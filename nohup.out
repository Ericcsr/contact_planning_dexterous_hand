pybullet build time: Jan 28 2022 20:17:22
pybullet build time: Jan 28 2022 20:17:22
Iteration 1 took 2.13 seconds (mean sampled reward: -7549.00). Current reward after update: -7228.07, Optimal reward -7228.07
Iteration 2 took 2.11 seconds (mean sampled reward: -7471.75). Current reward after update: -7154.89, Optimal reward -7154.89
Iteration 3 took 2.20 seconds (mean sampled reward: -7430.62). Current reward after update: -7236.02, Optimal reward -7154.89
Iteration 4 took 2.17 seconds (mean sampled reward: -7450.69). Current reward after update: -7145.08, Optimal reward -7145.08
Iteration 5 took 2.17 seconds (mean sampled reward: -7458.03). Current reward after update: -7193.10, Optimal reward -7145.08
Iteration 6 took 2.11 seconds (mean sampled reward: -7414.68). Current reward after update: -7157.59, Optimal reward -7145.08
Iteration 7 took 2.18 seconds (mean sampled reward: -7418.23). Current reward after update: -7180.53, Optimal reward -7145.08
Iteration 8 took 2.23 seconds (mean sampled reward: -7478.12). Current reward after update: -7172.88, Optimal reward -7145.08
Iteration 9 took 2.20 seconds (mean sampled reward: -7420.44). Current reward after update: -7161.44, Optimal reward -7145.08
Iteration 10 took 2.17 seconds (mean sampled reward: -7432.29). Current reward after update: -7163.58, Optimal reward -7145.08
Iteration 11 took 2.27 seconds (mean sampled reward: -7514.56). Current reward after update: -7178.40, Optimal reward -7145.08
Iteration 12 took 2.38 seconds (mean sampled reward: -7500.93). Current reward after update: -7170.88, Optimal reward -7145.08
Iteration 13 took 2.32 seconds (mean sampled reward: -7419.41). Current reward after update: -7180.14, Optimal reward -7145.08
Iteration 14 took 2.26 seconds (mean sampled reward: -7416.29). Current reward after update: -7174.61, Optimal reward -7145.08
Iteration 15 took 2.24 seconds (mean sampled reward: -7387.04). Current reward after update: -7177.61, Optimal reward -7145.08
Iteration 16 took 2.21 seconds (mean sampled reward: -7396.31). Current reward after update: -7175.95, Optimal reward -7145.08
Iteration 17 took 2.39 seconds (mean sampled reward: -7373.38). Current reward after update: -7157.94, Optimal reward -7145.08
Iteration 18 took 2.45 seconds (mean sampled reward: -7378.65). Current reward after update: -7147.05, Optimal reward -7145.08
Iteration 19 took 2.33 seconds (mean sampled reward: -7373.47). Current reward after update: -6832.12, Optimal reward -6832.12
Iteration 20 took 2.17 seconds (mean sampled reward: -7340.95). Current reward after update: -6764.89, Optimal reward -6764.89
Iteration 21 took 2.11 seconds (mean sampled reward: -7101.30). Current reward after update: -6604.51, Optimal reward -6604.51
Iteration 22 took 2.19 seconds (mean sampled reward: -6916.96). Current reward after update: -6311.89, Optimal reward -6311.89
Iteration 23 took 2.22 seconds (mean sampled reward: -6726.86). Current reward after update: -5555.63, Optimal reward -5555.63
Iteration 24 took 2.36 seconds (mean sampled reward: -6090.54). Current reward after update: -4906.89, Optimal reward -4906.89
Iteration 25 took 2.31 seconds (mean sampled reward: -5813.10). Current reward after update: -4840.84, Optimal reward -4840.84
Iteration 26 took 2.28 seconds (mean sampled reward: -5506.59). Current reward after update: -4582.85, Optimal reward -4582.85
Iteration 27 took 2.35 seconds (mean sampled reward: -5302.51). Current reward after update: -4582.41, Optimal reward -4582.41
Iteration 28 took 2.32 seconds (mean sampled reward: -5043.16). Current reward after update: -4229.53, Optimal reward -4229.53
Iteration 29 took 2.41 seconds (mean sampled reward: -5419.57). Current reward after update: -4204.19, Optimal reward -4204.19
Iteration 30 took 2.30 seconds (mean sampled reward: -5167.70). Current reward after update: -4005.56, Optimal reward -4005.56
Iteration 31 took 2.29 seconds (mean sampled reward: -4894.38). Current reward after update: -3785.60, Optimal reward -3785.60
Iteration 32 took 2.35 seconds (mean sampled reward: -5061.74). Current reward after update: -3676.59, Optimal reward -3676.59
Iteration 33 took 2.29 seconds (mean sampled reward: -4972.43). Current reward after update: -3750.48, Optimal reward -3676.59
Iteration 34 took 2.29 seconds (mean sampled reward: -5357.70). Current reward after update: -3979.37, Optimal reward -3676.59
Iteration 35 took 2.36 seconds (mean sampled reward: -5605.29). Current reward after update: -4031.25, Optimal reward -3676.59
Iteration 36 took 2.31 seconds (mean sampled reward: -5259.42). Current reward after update: -4299.37, Optimal reward -3676.59
Iteration 37 took 2.45 seconds (mean sampled reward: -5630.35). Current reward after update: -4248.95, Optimal reward -3676.59
Iteration 38 took 2.30 seconds (mean sampled reward: -5447.67). Current reward after update: -3867.82, Optimal reward -3676.59
Iteration 39 took 2.22 seconds (mean sampled reward: -5460.47). Current reward after update: -3820.51, Optimal reward -3676.59
Iteration 40 took 2.27 seconds (mean sampled reward: -5574.40). Current reward after update: -3860.81, Optimal reward -3676.59
Iteration 41 took 2.24 seconds (mean sampled reward: -5891.68). Current reward after update: -3783.41, Optimal reward -3676.59
Iteration 42 took 2.21 seconds (mean sampled reward: -5842.87). Current reward after update: -3785.43, Optimal reward -3676.59
Iteration 43 took 2.35 seconds (mean sampled reward: -6225.65). Current reward after update: -3945.68, Optimal reward -3676.59
Iteration 44 took 2.19 seconds (mean sampled reward: -5873.69). Current reward after update: -3824.59, Optimal reward -3676.59
Iteration 45 took 2.20 seconds (mean sampled reward: -5933.13). Current reward after update: -3791.49, Optimal reward -3676.59
Iteration 46 took 2.21 seconds (mean sampled reward: -6099.65). Current reward after update: -3842.35, Optimal reward -3676.59
Iteration 47 took 2.21 seconds (mean sampled reward: -6161.66). Current reward after update: -3862.49, Optimal reward -3676.59
Iteration 48 took 2.42 seconds (mean sampled reward: -5889.44). Current reward after update: -3778.55, Optimal reward -3676.59
Iteration 49 took 2.27 seconds (mean sampled reward: -5884.81). Current reward after update: -3587.99, Optimal reward -3587.99
Iteration 50 took 2.36 seconds (mean sampled reward: -5943.25). Current reward after update: -3823.28, Optimal reward -3587.99
Iteration 51 took 2.34 seconds (mean sampled reward: -6068.55). Current reward after update: -3872.39, Optimal reward -3587.99
Iteration 52 took 2.44 seconds (mean sampled reward: -6284.90). Current reward after update: -4616.85, Optimal reward -3587.99
Iteration 53 took 2.53 seconds (mean sampled reward: -5816.29). Current reward after update: -3677.34, Optimal reward -3587.99
Iteration 54 took 2.43 seconds (mean sampled reward: -5670.75). Current reward after update: -3541.50, Optimal reward -3541.50
Iteration 55 took 2.58 seconds (mean sampled reward: -5371.34). Current reward after update: -3773.33, Optimal reward -3541.50
Iteration 56 took 2.37 seconds (mean sampled reward: -5857.44). Current reward after update: -3756.64, Optimal reward -3541.50
Iteration 57 took 2.38 seconds (mean sampled reward: -6093.87). Current reward after update: -3764.96, Optimal reward -3541.50
Iteration 58 took 2.53 seconds (mean sampled reward: -5662.33). Current reward after update: -3707.03, Optimal reward -3541.50
Iteration 59 took 2.34 seconds (mean sampled reward: -5434.49). Current reward after update: -3675.42, Optimal reward -3541.50
Iteration 60 took 2.32 seconds (mean sampled reward: -5452.60). Current reward after update: -3702.53, Optimal reward -3541.50
Iteration 61 took 2.35 seconds (mean sampled reward: -5142.49). Current reward after update: -3601.67, Optimal reward -3541.50
Iteration 62 took 2.34 seconds (mean sampled reward: -4835.48). Current reward after update: -3128.80, Optimal reward -3128.80
Iteration 63 took 2.31 seconds (mean sampled reward: -5114.45). Current reward after update: -3333.11, Optimal reward -3128.80
Iteration 64 took 2.27 seconds (mean sampled reward: -5019.34). Current reward after update: -3620.91, Optimal reward -3128.80
Iteration 65 took 2.31 seconds (mean sampled reward: -5682.76). Current reward after update: -3218.34, Optimal reward -3128.80
Iteration 66 took 2.32 seconds (mean sampled reward: -5808.15). Current reward after update: -3666.10, Optimal reward -3128.80
Iteration 67 took 2.32 seconds (mean sampled reward: -5983.13). Current reward after update: -3718.16, Optimal reward -3128.80
Iteration 68 took 2.23 seconds (mean sampled reward: -5513.58). Current reward after update: -3408.82, Optimal reward -3128.80
Iteration 69 took 2.26 seconds (mean sampled reward: -5553.21). Current reward after update: -3672.51, Optimal reward -3128.80
Iteration 70 took 2.18 seconds (mean sampled reward: -5419.28). Current reward after update: -3683.08, Optimal reward -3128.80
Iteration 71 took 2.25 seconds (mean sampled reward: -5092.96). Current reward after update: -3594.08, Optimal reward -3128.80
Iteration 72 took 2.15 seconds (mean sampled reward: -5346.61). Current reward after update: -3836.15, Optimal reward -3128.80
Iteration 73 took 2.33 seconds (mean sampled reward: -5462.77). Current reward after update: -3770.03, Optimal reward -3128.80
Iteration 74 took 2.19 seconds (mean sampled reward: -5763.60). Current reward after update: -3831.47, Optimal reward -3128.80
Iteration 75 took 2.23 seconds (mean sampled reward: -5523.68). Current reward after update: -3633.40, Optimal reward -3128.80
Iteration 76 took 2.21 seconds (mean sampled reward: -5594.41). Current reward after update: -3798.25, Optimal reward -3128.80
Iteration 77 took 2.21 seconds (mean sampled reward: -5623.75). Current reward after update: -5968.66, Optimal reward -3128.80
Iteration 78 took 2.23 seconds (mean sampled reward: -5273.63). Current reward after update: -3765.19, Optimal reward -3128.80
Iteration 79 took 2.20 seconds (mean sampled reward: -5501.93). Current reward after update: -3901.09, Optimal reward -3128.80
Iteration 80 took 2.21 seconds (mean sampled reward: -5207.62). Current reward after update: -4440.43, Optimal reward -3128.80
Iteration 81 took 2.19 seconds (mean sampled reward: -5050.82). Current reward after update: -4120.99, Optimal reward -3128.80
Iteration 82 took 2.22 seconds (mean sampled reward: -5196.14). Current reward after update: -3844.16, Optimal reward -3128.80
Iteration 83 took 2.19 seconds (mean sampled reward: -4861.97). Current reward after update: -3874.86, Optimal reward -3128.80
Iteration 84 took 2.25 seconds (mean sampled reward: -5232.92). Current reward after update: -3792.31, Optimal reward -3128.80
Iteration 85 took 2.26 seconds (mean sampled reward: -5100.96). Current reward after update: -3845.84, Optimal reward -3128.80
Iteration 86 took 2.18 seconds (mean sampled reward: -5067.15). Current reward after update: -3776.29, Optimal reward -3128.80
Iteration 87 took 2.18 seconds (mean sampled reward: -4728.08). Current reward after update: -3781.89, Optimal reward -3128.80
Iteration 88 took 2.25 seconds (mean sampled reward: -5161.41). Current reward after update: -6036.63, Optimal reward -3128.80
Iteration 89 took 2.24 seconds (mean sampled reward: -5270.95). Current reward after update: -4207.21, Optimal reward -3128.80
Iteration 90 took 2.30 seconds (mean sampled reward: -5781.25). Current reward after update: -3998.97, Optimal reward -3128.80
Iteration 91 took 2.27 seconds (mean sampled reward: -5778.83). Current reward after update: -3962.84, Optimal reward -3128.80
Iteration 92 took 2.32 seconds (mean sampled reward: -5673.17). Current reward after update: -4004.14, Optimal reward -3128.80
Iteration 93 took 2.24 seconds (mean sampled reward: -5092.45). Current reward after update: -3941.21, Optimal reward -3128.80
Iteration 94 took 2.29 seconds (mean sampled reward: -5509.30). Current reward after update: -3989.04, Optimal reward -3128.80
Iteration 95 took 2.43 seconds (mean sampled reward: -5167.86). Current reward after update: -3788.30, Optimal reward -3128.80
Iteration 96 took 2.35 seconds (mean sampled reward: -5484.23). Current reward after update: -3788.05, Optimal reward -3128.80
Iteration 97 took 2.34 seconds (mean sampled reward: -5377.17). Current reward after update: -3842.55, Optimal reward -3128.80
Iteration 98 took 2.34 seconds (mean sampled reward: -5618.08). Current reward after update: -5467.71, Optimal reward -3128.80
Iteration 99 took 2.38 seconds (mean sampled reward: -5662.50). Current reward after update: -3935.82, Optimal reward -3128.80
Iteration 100 took 2.34 seconds (mean sampled reward: -5406.75). Current reward after update: -3908.29, Optimal reward -3128.80
Iteration 101 took 2.38 seconds (mean sampled reward: -5491.73). Current reward after update: -3743.19, Optimal reward -3128.80
Iteration 102 took 2.41 seconds (mean sampled reward: -5371.67). Current reward after update: -3867.71, Optimal reward -3128.80
Iteration 103 took 2.32 seconds (mean sampled reward: -5326.41). Current reward after update: -3872.69, Optimal reward -3128.80
Iteration 104 took 2.37 seconds (mean sampled reward: -5736.41). Current reward after update: -3858.59, Optimal reward -3128.80
Iteration 105 took 2.41 seconds (mean sampled reward: -5857.85). Current reward after update: -3708.12, Optimal reward -3128.80
Iteration 106 took 2.39 seconds (mean sampled reward: -5688.39). Current reward after update: -3725.17, Optimal reward -3128.80
Iteration 107 took 2.42 seconds (mean sampled reward: -5824.30). Current reward after update: -3763.08, Optimal reward -3128.80
Iteration 108 took 2.36 seconds (mean sampled reward: -6038.79). Current reward after update: -3658.69, Optimal reward -3128.80
Iteration 109 took 2.25 seconds (mean sampled reward: -6376.85). Current reward after update: -3805.77, Optimal reward -3128.80
Iteration 110 took 2.37 seconds (mean sampled reward: -6515.01). Current reward after update: -3925.20, Optimal reward -3128.80
Iteration 111 took 2.30 seconds (mean sampled reward: -6285.78). Current reward after update: -3751.72, Optimal reward -3128.80
Iteration 112 took 2.43 seconds (mean sampled reward: -5698.59). Current reward after update: -4077.50, Optimal reward -3128.80
Iteration 113 took 2.36 seconds (mean sampled reward: -5834.80). Current reward after update: -3798.98, Optimal reward -3128.80
Iteration 114 took 2.48 seconds (mean sampled reward: -5640.64). Current reward after update: -3513.03, Optimal reward -3128.80
Iteration 115 took 2.33 seconds (mean sampled reward: -5248.24). Current reward after update: -3829.17, Optimal reward -3128.80
Iteration 116 took 2.18 seconds (mean sampled reward: -5103.95). Current reward after update: -3822.75, Optimal reward -3128.80
Iteration 117 took 2.27 seconds (mean sampled reward: -5086.65). Current reward after update: -5026.33, Optimal reward -3128.80
Iteration 118 took 2.23 seconds (mean sampled reward: -4976.14). Current reward after update: -5023.57, Optimal reward -3128.80
Iteration 119 took 2.26 seconds (mean sampled reward: -5422.93). Current reward after update: -3762.92, Optimal reward -3128.80
Iteration 120 took 2.33 seconds (mean sampled reward: -5331.20). Current reward after update: -4079.88, Optimal reward -3128.80
Iteration 121 took 2.34 seconds (mean sampled reward: -5501.15). Current reward after update: -5494.37, Optimal reward -3128.80
Iteration 122 took 2.29 seconds (mean sampled reward: -5216.75). Current reward after update: -3789.49, Optimal reward -3128.80
Iteration 123 took 2.25 seconds (mean sampled reward: -5354.52). Current reward after update: -3698.64, Optimal reward -3128.80
Iteration 124 took 2.36 seconds (mean sampled reward: -5599.33). Current reward after update: -4323.70, Optimal reward -3128.80
Iteration 125 took 2.34 seconds (mean sampled reward: -5078.40). Current reward after update: -3788.97, Optimal reward -3128.80
Iteration 126 took 2.30 seconds (mean sampled reward: -4936.00). Current reward after update: -3699.09, Optimal reward -3128.80
Iteration 127 took 2.34 seconds (mean sampled reward: -5086.59). Current reward after update: -3534.47, Optimal reward -3128.80
Iteration 128 took 2.29 seconds (mean sampled reward: -4718.27). Current reward after update: -3646.24, Optimal reward -3128.80
Iteration 129 took 2.26 seconds (mean sampled reward: -5011.39). Current reward after update: -3659.36, Optimal reward -3128.80
Iteration 130 took 2.23 seconds (mean sampled reward: -5185.11). Current reward after update: -3696.95, Optimal reward -3128.80
Iteration 131 took 2.29 seconds (mean sampled reward: -5390.06). Current reward after update: -4335.64, Optimal reward -3128.80
Iteration 132 took 2.31 seconds (mean sampled reward: -5175.90). Current reward after update: -3780.00, Optimal reward -3128.80
Iteration 133 took 2.33 seconds (mean sampled reward: -5668.64). Current reward after update: -3790.04, Optimal reward -3128.80
Iteration 134 took 2.29 seconds (mean sampled reward: -5126.57). Current reward after update: -3693.60, Optimal reward -3128.80
Iteration 135 took 2.30 seconds (mean sampled reward: -5120.28). Current reward after update: -3868.20, Optimal reward -3128.80
Iteration 136 took 2.29 seconds (mean sampled reward: -5177.73). Current reward after update: -3598.61, Optimal reward -3128.80
Iteration 137 took 2.25 seconds (mean sampled reward: -4449.79). Current reward after update: -4178.63, Optimal reward -3128.80
Iteration 138 took 2.30 seconds (mean sampled reward: -4911.48). Current reward after update: -3652.22, Optimal reward -3128.80
Iteration 139 took 2.27 seconds (mean sampled reward: -4729.27). Current reward after update: -3644.83, Optimal reward -3128.80
Iteration 140 took 2.29 seconds (mean sampled reward: -4875.61). Current reward after update: -3911.21, Optimal reward -3128.80
Iteration 141 took 2.28 seconds (mean sampled reward: -4842.37). Current reward after update: -4319.81, Optimal reward -3128.80
Iteration 142 took 2.23 seconds (mean sampled reward: -4713.31). Current reward after update: -3621.55, Optimal reward -3128.80
Iteration 143 took 2.28 seconds (mean sampled reward: -4687.04). Current reward after update: -3745.06, Optimal reward -3128.80
Iteration 144 took 2.28 seconds (mean sampled reward: -4510.63). Current reward after update: -3756.26, Optimal reward -3128.80
Iteration 145 took 2.31 seconds (mean sampled reward: -4507.79). Current reward after update: -3571.66, Optimal reward -3128.80
Iteration 146 took 2.31 seconds (mean sampled reward: -4365.10). Current reward after update: -3433.20, Optimal reward -3128.80
Iteration 147 took 2.29 seconds (mean sampled reward: -4323.48). Current reward after update: -3302.69, Optimal reward -3128.80
Iteration 148 took 2.33 seconds (mean sampled reward: -4925.20). Current reward after update: -3180.14, Optimal reward -3128.80
Iteration 149 took 2.32 seconds (mean sampled reward: -5537.21). Current reward after update: -4368.13, Optimal reward -3128.80
Iteration 150 took 2.33 seconds (mean sampled reward: -5189.00). Current reward after update: -3577.96, Optimal reward -3128.80
Iteration 151 took 2.31 seconds (mean sampled reward: -5486.55). Current reward after update: -3735.88, Optimal reward -3128.80
Iteration 152 took 2.26 seconds (mean sampled reward: -4789.83). Current reward after update: -4464.23, Optimal reward -3128.80
Iteration 153 took 2.26 seconds (mean sampled reward: -4946.49). Current reward after update: -3540.46, Optimal reward -3128.80
Iteration 154 took 2.25 seconds (mean sampled reward: -4980.60). Current reward after update: -3637.54, Optimal reward -3128.80
Iteration 155 took 2.36 seconds (mean sampled reward: -5671.39). Current reward after update: -3972.21, Optimal reward -3128.80
Iteration 156 took 2.32 seconds (mean sampled reward: -4718.74). Current reward after update: -3208.00, Optimal reward -3128.80
Iteration 157 took 2.28 seconds (mean sampled reward: -5053.63). Current reward after update: -3191.02, Optimal reward -3128.80
Iteration 158 took 2.32 seconds (mean sampled reward: -5134.61). Current reward after update: -4154.42, Optimal reward -3128.80
Iteration 159 took 2.20 seconds (mean sampled reward: -4719.63). Current reward after update: -3542.00, Optimal reward -3128.80
Iteration 160 took 2.20 seconds (mean sampled reward: -4710.84). Current reward after update: -4034.14, Optimal reward -3128.80
Iteration 161 took 2.26 seconds (mean sampled reward: -4730.14). Current reward after update: -3630.11, Optimal reward -3128.80
Iteration 162 took 2.23 seconds (mean sampled reward: -4755.99). Current reward after update: -3515.36, Optimal reward -3128.80
Iteration 163 took 2.24 seconds (mean sampled reward: -4551.17). Current reward after update: -3302.92, Optimal reward -3128.80
Iteration 164 took 2.17 seconds (mean sampled reward: -4714.94). Current reward after update: -3695.22, Optimal reward -3128.80
Iteration 165 took 2.21 seconds (mean sampled reward: -4861.31). Current reward after update: -3574.91, Optimal reward -3128.80
Iteration 166 took 2.29 seconds (mean sampled reward: -4546.21). Current reward after update: -3730.46, Optimal reward -3128.80
Iteration 167 took 2.27 seconds (mean sampled reward: -4651.30). Current reward after update: -4131.68, Optimal reward -3128.80
Iteration 168 took 2.26 seconds (mean sampled reward: -4506.64). Current reward after update: -3444.12, Optimal reward -3128.80
Iteration 169 took 2.33 seconds (mean sampled reward: -4439.34). Current reward after update: -3331.30, Optimal reward -3128.80
Iteration 170 took 2.25 seconds (mean sampled reward: -5053.76). Current reward after update: -3649.38, Optimal reward -3128.80
Iteration 171 took 2.24 seconds (mean sampled reward: -5010.40). Current reward after update: -3146.17, Optimal reward -3128.80
Iteration 172 took 2.21 seconds (mean sampled reward: -4623.45). Current reward after update: -4002.22, Optimal reward -3128.80
Iteration 173 took 2.21 seconds (mean sampled reward: -4324.19). Current reward after update: -3537.93, Optimal reward -3128.80
Iteration 174 took 2.23 seconds (mean sampled reward: -4321.33). Current reward after update: -3194.70, Optimal reward -3128.80
Iteration 175 took 2.21 seconds (mean sampled reward: -4198.25). Current reward after update: -3598.42, Optimal reward -3128.80
Iteration 176 took 2.32 seconds (mean sampled reward: -4189.53). Current reward after update: -3567.15, Optimal reward -3128.80
Iteration 177 took 2.26 seconds (mean sampled reward: -4180.41). Current reward after update: -3618.20, Optimal reward -3128.80
Iteration 178 took 2.23 seconds (mean sampled reward: -4250.74). Current reward after update: -3481.62, Optimal reward -3128.80
Iteration 179 took 2.29 seconds (mean sampled reward: -4157.74). Current reward after update: -3485.09, Optimal reward -3128.80
Iteration 180 took 2.28 seconds (mean sampled reward: -4364.28). Current reward after update: -5000.93, Optimal reward -3128.80
Iteration 181 took 2.34 seconds (mean sampled reward: -4539.51). Current reward after update: -3717.86, Optimal reward -3128.80
Iteration 182 took 2.39 seconds (mean sampled reward: -4925.37). Current reward after update: -3731.04, Optimal reward -3128.80
Iteration 183 took 2.27 seconds (mean sampled reward: -4687.76). Current reward after update: -3444.41, Optimal reward -3128.80
Iteration 184 took 2.30 seconds (mean sampled reward: -4494.45). Current reward after update: -5045.50, Optimal reward -3128.80
Iteration 185 took 2.40 seconds (mean sampled reward: -4379.78). Current reward after update: -3533.26, Optimal reward -3128.80
Iteration 186 took 2.31 seconds (mean sampled reward: -4393.30). Current reward after update: -3504.35, Optimal reward -3128.80
Iteration 187 took 2.27 seconds (mean sampled reward: -4489.31). Current reward after update: -4151.16, Optimal reward -3128.80
Iteration 188 took 2.28 seconds (mean sampled reward: -4406.07). Current reward after update: -3750.48, Optimal reward -3128.80
Iteration 189 took 2.27 seconds (mean sampled reward: -5169.82). Current reward after update: -3465.15, Optimal reward -3128.80
Iteration 190 took 2.33 seconds (mean sampled reward: -4714.73). Current reward after update: -4104.24, Optimal reward -3128.80
Iteration 191 took 2.34 seconds (mean sampled reward: -4798.94). Current reward after update: -3506.30, Optimal reward -3128.80
Iteration 192 took 2.25 seconds (mean sampled reward: -4480.66). Current reward after update: -3509.01, Optimal reward -3128.80
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
Iteration 193 took 2.23 seconds (mean sampled reward: -4273.68). Current reward after update: -3537.63, Optimal reward -3128.80
Iteration 194 took 2.30 seconds (mean sampled reward: -4480.31). Current reward after update: -3367.68, Optimal reward -3128.80
Iteration 195 took 2.25 seconds (mean sampled reward: -4556.49). Current reward after update: -3529.40, Optimal reward -3128.80
Iteration 196 took 2.29 seconds (mean sampled reward: -4183.58). Current reward after update: -3483.99, Optimal reward -3128.80
Iteration 197 took 2.28 seconds (mean sampled reward: -4243.32). Current reward after update: -3377.21, Optimal reward -3128.80
Iteration 198 took 2.22 seconds (mean sampled reward: -4267.99). Current reward after update: -3324.39, Optimal reward -3128.80
Iteration 199 took 2.19 seconds (mean sampled reward: -4035.98). Current reward after update: -3089.64, Optimal reward -3089.64
Iteration 200 took 2.25 seconds (mean sampled reward: -4225.16). Current reward after update: -3179.63, Optimal reward -3089.64
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
Iteration 1 took 2.14 seconds (mean sampled reward: -7548.68). Current reward after update: -7531.89, Optimal reward -7531.89
Iteration 2 took 2.16 seconds (mean sampled reward: -7498.79). Current reward after update: -7384.85, Optimal reward -7384.85
Iteration 3 took 2.02 seconds (mean sampled reward: -7440.99). Current reward after update: -7221.70, Optimal reward -7221.70
Iteration 4 took 2.15 seconds (mean sampled reward: -7382.91). Current reward after update: -7182.50, Optimal reward -7182.50
Iteration 5 took 2.04 seconds (mean sampled reward: -7378.61). Current reward after update: -7153.05, Optimal reward -7153.05
Iteration 6 took 2.06 seconds (mean sampled reward: -7358.83). Current reward after update: -7154.08, Optimal reward -7153.05
Iteration 7 took 2.06 seconds (mean sampled reward: -7411.73). Current reward after update: -7150.02, Optimal reward -7150.02
Iteration 8 took 2.01 seconds (mean sampled reward: -7409.91). Current reward after update: -6914.25, Optimal reward -6914.25
Iteration 9 took 2.15 seconds (mean sampled reward: -7345.04). Current reward after update: -6148.92, Optimal reward -6148.92
Iteration 10 took 2.04 seconds (mean sampled reward: -6916.40). Current reward after update: -5989.51, Optimal reward -5989.51
Iteration 11 took 2.03 seconds (mean sampled reward: -6608.84). Current reward after update: -5960.59, Optimal reward -5960.59
Iteration 12 took 2.10 seconds (mean sampled reward: -6343.20). Current reward after update: -5925.77, Optimal reward -5925.77
Iteration 13 took 2.00 seconds (mean sampled reward: -6464.01). Current reward after update: -5913.26, Optimal reward -5913.26
Iteration 14 took 2.16 seconds (mean sampled reward: -6450.36). Current reward after update: -5978.65, Optimal reward -5913.26
Iteration 15 took 2.13 seconds (mean sampled reward: -6618.21). Current reward after update: -5885.07, Optimal reward -5885.07
Iteration 16 took 2.14 seconds (mean sampled reward: -6591.86). Current reward after update: -5913.30, Optimal reward -5885.07
Iteration 17 took 2.12 seconds (mean sampled reward: -6214.41). Current reward after update: -5887.47, Optimal reward -5885.07
Iteration 18 took 2.00 seconds (mean sampled reward: -6662.75). Current reward after update: -5914.96, Optimal reward -5885.07
Iteration 19 took 2.16 seconds (mean sampled reward: -6573.45). Current reward after update: -6035.23, Optimal reward -5885.07
Iteration 20 took 2.09 seconds (mean sampled reward: -6887.44). Current reward after update: -5889.49, Optimal reward -5885.07
Iteration 21 took 2.14 seconds (mean sampled reward: -6350.14). Current reward after update: -5875.46, Optimal reward -5875.46
Iteration 22 took 2.05 seconds (mean sampled reward: -6210.96). Current reward after update: -5877.88, Optimal reward -5875.46
Iteration 23 took 2.03 seconds (mean sampled reward: -6097.98). Current reward after update: -6022.94, Optimal reward -5875.46
Iteration 24 took 2.01 seconds (mean sampled reward: -6160.36). Current reward after update: -6027.57, Optimal reward -5875.46
Iteration 25 took 2.01 seconds (mean sampled reward: -6156.30). Current reward after update: -5909.64, Optimal reward -5875.46
Iteration 26 took 2.04 seconds (mean sampled reward: -6187.44). Current reward after update: -5920.52, Optimal reward -5875.46
Iteration 27 took 1.98 seconds (mean sampled reward: -6393.72). Current reward after update: -4926.72, Optimal reward -4926.72
Iteration 28 took 2.23 seconds (mean sampled reward: -6420.35). Current reward after update: -5026.07, Optimal reward -4926.72
Iteration 29 took 2.27 seconds (mean sampled reward: -5879.84). Current reward after update: -4716.88, Optimal reward -4716.88
Iteration 30 took 2.37 seconds (mean sampled reward: -6109.66). Current reward after update: -4628.07, Optimal reward -4628.07
Iteration 31 took 2.37 seconds (mean sampled reward: -6017.08). Current reward after update: -4375.29, Optimal reward -4375.29
Iteration 32 took 2.35 seconds (mean sampled reward: -5837.60). Current reward after update: -4029.78, Optimal reward -4029.78
Iteration 33 took 2.68 seconds (mean sampled reward: -5462.01). Current reward after update: -3917.72, Optimal reward -3917.72
Iteration 34 took 2.40 seconds (mean sampled reward: -5350.14). Current reward after update: -3909.20, Optimal reward -3909.20
Iteration 35 took 2.18 seconds (mean sampled reward: -5803.84). Current reward after update: -3976.66, Optimal reward -3909.20
Iteration 36 took 2.25 seconds (mean sampled reward: -5655.77). Current reward after update: -4017.13, Optimal reward -3909.20
Iteration 37 took 2.57 seconds (mean sampled reward: -4979.88). Current reward after update: -3824.90, Optimal reward -3824.90
Iteration 38 took 2.25 seconds (mean sampled reward: -5191.12). Current reward after update: -3852.93, Optimal reward -3824.90
Iteration 39 took 2.57 seconds (mean sampled reward: -5142.34). Current reward after update: -4039.09, Optimal reward -3824.90
Iteration 40 took 2.36 seconds (mean sampled reward: -5041.25). Current reward after update: -3852.24, Optimal reward -3824.90
Iteration 41 took 2.20 seconds (mean sampled reward: -5094.91). Current reward after update: -3893.70, Optimal reward -3824.90
Iteration 42 took 2.19 seconds (mean sampled reward: -4673.38). Current reward after update: -3843.43, Optimal reward -3824.90
Iteration 43 took 2.15 seconds (mean sampled reward: -4873.39). Current reward after update: -4096.53, Optimal reward -3824.90
Iteration 44 took 2.09 seconds (mean sampled reward: -4462.32). Current reward after update: -3902.47, Optimal reward -3824.90
Iteration 45 took 2.17 seconds (mean sampled reward: -4407.70). Current reward after update: -3774.80, Optimal reward -3774.80
Iteration 46 took 2.12 seconds (mean sampled reward: -4394.27). Current reward after update: -3775.85, Optimal reward -3774.80
Iteration 47 took 2.08 seconds (mean sampled reward: -4875.68). Current reward after update: -3948.36, Optimal reward -3774.80
Iteration 48 took 2.10 seconds (mean sampled reward: -4715.92). Current reward after update: -4248.48, Optimal reward -3774.80
Iteration 49 took 2.13 seconds (mean sampled reward: -4796.54). Current reward after update: -4104.60, Optimal reward -3774.80
Iteration 50 took 2.18 seconds (mean sampled reward: -4520.43). Current reward after update: -4049.13, Optimal reward -3774.80
Iteration 51 took 2.05 seconds (mean sampled reward: -4539.26). Current reward after update: -3980.03, Optimal reward -3774.80
Iteration 52 took 2.03 seconds (mean sampled reward: -4479.37). Current reward after update: -3851.51, Optimal reward -3774.80
Iteration 53 took 2.09 seconds (mean sampled reward: -4415.87). Current reward after update: -3894.97, Optimal reward -3774.80
Iteration 54 took 2.01 seconds (mean sampled reward: -4428.16). Current reward after update: -3897.33, Optimal reward -3774.80
Iteration 55 took 2.21 seconds (mean sampled reward: -4742.25). Current reward after update: -3676.57, Optimal reward -3676.57
Iteration 56 took 2.33 seconds (mean sampled reward: -4634.05). Current reward after update: -3508.57, Optimal reward -3508.57
Iteration 57 took 1.97 seconds (mean sampled reward: -4687.62). Current reward after update: -3522.65, Optimal reward -3508.57
Iteration 58 took 2.16 seconds (mean sampled reward: -4312.75). Current reward after update: -3308.94, Optimal reward -3308.94
Iteration 59 took 2.16 seconds (mean sampled reward: -4086.43). Current reward after update: -3743.69, Optimal reward -3308.94
Iteration 60 took 2.18 seconds (mean sampled reward: -3991.80). Current reward after update: -3425.18, Optimal reward -3308.94
Iteration 61 took 2.47 seconds (mean sampled reward: -3934.63). Current reward after update: -3114.09, Optimal reward -3114.09
Iteration 62 took 2.07 seconds (mean sampled reward: -4020.95). Current reward after update: -2747.06, Optimal reward -2747.06
Iteration 63 took 1.98 seconds (mean sampled reward: -4760.79). Current reward after update: -3723.97, Optimal reward -2747.06
Iteration 64 took 2.06 seconds (mean sampled reward: -4081.80). Current reward after update: -2699.22, Optimal reward -2699.22
Iteration 65 took 2.08 seconds (mean sampled reward: -4024.72). Current reward after update: -3264.01, Optimal reward -2699.22
Iteration 66 took 2.01 seconds (mean sampled reward: -4587.49). Current reward after update: -2783.00, Optimal reward -2699.22
Iteration 67 took 2.10 seconds (mean sampled reward: -4222.90). Current reward after update: -2561.31, Optimal reward -2561.31
Iteration 68 took 2.13 seconds (mean sampled reward: -4131.72). Current reward after update: -2589.99, Optimal reward -2561.31
Iteration 69 took 2.04 seconds (mean sampled reward: -3858.09). Current reward after update: -2556.96, Optimal reward -2556.96
Iteration 70 took 2.12 seconds (mean sampled reward: -3988.73). Current reward after update: -2633.71, Optimal reward -2556.96
Iteration 71 took 2.13 seconds (mean sampled reward: -3867.49). Current reward after update: -2516.11, Optimal reward -2516.11
Iteration 72 took 2.15 seconds (mean sampled reward: -3922.87). Current reward after update: -2421.96, Optimal reward -2421.96
Iteration 73 took 2.19 seconds (mean sampled reward: -3673.64). Current reward after update: -3835.26, Optimal reward -2421.96
Iteration 74 took 2.25 seconds (mean sampled reward: -3877.18). Current reward after update: -2417.86, Optimal reward -2417.86
Iteration 75 took 2.20 seconds (mean sampled reward: -4047.69). Current reward after update: -2393.51, Optimal reward -2393.51
Iteration 76 took 2.17 seconds (mean sampled reward: -4231.54). Current reward after update: -2389.82, Optimal reward -2389.82
Iteration 77 took 2.16 seconds (mean sampled reward: -4190.76). Current reward after update: -2508.72, Optimal reward -2389.82
Iteration 78 took 2.23 seconds (mean sampled reward: -4104.47). Current reward after update: -2316.58, Optimal reward -2316.58
Iteration 79 took 2.22 seconds (mean sampled reward: -4100.70). Current reward after update: -2578.03, Optimal reward -2316.58
Iteration 80 took 2.24 seconds (mean sampled reward: -4086.18). Current reward after update: -2490.33, Optimal reward -2316.58
Iteration 81 took 2.16 seconds (mean sampled reward: -3992.15). Current reward after update: -2640.54, Optimal reward -2316.58
Iteration 82 took 2.22 seconds (mean sampled reward: -3647.41). Current reward after update: -2508.80, Optimal reward -2316.58
Iteration 83 took 2.25 seconds (mean sampled reward: -4036.90). Current reward after update: -2169.08, Optimal reward -2169.08
Iteration 84 took 2.24 seconds (mean sampled reward: -3972.71). Current reward after update: -2276.29, Optimal reward -2169.08
Iteration 85 took 2.30 seconds (mean sampled reward: -3780.86). Current reward after update: -2240.46, Optimal reward -2169.08
Iteration 86 took 2.22 seconds (mean sampled reward: -3753.30). Current reward after update: -2205.57, Optimal reward -2169.08
Iteration 87 took 2.20 seconds (mean sampled reward: -3795.99). Current reward after update: -2232.61, Optimal reward -2169.08
Iteration 88 took 2.18 seconds (mean sampled reward: -3458.10). Current reward after update: -2065.04, Optimal reward -2065.04
Iteration 89 took 2.25 seconds (mean sampled reward: -3563.64). Current reward after update: -2192.24, Optimal reward -2065.04
Iteration 90 took 2.19 seconds (mean sampled reward: -3552.53). Current reward after update: -2124.18, Optimal reward -2065.04
Iteration 91 took 2.24 seconds (mean sampled reward: -3810.66). Current reward after update: -3152.70, Optimal reward -2065.04
Iteration 92 took 2.15 seconds (mean sampled reward: -3694.20). Current reward after update: -2765.92, Optimal reward -2065.04
Iteration 93 took 2.18 seconds (mean sampled reward: -3915.85). Current reward after update: -1981.33, Optimal reward -1981.33
Iteration 94 took 2.14 seconds (mean sampled reward: -4167.88). Current reward after update: -2263.06, Optimal reward -1981.33
Iteration 95 took 2.12 seconds (mean sampled reward: -4845.83). Current reward after update: -2108.85, Optimal reward -1981.33
Iteration 96 took 2.18 seconds (mean sampled reward: -4598.29). Current reward after update: -2025.05, Optimal reward -1981.33
Iteration 97 took 2.21 seconds (mean sampled reward: -4174.08). Current reward after update: -2202.68, Optimal reward -1981.33
Iteration 98 took 2.20 seconds (mean sampled reward: -3875.76). Current reward after update: -2176.76, Optimal reward -1981.33
Iteration 99 took 2.22 seconds (mean sampled reward: -3564.58). Current reward after update: -2126.33, Optimal reward -1981.33
Iteration 100 took 2.24 seconds (mean sampled reward: -3847.19). Current reward after update: -3253.12, Optimal reward -1981.33
Iteration 101 took 2.25 seconds (mean sampled reward: -3572.41). Current reward after update: -2021.96, Optimal reward -1981.33
Iteration 102 took 2.26 seconds (mean sampled reward: -3745.92). Current reward after update: -2126.35, Optimal reward -1981.33
Iteration 103 took 2.28 seconds (mean sampled reward: -3733.07). Current reward after update: -2026.42, Optimal reward -1981.33
Iteration 104 took 2.20 seconds (mean sampled reward: -3608.82). Current reward after update: -3014.42, Optimal reward -1981.33
Iteration 105 took 2.12 seconds (mean sampled reward: -4156.42). Current reward after update: -2129.75, Optimal reward -1981.33
Iteration 106 took 2.21 seconds (mean sampled reward: -4063.12). Current reward after update: -5107.03, Optimal reward -1981.33
Iteration 107 took 2.17 seconds (mean sampled reward: -3780.18). Current reward after update: -3843.80, Optimal reward -1981.33
Iteration 108 took 2.07 seconds (mean sampled reward: -3742.60). Current reward after update: -2130.97, Optimal reward -1981.33
Iteration 109 took 2.16 seconds (mean sampled reward: -3863.24). Current reward after update: -2199.97, Optimal reward -1981.33
Iteration 110 took 2.20 seconds (mean sampled reward: -3690.71). Current reward after update: -2097.60, Optimal reward -1981.33
Iteration 111 took 2.28 seconds (mean sampled reward: -3413.44). Current reward after update: -2112.60, Optimal reward -1981.33
Iteration 112 took 2.30 seconds (mean sampled reward: -4005.94). Current reward after update: -2048.85, Optimal reward -1981.33
Iteration 113 took 2.22 seconds (mean sampled reward: -3833.11). Current reward after update: -3354.22, Optimal reward -1981.33
Iteration 114 took 2.11 seconds (mean sampled reward: -3494.70). Current reward after update: -2888.03, Optimal reward -1981.33
Iteration 115 took 2.21 seconds (mean sampled reward: -3543.96). Current reward after update: -2142.05, Optimal reward -1981.33
Iteration 116 took 2.29 seconds (mean sampled reward: -3606.14). Current reward after update: -3221.41, Optimal reward -1981.33
Iteration 117 took 2.19 seconds (mean sampled reward: -3673.31). Current reward after update: -2738.35, Optimal reward -1981.33
Iteration 118 took 2.12 seconds (mean sampled reward: -3556.12). Current reward after update: -2405.92, Optimal reward -1981.33
Iteration 119 took 2.11 seconds (mean sampled reward: -3755.89). Current reward after update: -3235.09, Optimal reward -1981.33
Iteration 120 took 2.14 seconds (mean sampled reward: -3915.92). Current reward after update: -2140.17, Optimal reward -1981.33
Iteration 121 took 2.24 seconds (mean sampled reward: -3927.35). Current reward after update: -2130.40, Optimal reward -1981.33
Iteration 122 took 2.09 seconds (mean sampled reward: -3748.88). Current reward after update: -3006.96, Optimal reward -1981.33
Iteration 123 took 2.13 seconds (mean sampled reward: -3511.77). Current reward after update: -2106.11, Optimal reward -1981.33
Iteration 124 took 2.19 seconds (mean sampled reward: -3556.17). Current reward after update: -2147.95, Optimal reward -1981.33
Iteration 125 took 2.15 seconds (mean sampled reward: -3846.84). Current reward after update: -2103.61, Optimal reward -1981.33
Iteration 126 took 2.12 seconds (mean sampled reward: -3931.68). Current reward after update: -2112.00, Optimal reward -1981.33
Iteration 127 took 2.22 seconds (mean sampled reward: -3972.96). Current reward after update: -2049.24, Optimal reward -1981.33
Iteration 128 took 2.16 seconds (mean sampled reward: -4111.54). Current reward after update: -2102.06, Optimal reward -1981.33
Iteration 129 took 2.12 seconds (mean sampled reward: -3586.18). Current reward after update: -2043.81, Optimal reward -1981.33
Iteration 130 took 2.15 seconds (mean sampled reward: -3871.82). Current reward after update: -2781.25, Optimal reward -1981.33
Iteration 131 took 2.28 seconds (mean sampled reward: -3639.96). Current reward after update: -2079.19, Optimal reward -1981.33
Iteration 132 took 2.12 seconds (mean sampled reward: -3544.84). Current reward after update: -2251.26, Optimal reward -1981.33
Iteration 133 took 2.05 seconds (mean sampled reward: -3670.81). Current reward after update: -2054.02, Optimal reward -1981.33
Iteration 134 took 2.15 seconds (mean sampled reward: -3790.00). Current reward after update: -2141.03, Optimal reward -1981.33
Iteration 135 took 2.09 seconds (mean sampled reward: -3911.90). Current reward after update: -2190.76, Optimal reward -1981.33
Iteration 136 took 2.14 seconds (mean sampled reward: -3894.37). Current reward after update: -2499.16, Optimal reward -1981.33
Iteration 137 took 2.16 seconds (mean sampled reward: -3769.09). Current reward after update: -2707.20, Optimal reward -1981.33
Iteration 138 took 2.09 seconds (mean sampled reward: -4195.76). Current reward after update: -2223.72, Optimal reward -1981.33
Iteration 139 took 2.09 seconds (mean sampled reward: -4360.76). Current reward after update: -2296.43, Optimal reward -1981.33
Iteration 140 took 2.03 seconds (mean sampled reward: -4634.66). Current reward after update: -2308.24, Optimal reward -1981.33
Iteration 141 took 2.16 seconds (mean sampled reward: -4572.27). Current reward after update: -2178.44, Optimal reward -1981.33
Iteration 142 took 2.15 seconds (mean sampled reward: -4228.15). Current reward after update: -2212.24, Optimal reward -1981.33
Iteration 143 took 2.11 seconds (mean sampled reward: -4195.81). Current reward after update: -4739.01, Optimal reward -1981.33
Iteration 144 took 2.02 seconds (mean sampled reward: -4641.09). Current reward after update: -3855.33, Optimal reward -1981.33
Iteration 145 took 2.10 seconds (mean sampled reward: -4511.54). Current reward after update: -2261.25, Optimal reward -1981.33
Iteration 146 took 2.05 seconds (mean sampled reward: -4453.12). Current reward after update: -2231.94, Optimal reward -1981.33
Iteration 147 took 2.04 seconds (mean sampled reward: -4352.55). Current reward after update: -2157.17, Optimal reward -1981.33
Iteration 148 took 2.08 seconds (mean sampled reward: -4186.61). Current reward after update: -2187.58, Optimal reward -1981.33
Iteration 149 took 2.13 seconds (mean sampled reward: -3843.90). Current reward after update: -2173.07, Optimal reward -1981.33
Iteration 150 took 2.21 seconds (mean sampled reward: -3632.67). Current reward after update: -2139.69, Optimal reward -1981.33
Iteration 151 took 2.14 seconds (mean sampled reward: -4058.44). Current reward after update: -2215.49, Optimal reward -1981.33
Iteration 152 took 2.12 seconds (mean sampled reward: -4006.36). Current reward after update: -2184.93, Optimal reward -1981.33
Iteration 153 took 2.11 seconds (mean sampled reward: -3972.47). Current reward after update: -3809.72, Optimal reward -1981.33
Iteration 154 took 2.12 seconds (mean sampled reward: -4455.20). Current reward after update: -3433.85, Optimal reward -1981.33
Iteration 155 took 2.19 seconds (mean sampled reward: -4224.30). Current reward after update: -3463.30, Optimal reward -1981.33
Iteration 156 took 2.14 seconds (mean sampled reward: -4336.04). Current reward after update: -3951.82, Optimal reward -1981.33
Iteration 157 took 2.20 seconds (mean sampled reward: -3791.56). Current reward after update: -2763.64, Optimal reward -1981.33
Iteration 158 took 2.21 seconds (mean sampled reward: -3663.38). Current reward after update: -2151.08, Optimal reward -1981.33
Iteration 159 took 2.15 seconds (mean sampled reward: -3716.21). Current reward after update: -2412.48, Optimal reward -1981.33
Iteration 160 took 2.21 seconds (mean sampled reward: -3691.03). Current reward after update: -2148.06, Optimal reward -1981.33
Iteration 161 took 2.14 seconds (mean sampled reward: -3651.25). Current reward after update: -3864.29, Optimal reward -1981.33
Iteration 162 took 2.15 seconds (mean sampled reward: -3696.86). Current reward after update: -3672.65, Optimal reward -1981.33
Iteration 163 took 2.15 seconds (mean sampled reward: -4169.01). Current reward after update: -2143.85, Optimal reward -1981.33
Iteration 164 took 2.23 seconds (mean sampled reward: -4039.18). Current reward after update: -3533.12, Optimal reward -1981.33
Iteration 165 took 2.01 seconds (mean sampled reward: -4445.26). Current reward after update: -2150.24, Optimal reward -1981.33
Iteration 166 took 2.01 seconds (mean sampled reward: -4586.74). Current reward after update: -2198.73, Optimal reward -1981.33
Iteration 167 took 2.03 seconds (mean sampled reward: -4655.38). Current reward after update: -3695.22, Optimal reward -1981.33
Iteration 168 took 2.13 seconds (mean sampled reward: -4233.60). Current reward after update: -2138.69, Optimal reward -1981.33
Iteration 169 took 2.18 seconds (mean sampled reward: -4188.38). Current reward after update: -2120.54, Optimal reward -1981.33
Iteration 170 took 2.12 seconds (mean sampled reward: -4136.19). Current reward after update: -3445.30, Optimal reward -1981.33
Iteration 171 took 2.18 seconds (mean sampled reward: -4110.72). Current reward after update: -3970.31, Optimal reward -1981.33
Iteration 172 took 2.14 seconds (mean sampled reward: -4048.36). Current reward after update: -3248.19, Optimal reward -1981.33
Iteration 173 took 2.09 seconds (mean sampled reward: -4294.75). Current reward after update: -2150.98, Optimal reward -1981.33
Iteration 174 took 2.13 seconds (mean sampled reward: -4074.16). Current reward after update: -2102.07, Optimal reward -1981.33
Iteration 175 took 2.13 seconds (mean sampled reward: -3946.70). Current reward after update: -2157.62, Optimal reward -1981.33
Iteration 176 took 2.19 seconds (mean sampled reward: -4072.21). Current reward after update: -3365.99, Optimal reward -1981.33
Iteration 177 took 2.15 seconds (mean sampled reward: -4180.17). Current reward after update: -2213.46, Optimal reward -1981.33
Iteration 178 took 2.16 seconds (mean sampled reward: -4212.49). Current reward after update: -3443.68, Optimal reward -1981.33
Iteration 179 took 2.12 seconds (mean sampled reward: -4323.35). Current reward after update: -2118.33, Optimal reward -1981.33
Iteration 180 took 2.18 seconds (mean sampled reward: -3864.13). Current reward after update: -3707.81, Optimal reward -1981.33
Iteration 181 took 2.16 seconds (mean sampled reward: -3733.36). Current reward after update: -2096.28, Optimal reward -1981.33
Iteration 182 took 2.09 seconds (mean sampled reward: -4339.99). Current reward after update: -3834.65, Optimal reward -1981.33
Iteration 183 took 2.06 seconds (mean sampled reward: -4713.42). Current reward after update: -2233.75, Optimal reward -1981.33
Iteration 184 took 2.11 seconds (mean sampled reward: -4495.55). Current reward after update: -3821.27, Optimal reward -1981.33
Iteration 185 took 2.16 seconds (mean sampled reward: -4053.83). Current reward after update: -3470.00, Optimal reward -1981.33
Iteration 186 took 2.10 seconds (mean sampled reward: -4302.20). Current reward after update: -2175.41, Optimal reward -1981.33
Iteration 187 took 2.18 seconds (mean sampled reward: -4123.32). Current reward after update: -3310.96, Optimal reward -1981.33
Iteration 188 took 2.32 seconds (mean sampled reward: -4339.46). Current reward after update: -2192.41, Optimal reward -1981.33
Iteration 189 took 2.19 seconds (mean sampled reward: -4516.95). Current reward after update: -3401.12, Optimal reward -1981.33
Iteration 190 took 2.23 seconds (mean sampled reward: -4298.78). Current reward after update: -2179.22, Optimal reward -1981.33
Iteration 191 took 2.19 seconds (mean sampled reward: -4913.91). Current reward after update: -2364.97, Optimal reward -1981.33
Iteration 192 took 2.25 seconds (mean sampled reward: -4695.99). Current reward after update: -3633.19, Optimal reward -1981.33
Iteration 193 took 2.21 seconds (mean sampled reward: -4132.19). Current reward after update: -2277.37, Optimal reward -1981.33
Iteration 194 took 2.14 seconds (mean sampled reward: -4922.00). Current reward after update: -2216.58, Optimal reward -1981.33
Iteration 195 took 2.18 seconds (mean sampled reward: -4878.10). Current reward after update: -2665.92, Optimal reward -1981.33
Iteration 196 took 2.15 seconds (mean sampled reward: -4843.61). Current reward after update: -2250.59, Optimal reward -1981.33
Iteration 197 took 2.17 seconds (mean sampled reward: -4836.89). Current reward after update: -2150.88, Optimal reward -1981.33
Iteration 198 took 2.20 seconds (mean sampled reward: -4681.85). Current reward after update: -2138.07, Optimal reward -1981.33
Iteration 199 took 2.17 seconds (mean sampled reward: -4428.06). Current reward after update: -3736.97, Optimal reward -1981.33
Iteration 200 took 2.17 seconds (mean sampled reward: -4233.69). Current reward after update: -2184.16, Optimal reward -1981.33
Iteration 1 took 2.14 seconds (mean sampled reward: -7531.86). Current reward after update: -7289.42, Optimal reward -7289.42
Iteration 2 took 2.07 seconds (mean sampled reward: -7509.80). Current reward after update: -7271.75, Optimal reward -7271.75
Iteration 3 took 1.96 seconds (mean sampled reward: -7460.77). Current reward after update: -7215.93, Optimal reward -7215.93
Iteration 4 took 1.97 seconds (mean sampled reward: -7373.24). Current reward after update: -7171.12, Optimal reward -7171.12
Iteration 5 took 1.93 seconds (mean sampled reward: -7334.17). Current reward after update: -7140.18, Optimal reward -7140.18
Iteration 6 took 2.01 seconds (mean sampled reward: -7324.59). Current reward after update: -7161.71, Optimal reward -7140.18
Iteration 7 took 2.05 seconds (mean sampled reward: -7315.17). Current reward after update: -7186.78, Optimal reward -7140.18
Iteration 8 took 2.08 seconds (mean sampled reward: -7306.91). Current reward after update: -7162.17, Optimal reward -7140.18
Iteration 9 took 2.14 seconds (mean sampled reward: -7313.28). Current reward after update: -7136.35, Optimal reward -7136.35
Iteration 10 took 2.17 seconds (mean sampled reward: -7300.06). Current reward after update: -6945.09, Optimal reward -6945.09
Iteration 11 took 2.21 seconds (mean sampled reward: -7274.75). Current reward after update: -6716.44, Optimal reward -6716.44
Iteration 12 took 2.18 seconds (mean sampled reward: -7273.14). Current reward after update: -6344.51, Optimal reward -6344.51
Iteration 13 took 2.14 seconds (mean sampled reward: -7311.90). Current reward after update: -6307.91, Optimal reward -6307.91
Iteration 14 took 2.08 seconds (mean sampled reward: -6950.72). Current reward after update: -6282.06, Optimal reward -6282.06
Iteration 15 took 2.18 seconds (mean sampled reward: -7096.26). Current reward after update: -6268.69, Optimal reward -6268.69
Iteration 16 took 2.23 seconds (mean sampled reward: -7085.69). Current reward after update: -6281.14, Optimal reward -6268.69
Iteration 17 took 2.26 seconds (mean sampled reward: -6839.73). Current reward after update: -6228.82, Optimal reward -6228.82
Iteration 18 took 2.24 seconds (mean sampled reward: -6560.99). Current reward after update: -6099.64, Optimal reward -6099.64
Iteration 19 took 2.23 seconds (mean sampled reward: -6971.18). Current reward after update: -5928.22, Optimal reward -5928.22
Iteration 20 took 2.11 seconds (mean sampled reward: -6642.57). Current reward after update: -5912.09, Optimal reward -5912.09
Iteration 21 took 2.05 seconds (mean sampled reward: -6714.42). Current reward after update: -5930.79, Optimal reward -5912.09
Iteration 22 took 2.11 seconds (mean sampled reward: -6830.71). Current reward after update: -5963.06, Optimal reward -5912.09
Iteration 23 took 2.21 seconds (mean sampled reward: -6673.68). Current reward after update: -5680.41, Optimal reward -5680.41
Iteration 24 took 2.14 seconds (mean sampled reward: -6427.20). Current reward after update: -5884.84, Optimal reward -5680.41
Iteration 25 took 2.07 seconds (mean sampled reward: -6064.50). Current reward after update: -5263.64, Optimal reward -5263.64
Iteration 26 took 2.19 seconds (mean sampled reward: -6336.50). Current reward after update: -4768.95, Optimal reward -4768.95
Iteration 27 took 2.09 seconds (mean sampled reward: -5754.47). Current reward after update: -4689.47, Optimal reward -4689.47
Iteration 28 took 2.10 seconds (mean sampled reward: -5477.22). Current reward after update: -4109.74, Optimal reward -4109.74
Iteration 29 took 2.20 seconds (mean sampled reward: -5376.55). Current reward after update: -4205.00, Optimal reward -4109.74
Iteration 30 took 2.21 seconds (mean sampled reward: -5747.96). Current reward after update: -4222.28, Optimal reward -4109.74
Iteration 31 took 2.28 seconds (mean sampled reward: -5550.09). Current reward after update: -4187.88, Optimal reward -4109.74
Iteration 32 took 2.17 seconds (mean sampled reward: -5692.50). Current reward after update: -4177.49, Optimal reward -4109.74
Iteration 33 took 2.30 seconds (mean sampled reward: -5305.01). Current reward after update: -3685.34, Optimal reward -3685.34
Iteration 34 took 2.21 seconds (mean sampled reward: -5386.38). Current reward after update: -3855.38, Optimal reward -3685.34
Iteration 35 took 2.18 seconds (mean sampled reward: -5132.53). Current reward after update: -3496.00, Optimal reward -3496.00
Iteration 36 took 2.12 seconds (mean sampled reward: -5070.58). Current reward after update: -3845.27, Optimal reward -3496.00
Iteration 37 took 1.94 seconds (mean sampled reward: -6205.68). Current reward after update: -3076.57, Optimal reward -3076.57
Iteration 38 took 1.98 seconds (mean sampled reward: -6061.55). Current reward after update: -3142.39, Optimal reward -3076.57
Iteration 39 took 2.05 seconds (mean sampled reward: -5946.27). Current reward after update: -3133.80, Optimal reward -3076.57
Iteration 40 took 1.98 seconds (mean sampled reward: -6076.91). Current reward after update: -3353.31, Optimal reward -3076.57
Iteration 41 took 1.96 seconds (mean sampled reward: -6103.61). Current reward after update: -3011.23, Optimal reward -3011.23
Iteration 42 took 1.90 seconds (mean sampled reward: -6131.43). Current reward after update: -2886.79, Optimal reward -2886.79
Iteration 43 took 1.84 seconds (mean sampled reward: -6283.19). Current reward after update: -2778.36, Optimal reward -2778.36
Iteration 44 took 1.82 seconds (mean sampled reward: -6489.38). Current reward after update: -2942.50, Optimal reward -2778.36
Iteration 45 took 1.90 seconds (mean sampled reward: -5846.78). Current reward after update: -2809.47, Optimal reward -2778.36
Iteration 46 took 1.80 seconds (mean sampled reward: -5610.33). Current reward after update: -2764.60, Optimal reward -2764.60
Iteration 47 took 1.87 seconds (mean sampled reward: -5433.99). Current reward after update: -3133.42, Optimal reward -2764.60
Iteration 48 took 1.93 seconds (mean sampled reward: -5598.29). Current reward after update: -2641.61, Optimal reward -2641.61
Iteration 49 took 1.90 seconds (mean sampled reward: -5313.07). Current reward after update: -2725.77, Optimal reward -2641.61
Iteration 50 took 1.83 seconds (mean sampled reward: -5827.42). Current reward after update: -2934.19, Optimal reward -2641.61
Iteration 51 took 1.90 seconds (mean sampled reward: -5292.30). Current reward after update: -2694.70, Optimal reward -2641.61
Iteration 52 took 1.91 seconds (mean sampled reward: -5125.83). Current reward after update: -2670.41, Optimal reward -2641.61
Iteration 53 took 1.88 seconds (mean sampled reward: -5851.78). Current reward after update: -2280.43, Optimal reward -2280.43
Iteration 54 took 2.25 seconds (mean sampled reward: -5684.25). Current reward after update: -2239.71, Optimal reward -2239.71
Iteration 55 took 2.17 seconds (mean sampled reward: -5474.57). Current reward after update: -2157.52, Optimal reward -2157.52
Iteration 56 took 2.24 seconds (mean sampled reward: -5375.51). Current reward after update: -2146.08, Optimal reward -2146.08
Iteration 57 took 2.09 seconds (mean sampled reward: -5894.54). Current reward after update: -2168.93, Optimal reward -2146.08
Iteration 58 took 2.10 seconds (mean sampled reward: -6004.58). Current reward after update: -2012.67, Optimal reward -2012.67
Iteration 59 took 2.40 seconds (mean sampled reward: -6415.56). Current reward after update: -1945.91, Optimal reward -1945.91
Iteration 60 took 2.14 seconds (mean sampled reward: -5979.11). Current reward after update: -1943.75, Optimal reward -1943.75
Iteration 61 took 2.02 seconds (mean sampled reward: -6019.27). Current reward after update: -1925.89, Optimal reward -1925.89
Iteration 62 took 1.96 seconds (mean sampled reward: -6249.31). Current reward after update: -1868.65, Optimal reward -1868.65
Iteration 63 took 1.86 seconds (mean sampled reward: -6738.22). Current reward after update: -1833.62, Optimal reward -1833.62
Iteration 64 took 1.90 seconds (mean sampled reward: -6367.94). Current reward after update: -1759.21, Optimal reward -1759.21
Iteration 65 took 1.97 seconds (mean sampled reward: -5815.84). Current reward after update: -1804.90, Optimal reward -1759.21
Iteration 66 took 1.99 seconds (mean sampled reward: -5798.03). Current reward after update: -1905.60, Optimal reward -1759.21
Iteration 67 took 1.88 seconds (mean sampled reward: -6250.43). Current reward after update: -1843.91, Optimal reward -1759.21
Iteration 68 took 1.92 seconds (mean sampled reward: -5928.64). Current reward after update: -6369.85, Optimal reward -1759.21
Iteration 69 took 1.84 seconds (mean sampled reward: -6379.51). Current reward after update: -1681.70, Optimal reward -1681.70
Iteration 70 took 1.90 seconds (mean sampled reward: -6873.07). Current reward after update: -1672.31, Optimal reward -1672.31
Iteration 71 took 1.97 seconds (mean sampled reward: -6630.09). Current reward after update: -1572.13, Optimal reward -1572.13
Iteration 72 took 1.91 seconds (mean sampled reward: -7052.04). Current reward after update: -1562.65, Optimal reward -1562.65
Iteration 73 took 1.89 seconds (mean sampled reward: -6506.33). Current reward after update: -1606.36, Optimal reward -1562.65
Iteration 74 took 1.87 seconds (mean sampled reward: -6537.63). Current reward after update: -1627.88, Optimal reward -1562.65
Iteration 75 took 1.93 seconds (mean sampled reward: -6909.03). Current reward after update: -1669.27, Optimal reward -1562.65
Iteration 76 took 1.97 seconds (mean sampled reward: -6447.04). Current reward after update: -1738.94, Optimal reward -1562.65
Iteration 77 took 2.02 seconds (mean sampled reward: -6441.95). Current reward after update: -1738.33, Optimal reward -1562.65
Iteration 78 took 1.89 seconds (mean sampled reward: -6429.83). Current reward after update: -1644.09, Optimal reward -1562.65
Iteration 79 took 1.88 seconds (mean sampled reward: -6038.16). Current reward after update: -1627.04, Optimal reward -1562.65
Iteration 80 took 2.04 seconds (mean sampled reward: -5736.09). Current reward after update: -1579.03, Optimal reward -1562.65
Iteration 81 took 1.97 seconds (mean sampled reward: -5659.71). Current reward after update: -1671.55, Optimal reward -1562.65
Iteration 82 took 1.96 seconds (mean sampled reward: -5825.14). Current reward after update: -1642.58, Optimal reward -1562.65
Iteration 83 took 2.01 seconds (mean sampled reward: -6144.09). Current reward after update: -1720.28, Optimal reward -1562.65
Iteration 84 took 1.92 seconds (mean sampled reward: -6121.04). Current reward after update: -1704.97, Optimal reward -1562.65
Iteration 85 took 2.04 seconds (mean sampled reward: -6533.84). Current reward after update: -1605.71, Optimal reward -1562.65
Iteration 86 took 1.95 seconds (mean sampled reward: -6722.34). Current reward after update: -1536.84, Optimal reward -1536.84
Iteration 87 took 1.99 seconds (mean sampled reward: -6915.68). Current reward after update: -1942.63, Optimal reward -1536.84
Iteration 88 took 1.98 seconds (mean sampled reward: -6857.73). Current reward after update: -1786.92, Optimal reward -1536.84
Iteration 89 took 1.99 seconds (mean sampled reward: -6583.06). Current reward after update: -1658.99, Optimal reward -1536.84
Iteration 90 took 2.03 seconds (mean sampled reward: -6432.67). Current reward after update: -1500.09, Optimal reward -1500.09
Iteration 91 took 2.05 seconds (mean sampled reward: -6374.86). Current reward after update: -1666.38, Optimal reward -1500.09
Iteration 92 took 2.14 seconds (mean sampled reward: -6211.05). Current reward after update: -1657.45, Optimal reward -1500.09
Iteration 93 took 2.13 seconds (mean sampled reward: -6162.32). Current reward after update: -1705.67, Optimal reward -1500.09
Iteration 94 took 2.15 seconds (mean sampled reward: -5923.87). Current reward after update: -1568.24, Optimal reward -1500.09
Iteration 95 took 2.09 seconds (mean sampled reward: -6200.96). Current reward after update: -1501.56, Optimal reward -1500.09
Iteration 96 took 2.04 seconds (mean sampled reward: -6588.26). Current reward after update: -1604.35, Optimal reward -1500.09
Iteration 97 took 1.98 seconds (mean sampled reward: -6286.43). Current reward after update: -1334.95, Optimal reward -1334.95
Iteration 98 took 2.10 seconds (mean sampled reward: -6404.95). Current reward after update: -1386.04, Optimal reward -1334.95
Iteration 99 took 2.09 seconds (mean sampled reward: -6323.05). Current reward after update: -1620.38, Optimal reward -1334.95
Iteration 100 took 2.24 seconds (mean sampled reward: -5652.51). Current reward after update: -1552.22, Optimal reward -1334.95
Iteration 101 took 2.15 seconds (mean sampled reward: -5968.21). Current reward after update: -1550.14, Optimal reward -1334.95
Iteration 102 took 2.04 seconds (mean sampled reward: -6026.30). Current reward after update: -1590.21, Optimal reward -1334.95
Iteration 103 took 2.06 seconds (mean sampled reward: -6054.74). Current reward after update: -1470.83, Optimal reward -1334.95
Iteration 104 took 2.03 seconds (mean sampled reward: -6265.45). Current reward after update: -1424.11, Optimal reward -1334.95
Iteration 105 took 2.06 seconds (mean sampled reward: -6302.69). Current reward after update: -1433.55, Optimal reward -1334.95
Iteration 106 took 2.07 seconds (mean sampled reward: -5819.70). Current reward after update: -1454.75, Optimal reward -1334.95
Iteration 107 took 2.03 seconds (mean sampled reward: -5961.04). Current reward after update: -1328.72, Optimal reward -1328.72
Iteration 108 took 2.17 seconds (mean sampled reward: -6374.59). Current reward after update: -1370.64, Optimal reward -1328.72
Iteration 109 took 2.07 seconds (mean sampled reward: -5928.31). Current reward after update: -1251.82, Optimal reward -1251.82
Iteration 110 took 2.13 seconds (mean sampled reward: -6032.19). Current reward after update: -1212.35, Optimal reward -1212.35
Iteration 111 took 2.07 seconds (mean sampled reward: -5830.30). Current reward after update: -1214.06, Optimal reward -1212.35
Iteration 112 took 2.35 seconds (mean sampled reward: -5563.31). Current reward after update: -1241.15, Optimal reward -1212.35
Iteration 113 took 2.16 seconds (mean sampled reward: -5804.71). Current reward after update: -1211.68, Optimal reward -1211.68
Iteration 114 took 2.08 seconds (mean sampled reward: -6055.62). Current reward after update: -1195.06, Optimal reward -1195.06
Iteration 115 took 2.14 seconds (mean sampled reward: -6071.79). Current reward after update: -1163.62, Optimal reward -1163.62
Iteration 116 took 1.98 seconds (mean sampled reward: -6037.78). Current reward after update: -1114.04, Optimal reward -1114.04
Iteration 117 took 1.93 seconds (mean sampled reward: -6364.94). Current reward after update: -1189.69, Optimal reward -1114.04
Iteration 118 took 2.02 seconds (mean sampled reward: -5634.09). Current reward after update: -1264.35, Optimal reward -1114.04
Iteration 119 took 2.06 seconds (mean sampled reward: -5307.71). Current reward after update: -1256.17, Optimal reward -1114.04
Iteration 120 took 2.00 seconds (mean sampled reward: -5661.69). Current reward after update: -1490.99, Optimal reward -1114.04
Iteration 121 took 2.06 seconds (mean sampled reward: -5068.25). Current reward after update: -1102.09, Optimal reward -1102.09
Iteration 122 took 1.98 seconds (mean sampled reward: -5632.54). Current reward after update: -1148.27, Optimal reward -1102.09
Iteration 123 took 1.86 seconds (mean sampled reward: -5886.07). Current reward after update: -1144.99, Optimal reward -1102.09
Iteration 124 took 1.98 seconds (mean sampled reward: -5548.43). Current reward after update: -1208.28, Optimal reward -1102.09
Iteration 125 took 2.04 seconds (mean sampled reward: -5607.85). Current reward after update: -1141.96, Optimal reward -1102.09
Iteration 126 took 1.88 seconds (mean sampled reward: -5567.81). Current reward after update: -1122.92, Optimal reward -1102.09
Iteration 127 took 1.87 seconds (mean sampled reward: -5625.75). Current reward after update: -1090.64, Optimal reward -1090.64
Iteration 128 took 1.89 seconds (mean sampled reward: -5420.05). Current reward after update: -1024.78, Optimal reward -1024.78
Iteration 129 took 1.87 seconds (mean sampled reward: -5619.22). Current reward after update: -1023.88, Optimal reward -1023.88
Iteration 130 took 1.95 seconds (mean sampled reward: -5977.61). Current reward after update: -999.77, Optimal reward -999.77
Iteration 131 took 2.06 seconds (mean sampled reward: -5807.92). Current reward after update: -1801.73, Optimal reward -999.77
Iteration 132 took 1.91 seconds (mean sampled reward: -5817.60). Current reward after update: -999.21, Optimal reward -999.21
Iteration 133 took 1.88 seconds (mean sampled reward: -6080.90). Current reward after update: -917.07, Optimal reward -917.07
Iteration 134 took 2.01 seconds (mean sampled reward: -6243.82). Current reward after update: -1059.56, Optimal reward -917.07
Iteration 135 took 1.88 seconds (mean sampled reward: -5925.87). Current reward after update: -1442.22, Optimal reward -917.07
Iteration 136 took 1.90 seconds (mean sampled reward: -6007.94). Current reward after update: -902.99, Optimal reward -902.99
Iteration 137 took 1.96 seconds (mean sampled reward: -5331.57). Current reward after update: -875.61, Optimal reward -875.61
Iteration 138 took 1.98 seconds (mean sampled reward: -5745.41). Current reward after update: -912.16, Optimal reward -875.61
Iteration 139 took 1.95 seconds (mean sampled reward: -5438.84). Current reward after update: -933.97, Optimal reward -875.61
Iteration 140 took 1.92 seconds (mean sampled reward: -5645.78). Current reward after update: -867.79, Optimal reward -867.79
Iteration 141 took 1.95 seconds (mean sampled reward: -5821.74). Current reward after update: -991.11, Optimal reward -867.79
Iteration 142 took 2.03 seconds (mean sampled reward: -6215.55). Current reward after update: -1042.53, Optimal reward -867.79
Iteration 143 took 1.92 seconds (mean sampled reward: -6696.26). Current reward after update: -1098.54, Optimal reward -867.79
Iteration 144 took 1.91 seconds (mean sampled reward: -6561.80). Current reward after update: -960.26, Optimal reward -867.79
Iteration 145 took 1.94 seconds (mean sampled reward: -6046.65). Current reward after update: -1012.17, Optimal reward -867.79
Iteration 146 took 1.93 seconds (mean sampled reward: -5946.69). Current reward after update: -930.96, Optimal reward -867.79
Iteration 147 took 1.87 seconds (mean sampled reward: -6026.38). Current reward after update: -1740.35, Optimal reward -867.79
Iteration 148 took 1.92 seconds (mean sampled reward: -5682.16). Current reward after update: -1233.27, Optimal reward -867.79
Iteration 149 took 1.85 seconds (mean sampled reward: -5621.97). Current reward after update: -1689.24, Optimal reward -867.79
Iteration 150 took 1.90 seconds (mean sampled reward: -6144.66). Current reward after update: -1032.99, Optimal reward -867.79
Iteration 151 took 2.05 seconds (mean sampled reward: -6130.99). Current reward after update: -986.48, Optimal reward -867.79
Iteration 152 took 2.09 seconds (mean sampled reward: -5976.06). Current reward after update: -1554.88, Optimal reward -867.79
Iteration 153 took 2.06 seconds (mean sampled reward: -6304.73). Current reward after update: -1010.20, Optimal reward -867.79
Iteration 154 took 2.05 seconds (mean sampled reward: -5926.42). Current reward after update: -1087.12, Optimal reward -867.79
Iteration 155 took 2.04 seconds (mean sampled reward: -6264.16). Current reward after update: -1123.50, Optimal reward -867.79
Iteration 156 took 2.07 seconds (mean sampled reward: -5838.46). Current reward after update: -1052.40, Optimal reward -867.79
Iteration 157 took 2.03 seconds (mean sampled reward: -6446.59). Current reward after update: -1045.63, Optimal reward -867.79
Iteration 158 took 2.07 seconds (mean sampled reward: -6972.50). Current reward after update: -1143.18, Optimal reward -867.79
Iteration 159 took 2.03 seconds (mean sampled reward: -6359.91). Current reward after update: -1048.02, Optimal reward -867.79
Iteration 160 took 2.03 seconds (mean sampled reward: -6120.66). Current reward after update: -962.53, Optimal reward -867.79
Iteration 161 took 2.02 seconds (mean sampled reward: -6558.82). Current reward after update: -1000.36, Optimal reward -867.79
Iteration 162 took 2.05 seconds (mean sampled reward: -6560.77). Current reward after update: -1189.45, Optimal reward -867.79
Iteration 163 took 2.07 seconds (mean sampled reward: -5944.25). Current reward after update: -1151.68, Optimal reward -867.79
Iteration 164 took 2.01 seconds (mean sampled reward: -6045.62). Current reward after update: -1233.01, Optimal reward -867.79
Iteration 165 took 2.07 seconds (mean sampled reward: -5948.57). Current reward after update: -1196.04, Optimal reward -867.79
Iteration 166 took 2.06 seconds (mean sampled reward: -5804.18). Current reward after update: -1222.20, Optimal reward -867.79
Iteration 167 took 2.06 seconds (mean sampled reward: -5783.27). Current reward after update: -6549.85, Optimal reward -867.79
Iteration 168 took 2.06 seconds (mean sampled reward: -6023.28). Current reward after update: -1763.61, Optimal reward -867.79
Iteration 169 took 2.20 seconds (mean sampled reward: -5624.49). Current reward after update: -1123.26, Optimal reward -867.79
Iteration 170 took 2.02 seconds (mean sampled reward: -6102.24). Current reward after update: -1156.03, Optimal reward -867.79
Iteration 171 took 2.03 seconds (mean sampled reward: -6156.53). Current reward after update: -1072.42, Optimal reward -867.79
Iteration 172 took 2.10 seconds (mean sampled reward: -6110.33). Current reward after update: -1061.24, Optimal reward -867.79
Iteration 173 took 2.03 seconds (mean sampled reward: -5748.28). Current reward after update: -1293.22, Optimal reward -867.79
Iteration 174 took 2.06 seconds (mean sampled reward: -5601.15). Current reward after update: -1203.50, Optimal reward -867.79
Iteration 175 took 2.04 seconds (mean sampled reward: -5505.19). Current reward after update: -1082.12, Optimal reward -867.79
Iteration 176 took 2.09 seconds (mean sampled reward: -6045.32). Current reward after update: -1133.64, Optimal reward -867.79
Iteration 177 took 2.02 seconds (mean sampled reward: -5932.30). Current reward after update: -1045.95, Optimal reward -867.79
Iteration 178 took 2.08 seconds (mean sampled reward: -5867.17). Current reward after update: -932.04, Optimal reward -867.79
Iteration 179 took 2.07 seconds (mean sampled reward: -6622.97). Current reward after update: -931.88, Optimal reward -867.79
Iteration 180 took 2.06 seconds (mean sampled reward: -5853.08). Current reward after update: -1532.68, Optimal reward -867.79
Iteration 181 took 2.06 seconds (mean sampled reward: -5684.17). Current reward after update: -1362.26, Optimal reward -867.79
Iteration 182 took 2.03 seconds (mean sampled reward: -5957.75). Current reward after update: -1049.99, Optimal reward -867.79
Iteration 183 took 2.05 seconds (mean sampled reward: -6762.94). Current reward after update: -1186.96, Optimal reward -867.79
Iteration 184 took 2.04 seconds (mean sampled reward: -6494.13). Current reward after update: -1052.69, Optimal reward -867.79
Iteration 185 took 2.08 seconds (mean sampled reward: -6654.06). Current reward after update: -986.82, Optimal reward -867.79
Iteration 186 took 1.99 seconds (mean sampled reward: -6404.85). Current reward after update: -1105.79, Optimal reward -867.79
Iteration 187 took 2.06 seconds (mean sampled reward: -5871.23). Current reward after update: -1025.60, Optimal reward -867.79
Iteration 188 took 2.05 seconds (mean sampled reward: -6204.04). Current reward after update: -1050.70, Optimal reward -867.79
Iteration 189 took 2.06 seconds (mean sampled reward: -5695.87). Current reward after update: -1020.10, Optimal reward -867.79
Iteration 190 took 2.06 seconds (mean sampled reward: -5771.82). Current reward after update: -2287.36, Optimal reward -867.79
Iteration 191 took 2.08 seconds (mean sampled reward: -5963.32). Current reward after update: -1034.07, Optimal reward -867.79
Iteration 192 took 2.06 seconds (mean sampled reward: -5556.97). Current reward after update: -1021.57, Optimal reward -867.79
Iteration 193 took 2.03 seconds (mean sampled reward: -5571.13). Current reward after update: -994.59, Optimal reward -867.79
Iteration 194 took 2.09 seconds (mean sampled reward: -5783.26). Current reward after update: -1015.06, Optimal reward -867.79
Iteration 195 took 2.05 seconds (mean sampled reward: -6155.06). Current reward after update: -1443.48, Optimal reward -867.79
Iteration 196 took 2.12 seconds (mean sampled reward: -6118.04). Current reward after update: -1166.97, Optimal reward -867.79
Iteration 197 took 2.07 seconds (mean sampled reward: -6430.07). Current reward after update: -1040.52, Optimal reward -867.79
Iteration 198 took 2.07 seconds (mean sampled reward: -6547.59). Current reward after update: -1006.57, Optimal reward -867.79
Iteration 199 took 2.04 seconds (mean sampled reward: -6591.27). Current reward after update: -1000.14, Optimal reward -867.79
Iteration 200 took 2.06 seconds (mean sampled reward: -6640.01). Current reward after update: -1031.56, Optimal reward -867.79
Max force: 10 Sigma: 0.1 mean rewards: -1979.5888029661528, best rewards:-867.794781380316

Iteration 1 took 2.11 seconds (mean sampled reward: -7530.72). Current reward after update: -7257.93, Optimal reward -7257.93
Iteration 2 took 2.02 seconds (mean sampled reward: -7463.07). Current reward after update: -7218.07, Optimal reward -7218.07
Iteration 3 took 1.90 seconds (mean sampled reward: -7408.16). Current reward after update: -6141.88, Optimal reward -6141.88
Iteration 4 took 2.12 seconds (mean sampled reward: -7355.89). Current reward after update: -5932.20, Optimal reward -5932.20
Iteration 5 took 2.10 seconds (mean sampled reward: -7274.49). Current reward after update: -5553.60, Optimal reward -5553.60
Iteration 6 took 2.22 seconds (mean sampled reward: -6652.00). Current reward after update: -4533.20, Optimal reward -4533.20
Iteration 7 took 2.14 seconds (mean sampled reward: -6612.08). Current reward after update: -4473.93, Optimal reward -4473.93
Iteration 8 took 2.38 seconds (mean sampled reward: -6298.14). Current reward after update: -4182.33, Optimal reward -4182.33
Iteration 9 took 2.24 seconds (mean sampled reward: -5847.44). Current reward after update: -4219.59, Optimal reward -4182.33
Iteration 10 took 2.24 seconds (mean sampled reward: -6103.80). Current reward after update: -4326.58, Optimal reward -4182.33
Iteration 11 took 2.31 seconds (mean sampled reward: -5559.13). Current reward after update: -4101.92, Optimal reward -4101.92
Iteration 12 took 2.42 seconds (mean sampled reward: -6073.74). Current reward after update: -3844.15, Optimal reward -3844.15
Iteration 13 took 2.19 seconds (mean sampled reward: -5719.65). Current reward after update: -3472.72, Optimal reward -3472.72
Iteration 14 took 2.46 seconds (mean sampled reward: -5781.10). Current reward after update: -3286.75, Optimal reward -3286.75
Iteration 15 took 2.31 seconds (mean sampled reward: -6394.14). Current reward after update: -3305.87, Optimal reward -3286.75
Iteration 16 took 2.32 seconds (mean sampled reward: -5575.35). Current reward after update: -3114.87, Optimal reward -3114.87
Iteration 17 took 2.43 seconds (mean sampled reward: -5822.16). Current reward after update: -3022.79, Optimal reward -3022.79
Iteration 18 took 2.22 seconds (mean sampled reward: -5051.06). Current reward after update: -2805.71, Optimal reward -2805.71
Iteration 19 took 2.33 seconds (mean sampled reward: -4649.75). Current reward after update: -2677.49, Optimal reward -2677.49
Iteration 20 took 2.32 seconds (mean sampled reward: -5271.15). Current reward after update: -2693.03, Optimal reward -2677.49
Iteration 21 took 2.24 seconds (mean sampled reward: -5165.16). Current reward after update: -2734.01, Optimal reward -2677.49
Iteration 22 took 2.29 seconds (mean sampled reward: -5182.82). Current reward after update: -2457.84, Optimal reward -2457.84
Iteration 23 took 2.31 seconds (mean sampled reward: -5834.04). Current reward after update: -2780.70, Optimal reward -2457.84
Iteration 24 took 2.43 seconds (mean sampled reward: -4605.88). Current reward after update: -2402.57, Optimal reward -2402.57
Iteration 25 took 2.19 seconds (mean sampled reward: -4715.68). Current reward after update: -2503.56, Optimal reward -2402.57
Iteration 26 took 2.23 seconds (mean sampled reward: -4136.29). Current reward after update: -2294.84, Optimal reward -2294.84
Iteration 27 took 2.32 seconds (mean sampled reward: -3766.07). Current reward after update: -2468.86, Optimal reward -2294.84
Iteration 28 took 2.21 seconds (mean sampled reward: -4794.52). Current reward after update: -2396.65, Optimal reward -2294.84
Iteration 29 took 2.09 seconds (mean sampled reward: -4665.83). Current reward after update: -2293.76, Optimal reward -2293.76
Iteration 30 took 2.21 seconds (mean sampled reward: -5072.97). Current reward after update: -2449.81, Optimal reward -2293.76
Iteration 31 took 2.15 seconds (mean sampled reward: -5024.10). Current reward after update: -2678.67, Optimal reward -2293.76
Iteration 32 took 2.29 seconds (mean sampled reward: -4880.17). Current reward after update: -2257.23, Optimal reward -2257.23
Iteration 33 took 2.23 seconds (mean sampled reward: -5440.04). Current reward after update: -2366.70, Optimal reward -2257.23
Iteration 34 took 2.20 seconds (mean sampled reward: -4585.58). Current reward after update: -2252.45, Optimal reward -2252.45
Iteration 35 took 2.34 seconds (mean sampled reward: -4357.88). Current reward after update: -3200.34, Optimal reward -2252.45
Iteration 36 took 2.25 seconds (mean sampled reward: -4660.47). Current reward after update: -2212.97, Optimal reward -2212.97
Iteration 37 took 2.38 seconds (mean sampled reward: -4633.36). Current reward after update: -2106.74, Optimal reward -2106.74
Iteration 38 took 2.42 seconds (mean sampled reward: -4366.95). Current reward after update: -2170.60, Optimal reward -2106.74
Iteration 39 took 2.23 seconds (mean sampled reward: -4811.59). Current reward after update: -2387.14, Optimal reward -2106.74
Iteration 40 took 2.19 seconds (mean sampled reward: -4724.54). Current reward after update: -2540.39, Optimal reward -2106.74
Iteration 41 took 2.34 seconds (mean sampled reward: -4368.76). Current reward after update: -2740.48, Optimal reward -2106.74
Iteration 42 took 2.30 seconds (mean sampled reward: -4587.83). Current reward after update: -2072.65, Optimal reward -2072.65
Iteration 43 took 2.26 seconds (mean sampled reward: -4841.10). Current reward after update: -2391.28, Optimal reward -2072.65
Iteration 44 took 2.33 seconds (mean sampled reward: -4457.70). Current reward after update: -1989.34, Optimal reward -1989.34
Iteration 45 took 2.36 seconds (mean sampled reward: -3527.15). Current reward after update: -2869.44, Optimal reward -1989.34
Iteration 46 took 2.27 seconds (mean sampled reward: -3243.26). Current reward after update: -2507.37, Optimal reward -1989.34
Iteration 47 took 2.25 seconds (mean sampled reward: -4730.90). Current reward after update: -1964.14, Optimal reward -1964.14
Iteration 48 took 2.37 seconds (mean sampled reward: -4367.54). Current reward after update: -1976.78, Optimal reward -1964.14
Iteration 49 took 2.28 seconds (mean sampled reward: -3507.37). Current reward after update: -2371.16, Optimal reward -1964.14
Iteration 50 took 2.20 seconds (mean sampled reward: -2983.32). Current reward after update: -2475.98, Optimal reward -1964.14
Iteration 51 took 2.24 seconds (mean sampled reward: -3485.86). Current reward after update: -2109.34, Optimal reward -1964.14
Iteration 52 took 2.20 seconds (mean sampled reward: -3184.37). Current reward after update: -2197.85, Optimal reward -1964.14
Iteration 53 took 2.36 seconds (mean sampled reward: -2767.34). Current reward after update: -2269.89, Optimal reward -1964.14
Iteration 54 took 2.20 seconds (mean sampled reward: -2907.54). Current reward after update: -2615.81, Optimal reward -1964.14
Iteration 55 took 2.46 seconds (mean sampled reward: -2963.50). Current reward after update: -2393.31, Optimal reward -1964.14
Iteration 56 took 2.37 seconds (mean sampled reward: -2885.32). Current reward after update: -1918.81, Optimal reward -1918.81
Iteration 57 took 2.42 seconds (mean sampled reward: -2971.88). Current reward after update: -1956.14, Optimal reward -1918.81
Iteration 58 took 2.29 seconds (mean sampled reward: -3262.13). Current reward after update: -3656.42, Optimal reward -1918.81
Iteration 59 took 2.33 seconds (mean sampled reward: -3585.46). Current reward after update: -2248.79, Optimal reward -1918.81
Iteration 60 took 2.44 seconds (mean sampled reward: -3237.91). Current reward after update: -2161.03, Optimal reward -1918.81
Iteration 61 took 2.23 seconds (mean sampled reward: -3259.68). Current reward after update: -1888.10, Optimal reward -1888.10
Iteration 62 took 2.13 seconds (mean sampled reward: -3317.47). Current reward after update: -1884.01, Optimal reward -1884.01
Iteration 63 took 2.19 seconds (mean sampled reward: -4471.38). Current reward after update: -2467.77, Optimal reward -1884.01
Iteration 64 took 2.10 seconds (mean sampled reward: -4418.04). Current reward after update: -2317.38, Optimal reward -1884.01
Iteration 65 took 2.17 seconds (mean sampled reward: -5007.98). Current reward after update: -1856.70, Optimal reward -1856.70
Iteration 66 took 2.18 seconds (mean sampled reward: -4751.88). Current reward after update: -2157.40, Optimal reward -1856.70
Iteration 67 took 2.27 seconds (mean sampled reward: -4276.01). Current reward after update: -1812.08, Optimal reward -1812.08
Iteration 68 took 2.27 seconds (mean sampled reward: -4277.25). Current reward after update: -1822.10, Optimal reward -1812.08
Iteration 69 took 2.18 seconds (mean sampled reward: -3540.74). Current reward after update: -1757.45, Optimal reward -1757.45
Iteration 70 took 2.24 seconds (mean sampled reward: -2769.94). Current reward after update: -1685.44, Optimal reward -1685.44
Iteration 71 took 2.26 seconds (mean sampled reward: -2926.96). Current reward after update: -1497.58, Optimal reward -1497.58
Iteration 72 took 2.25 seconds (mean sampled reward: -2767.14). Current reward after update: -1678.90, Optimal reward -1497.58
Iteration 73 took 2.22 seconds (mean sampled reward: -2501.55). Current reward after update: -1509.77, Optimal reward -1497.58
Iteration 74 took 2.22 seconds (mean sampled reward: -2265.82). Current reward after update: -1581.58, Optimal reward -1497.58
Iteration 75 took 2.19 seconds (mean sampled reward: -2090.63). Current reward after update: -2126.83, Optimal reward -1497.58
Iteration 76 took 2.21 seconds (mean sampled reward: -2127.70). Current reward after update: -1604.78, Optimal reward -1497.58
Iteration 77 took 2.20 seconds (mean sampled reward: -2138.10). Current reward after update: -1464.01, Optimal reward -1464.01
Iteration 78 took 2.19 seconds (mean sampled reward: -2239.97). Current reward after update: -2684.08, Optimal reward -1464.01
Iteration 79 took 2.28 seconds (mean sampled reward: -1982.63). Current reward after update: -1530.18, Optimal reward -1464.01
Iteration 80 took 2.17 seconds (mean sampled reward: -2035.27). Current reward after update: -1608.36, Optimal reward -1464.01
Iteration 81 took 2.25 seconds (mean sampled reward: -2181.35). Current reward after update: -1461.63, Optimal reward -1461.63
Iteration 82 took 2.14 seconds (mean sampled reward: -1980.27). Current reward after update: -1417.88, Optimal reward -1417.88
Iteration 83 took 2.05 seconds (mean sampled reward: -2236.37). Current reward after update: -1406.13, Optimal reward -1406.13
Iteration 84 took 2.11 seconds (mean sampled reward: -2692.45). Current reward after update: -1348.70, Optimal reward -1348.70
Iteration 85 took 2.11 seconds (mean sampled reward: -2117.32). Current reward after update: -1520.28, Optimal reward -1348.70
Iteration 86 took 2.09 seconds (mean sampled reward: -2096.34). Current reward after update: -1383.79, Optimal reward -1348.70
Iteration 87 took 2.17 seconds (mean sampled reward: -2027.63). Current reward after update: -1438.68, Optimal reward -1348.70
Iteration 88 took 2.07 seconds (mean sampled reward: -2096.47). Current reward after update: -1392.07, Optimal reward -1348.70
Iteration 89 took 2.15 seconds (mean sampled reward: -2376.84). Current reward after update: -1418.89, Optimal reward -1348.70
Iteration 90 took 2.10 seconds (mean sampled reward: -2490.80). Current reward after update: -1933.46, Optimal reward -1348.70
Iteration 91 took 2.13 seconds (mean sampled reward: -2157.74). Current reward after update: -1403.73, Optimal reward -1348.70
Iteration 92 took 2.06 seconds (mean sampled reward: -2318.45). Current reward after update: -1706.17, Optimal reward -1348.70
Iteration 93 took 2.19 seconds (mean sampled reward: -2274.36). Current reward after update: -1504.12, Optimal reward -1348.70
Iteration 94 took 2.22 seconds (mean sampled reward: -2004.79). Current reward after update: -1493.87, Optimal reward -1348.70
Iteration 95 took 2.16 seconds (mean sampled reward: -2066.71). Current reward after update: -1396.04, Optimal reward -1348.70
Iteration 96 took 2.05 seconds (mean sampled reward: -2166.84). Current reward after update: -1432.40, Optimal reward -1348.70
Iteration 97 took 2.11 seconds (mean sampled reward: -1981.15). Current reward after update: -1550.13, Optimal reward -1348.70
Iteration 98 took 2.21 seconds (mean sampled reward: -2409.62). Current reward after update: -1686.03, Optimal reward -1348.70
Iteration 99 took 2.08 seconds (mean sampled reward: -3450.65). Current reward after update: -1462.79, Optimal reward -1348.70
Iteration 100 took 2.14 seconds (mean sampled reward: -3041.34). Current reward after update: -1486.65, Optimal reward -1348.70
Iteration 101 took 2.09 seconds (mean sampled reward: -2701.20). Current reward after update: -1492.38, Optimal reward -1348.70
Iteration 102 took 2.19 seconds (mean sampled reward: -2167.92). Current reward after update: -1609.86, Optimal reward -1348.70
Iteration 103 took 2.18 seconds (mean sampled reward: -2593.95). Current reward after update: -1417.96, Optimal reward -1348.70
Iteration 104 took 2.25 seconds (mean sampled reward: -2631.02). Current reward after update: -2004.27, Optimal reward -1348.70
Iteration 105 took 2.25 seconds (mean sampled reward: -2051.38). Current reward after update: -1446.05, Optimal reward -1348.70
Iteration 106 took 2.27 seconds (mean sampled reward: -2261.71). Current reward after update: -1559.32, Optimal reward -1348.70
Iteration 107 took 2.33 seconds (mean sampled reward: -3171.48). Current reward after update: -1489.17, Optimal reward -1348.70
Iteration 108 took 2.31 seconds (mean sampled reward: -2560.92). Current reward after update: -2033.51, Optimal reward -1348.70
Iteration 109 took 2.08 seconds (mean sampled reward: -2268.19). Current reward after update: -1634.05, Optimal reward -1348.70
Iteration 110 took 2.22 seconds (mean sampled reward: -2373.44). Current reward after update: -1553.20, Optimal reward -1348.70
Iteration 111 took 2.21 seconds (mean sampled reward: -3305.77). Current reward after update: -1478.74, Optimal reward -1348.70
Iteration 112 took 2.04 seconds (mean sampled reward: -3980.82). Current reward after update: -1945.30, Optimal reward -1348.70
Iteration 113 took 2.12 seconds (mean sampled reward: -4207.55). Current reward after update: -1593.06, Optimal reward -1348.70
Iteration 114 took 2.05 seconds (mean sampled reward: -3196.19). Current reward after update: -1466.48, Optimal reward -1348.70
Iteration 115 took 2.16 seconds (mean sampled reward: -2533.63). Current reward after update: -1542.80, Optimal reward -1348.70
Iteration 116 took 2.15 seconds (mean sampled reward: -2547.90). Current reward after update: -1416.66, Optimal reward -1348.70
Iteration 117 took 2.24 seconds (mean sampled reward: -2427.97). Current reward after update: -1485.29, Optimal reward -1348.70
Iteration 118 took 2.13 seconds (mean sampled reward: -2549.07). Current reward after update: -1320.74, Optimal reward -1320.74
Iteration 119 took 2.15 seconds (mean sampled reward: -2613.22). Current reward after update: -1379.05, Optimal reward -1320.74
Iteration 120 took 2.06 seconds (mean sampled reward: -2837.09). Current reward after update: -1414.53, Optimal reward -1320.74
Iteration 121 took 2.20 seconds (mean sampled reward: -3503.66). Current reward after update: -1988.97, Optimal reward -1320.74
Iteration 122 took 2.12 seconds (mean sampled reward: -3373.92). Current reward after update: -1802.42, Optimal reward -1320.74
Iteration 123 took 2.19 seconds (mean sampled reward: -2701.11). Current reward after update: -1483.04, Optimal reward -1320.74
Iteration 124 took 2.02 seconds (mean sampled reward: -2221.32). Current reward after update: -1506.44, Optimal reward -1320.74
Iteration 125 took 2.03 seconds (mean sampled reward: -2426.24). Current reward after update: -1376.32, Optimal reward -1320.74
Iteration 126 took 2.10 seconds (mean sampled reward: -2280.08). Current reward after update: -1495.05, Optimal reward -1320.74
Iteration 127 took 2.05 seconds (mean sampled reward: -2744.58). Current reward after update: -1906.07, Optimal reward -1320.74
Iteration 128 took 2.01 seconds (mean sampled reward: -2905.92). Current reward after update: -1885.85, Optimal reward -1320.74
Iteration 129 took 1.94 seconds (mean sampled reward: -4007.51). Current reward after update: -1477.70, Optimal reward -1320.74
Iteration 130 took 2.03 seconds (mean sampled reward: -3186.31). Current reward after update: -1752.18, Optimal reward -1320.74
Iteration 131 took 2.08 seconds (mean sampled reward: -3444.08). Current reward after update: -1442.23, Optimal reward -1320.74
Iteration 132 took 2.02 seconds (mean sampled reward: -3127.88). Current reward after update: -1419.26, Optimal reward -1320.74
Iteration 133 took 2.03 seconds (mean sampled reward: -3356.36). Current reward after update: -1993.40, Optimal reward -1320.74
Iteration 134 took 2.01 seconds (mean sampled reward: -2882.57). Current reward after update: -1499.37, Optimal reward -1320.74
Iteration 135 took 2.07 seconds (mean sampled reward: -2823.25). Current reward after update: -1477.88, Optimal reward -1320.74
Iteration 136 took 2.00 seconds (mean sampled reward: -3503.38). Current reward after update: -1428.56, Optimal reward -1320.74
Iteration 137 took 2.02 seconds (mean sampled reward: -3305.08). Current reward after update: -1605.91, Optimal reward -1320.74
Iteration 138 took 2.00 seconds (mean sampled reward: -3549.81). Current reward after update: -1404.98, Optimal reward -1320.74
Iteration 139 took 2.09 seconds (mean sampled reward: -4299.48). Current reward after update: -1394.64, Optimal reward -1320.74
Iteration 140 took 2.00 seconds (mean sampled reward: -3510.85). Current reward after update: -1344.81, Optimal reward -1320.74
Iteration 141 took 1.93 seconds (mean sampled reward: -4558.09). Current reward after update: -1474.95, Optimal reward -1320.74
Iteration 142 took 1.98 seconds (mean sampled reward: -3887.71). Current reward after update: -1384.33, Optimal reward -1320.74
Iteration 143 took 2.07 seconds (mean sampled reward: -2568.69). Current reward after update: -2061.91, Optimal reward -1320.74
Iteration 144 took 2.03 seconds (mean sampled reward: -2480.25). Current reward after update: -1274.71, Optimal reward -1274.71
Iteration 145 took 2.08 seconds (mean sampled reward: -2479.94). Current reward after update: -2074.81, Optimal reward -1274.71
Iteration 146 took 2.06 seconds (mean sampled reward: -2675.12). Current reward after update: -1531.76, Optimal reward -1274.71
Iteration 147 took 2.04 seconds (mean sampled reward: -2981.19). Current reward after update: -1463.08, Optimal reward -1274.71
Iteration 148 took 2.12 seconds (mean sampled reward: -2763.14). Current reward after update: -1421.29, Optimal reward -1274.71
Iteration 149 took 2.03 seconds (mean sampled reward: -3933.74). Current reward after update: -1431.74, Optimal reward -1274.71
Iteration 150 took 2.02 seconds (mean sampled reward: -4554.77). Current reward after update: -1702.93, Optimal reward -1274.71
Iteration 151 took 2.01 seconds (mean sampled reward: -4000.80). Current reward after update: -1449.56, Optimal reward -1274.71
Iteration 152 took 2.12 seconds (mean sampled reward: -3353.41). Current reward after update: -1429.63, Optimal reward -1274.71
Iteration 153 took 2.05 seconds (mean sampled reward: -4150.10). Current reward after update: -1623.81, Optimal reward -1274.71
Iteration 154 took 2.13 seconds (mean sampled reward: -2883.94). Current reward after update: -1486.95, Optimal reward -1274.71
Iteration 155 took 2.04 seconds (mean sampled reward: -3747.85). Current reward after update: -1460.47, Optimal reward -1274.71
Iteration 156 took 2.05 seconds (mean sampled reward: -4754.03). Current reward after update: -1518.30, Optimal reward -1274.71
Iteration 157 took 2.10 seconds (mean sampled reward: -3092.87). Current reward after update: -1421.37, Optimal reward -1274.71
Iteration 158 took 2.11 seconds (mean sampled reward: -3740.72). Current reward after update: -1369.39, Optimal reward -1274.71
Iteration 159 took 2.17 seconds (mean sampled reward: -2912.91). Current reward after update: -1297.47, Optimal reward -1274.71
Iteration 160 took 2.23 seconds (mean sampled reward: -3433.00). Current reward after update: -1323.90, Optimal reward -1274.71
Iteration 161 took 2.15 seconds (mean sampled reward: -3330.12). Current reward after update: -1393.33, Optimal reward -1274.71
Iteration 162 took 2.21 seconds (mean sampled reward: -4008.94). Current reward after update: -1352.19, Optimal reward -1274.71
Iteration 163 took 2.18 seconds (mean sampled reward: -3126.50). Current reward after update: -1420.13, Optimal reward -1274.71
Iteration 164 took 2.20 seconds (mean sampled reward: -3222.96). Current reward after update: -1338.81, Optimal reward -1274.71
Iteration 165 took 2.24 seconds (mean sampled reward: -3866.45). Current reward after update: -1469.41, Optimal reward -1274.71
Iteration 166 took 2.16 seconds (mean sampled reward: -3144.69). Current reward after update: -1228.96, Optimal reward -1228.96
Iteration 167 took 2.21 seconds (mean sampled reward: -2811.01). Current reward after update: -1422.67, Optimal reward -1228.96
Iteration 168 took 2.20 seconds (mean sampled reward: -3001.77). Current reward after update: -1334.88, Optimal reward -1228.96
Iteration 169 took 2.15 seconds (mean sampled reward: -3027.44). Current reward after update: -1806.30, Optimal reward -1228.96
Iteration 170 took 2.17 seconds (mean sampled reward: -2731.44). Current reward after update: -1539.07, Optimal reward -1228.96
Iteration 171 took 2.23 seconds (mean sampled reward: -2462.74). Current reward after update: -1425.11, Optimal reward -1228.96
Iteration 172 took 2.10 seconds (mean sampled reward: -2812.87). Current reward after update: -1397.88, Optimal reward -1228.96
Iteration 173 took 2.14 seconds (mean sampled reward: -2938.62). Current reward after update: -1442.97, Optimal reward -1228.96
Iteration 174 took 2.14 seconds (mean sampled reward: -3011.29). Current reward after update: -1360.74, Optimal reward -1228.96
Iteration 175 took 2.12 seconds (mean sampled reward: -3225.81). Current reward after update: -1366.21, Optimal reward -1228.96
Iteration 176 took 2.16 seconds (mean sampled reward: -3885.41). Current reward after update: -2474.20, Optimal reward -1228.96
Iteration 177 took 2.16 seconds (mean sampled reward: -2590.12). Current reward after update: -1646.03, Optimal reward -1228.96
Iteration 178 took 2.08 seconds (mean sampled reward: -3373.52). Current reward after update: -1289.80, Optimal reward -1228.96
Iteration 179 took 2.04 seconds (mean sampled reward: -3890.54). Current reward after update: -1468.34, Optimal reward -1228.96
Iteration 180 took 2.12 seconds (mean sampled reward: -4031.19). Current reward after update: -1410.41, Optimal reward -1228.96
Iteration 181 took 2.06 seconds (mean sampled reward: -3661.02). Current reward after update: -1301.03, Optimal reward -1228.96
Iteration 182 took 2.13 seconds (mean sampled reward: -2972.43). Current reward after update: -1406.20, Optimal reward -1228.96
Iteration 183 took 2.07 seconds (mean sampled reward: -3187.69). Current reward after update: -2833.53, Optimal reward -1228.96
Iteration 184 took 2.04 seconds (mean sampled reward: -3637.20). Current reward after update: -1257.68, Optimal reward -1228.96
Iteration 185 took 2.06 seconds (mean sampled reward: -3173.05). Current reward after update: -1888.73, Optimal reward -1228.96
Iteration 186 took 2.15 seconds (mean sampled reward: -2789.36). Current reward after update: -1684.63, Optimal reward -1228.96
Iteration 187 took 2.02 seconds (mean sampled reward: -3959.96). Current reward after update: -1411.55, Optimal reward -1228.96
Iteration 188 took 2.00 seconds (mean sampled reward: -4535.27). Current reward after update: -1324.01, Optimal reward -1228.96
Iteration 189 took 2.06 seconds (mean sampled reward: -3573.40). Current reward after update: -2642.23, Optimal reward -1228.96
Iteration 190 took 2.05 seconds (mean sampled reward: -3936.92). Current reward after update: -2509.30, Optimal reward -1228.96
Iteration 191 took 2.15 seconds (mean sampled reward: -3061.04). Current reward after update: -1220.11, Optimal reward -1220.11
Iteration 192 took 2.01 seconds (mean sampled reward: -3966.03). Current reward after update: -1549.60, Optimal reward -1220.11
Iteration 193 took 2.08 seconds (mean sampled reward: -4330.98). Current reward after update: -1226.02, Optimal reward -1220.11
Iteration 194 took 2.07 seconds (mean sampled reward: -3560.27). Current reward after update: -2131.65, Optimal reward -1220.11
Iteration 195 took 2.11 seconds (mean sampled reward: -3052.56). Current reward after update: -1279.25, Optimal reward -1220.11
Iteration 196 took 2.18 seconds (mean sampled reward: -2969.87). Current reward after update: -1293.73, Optimal reward -1220.11
Iteration 197 took 2.05 seconds (mean sampled reward: -3852.56). Current reward after update: -1957.81, Optimal reward -1220.11
Iteration 198 took 2.00 seconds (mean sampled reward: -3778.22). Current reward after update: -1296.11, Optimal reward -1220.11
Iteration 199 took 2.12 seconds (mean sampled reward: -3307.78). Current reward after update: -1294.22, Optimal reward -1220.11
Iteration 200 took 2.14 seconds (mean sampled reward: -2831.84). Current reward after update: -1316.31, Optimal reward -1220.11
Iteration 1 took 2.18 seconds (mean sampled reward: -7536.81). Current reward after update: -7284.99, Optimal reward -7284.99
Iteration 2 took 2.06 seconds (mean sampled reward: -7452.14). Current reward after update: -6900.84, Optimal reward -6900.84
Iteration 3 took 1.98 seconds (mean sampled reward: -7412.33). Current reward after update: -6753.43, Optimal reward -6753.43
Iteration 4 took 2.15 seconds (mean sampled reward: -7348.18). Current reward after update: -6519.18, Optimal reward -6519.18
Iteration 5 took 2.17 seconds (mean sampled reward: -7094.68). Current reward after update: -5581.48, Optimal reward -5581.48
Iteration 6 took 2.14 seconds (mean sampled reward: -6666.68). Current reward after update: -4588.42, Optimal reward -4588.42
Iteration 7 took 2.19 seconds (mean sampled reward: -5810.26). Current reward after update: -4169.69, Optimal reward -4169.69
Iteration 8 took 2.12 seconds (mean sampled reward: -5344.56). Current reward after update: -3917.51, Optimal reward -3917.51
Iteration 9 took 2.29 seconds (mean sampled reward: -5119.44). Current reward after update: -3828.40, Optimal reward -3828.40
Iteration 10 took 2.20 seconds (mean sampled reward: -5764.60). Current reward after update: -4198.49, Optimal reward -3828.40
Iteration 11 took 2.40 seconds (mean sampled reward: -6001.73). Current reward after update: -4190.64, Optimal reward -3828.40
Iteration 12 took 2.21 seconds (mean sampled reward: -5840.94). Current reward after update: -4049.41, Optimal reward -3828.40
Iteration 13 took 2.21 seconds (mean sampled reward: -5331.81). Current reward after update: -4050.87, Optimal reward -3828.40
Iteration 14 took 2.25 seconds (mean sampled reward: -6043.95). Current reward after update: -3989.34, Optimal reward -3828.40
Iteration 15 took 2.24 seconds (mean sampled reward: -5851.16). Current reward after update: -4016.57, Optimal reward -3828.40
Iteration 16 took 2.29 seconds (mean sampled reward: -5607.83). Current reward after update: -3942.03, Optimal reward -3828.40
Iteration 17 took 2.28 seconds (mean sampled reward: -6238.70). Current reward after update: -3573.28, Optimal reward -3573.28
Iteration 18 took 2.30 seconds (mean sampled reward: -6039.00). Current reward after update: -3443.83, Optimal reward -3443.83
Iteration 19 took 2.40 seconds (mean sampled reward: -5930.01). Current reward after update: -3399.29, Optimal reward -3399.29
Iteration 20 took 2.18 seconds (mean sampled reward: -6384.41). Current reward after update: -3412.58, Optimal reward -3399.29
Iteration 21 took 2.31 seconds (mean sampled reward: -6302.38). Current reward after update: -3256.84, Optimal reward -3256.84
Iteration 22 took 2.19 seconds (mean sampled reward: -5651.06). Current reward after update: -3356.67, Optimal reward -3256.84
Iteration 23 took 2.15 seconds (mean sampled reward: -5708.89). Current reward after update: -2683.14, Optimal reward -2683.14
Iteration 24 took 2.24 seconds (mean sampled reward: -5821.38). Current reward after update: -2662.40, Optimal reward -2662.40
Iteration 25 took 2.14 seconds (mean sampled reward: -5518.63). Current reward after update: -2653.92, Optimal reward -2653.92
Iteration 26 took 2.17 seconds (mean sampled reward: -4602.88). Current reward after update: -2840.06, Optimal reward -2653.92
Iteration 27 took 2.07 seconds (mean sampled reward: -5150.61). Current reward after update: -2545.99, Optimal reward -2545.99
Iteration 28 took 2.15 seconds (mean sampled reward: -4775.11). Current reward after update: -2321.10, Optimal reward -2321.10
Iteration 29 took 2.11 seconds (mean sampled reward: -4609.60). Current reward after update: -2363.77, Optimal reward -2321.10
Iteration 30 took 2.22 seconds (mean sampled reward: -4301.38). Current reward after update: -2418.81, Optimal reward -2321.10
Iteration 31 took 2.36 seconds (mean sampled reward: -3417.19). Current reward after update: -2217.72, Optimal reward -2217.72
Iteration 32 took 2.31 seconds (mean sampled reward: -3333.80). Current reward after update: -2090.83, Optimal reward -2090.83
Iteration 33 took 2.09 seconds (mean sampled reward: -4249.56). Current reward after update: -2121.07, Optimal reward -2090.83
Iteration 34 took 2.22 seconds (mean sampled reward: -4004.92). Current reward after update: -2071.02, Optimal reward -2071.02
Iteration 35 took 2.19 seconds (mean sampled reward: -3229.59). Current reward after update: -2170.17, Optimal reward -2071.02
Iteration 36 took 2.24 seconds (mean sampled reward: -2961.50). Current reward after update: -2262.28, Optimal reward -2071.02
Iteration 37 took 2.12 seconds (mean sampled reward: -4043.97). Current reward after update: -2469.41, Optimal reward -2071.02
Iteration 38 took 2.20 seconds (mean sampled reward: -4137.06). Current reward after update: -2285.72, Optimal reward -2071.02
Iteration 39 took 2.05 seconds (mean sampled reward: -4654.74). Current reward after update: -2020.36, Optimal reward -2020.36
Iteration 40 took 2.11 seconds (mean sampled reward: -4205.43). Current reward after update: -1989.83, Optimal reward -1989.83
Iteration 41 took 2.00 seconds (mean sampled reward: -6100.29). Current reward after update: -2045.40, Optimal reward -1989.83
Iteration 42 took 2.15 seconds (mean sampled reward: -3516.24). Current reward after update: -1907.80, Optimal reward -1907.80
Iteration 43 took 2.11 seconds (mean sampled reward: -4383.67). Current reward after update: -2023.76, Optimal reward -1907.80
Iteration 44 took 2.08 seconds (mean sampled reward: -4443.84). Current reward after update: -1971.90, Optimal reward -1907.80
Iteration 45 took 2.05 seconds (mean sampled reward: -4611.28). Current reward after update: -2005.96, Optimal reward -1907.80
Iteration 46 took 2.11 seconds (mean sampled reward: -4498.82). Current reward after update: -1930.31, Optimal reward -1907.80
Iteration 47 took 2.19 seconds (mean sampled reward: -3563.93). Current reward after update: -1937.25, Optimal reward -1907.80
Iteration 48 took 2.22 seconds (mean sampled reward: -3067.99). Current reward after update: -1953.17, Optimal reward -1907.80
Iteration 49 took 2.05 seconds (mean sampled reward: -5022.58). Current reward after update: -1964.33, Optimal reward -1907.80
Iteration 50 took 2.04 seconds (mean sampled reward: -4862.24). Current reward after update: -1991.15, Optimal reward -1907.80
Iteration 51 took 2.26 seconds (mean sampled reward: -3350.00). Current reward after update: -2147.61, Optimal reward -1907.80
Iteration 52 took 2.32 seconds (mean sampled reward: -3313.78). Current reward after update: -1929.12, Optimal reward -1907.80
Iteration 53 took 2.29 seconds (mean sampled reward: -3907.34). Current reward after update: -2067.98, Optimal reward -1907.80
Iteration 54 took 2.12 seconds (mean sampled reward: -3783.85). Current reward after update: -2021.50, Optimal reward -1907.80
Iteration 55 took 2.30 seconds (mean sampled reward: -4395.35). Current reward after update: -1950.26, Optimal reward -1907.80
Iteration 56 took 2.28 seconds (mean sampled reward: -3531.46). Current reward after update: -2649.98, Optimal reward -1907.80
Iteration 57 took 2.42 seconds (mean sampled reward: -2834.82). Current reward after update: -1933.84, Optimal reward -1907.80
Iteration 58 took 2.20 seconds (mean sampled reward: -3017.18). Current reward after update: -1945.62, Optimal reward -1907.80
Iteration 59 took 2.31 seconds (mean sampled reward: -3099.36). Current reward after update: -1860.21, Optimal reward -1860.21
Iteration 60 took 2.25 seconds (mean sampled reward: -3114.43). Current reward after update: -2159.14, Optimal reward -1860.21
Iteration 61 took 2.23 seconds (mean sampled reward: -3124.90). Current reward after update: -2037.54, Optimal reward -1860.21
Iteration 62 took 2.10 seconds (mean sampled reward: -3377.93). Current reward after update: -2056.80, Optimal reward -1860.21
Iteration 63 took 2.19 seconds (mean sampled reward: -3198.49). Current reward after update: -1931.54, Optimal reward -1860.21
Iteration 64 took 2.12 seconds (mean sampled reward: -3016.88). Current reward after update: -1878.88, Optimal reward -1860.21
Iteration 65 took 2.08 seconds (mean sampled reward: -3813.87). Current reward after update: -1936.59, Optimal reward -1860.21
Iteration 66 took 2.13 seconds (mean sampled reward: -3368.70). Current reward after update: -1872.54, Optimal reward -1860.21
Iteration 67 took 2.04 seconds (mean sampled reward: -3401.50). Current reward after update: -1841.82, Optimal reward -1841.82
Iteration 68 took 2.20 seconds (mean sampled reward: -3015.12). Current reward after update: -2751.40, Optimal reward -1841.82
Iteration 69 took 2.31 seconds (mean sampled reward: -2833.41). Current reward after update: -1878.51, Optimal reward -1841.82
Iteration 70 took 2.20 seconds (mean sampled reward: -2972.67). Current reward after update: -1844.54, Optimal reward -1841.82
Iteration 71 took 2.12 seconds (mean sampled reward: -2966.77). Current reward after update: -1898.55, Optimal reward -1841.82
Iteration 72 took 2.13 seconds (mean sampled reward: -2896.17). Current reward after update: -2333.12, Optimal reward -1841.82
Iteration 73 took 2.20 seconds (mean sampled reward: -2927.62). Current reward after update: -2081.92, Optimal reward -1841.82
Iteration 74 took 2.18 seconds (mean sampled reward: -2598.84). Current reward after update: -2029.90, Optimal reward -1841.82
Iteration 75 took 2.40 seconds (mean sampled reward: -2898.54). Current reward after update: -1841.40, Optimal reward -1841.40
Iteration 76 took 2.18 seconds (mean sampled reward: -2825.23). Current reward after update: -2810.72, Optimal reward -1841.40
Iteration 77 took 2.05 seconds (mean sampled reward: -4804.74). Current reward after update: -1727.28, Optimal reward -1727.28
Iteration 78 took 2.08 seconds (mean sampled reward: -4646.72). Current reward after update: -1761.83, Optimal reward -1727.28
Iteration 79 took 2.08 seconds (mean sampled reward: -4979.33). Current reward after update: -1720.37, Optimal reward -1720.37
Iteration 80 took 2.02 seconds (mean sampled reward: -5787.81). Current reward after update: -1751.92, Optimal reward -1720.37
Iteration 81 took 2.14 seconds (mean sampled reward: -4118.50). Current reward after update: -1721.45, Optimal reward -1720.37
Iteration 82 took 2.12 seconds (mean sampled reward: -5363.45). Current reward after update: -1824.46, Optimal reward -1720.37
Iteration 83 took 2.08 seconds (mean sampled reward: -3590.98). Current reward after update: -1866.26, Optimal reward -1720.37
Iteration 84 took 2.09 seconds (mean sampled reward: -4419.92). Current reward after update: -1843.48, Optimal reward -1720.37
Iteration 85 took 2.16 seconds (mean sampled reward: -3257.30). Current reward after update: -1735.74, Optimal reward -1720.37
Iteration 86 took 2.02 seconds (mean sampled reward: -4700.42). Current reward after update: -1733.89, Optimal reward -1720.37
Iteration 87 took 2.09 seconds (mean sampled reward: -4096.49). Current reward after update: -1758.87, Optimal reward -1720.37
Iteration 88 took 2.11 seconds (mean sampled reward: -4630.78). Current reward after update: -1779.11, Optimal reward -1720.37
Iteration 89 took 2.14 seconds (mean sampled reward: -2337.01). Current reward after update: -1653.07, Optimal reward -1653.07
Iteration 90 took 2.32 seconds (mean sampled reward: -2670.14). Current reward after update: -1571.40, Optimal reward -1571.40
Iteration 91 took 2.08 seconds (mean sampled reward: -3893.18). Current reward after update: -1640.94, Optimal reward -1571.40
Iteration 92 took 2.20 seconds (mean sampled reward: -3797.33). Current reward after update: -2171.60, Optimal reward -1571.40
Iteration 93 took 2.11 seconds (mean sampled reward: -4168.79). Current reward after update: -1928.21, Optimal reward -1571.40
Iteration 94 took 2.01 seconds (mean sampled reward: -4914.36). Current reward after update: -2061.01, Optimal reward -1571.40
Iteration 95 took 2.03 seconds (mean sampled reward: -4203.62). Current reward after update: -1600.87, Optimal reward -1571.40
Iteration 96 took 2.10 seconds (mean sampled reward: -3226.94). Current reward after update: -1565.16, Optimal reward -1565.16
Iteration 97 took 2.09 seconds (mean sampled reward: -3460.97). Current reward after update: -1923.49, Optimal reward -1565.16
Iteration 98 took 2.17 seconds (mean sampled reward: -3257.65). Current reward after update: -1866.76, Optimal reward -1565.16
Iteration 99 took 2.11 seconds (mean sampled reward: -2843.14). Current reward after update: -1588.01, Optimal reward -1565.16
Iteration 100 took 2.33 seconds (mean sampled reward: -2413.57). Current reward after update: -1664.61, Optimal reward -1565.16
Iteration 101 took 2.25 seconds (mean sampled reward: -2519.46). Current reward after update: -1559.29, Optimal reward -1559.29
Iteration 102 took 2.18 seconds (mean sampled reward: -3844.83). Current reward after update: -1591.64, Optimal reward -1559.29
Iteration 103 took 2.18 seconds (mean sampled reward: -3241.35). Current reward after update: -1574.76, Optimal reward -1559.29
Iteration 104 took 2.19 seconds (mean sampled reward: -3637.42). Current reward after update: -1561.47, Optimal reward -1559.29
Iteration 105 took 2.19 seconds (mean sampled reward: -3331.51). Current reward after update: -1576.81, Optimal reward -1559.29
Iteration 106 took 2.25 seconds (mean sampled reward: -3850.27). Current reward after update: -1883.41, Optimal reward -1559.29
Iteration 107 took 2.32 seconds (mean sampled reward: -3249.38). Current reward after update: -2144.50, Optimal reward -1559.29
Iteration 108 took 2.20 seconds (mean sampled reward: -2183.25). Current reward after update: -1749.27, Optimal reward -1559.29
Iteration 109 took 2.17 seconds (mean sampled reward: -2348.38). Current reward after update: -1527.54, Optimal reward -1527.54
Iteration 110 took 2.32 seconds (mean sampled reward: -2066.34). Current reward after update: -1868.75, Optimal reward -1527.54
Iteration 111 took 2.29 seconds (mean sampled reward: -2242.78). Current reward after update: -1603.23, Optimal reward -1527.54
Iteration 112 took 2.10 seconds (mean sampled reward: -3091.20). Current reward after update: -1626.72, Optimal reward -1527.54
Iteration 113 took 2.24 seconds (mean sampled reward: -2567.21). Current reward after update: -2204.03, Optimal reward -1527.54
Iteration 114 took 2.10 seconds (mean sampled reward: -4174.29). Current reward after update: -1791.05, Optimal reward -1527.54
Iteration 115 took 2.24 seconds (mean sampled reward: -2209.93). Current reward after update: -1833.01, Optimal reward -1527.54
Iteration 116 took 2.22 seconds (mean sampled reward: -2018.45). Current reward after update: -1567.83, Optimal reward -1527.54
Iteration 117 took 2.24 seconds (mean sampled reward: -2381.04). Current reward after update: -1590.93, Optimal reward -1527.54
Iteration 118 took 2.16 seconds (mean sampled reward: -3189.83). Current reward after update: -1588.85, Optimal reward -1527.54
Iteration 119 took 2.20 seconds (mean sampled reward: -2553.12). Current reward after update: -2041.08, Optimal reward -1527.54
Iteration 120 took 2.30 seconds (mean sampled reward: -3072.82). Current reward after update: -1653.09, Optimal reward -1527.54
Iteration 121 took 2.14 seconds (mean sampled reward: -2946.86). Current reward after update: -2088.89, Optimal reward -1527.54
Iteration 122 took 2.31 seconds (mean sampled reward: -2535.03). Current reward after update: -1568.48, Optimal reward -1527.54
Iteration 123 took 2.19 seconds (mean sampled reward: -2158.26). Current reward after update: -1900.34, Optimal reward -1527.54
Iteration 124 took 2.30 seconds (mean sampled reward: -2142.44). Current reward after update: -1543.94, Optimal reward -1527.54
Iteration 125 took 2.23 seconds (mean sampled reward: -2168.01). Current reward after update: -1581.04, Optimal reward -1527.54
Iteration 126 took 2.20 seconds (mean sampled reward: -2071.81). Current reward after update: -1899.39, Optimal reward -1527.54
Iteration 127 took 2.23 seconds (mean sampled reward: -2159.71). Current reward after update: -1883.46, Optimal reward -1527.54
Iteration 128 took 2.26 seconds (mean sampled reward: -2910.73). Current reward after update: -1668.89, Optimal reward -1527.54
Iteration 129 took 2.23 seconds (mean sampled reward: -2302.53). Current reward after update: -1518.03, Optimal reward -1518.03
Iteration 130 took 2.22 seconds (mean sampled reward: -2030.18). Current reward after update: -1560.93, Optimal reward -1518.03
Iteration 131 took 2.21 seconds (mean sampled reward: -2054.65). Current reward after update: -2218.71, Optimal reward -1518.03
Iteration 132 took 2.16 seconds (mean sampled reward: -2179.60). Current reward after update: -1852.08, Optimal reward -1518.03
Iteration 133 took 2.15 seconds (mean sampled reward: -2159.57). Current reward after update: -1898.13, Optimal reward -1518.03
Iteration 134 took 2.15 seconds (mean sampled reward: -2257.23). Current reward after update: -1586.91, Optimal reward -1518.03
Iteration 135 took 2.08 seconds (mean sampled reward: -2417.83). Current reward after update: -2110.80, Optimal reward -1518.03
Iteration 136 took 2.18 seconds (mean sampled reward: -2276.38). Current reward after update: -2441.16, Optimal reward -1518.03
Iteration 137 took 2.13 seconds (mean sampled reward: -2389.72). Current reward after update: -1881.79, Optimal reward -1518.03
Iteration 138 took 2.21 seconds (mean sampled reward: -2024.32). Current reward after update: -2072.85, Optimal reward -1518.03
Iteration 139 took 2.28 seconds (mean sampled reward: -2000.20). Current reward after update: -1806.19, Optimal reward -1518.03
Iteration 140 took 2.20 seconds (mean sampled reward: -2172.02). Current reward after update: -1785.53, Optimal reward -1518.03
Iteration 141 took 2.25 seconds (mean sampled reward: -2318.47). Current reward after update: -1919.86, Optimal reward -1518.03
Iteration 142 took 2.22 seconds (mean sampled reward: -2006.62). Current reward after update: -1637.14, Optimal reward -1518.03
Iteration 143 took 2.20 seconds (mean sampled reward: -1957.53). Current reward after update: -1605.44, Optimal reward -1518.03
Iteration 144 took 2.20 seconds (mean sampled reward: -1968.96). Current reward after update: -1513.83, Optimal reward -1513.83
Iteration 145 took 2.26 seconds (mean sampled reward: -2016.26). Current reward after update: -1926.26, Optimal reward -1513.83
Iteration 146 took 2.19 seconds (mean sampled reward: -2002.79). Current reward after update: -1576.00, Optimal reward -1513.83
Iteration 147 took 2.20 seconds (mean sampled reward: -1988.47). Current reward after update: -1536.87, Optimal reward -1513.83
Iteration 148 took 2.18 seconds (mean sampled reward: -2026.17). Current reward after update: -1990.28, Optimal reward -1513.83
Iteration 149 took 2.26 seconds (mean sampled reward: -1988.94). Current reward after update: -1580.98, Optimal reward -1513.83
Iteration 150 took 2.21 seconds (mean sampled reward: -1990.66). Current reward after update: -1940.03, Optimal reward -1513.83
Iteration 151 took 2.22 seconds (mean sampled reward: -2098.19). Current reward after update: -1907.44, Optimal reward -1513.83
Iteration 152 took 2.18 seconds (mean sampled reward: -2254.48). Current reward after update: -1497.28, Optimal reward -1497.28
Iteration 153 took 2.21 seconds (mean sampled reward: -2418.88). Current reward after update: -1505.32, Optimal reward -1497.28
Iteration 154 took 2.24 seconds (mean sampled reward: -2061.72). Current reward after update: -1505.55, Optimal reward -1497.28
Iteration 155 took 2.25 seconds (mean sampled reward: -2076.87). Current reward after update: -2061.49, Optimal reward -1497.28
Iteration 156 took 2.21 seconds (mean sampled reward: -2016.01). Current reward after update: -1466.31, Optimal reward -1466.31
Iteration 157 took 2.29 seconds (mean sampled reward: -2091.06). Current reward after update: -1740.83, Optimal reward -1466.31
Iteration 158 took 2.26 seconds (mean sampled reward: -2064.41). Current reward after update: -1855.95, Optimal reward -1466.31
Iteration 159 took 2.27 seconds (mean sampled reward: -2076.88). Current reward after update: -1513.58, Optimal reward -1466.31
Iteration 160 took 2.23 seconds (mean sampled reward: -2075.57). Current reward after update: -1525.44, Optimal reward -1466.31
Iteration 161 took 2.19 seconds (mean sampled reward: -2245.40). Current reward after update: -2042.24, Optimal reward -1466.31
Iteration 162 took 2.15 seconds (mean sampled reward: -2467.95). Current reward after update: -1759.01, Optimal reward -1466.31
Iteration 163 took 2.15 seconds (mean sampled reward: -2134.13). Current reward after update: -2102.43, Optimal reward -1466.31
Iteration 164 took 2.18 seconds (mean sampled reward: -2358.40). Current reward after update: -1660.05, Optimal reward -1466.31
Iteration 165 took 2.22 seconds (mean sampled reward: -2811.29). Current reward after update: -1812.52, Optimal reward -1466.31
Iteration 166 took 2.21 seconds (mean sampled reward: -2699.47). Current reward after update: -1634.38, Optimal reward -1466.31
Iteration 167 took 2.23 seconds (mean sampled reward: -2520.35). Current reward after update: -1533.30, Optimal reward -1466.31
Iteration 168 took 2.14 seconds (mean sampled reward: -3802.97). Current reward after update: -1998.04, Optimal reward -1466.31
Iteration 169 took 2.12 seconds (mean sampled reward: -2774.70). Current reward after update: -1618.64, Optimal reward -1466.31
Iteration 170 took 2.19 seconds (mean sampled reward: -3360.88). Current reward after update: -1666.61, Optimal reward -1466.31
Iteration 171 took 2.14 seconds (mean sampled reward: -2886.38). Current reward after update: -1635.51, Optimal reward -1466.31
Iteration 172 took 2.19 seconds (mean sampled reward: -2844.00). Current reward after update: -1553.11, Optimal reward -1466.31
Iteration 173 took 2.21 seconds (mean sampled reward: -2315.08). Current reward after update: -1741.89, Optimal reward -1466.31
Iteration 174 took 2.16 seconds (mean sampled reward: -2263.38). Current reward after update: -1747.87, Optimal reward -1466.31
Iteration 175 took 2.16 seconds (mean sampled reward: -2317.30). Current reward after update: -1506.83, Optimal reward -1466.31
Iteration 176 took 2.26 seconds (mean sampled reward: -2420.47). Current reward after update: -1497.52, Optimal reward -1466.31
Iteration 177 took 2.21 seconds (mean sampled reward: -2799.55). Current reward after update: -1511.98, Optimal reward -1466.31
Iteration 178 took 2.24 seconds (mean sampled reward: -2470.83). Current reward after update: -1807.95, Optimal reward -1466.31
Iteration 179 took 2.15 seconds (mean sampled reward: -2816.60). Current reward after update: -1504.01, Optimal reward -1466.31
Iteration 180 took 2.16 seconds (mean sampled reward: -2502.63). Current reward after update: -1542.75, Optimal reward -1466.31
Iteration 181 took 2.15 seconds (mean sampled reward: -2515.20). Current reward after update: -1529.95, Optimal reward -1466.31
Iteration 182 took 2.11 seconds (mean sampled reward: -3778.00). Current reward after update: -1531.02, Optimal reward -1466.31
Iteration 183 took 2.07 seconds (mean sampled reward: -4369.71). Current reward after update: -1542.35, Optimal reward -1466.31
Iteration 184 took 2.16 seconds (mean sampled reward: -3413.45). Current reward after update: -1630.37, Optimal reward -1466.31
Iteration 185 took 2.09 seconds (mean sampled reward: -4101.11). Current reward after update: -1534.25, Optimal reward -1466.31
Iteration 186 took 2.15 seconds (mean sampled reward: -4123.33). Current reward after update: -1954.48, Optimal reward -1466.31
Iteration 187 took 2.15 seconds (mean sampled reward: -2218.25). Current reward after update: -1619.50, Optimal reward -1466.31
Iteration 188 took 2.12 seconds (mean sampled reward: -4436.60). Current reward after update: -1975.90, Optimal reward -1466.31
Iteration 189 took 2.16 seconds (mean sampled reward: -4125.57). Current reward after update: -1511.27, Optimal reward -1466.31
Iteration 190 took 2.11 seconds (mean sampled reward: -3967.12). Current reward after update: -1542.27, Optimal reward -1466.31
Iteration 191 took 2.09 seconds (mean sampled reward: -3742.47). Current reward after update: -1735.18, Optimal reward -1466.31
Iteration 192 took 2.15 seconds (mean sampled reward: -3576.80). Current reward after update: -1596.85, Optimal reward -1466.31
Iteration 193 took 2.17 seconds (mean sampled reward: -3701.26). Current reward after update: -1782.82, Optimal reward -1466.31
Iteration 194 took 2.27 seconds (mean sampled reward: -2192.42). Current reward after update: -1967.33, Optimal reward -1466.31
Iteration 195 took 2.24 seconds (mean sampled reward: -2168.76). Current reward after update: -2258.40, Optimal reward -1466.31
Iteration 196 took 2.18 seconds (mean sampled reward: -2229.66). Current reward after update: -1470.03, Optimal reward -1466.31
Iteration 197 took 2.23 seconds (mean sampled reward: -2156.14). Current reward after update: -1788.36, Optimal reward -1466.31
Iteration 198 took 2.22 seconds (mean sampled reward: -2201.82). Current reward after update: -1443.40, Optimal reward -1443.40
Iteration 199 took 2.27 seconds (mean sampled reward: -2089.81). Current reward after update: -2881.29, Optimal reward -1443.40
Iteration 200 took 2.31 seconds (mean sampled reward: -2151.92). Current reward after update: -2028.00, Optimal reward -1443.40
Iteration 1 took 2.13 seconds (mean sampled reward: -7546.79). Current reward after update: -7260.36, Optimal reward -7260.36
Iteration 2 took 2.06 seconds (mean sampled reward: -7392.39). Current reward after update: -6432.12, Optimal reward -6432.12
Iteration 3 took 2.28 seconds (mean sampled reward: -7165.54). Current reward after update: -5972.69, Optimal reward -5972.69
Iteration 4 took 2.31 seconds (mean sampled reward: -6576.65). Current reward after update: -5391.40, Optimal reward -5391.40
Iteration 5 took 2.24 seconds (mean sampled reward: -6101.54). Current reward after update: -4928.19, Optimal reward -4928.19
Iteration 6 took 2.25 seconds (mean sampled reward: -5978.06). Current reward after update: -4602.65, Optimal reward -4602.65
Iteration 7 took 2.33 seconds (mean sampled reward: -5919.00). Current reward after update: -4054.46, Optimal reward -4054.46
Iteration 8 took 2.22 seconds (mean sampled reward: -5405.39). Current reward after update: -3839.44, Optimal reward -3839.44
Iteration 9 took 2.25 seconds (mean sampled reward: -5436.03). Current reward after update: -3813.38, Optimal reward -3813.38
Iteration 10 took 2.26 seconds (mean sampled reward: -5436.18). Current reward after update: -3854.24, Optimal reward -3813.38
Iteration 11 took 2.14 seconds (mean sampled reward: -5406.31). Current reward after update: -3402.12, Optimal reward -3402.12
Iteration 12 took 2.15 seconds (mean sampled reward: -4828.82). Current reward after update: -2866.08, Optimal reward -2866.08
Iteration 13 took 2.18 seconds (mean sampled reward: -4712.85). Current reward after update: -2647.51, Optimal reward -2647.51
Iteration 14 took 2.14 seconds (mean sampled reward: -5318.56). Current reward after update: -2491.29, Optimal reward -2491.29
Iteration 15 took 2.41 seconds (mean sampled reward: -4579.68). Current reward after update: -2427.56, Optimal reward -2427.56
Iteration 16 took 2.29 seconds (mean sampled reward: -4348.07). Current reward after update: -3540.36, Optimal reward -2427.56
Iteration 17 took 1.96 seconds (mean sampled reward: -6267.72). Current reward after update: -2386.88, Optimal reward -2386.88
Iteration 18 took 2.14 seconds (mean sampled reward: -5466.95). Current reward after update: -2575.21, Optimal reward -2386.88
Iteration 19 took 2.03 seconds (mean sampled reward: -5468.77). Current reward after update: -2346.67, Optimal reward -2346.67
Iteration 20 took 2.18 seconds (mean sampled reward: -4817.10). Current reward after update: -2392.80, Optimal reward -2346.67
Iteration 21 took 2.06 seconds (mean sampled reward: -4914.24). Current reward after update: -2189.26, Optimal reward -2189.26
Iteration 22 took 2.06 seconds (mean sampled reward: -4112.68). Current reward after update: -1994.19, Optimal reward -1994.19
Iteration 23 took 2.13 seconds (mean sampled reward: -3225.68). Current reward after update: -1875.89, Optimal reward -1875.89
Iteration 24 took 2.24 seconds (mean sampled reward: -3563.00). Current reward after update: -1784.49, Optimal reward -1784.49
Iteration 25 took 2.29 seconds (mean sampled reward: -2694.80). Current reward after update: -1750.83, Optimal reward -1750.83
Iteration 26 took 2.26 seconds (mean sampled reward: -2934.28). Current reward after update: -1891.48, Optimal reward -1750.83
Iteration 27 took 2.29 seconds (mean sampled reward: -2983.52). Current reward after update: -2026.14, Optimal reward -1750.83
Iteration 28 took 2.27 seconds (mean sampled reward: -2527.63). Current reward after update: -1677.20, Optimal reward -1677.20
Iteration 29 took 2.37 seconds (mean sampled reward: -3125.60). Current reward after update: -1687.42, Optimal reward -1677.20
Iteration 30 took 2.32 seconds (mean sampled reward: -3908.76). Current reward after update: -1670.11, Optimal reward -1670.11
Iteration 31 took 2.19 seconds (mean sampled reward: -2693.13). Current reward after update: -2656.88, Optimal reward -1670.11
Iteration 32 took 2.27 seconds (mean sampled reward: -2809.37). Current reward after update: -1864.02, Optimal reward -1670.11
Iteration 33 took 2.18 seconds (mean sampled reward: -3493.99). Current reward after update: -1804.16, Optimal reward -1670.11
Iteration 34 took 2.17 seconds (mean sampled reward: -3821.62). Current reward after update: -1699.61, Optimal reward -1670.11
Iteration 35 took 2.22 seconds (mean sampled reward: -3766.57). Current reward after update: -1676.78, Optimal reward -1670.11
Iteration 36 took 2.21 seconds (mean sampled reward: -2616.83). Current reward after update: -1561.97, Optimal reward -1561.97
Iteration 37 took 2.33 seconds (mean sampled reward: -3345.00). Current reward after update: -1630.30, Optimal reward -1561.97
Iteration 38 took 2.27 seconds (mean sampled reward: -3240.39). Current reward after update: -1475.57, Optimal reward -1475.57
Iteration 39 took 2.20 seconds (mean sampled reward: -2902.18). Current reward after update: -1481.31, Optimal reward -1475.57
Iteration 40 took 2.22 seconds (mean sampled reward: -2539.97). Current reward after update: -1435.48, Optimal reward -1435.48
Iteration 41 took 2.20 seconds (mean sampled reward: -2375.99). Current reward after update: -1402.23, Optimal reward -1402.23
Iteration 42 took 2.29 seconds (mean sampled reward: -2858.68). Current reward after update: -1451.86, Optimal reward -1402.23
Iteration 43 took 2.55 seconds (mean sampled reward: -3153.67). Current reward after update: -1387.44, Optimal reward -1387.44
Iteration 44 took 2.24 seconds (mean sampled reward: -3490.56). Current reward after update: -1461.22, Optimal reward -1387.44
Iteration 45 took 2.17 seconds (mean sampled reward: -3829.07). Current reward after update: -1455.27, Optimal reward -1387.44
Iteration 46 took 2.17 seconds (mean sampled reward: -4581.05). Current reward after update: -1703.93, Optimal reward -1387.44
Iteration 47 took 2.28 seconds (mean sampled reward: -4223.25). Current reward after update: -1363.29, Optimal reward -1363.29
Iteration 48 took 2.24 seconds (mean sampled reward: -3667.60). Current reward after update: -1490.98, Optimal reward -1363.29
Iteration 49 took 2.22 seconds (mean sampled reward: -3691.31). Current reward after update: -1447.84, Optimal reward -1363.29
Iteration 50 took 2.28 seconds (mean sampled reward: -3701.66). Current reward after update: -1300.33, Optimal reward -1300.33
Iteration 51 took 2.22 seconds (mean sampled reward: -5361.59). Current reward after update: -1901.52, Optimal reward -1300.33
Iteration 52 took 2.27 seconds (mean sampled reward: -2970.83). Current reward after update: -1323.23, Optimal reward -1300.33
Iteration 53 took 2.34 seconds (mean sampled reward: -3900.44). Current reward after update: -1357.92, Optimal reward -1300.33
Iteration 54 took 2.34 seconds (mean sampled reward: -4102.65). Current reward after update: -1488.44, Optimal reward -1300.33
Iteration 55 took 2.22 seconds (mean sampled reward: -3766.38). Current reward after update: -1507.92, Optimal reward -1300.33
Iteration 56 took 2.30 seconds (mean sampled reward: -4001.80). Current reward after update: -1270.95, Optimal reward -1270.95
Iteration 57 took 2.27 seconds (mean sampled reward: -2879.37). Current reward after update: -1283.45, Optimal reward -1270.95
Iteration 58 took 2.53 seconds (mean sampled reward: -2902.12). Current reward after update: -1276.28, Optimal reward -1270.95
Iteration 59 took 2.26 seconds (mean sampled reward: -2580.28). Current reward after update: -1205.88, Optimal reward -1205.88
Iteration 60 took 2.36 seconds (mean sampled reward: -2297.97). Current reward after update: -1301.62, Optimal reward -1205.88
Iteration 61 took 2.39 seconds (mean sampled reward: -1980.22). Current reward after update: -1250.92, Optimal reward -1205.88
Iteration 62 took 2.31 seconds (mean sampled reward: -2107.72). Current reward after update: -1255.31, Optimal reward -1205.88
Iteration 63 took 2.26 seconds (mean sampled reward: -2221.73). Current reward after update: -1336.91, Optimal reward -1205.88
Iteration 64 took 2.29 seconds (mean sampled reward: -2854.74). Current reward after update: -1168.17, Optimal reward -1168.17
Iteration 65 took 2.32 seconds (mean sampled reward: -2208.34). Current reward after update: -1787.50, Optimal reward -1168.17
Iteration 66 took 2.28 seconds (mean sampled reward: -2512.42). Current reward after update: -1640.08, Optimal reward -1168.17
Iteration 67 took 2.26 seconds (mean sampled reward: -3067.73). Current reward after update: -1880.72, Optimal reward -1168.17
Iteration 68 took 2.18 seconds (mean sampled reward: -3630.21). Current reward after update: -1409.73, Optimal reward -1168.17
Iteration 69 took 2.13 seconds (mean sampled reward: -3916.29). Current reward after update: -1496.83, Optimal reward -1168.17
Iteration 70 took 2.08 seconds (mean sampled reward: -5280.67). Current reward after update: -1216.47, Optimal reward -1168.17
Iteration 71 took 2.10 seconds (mean sampled reward: -4556.31). Current reward after update: -1176.38, Optimal reward -1168.17
Iteration 72 took 2.07 seconds (mean sampled reward: -3933.16). Current reward after update: -1251.36, Optimal reward -1168.17
Iteration 73 took 2.10 seconds (mean sampled reward: -4999.83). Current reward after update: -1159.39, Optimal reward -1159.39
Iteration 74 took 2.15 seconds (mean sampled reward: -4108.97). Current reward after update: -1177.25, Optimal reward -1159.39
Iteration 75 took 2.08 seconds (mean sampled reward: -4209.25). Current reward after update: -1145.45, Optimal reward -1145.45
Iteration 76 took 2.07 seconds (mean sampled reward: -3540.30). Current reward after update: -1259.96, Optimal reward -1145.45
Iteration 77 took 2.23 seconds (mean sampled reward: -3882.71). Current reward after update: -1231.59, Optimal reward -1145.45
Iteration 78 took 2.04 seconds (mean sampled reward: -5229.13). Current reward after update: -1196.54, Optimal reward -1145.45
Iteration 79 took 2.17 seconds (mean sampled reward: -3314.41). Current reward after update: -1168.73, Optimal reward -1145.45
Iteration 80 took 2.27 seconds (mean sampled reward: -2009.52). Current reward after update: -1280.58, Optimal reward -1145.45
Iteration 81 took 2.28 seconds (mean sampled reward: -2270.09). Current reward after update: -1152.43, Optimal reward -1145.45
Iteration 82 took 2.09 seconds (mean sampled reward: -4600.08). Current reward after update: -1421.75, Optimal reward -1145.45
Iteration 83 took 2.21 seconds (mean sampled reward: -4134.31). Current reward after update: -1205.93, Optimal reward -1145.45
Iteration 84 took 2.25 seconds (mean sampled reward: -3933.39). Current reward after update: -1098.87, Optimal reward -1098.87
Iteration 85 took 2.34 seconds (mean sampled reward: -3261.00). Current reward after update: -1192.52, Optimal reward -1098.87
Iteration 86 took 2.54 seconds (mean sampled reward: -3507.66). Current reward after update: -1167.04, Optimal reward -1098.87
Iteration 87 took 2.33 seconds (mean sampled reward: -3779.10). Current reward after update: -1118.00, Optimal reward -1098.87
Iteration 88 took 2.38 seconds (mean sampled reward: -3531.61). Current reward after update: -1181.09, Optimal reward -1098.87
Iteration 89 took 2.10 seconds (mean sampled reward: -3293.92). Current reward after update: -1122.63, Optimal reward -1098.87
Iteration 90 took 1.99 seconds (mean sampled reward: -5133.80). Current reward after update: -999.09, Optimal reward -999.09
Iteration 91 took 2.08 seconds (mean sampled reward: -4250.09). Current reward after update: -1089.15, Optimal reward -999.09
Iteration 92 took 2.15 seconds (mean sampled reward: -3710.24). Current reward after update: -1352.22, Optimal reward -999.09
Iteration 93 took 2.21 seconds (mean sampled reward: -2466.12). Current reward after update: -2083.79, Optimal reward -999.09
Iteration 94 took 2.24 seconds (mean sampled reward: -3092.32). Current reward after update: -1055.46, Optimal reward -999.09
Iteration 95 took 2.16 seconds (mean sampled reward: -3302.36). Current reward after update: -1072.69, Optimal reward -999.09
Iteration 96 took 2.13 seconds (mean sampled reward: -3352.85). Current reward after update: -1566.77, Optimal reward -999.09
Iteration 97 took 2.03 seconds (mean sampled reward: -4524.36). Current reward after update: -1653.13, Optimal reward -999.09
Iteration 98 took 2.15 seconds (mean sampled reward: -2794.53). Current reward after update: -1232.94, Optimal reward -999.09
Iteration 99 took 2.13 seconds (mean sampled reward: -3680.69). Current reward after update: -1092.84, Optimal reward -999.09
Iteration 100 took 2.14 seconds (mean sampled reward: -3924.36). Current reward after update: -1247.25, Optimal reward -999.09
Iteration 101 took 2.19 seconds (mean sampled reward: -2561.57). Current reward after update: -1088.20, Optimal reward -999.09
Iteration 102 took 2.19 seconds (mean sampled reward: -3442.96). Current reward after update: -1138.96, Optimal reward -999.09
Iteration 103 took 2.13 seconds (mean sampled reward: -4494.09). Current reward after update: -1694.62, Optimal reward -999.09
Iteration 104 took 1.94 seconds (mean sampled reward: -5186.19). Current reward after update: -1084.89, Optimal reward -999.09
Iteration 105 took 2.15 seconds (mean sampled reward: -3447.68). Current reward after update: -1063.87, Optimal reward -999.09
Iteration 106 took 2.19 seconds (mean sampled reward: -3181.29). Current reward after update: -2804.04, Optimal reward -999.09
Iteration 107 took 2.31 seconds (mean sampled reward: -3685.98). Current reward after update: -1006.61, Optimal reward -999.09
Iteration 108 took 2.22 seconds (mean sampled reward: -2403.81). Current reward after update: -1134.60, Optimal reward -999.09
Iteration 109 took 2.30 seconds (mean sampled reward: -1691.84). Current reward after update: -1039.72, Optimal reward -999.09
Iteration 110 took 2.57 seconds (mean sampled reward: -2011.95). Current reward after update: -941.32, Optimal reward -941.32
Iteration 111 took 2.33 seconds (mean sampled reward: -1890.65). Current reward after update: -1206.76, Optimal reward -941.32
Iteration 112 took 2.30 seconds (mean sampled reward: -1805.78). Current reward after update: -1015.71, Optimal reward -941.32
Iteration 113 took 2.25 seconds (mean sampled reward: -2563.04). Current reward after update: -1362.11, Optimal reward -941.32
Iteration 114 took 2.43 seconds (mean sampled reward: -2822.82). Current reward after update: -1012.98, Optimal reward -941.32
Iteration 115 took 2.17 seconds (mean sampled reward: -2273.04). Current reward after update: -917.18, Optimal reward -917.18
Iteration 116 took 2.32 seconds (mean sampled reward: -1668.83). Current reward after update: -1274.50, Optimal reward -917.18
Iteration 117 took 2.25 seconds (mean sampled reward: -1671.36). Current reward after update: -1006.04, Optimal reward -917.18
Iteration 118 took 2.19 seconds (mean sampled reward: -1881.25). Current reward after update: -908.83, Optimal reward -908.83
Iteration 119 took 2.26 seconds (mean sampled reward: -2232.91). Current reward after update: -974.42, Optimal reward -908.83
Iteration 120 took 2.19 seconds (mean sampled reward: -1995.03). Current reward after update: -1206.88, Optimal reward -908.83
Iteration 121 took 2.30 seconds (mean sampled reward: -1842.49). Current reward after update: -976.07, Optimal reward -908.83
Iteration 122 took 2.22 seconds (mean sampled reward: -1637.98). Current reward after update: -1129.65, Optimal reward -908.83
Iteration 123 took 2.21 seconds (mean sampled reward: -1562.58). Current reward after update: -1189.21, Optimal reward -908.83
Iteration 124 took 2.22 seconds (mean sampled reward: -1954.21). Current reward after update: -987.34, Optimal reward -908.83
Iteration 125 took 2.26 seconds (mean sampled reward: -1991.49). Current reward after update: -1029.31, Optimal reward -908.83
Iteration 126 took 2.30 seconds (mean sampled reward: -1794.37). Current reward after update: -1184.71, Optimal reward -908.83
Iteration 127 took 2.24 seconds (mean sampled reward: -1718.68). Current reward after update: -965.31, Optimal reward -908.83
Iteration 128 took 2.21 seconds (mean sampled reward: -1798.69). Current reward after update: -953.75, Optimal reward -908.83
Iteration 129 took 2.27 seconds (mean sampled reward: -1844.58). Current reward after update: -969.72, Optimal reward -908.83
Iteration 130 took 2.19 seconds (mean sampled reward: -2271.66). Current reward after update: -1002.22, Optimal reward -908.83
Iteration 131 took 2.16 seconds (mean sampled reward: -2258.14). Current reward after update: -924.85, Optimal reward -908.83
Iteration 132 took 2.27 seconds (mean sampled reward: -1665.59). Current reward after update: -1028.65, Optimal reward -908.83
Iteration 133 took 2.21 seconds (mean sampled reward: -1528.55). Current reward after update: -887.62, Optimal reward -887.62
Iteration 134 took 2.15 seconds (mean sampled reward: -1951.72). Current reward after update: -933.71, Optimal reward -887.62
Iteration 135 took 2.17 seconds (mean sampled reward: -1782.62). Current reward after update: -945.76, Optimal reward -887.62
Iteration 136 took 2.19 seconds (mean sampled reward: -1835.26). Current reward after update: -923.13, Optimal reward -887.62
Iteration 137 took 2.25 seconds (mean sampled reward: -2058.46). Current reward after update: -865.83, Optimal reward -865.83
Iteration 138 took 2.15 seconds (mean sampled reward: -1948.97). Current reward after update: -1187.49, Optimal reward -865.83
Iteration 139 took 2.10 seconds (mean sampled reward: -2824.83). Current reward after update: -1035.62, Optimal reward -865.83
Iteration 140 took 2.27 seconds (mean sampled reward: -1772.99). Current reward after update: -860.23, Optimal reward -860.23
Iteration 141 took 2.14 seconds (mean sampled reward: -1804.14). Current reward after update: -2130.70, Optimal reward -860.23
Iteration 142 took 2.13 seconds (mean sampled reward: -1853.81). Current reward after update: -913.69, Optimal reward -860.23
Iteration 143 took 2.11 seconds (mean sampled reward: -2386.28). Current reward after update: -906.96, Optimal reward -860.23
Iteration 144 took 2.07 seconds (mean sampled reward: -2695.07). Current reward after update: -2001.80, Optimal reward -860.23
Iteration 145 took 2.15 seconds (mean sampled reward: -2124.84). Current reward after update: -878.48, Optimal reward -860.23
Iteration 146 took 2.17 seconds (mean sampled reward: -1721.01). Current reward after update: -949.39, Optimal reward -860.23
Iteration 147 took 2.11 seconds (mean sampled reward: -1832.45). Current reward after update: -1904.48, Optimal reward -860.23
Iteration 148 took 2.14 seconds (mean sampled reward: -1768.84). Current reward after update: -1543.70, Optimal reward -860.23
Iteration 149 took 2.10 seconds (mean sampled reward: -2247.96). Current reward after update: -906.17, Optimal reward -860.23
Iteration 150 took 2.10 seconds (mean sampled reward: -2657.93). Current reward after update: -937.35, Optimal reward -860.23
Iteration 151 took 2.10 seconds (mean sampled reward: -2411.54). Current reward after update: -924.07, Optimal reward -860.23
Iteration 152 took 2.06 seconds (mean sampled reward: -2739.72). Current reward after update: -928.24, Optimal reward -860.23
Iteration 153 took 2.07 seconds (mean sampled reward: -2928.23). Current reward after update: -1040.43, Optimal reward -860.23
Iteration 154 took 2.11 seconds (mean sampled reward: -2019.07). Current reward after update: -799.02, Optimal reward -799.02
Iteration 155 took 2.08 seconds (mean sampled reward: -2336.17). Current reward after update: -1008.22, Optimal reward -799.02
Iteration 156 took 2.11 seconds (mean sampled reward: -2392.25). Current reward after update: -1358.20, Optimal reward -799.02
Iteration 157 took 2.07 seconds (mean sampled reward: -3002.20). Current reward after update: -944.78, Optimal reward -799.02
Iteration 158 took 2.05 seconds (mean sampled reward: -2722.57). Current reward after update: -1603.41, Optimal reward -799.02
Iteration 159 took 2.11 seconds (mean sampled reward: -2159.49). Current reward after update: -836.34, Optimal reward -799.02
Iteration 160 took 2.05 seconds (mean sampled reward: -3655.33). Current reward after update: -832.82, Optimal reward -799.02
Iteration 161 took 1.99 seconds (mean sampled reward: -3860.52). Current reward after update: -911.63, Optimal reward -799.02
Iteration 162 took 2.19 seconds (mean sampled reward: -1770.08). Current reward after update: -844.33, Optimal reward -799.02
Iteration 163 took 2.18 seconds (mean sampled reward: -1907.80). Current reward after update: -1038.82, Optimal reward -799.02
Iteration 164 took 2.18 seconds (mean sampled reward: -1800.45). Current reward after update: -1177.73, Optimal reward -799.02
Iteration 165 took 2.15 seconds (mean sampled reward: -2160.82). Current reward after update: -896.51, Optimal reward -799.02
Iteration 166 took 2.27 seconds (mean sampled reward: -1419.91). Current reward after update: -868.73, Optimal reward -799.02
Iteration 167 took 2.17 seconds (mean sampled reward: -1446.02). Current reward after update: -812.14, Optimal reward -799.02
Iteration 168 took 2.25 seconds (mean sampled reward: -1415.57). Current reward after update: -809.92, Optimal reward -799.02
Iteration 169 took 2.18 seconds (mean sampled reward: -1282.85). Current reward after update: -745.46, Optimal reward -745.46
Iteration 170 took 2.13 seconds (mean sampled reward: -1464.93). Current reward after update: -916.35, Optimal reward -745.46
Iteration 171 took 2.15 seconds (mean sampled reward: -1620.14). Current reward after update: -763.87, Optimal reward -745.46
Iteration 172 took 2.15 seconds (mean sampled reward: -1764.64). Current reward after update: -785.26, Optimal reward -745.46
Iteration 173 took 2.18 seconds (mean sampled reward: -1547.32). Current reward after update: -738.46, Optimal reward -738.46
Iteration 174 took 2.11 seconds (mean sampled reward: -2670.33). Current reward after update: -827.74, Optimal reward -738.46
Iteration 175 took 2.16 seconds (mean sampled reward: -2849.75). Current reward after update: -933.50, Optimal reward -738.46
Iteration 176 took 2.12 seconds (mean sampled reward: -2273.03). Current reward after update: -859.10, Optimal reward -738.46
Iteration 177 took 2.22 seconds (mean sampled reward: -1736.92). Current reward after update: -855.68, Optimal reward -738.46
Iteration 178 took 2.15 seconds (mean sampled reward: -1570.39). Current reward after update: -715.18, Optimal reward -715.18
Iteration 179 took 2.12 seconds (mean sampled reward: -2390.53). Current reward after update: -932.90, Optimal reward -715.18
Iteration 180 took 2.10 seconds (mean sampled reward: -2829.01). Current reward after update: -889.72, Optimal reward -715.18
Iteration 181 took 2.12 seconds (mean sampled reward: -2507.78). Current reward after update: -903.64, Optimal reward -715.18
Iteration 182 took 2.11 seconds (mean sampled reward: -2138.13). Current reward after update: -744.12, Optimal reward -715.18
Iteration 183 took 2.14 seconds (mean sampled reward: -2148.64). Current reward after update: -733.71, Optimal reward -715.18
Iteration 184 took 2.11 seconds (mean sampled reward: -2556.50). Current reward after update: -1747.31, Optimal reward -715.18
Iteration 185 took 2.16 seconds (mean sampled reward: -2205.52). Current reward after update: -774.24, Optimal reward -715.18
Iteration 186 took 2.20 seconds (mean sampled reward: -1805.35). Current reward after update: -1555.06, Optimal reward -715.18
Iteration 187 took 2.17 seconds (mean sampled reward: -1517.33). Current reward after update: -1090.82, Optimal reward -715.18
Iteration 188 took 2.16 seconds (mean sampled reward: -2095.76). Current reward after update: -867.48, Optimal reward -715.18
Iteration 189 took 2.18 seconds (mean sampled reward: -3084.06). Current reward after update: -895.95, Optimal reward -715.18
Iteration 190 took 2.17 seconds (mean sampled reward: -2277.85). Current reward after update: -803.09, Optimal reward -715.18
Iteration 191 took 2.24 seconds (mean sampled reward: -2017.61). Current reward after update: -795.17, Optimal reward -715.18
Iteration 192 took 2.09 seconds (mean sampled reward: -3167.51). Current reward after update: -816.25, Optimal reward -715.18
Iteration 193 took 2.17 seconds (mean sampled reward: -1814.17). Current reward after update: -1045.49, Optimal reward -715.18
Iteration 194 took 2.17 seconds (mean sampled reward: -1819.10). Current reward after update: -765.11, Optimal reward -715.18
Iteration 195 took 2.20 seconds (mean sampled reward: -1655.63). Current reward after update: -1051.04, Optimal reward -715.18
Iteration 196 took 2.20 seconds (mean sampled reward: -1682.65). Current reward after update: -792.61, Optimal reward -715.18
Iteration 197 took 2.16 seconds (mean sampled reward: -1821.02). Current reward after update: -893.03, Optimal reward -715.18
Iteration 198 took 2.19 seconds (mean sampled reward: -1620.69). Current reward after update: -795.31, Optimal reward -715.18
Iteration 199 took 2.17 seconds (mean sampled reward: -2131.73). Current reward after update: -1412.75, Optimal reward -715.18
Iteration 200 took 2.23 seconds (mean sampled reward: -2230.82). Current reward after update: -1711.94, Optimal reward -715.18
Max force: 10 Sigma: 0.2 mean rewards: -1126.2290822771542, best rewards:-715.1784482405712

Iteration 1 took 2.19 seconds (mean sampled reward: -7562.20). Current reward after update: -6439.56, Optimal reward -6439.56
Iteration 2 took 2.15 seconds (mean sampled reward: -7300.30). Current reward after update: -5836.09, Optimal reward -5836.09
Iteration 3 took 2.10 seconds (mean sampled reward: -6728.33). Current reward after update: -4782.79, Optimal reward -4782.79
Iteration 4 took 2.33 seconds (mean sampled reward: -6778.25). Current reward after update: -4509.10, Optimal reward -4509.10
Iteration 5 took 2.24 seconds (mean sampled reward: -6139.45). Current reward after update: -3940.48, Optimal reward -3940.48
Iteration 6 took 2.12 seconds (mean sampled reward: -6103.20). Current reward after update: -3566.48, Optimal reward -3566.48
Iteration 7 took 2.15 seconds (mean sampled reward: -5436.26). Current reward after update: -3234.47, Optimal reward -3234.47
Iteration 8 took 2.09 seconds (mean sampled reward: -5313.22). Current reward after update: -2615.66, Optimal reward -2615.66
Iteration 9 took 2.22 seconds (mean sampled reward: -5866.94). Current reward after update: -2511.52, Optimal reward -2511.52
Iteration 10 took 2.30 seconds (mean sampled reward: -5374.44). Current reward after update: -2438.25, Optimal reward -2438.25
Iteration 11 took 1.97 seconds (mean sampled reward: -6220.87). Current reward after update: -2623.73, Optimal reward -2438.25
Iteration 12 took 2.09 seconds (mean sampled reward: -5664.89). Current reward after update: -2473.89, Optimal reward -2438.25
Iteration 13 took 2.03 seconds (mean sampled reward: -5753.76). Current reward after update: -4383.39, Optimal reward -2438.25
Iteration 14 took 2.02 seconds (mean sampled reward: -6547.71). Current reward after update: -2693.09, Optimal reward -2438.25
Iteration 15 took 2.06 seconds (mean sampled reward: -6855.47). Current reward after update: -2597.21, Optimal reward -2438.25
Iteration 16 took 1.98 seconds (mean sampled reward: -5856.79). Current reward after update: -2465.10, Optimal reward -2438.25
Iteration 17 took 2.07 seconds (mean sampled reward: -6196.91). Current reward after update: -2484.98, Optimal reward -2438.25
Iteration 18 took 2.06 seconds (mean sampled reward: -5151.21). Current reward after update: -2346.33, Optimal reward -2346.33
Iteration 19 took 2.00 seconds (mean sampled reward: -5235.51). Current reward after update: -2312.32, Optimal reward -2312.32
Iteration 20 took 2.07 seconds (mean sampled reward: -5868.44). Current reward after update: -2171.02, Optimal reward -2171.02
Iteration 21 took 2.12 seconds (mean sampled reward: -5637.69). Current reward after update: -2052.38, Optimal reward -2052.38
Iteration 22 took 2.12 seconds (mean sampled reward: -6223.65). Current reward after update: -2209.05, Optimal reward -2052.38
Iteration 23 took 2.09 seconds (mean sampled reward: -5790.48). Current reward after update: -2116.11, Optimal reward -2052.38
Iteration 24 took 2.09 seconds (mean sampled reward: -5978.89). Current reward after update: -1979.94, Optimal reward -1979.94
Iteration 25 took 2.09 seconds (mean sampled reward: -5501.27). Current reward after update: -2019.02, Optimal reward -1979.94
Iteration 26 took 2.15 seconds (mean sampled reward: -6196.84). Current reward after update: -2112.78, Optimal reward -1979.94
Iteration 27 took 2.10 seconds (mean sampled reward: -5416.95). Current reward after update: -2005.32, Optimal reward -1979.94
Iteration 28 took 2.24 seconds (mean sampled reward: -4900.33). Current reward after update: -1931.89, Optimal reward -1931.89
Iteration 29 took 2.36 seconds (mean sampled reward: -4116.85). Current reward after update: -1689.18, Optimal reward -1689.18
Iteration 30 took 2.17 seconds (mean sampled reward: -4356.95). Current reward after update: -1622.13, Optimal reward -1622.13
Iteration 31 took 2.24 seconds (mean sampled reward: -3175.13). Current reward after update: -1577.63, Optimal reward -1577.63
Iteration 32 took 2.35 seconds (mean sampled reward: -3557.29). Current reward after update: -1723.35, Optimal reward -1577.63
Iteration 33 took 2.22 seconds (mean sampled reward: -2493.62). Current reward after update: -1586.04, Optimal reward -1577.63
Iteration 34 took 2.21 seconds (mean sampled reward: -2778.40). Current reward after update: -1577.75, Optimal reward -1577.63
Iteration 35 took 2.03 seconds (mean sampled reward: -2661.89). Current reward after update: -1535.53, Optimal reward -1535.53
Iteration 36 took 2.16 seconds (mean sampled reward: -2621.38). Current reward after update: -1611.11, Optimal reward -1535.53
Iteration 37 took 2.04 seconds (mean sampled reward: -2349.89). Current reward after update: -1536.50, Optimal reward -1535.53
Iteration 38 took 2.21 seconds (mean sampled reward: -2765.12). Current reward after update: -1537.91, Optimal reward -1535.53
Iteration 39 took 2.12 seconds (mean sampled reward: -3606.23). Current reward after update: -1468.10, Optimal reward -1468.10
Iteration 40 took 2.25 seconds (mean sampled reward: -3753.22). Current reward after update: -1476.93, Optimal reward -1468.10
Iteration 41 took 2.30 seconds (mean sampled reward: -3294.17). Current reward after update: -1546.92, Optimal reward -1468.10
Iteration 42 took 2.18 seconds (mean sampled reward: -2933.03). Current reward after update: -1601.15, Optimal reward -1468.10
Iteration 43 took 2.20 seconds (mean sampled reward: -3796.52). Current reward after update: -1530.94, Optimal reward -1468.10
Iteration 44 took 2.07 seconds (mean sampled reward: -3827.48). Current reward after update: -1421.85, Optimal reward -1421.85
Iteration 45 took 2.13 seconds (mean sampled reward: -4081.70). Current reward after update: -1473.07, Optimal reward -1421.85
Iteration 46 took 2.09 seconds (mean sampled reward: -4759.09). Current reward after update: -2689.42, Optimal reward -1421.85
Iteration 47 took 2.15 seconds (mean sampled reward: -4917.27). Current reward after update: -1378.58, Optimal reward -1378.58
Iteration 48 took 2.16 seconds (mean sampled reward: -2966.35). Current reward after update: -1509.86, Optimal reward -1378.58
Iteration 49 took 2.26 seconds (mean sampled reward: -4974.25). Current reward after update: -1508.18, Optimal reward -1378.58
Iteration 50 took 2.10 seconds (mean sampled reward: -5194.29). Current reward after update: -1583.69, Optimal reward -1378.58
Iteration 51 took 2.10 seconds (mean sampled reward: -5372.46). Current reward after update: -1586.75, Optimal reward -1378.58
Iteration 52 took 2.09 seconds (mean sampled reward: -5954.72). Current reward after update: -1509.72, Optimal reward -1378.58
Iteration 53 took 2.15 seconds (mean sampled reward: -5038.99). Current reward after update: -1617.91, Optimal reward -1378.58
Iteration 54 took 2.20 seconds (mean sampled reward: -6143.60). Current reward after update: -1521.35, Optimal reward -1378.58
Iteration 55 took 2.23 seconds (mean sampled reward: -5409.98). Current reward after update: -1596.98, Optimal reward -1378.58
Iteration 56 took 2.31 seconds (mean sampled reward: -5342.05). Current reward after update: -1557.63, Optimal reward -1378.58
Iteration 57 took 2.13 seconds (mean sampled reward: -4917.11). Current reward after update: -2390.34, Optimal reward -1378.58
Iteration 58 took 2.28 seconds (mean sampled reward: -5853.58). Current reward after update: -1521.77, Optimal reward -1378.58
Iteration 59 took 2.14 seconds (mean sampled reward: -5776.01). Current reward after update: -1392.11, Optimal reward -1378.58
Iteration 60 took 2.23 seconds (mean sampled reward: -4996.08). Current reward after update: -1653.97, Optimal reward -1378.58
Iteration 61 took 2.20 seconds (mean sampled reward: -4965.67). Current reward after update: -1600.21, Optimal reward -1378.58
Iteration 62 took 2.29 seconds (mean sampled reward: -4836.43). Current reward after update: -1574.20, Optimal reward -1378.58
Iteration 63 took 2.20 seconds (mean sampled reward: -4617.63). Current reward after update: -1610.15, Optimal reward -1378.58
Iteration 64 took 2.17 seconds (mean sampled reward: -5510.14). Current reward after update: -1719.84, Optimal reward -1378.58
Iteration 65 took 2.12 seconds (mean sampled reward: -4757.60). Current reward after update: -2298.63, Optimal reward -1378.58
Iteration 66 took 2.04 seconds (mean sampled reward: -4900.48). Current reward after update: -1836.78, Optimal reward -1378.58
Iteration 67 took 2.11 seconds (mean sampled reward: -5140.23). Current reward after update: -1746.34, Optimal reward -1378.58
Iteration 68 took 2.16 seconds (mean sampled reward: -4698.18). Current reward after update: -1724.62, Optimal reward -1378.58
Iteration 69 took 2.17 seconds (mean sampled reward: -4025.26). Current reward after update: -1678.96, Optimal reward -1378.58
Iteration 70 took 2.08 seconds (mean sampled reward: -4808.38). Current reward after update: -1684.96, Optimal reward -1378.58
Iteration 71 took 2.02 seconds (mean sampled reward: -6096.36). Current reward after update: -1825.51, Optimal reward -1378.58
Iteration 72 took 2.09 seconds (mean sampled reward: -4893.27). Current reward after update: -2327.26, Optimal reward -1378.58
Iteration 73 took 2.11 seconds (mean sampled reward: -5272.11). Current reward after update: -1780.07, Optimal reward -1378.58
Iteration 74 took 2.09 seconds (mean sampled reward: -4920.79). Current reward after update: -1777.58, Optimal reward -1378.58
Iteration 75 took 2.12 seconds (mean sampled reward: -5183.57). Current reward after update: -1703.63, Optimal reward -1378.58
Iteration 76 took 2.13 seconds (mean sampled reward: -5255.68). Current reward after update: -1787.07, Optimal reward -1378.58
Iteration 77 took 2.09 seconds (mean sampled reward: -5411.15). Current reward after update: -3823.24, Optimal reward -1378.58
Iteration 78 took 2.05 seconds (mean sampled reward: -4913.75). Current reward after update: -1631.66, Optimal reward -1378.58
Iteration 79 took 2.09 seconds (mean sampled reward: -4657.24). Current reward after update: -1691.46, Optimal reward -1378.58
Iteration 80 took 2.03 seconds (mean sampled reward: -4067.11). Current reward after update: -1364.26, Optimal reward -1364.26
Iteration 81 took 2.03 seconds (mean sampled reward: -4574.69). Current reward after update: -1402.48, Optimal reward -1364.26
Iteration 82 took 2.02 seconds (mean sampled reward: -4193.00). Current reward after update: -1343.58, Optimal reward -1343.58
Iteration 83 took 2.05 seconds (mean sampled reward: -5229.93). Current reward after update: -1342.86, Optimal reward -1342.86
Iteration 84 took 2.04 seconds (mean sampled reward: -4030.58). Current reward after update: -1416.87, Optimal reward -1342.86
Iteration 85 took 2.09 seconds (mean sampled reward: -4918.16). Current reward after update: -1250.19, Optimal reward -1250.19
Iteration 86 took 2.05 seconds (mean sampled reward: -4522.16). Current reward after update: -1131.36, Optimal reward -1131.36
Iteration 87 took 2.14 seconds (mean sampled reward: -5037.45). Current reward after update: -1259.78, Optimal reward -1131.36
Iteration 88 took 2.12 seconds (mean sampled reward: -4778.35). Current reward after update: -1322.77, Optimal reward -1131.36
Iteration 89 took 2.14 seconds (mean sampled reward: -4546.41). Current reward after update: -1340.96, Optimal reward -1131.36
Iteration 90 took 2.09 seconds (mean sampled reward: -5110.37). Current reward after update: -1182.61, Optimal reward -1131.36
Iteration 91 took 2.07 seconds (mean sampled reward: -5393.67). Current reward after update: -1279.86, Optimal reward -1131.36
Iteration 92 took 2.07 seconds (mean sampled reward: -6001.31). Current reward after update: -1223.14, Optimal reward -1131.36
Iteration 93 took 2.12 seconds (mean sampled reward: -4763.38). Current reward after update: -1660.84, Optimal reward -1131.36
Iteration 94 took 2.06 seconds (mean sampled reward: -4053.67). Current reward after update: -3294.14, Optimal reward -1131.36
Iteration 95 took 2.14 seconds (mean sampled reward: -5630.73). Current reward after update: -1032.98, Optimal reward -1032.98
Iteration 96 took 2.04 seconds (mean sampled reward: -5233.42). Current reward after update: -1351.76, Optimal reward -1032.98
Iteration 97 took 2.08 seconds (mean sampled reward: -4724.03). Current reward after update: -751.59, Optimal reward -751.59
Iteration 98 took 2.01 seconds (mean sampled reward: -5621.29). Current reward after update: -1258.69, Optimal reward -751.59
Iteration 99 took 2.02 seconds (mean sampled reward: -4762.45). Current reward after update: -918.92, Optimal reward -751.59
Iteration 100 took 2.03 seconds (mean sampled reward: -4166.36). Current reward after update: -1119.93, Optimal reward -751.59
Iteration 101 took 2.07 seconds (mean sampled reward: -5101.61). Current reward after update: -1243.60, Optimal reward -751.59
Iteration 102 took 2.07 seconds (mean sampled reward: -5428.69). Current reward after update: -1109.61, Optimal reward -751.59
Iteration 103 took 2.07 seconds (mean sampled reward: -5265.40). Current reward after update: -1221.23, Optimal reward -751.59
Iteration 104 took 2.05 seconds (mean sampled reward: -5054.64). Current reward after update: -1258.57, Optimal reward -751.59
Iteration 105 took 2.05 seconds (mean sampled reward: -4317.29). Current reward after update: -2804.87, Optimal reward -751.59
Iteration 106 took 2.22 seconds (mean sampled reward: -4716.25). Current reward after update: -1520.97, Optimal reward -751.59
Iteration 107 took 2.08 seconds (mean sampled reward: -4864.63). Current reward after update: -1124.80, Optimal reward -751.59
Iteration 108 took 2.18 seconds (mean sampled reward: -4221.40). Current reward after update: -1236.32, Optimal reward -751.59
Iteration 109 took 2.08 seconds (mean sampled reward: -4237.89). Current reward after update: -1415.83, Optimal reward -751.59
Iteration 110 took 2.36 seconds (mean sampled reward: -3410.99). Current reward after update: -839.66, Optimal reward -751.59
Iteration 111 took 2.11 seconds (mean sampled reward: -3499.36). Current reward after update: -3131.53, Optimal reward -751.59
Iteration 112 took 2.08 seconds (mean sampled reward: -4069.13). Current reward after update: -1414.64, Optimal reward -751.59
Iteration 113 took 2.23 seconds (mean sampled reward: -3560.09). Current reward after update: -1349.66, Optimal reward -751.59
Iteration 114 took 2.12 seconds (mean sampled reward: -3600.31). Current reward after update: -1089.92, Optimal reward -751.59
Iteration 115 took 2.07 seconds (mean sampled reward: -4519.49). Current reward after update: -1511.35, Optimal reward -751.59
Iteration 116 took 2.05 seconds (mean sampled reward: -4459.99). Current reward after update: -1059.76, Optimal reward -751.59
Iteration 117 took 2.07 seconds (mean sampled reward: -4141.15). Current reward after update: -1282.08, Optimal reward -751.59
Iteration 118 took 2.02 seconds (mean sampled reward: -3891.43). Current reward after update: -1479.35, Optimal reward -751.59
Iteration 119 took 2.00 seconds (mean sampled reward: -3358.88). Current reward after update: -5922.66, Optimal reward -751.59
Iteration 120 took 1.95 seconds (mean sampled reward: -3577.22). Current reward after update: -1505.98, Optimal reward -751.59
Iteration 121 took 2.18 seconds (mean sampled reward: -3292.85). Current reward after update: -1633.30, Optimal reward -751.59
Iteration 122 took 2.03 seconds (mean sampled reward: -3826.22). Current reward after update: -1420.33, Optimal reward -751.59
Iteration 123 took 1.99 seconds (mean sampled reward: -3863.02). Current reward after update: -1502.91, Optimal reward -751.59
Iteration 124 took 1.92 seconds (mean sampled reward: -3437.82). Current reward after update: -2018.30, Optimal reward -751.59
Iteration 125 took 2.01 seconds (mean sampled reward: -3658.86). Current reward after update: -2495.50, Optimal reward -751.59
Iteration 126 took 2.11 seconds (mean sampled reward: -3011.48). Current reward after update: -1545.05, Optimal reward -751.59
Iteration 127 took 2.00 seconds (mean sampled reward: -3850.57). Current reward after update: -1431.96, Optimal reward -751.59
Iteration 128 took 2.07 seconds (mean sampled reward: -4395.73). Current reward after update: -1438.56, Optimal reward -751.59
Iteration 129 took 2.25 seconds (mean sampled reward: -3534.17). Current reward after update: -1397.71, Optimal reward -751.59
Iteration 130 took 2.00 seconds (mean sampled reward: -3935.84). Current reward after update: -1218.11, Optimal reward -751.59
Iteration 131 took 2.00 seconds (mean sampled reward: -4291.84). Current reward after update: -1558.71, Optimal reward -751.59
Iteration 132 took 2.00 seconds (mean sampled reward: -3676.42). Current reward after update: -1602.47, Optimal reward -751.59
Iteration 133 took 2.04 seconds (mean sampled reward: -3476.30). Current reward after update: -1503.00, Optimal reward -751.59
Iteration 134 took 2.06 seconds (mean sampled reward: -3399.02). Current reward after update: -2202.02, Optimal reward -751.59
Iteration 135 took 2.15 seconds (mean sampled reward: -3392.26). Current reward after update: -1428.54, Optimal reward -751.59
Iteration 136 took 2.05 seconds (mean sampled reward: -3798.15). Current reward after update: -1392.06, Optimal reward -751.59
Iteration 137 took 2.10 seconds (mean sampled reward: -3408.18). Current reward after update: -1390.98, Optimal reward -751.59
Iteration 138 took 2.12 seconds (mean sampled reward: -3177.05). Current reward after update: -1486.96, Optimal reward -751.59
Iteration 139 took 2.08 seconds (mean sampled reward: -3711.54). Current reward after update: -1466.83, Optimal reward -751.59
Iteration 140 took 2.08 seconds (mean sampled reward: -4048.73). Current reward after update: -1465.52, Optimal reward -751.59
Iteration 141 took 2.11 seconds (mean sampled reward: -3853.64). Current reward after update: -1634.56, Optimal reward -751.59
Iteration 142 took 2.09 seconds (mean sampled reward: -3830.81). Current reward after update: -1513.87, Optimal reward -751.59
Iteration 143 took 2.05 seconds (mean sampled reward: -3414.28). Current reward after update: -1489.80, Optimal reward -751.59
Iteration 144 took 2.09 seconds (mean sampled reward: -3100.37). Current reward after update: -1361.23, Optimal reward -751.59
Iteration 145 took 2.09 seconds (mean sampled reward: -3118.82). Current reward after update: -1344.74, Optimal reward -751.59
Iteration 146 took 2.08 seconds (mean sampled reward: -4172.27). Current reward after update: -1228.28, Optimal reward -751.59
Iteration 147 took 2.06 seconds (mean sampled reward: -3467.44). Current reward after update: -1505.30, Optimal reward -751.59
Iteration 148 took 2.07 seconds (mean sampled reward: -3185.02). Current reward after update: -1394.99, Optimal reward -751.59
Iteration 149 took 2.13 seconds (mean sampled reward: -3788.56). Current reward after update: -1525.44, Optimal reward -751.59
Iteration 150 took 2.11 seconds (mean sampled reward: -3241.67). Current reward after update: -1497.62, Optimal reward -751.59
Iteration 151 took 2.13 seconds (mean sampled reward: -2862.76). Current reward after update: -1275.96, Optimal reward -751.59
Iteration 152 took 2.13 seconds (mean sampled reward: -3406.54). Current reward after update: -1988.24, Optimal reward -751.59
Iteration 153 took 2.11 seconds (mean sampled reward: -3090.54). Current reward after update: -1868.39, Optimal reward -751.59
Iteration 154 took 2.06 seconds (mean sampled reward: -2946.74). Current reward after update: -2134.66, Optimal reward -751.59
Iteration 155 took 2.09 seconds (mean sampled reward: -2945.60). Current reward after update: -3184.99, Optimal reward -751.59
Iteration 156 took 2.07 seconds (mean sampled reward: -3194.78). Current reward after update: -2093.58, Optimal reward -751.59
Iteration 157 took 2.18 seconds (mean sampled reward: -4848.24). Current reward after update: -1647.49, Optimal reward -751.59
Iteration 158 took 2.06 seconds (mean sampled reward: -3423.93). Current reward after update: -1168.11, Optimal reward -751.59
Iteration 159 took 2.22 seconds (mean sampled reward: -3674.63). Current reward after update: -1650.55, Optimal reward -751.59
Iteration 160 took 2.20 seconds (mean sampled reward: -3389.31). Current reward after update: -975.19, Optimal reward -751.59
Iteration 161 took 2.16 seconds (mean sampled reward: -4497.65). Current reward after update: -943.39, Optimal reward -751.59
Iteration 162 took 2.16 seconds (mean sampled reward: -3793.95). Current reward after update: -1068.20, Optimal reward -751.59
Iteration 163 took 2.12 seconds (mean sampled reward: -2856.10). Current reward after update: -895.61, Optimal reward -751.59
Iteration 164 took 2.15 seconds (mean sampled reward: -2781.16). Current reward after update: -896.62, Optimal reward -751.59
Iteration 165 took 2.10 seconds (mean sampled reward: -3777.15). Current reward after update: -881.64, Optimal reward -751.59
Iteration 166 took 2.07 seconds (mean sampled reward: -3524.08). Current reward after update: -843.02, Optimal reward -751.59
Iteration 167 took 2.00 seconds (mean sampled reward: -3748.77). Current reward after update: -724.27, Optimal reward -724.27
Iteration 168 took 2.04 seconds (mean sampled reward: -4954.04). Current reward after update: -879.55, Optimal reward -724.27
Iteration 169 took 2.02 seconds (mean sampled reward: -3377.73). Current reward after update: -913.18, Optimal reward -724.27
Iteration 170 took 2.06 seconds (mean sampled reward: -3761.78). Current reward after update: -795.42, Optimal reward -724.27
Iteration 171 took 1.93 seconds (mean sampled reward: -5809.88). Current reward after update: -891.06, Optimal reward -724.27
Iteration 172 took 2.00 seconds (mean sampled reward: -5089.12). Current reward after update: -765.06, Optimal reward -724.27
Iteration 173 took 2.09 seconds (mean sampled reward: -3822.17). Current reward after update: -857.34, Optimal reward -724.27
Iteration 174 took 2.05 seconds (mean sampled reward: -4674.24). Current reward after update: -723.39, Optimal reward -723.39
Iteration 175 took 2.00 seconds (mean sampled reward: -3324.59). Current reward after update: -1975.24, Optimal reward -723.39
Iteration 176 took 2.01 seconds (mean sampled reward: -3653.23). Current reward after update: -636.37, Optimal reward -636.37
Iteration 177 took 1.95 seconds (mean sampled reward: -4387.36). Current reward after update: -751.71, Optimal reward -636.37
Iteration 178 took 1.94 seconds (mean sampled reward: -3620.91). Current reward after update: -777.65, Optimal reward -636.37
Iteration 179 took 1.99 seconds (mean sampled reward: -4080.93). Current reward after update: -798.79, Optimal reward -636.37
Iteration 180 took 1.99 seconds (mean sampled reward: -3919.56). Current reward after update: -809.11, Optimal reward -636.37
Iteration 181 took 1.98 seconds (mean sampled reward: -3566.33). Current reward after update: -857.49, Optimal reward -636.37
Iteration 182 took 2.01 seconds (mean sampled reward: -4090.47). Current reward after update: -859.22, Optimal reward -636.37
Iteration 183 took 2.05 seconds (mean sampled reward: -2719.45). Current reward after update: -857.68, Optimal reward -636.37
Iteration 184 took 2.03 seconds (mean sampled reward: -2641.51). Current reward after update: -713.62, Optimal reward -636.37
Iteration 185 took 2.03 seconds (mean sampled reward: -2424.74). Current reward after update: -770.52, Optimal reward -636.37
Iteration 186 took 2.06 seconds (mean sampled reward: -2595.79). Current reward after update: -718.37, Optimal reward -636.37
Iteration 187 took 2.03 seconds (mean sampled reward: -2796.20). Current reward after update: -720.49, Optimal reward -636.37
Iteration 188 took 2.02 seconds (mean sampled reward: -2969.93). Current reward after update: -760.28, Optimal reward -636.37
Iteration 189 took 2.07 seconds (mean sampled reward: -2920.72). Current reward after update: -971.20, Optimal reward -636.37
Iteration 190 took 2.07 seconds (mean sampled reward: -3019.52). Current reward after update: -821.41, Optimal reward -636.37
Iteration 191 took 2.05 seconds (mean sampled reward: -2805.07). Current reward after update: -830.96, Optimal reward -636.37
Iteration 192 took 2.01 seconds (mean sampled reward: -2954.82). Current reward after update: -703.98, Optimal reward -636.37
Iteration 193 took 2.14 seconds (mean sampled reward: -3261.04). Current reward after update: -778.28, Optimal reward -636.37
Iteration 194 took 2.06 seconds (mean sampled reward: -2776.97). Current reward after update: -3682.99, Optimal reward -636.37
Iteration 195 took 2.05 seconds (mean sampled reward: -3317.44). Current reward after update: -711.92, Optimal reward -636.37
Iteration 196 took 2.08 seconds (mean sampled reward: -3154.90). Current reward after update: -735.12, Optimal reward -636.37
Iteration 197 took 2.08 seconds (mean sampled reward: -3408.27). Current reward after update: -1637.57, Optimal reward -636.37
Iteration 198 took 2.08 seconds (mean sampled reward: -3115.84). Current reward after update: -763.79, Optimal reward -636.37
Iteration 199 took 2.07 seconds (mean sampled reward: -2905.63). Current reward after update: -707.31, Optimal reward -636.37
Iteration 200 took 2.04 seconds (mean sampled reward: -2670.41). Current reward after update: -718.51, Optimal reward -636.37
Iteration 1 took 2.19 seconds (mean sampled reward: -7545.33). Current reward after update: -6225.40, Optimal reward -6225.40
Iteration 2 took 2.15 seconds (mean sampled reward: -7383.43). Current reward after update: -5472.67, Optimal reward -5472.67
Iteration 3 took 2.15 seconds (mean sampled reward: -6967.44). Current reward after update: -5068.75, Optimal reward -5068.75
Iteration 4 took 2.23 seconds (mean sampled reward: -5973.01). Current reward after update: -4496.91, Optimal reward -4496.91
Iteration 5 took 2.40 seconds (mean sampled reward: -5557.01). Current reward after update: -3838.13, Optimal reward -3838.13
Iteration 6 took 2.14 seconds (mean sampled reward: -5529.29). Current reward after update: -3507.14, Optimal reward -3507.14
Iteration 7 took 2.25 seconds (mean sampled reward: -4707.70). Current reward after update: -3368.69, Optimal reward -3368.69
Iteration 8 took 2.35 seconds (mean sampled reward: -4555.41). Current reward after update: -3497.42, Optimal reward -3368.69
Iteration 9 took 2.25 seconds (mean sampled reward: -4378.49). Current reward after update: -3333.75, Optimal reward -3333.75
Iteration 10 took 2.27 seconds (mean sampled reward: -4922.61). Current reward after update: -3218.30, Optimal reward -3218.30
Iteration 11 took 2.34 seconds (mean sampled reward: -5380.19). Current reward after update: -2856.74, Optimal reward -2856.74
Iteration 12 took 2.19 seconds (mean sampled reward: -5211.42). Current reward after update: -2865.89, Optimal reward -2856.74
Iteration 13 took 2.39 seconds (mean sampled reward: -4951.06). Current reward after update: -2786.40, Optimal reward -2786.40
Iteration 14 took 2.25 seconds (mean sampled reward: -4617.23). Current reward after update: -2810.24, Optimal reward -2786.40
Iteration 15 took 2.33 seconds (mean sampled reward: -4259.39). Current reward after update: -2653.09, Optimal reward -2653.09
Iteration 16 took 2.22 seconds (mean sampled reward: -4755.20). Current reward after update: -2674.69, Optimal reward -2653.09
Iteration 17 took 2.20 seconds (mean sampled reward: -5390.20). Current reward after update: -2311.57, Optimal reward -2311.57
Iteration 18 took 2.18 seconds (mean sampled reward: -5310.36). Current reward after update: -2233.56, Optimal reward -2233.56
Iteration 19 took 2.07 seconds (mean sampled reward: -5867.42). Current reward after update: -2194.00, Optimal reward -2194.00
Iteration 20 took 2.17 seconds (mean sampled reward: -6012.51). Current reward after update: -2102.47, Optimal reward -2102.47
Iteration 21 took 2.01 seconds (mean sampled reward: -5571.63). Current reward after update: -2034.48, Optimal reward -2034.48
Iteration 22 took 2.32 seconds (mean sampled reward: -4740.46). Current reward after update: -2571.55, Optimal reward -2034.48
Iteration 23 took 2.02 seconds (mean sampled reward: -5276.76). Current reward after update: -1905.09, Optimal reward -1905.09
Iteration 24 took 1.99 seconds (mean sampled reward: -5437.40). Current reward after update: -1730.56, Optimal reward -1730.56
Iteration 25 took 2.11 seconds (mean sampled reward: -4365.43). Current reward after update: -1697.24, Optimal reward -1697.24
Iteration 26 took 2.09 seconds (mean sampled reward: -5467.01). Current reward after update: -1759.04, Optimal reward -1697.24
Iteration 27 took 1.98 seconds (mean sampled reward: -5167.75). Current reward after update: -1841.23, Optimal reward -1697.24
Iteration 28 took 2.03 seconds (mean sampled reward: -6482.51). Current reward after update: -1744.79, Optimal reward -1697.24
Iteration 29 took 1.98 seconds (mean sampled reward: -5704.99). Current reward after update: -1730.15, Optimal reward -1697.24
Iteration 30 took 2.03 seconds (mean sampled reward: -5073.26). Current reward after update: -1606.74, Optimal reward -1606.74
Iteration 31 took 2.07 seconds (mean sampled reward: -3399.87). Current reward after update: -1580.36, Optimal reward -1580.36
Iteration 32 took 2.00 seconds (mean sampled reward: -3787.31). Current reward after update: -1593.62, Optimal reward -1580.36
Iteration 33 took 2.02 seconds (mean sampled reward: -4936.97). Current reward after update: -1536.37, Optimal reward -1536.37
Iteration 34 took 2.11 seconds (mean sampled reward: -3837.58). Current reward after update: -1607.36, Optimal reward -1536.37
Iteration 35 took 2.15 seconds (mean sampled reward: -3772.63). Current reward after update: -1504.53, Optimal reward -1504.53
Iteration 36 took 2.07 seconds (mean sampled reward: -3325.10). Current reward after update: -1585.39, Optimal reward -1504.53
Iteration 37 took 1.99 seconds (mean sampled reward: -4331.47). Current reward after update: -1543.57, Optimal reward -1504.53
Iteration 38 took 2.00 seconds (mean sampled reward: -4872.93). Current reward after update: -1565.33, Optimal reward -1504.53
Iteration 39 took 2.20 seconds (mean sampled reward: -5394.17). Current reward after update: -1468.25, Optimal reward -1468.25
Iteration 40 took 1.92 seconds (mean sampled reward: -5650.61). Current reward after update: -1512.49, Optimal reward -1468.25
Iteration 41 took 1.98 seconds (mean sampled reward: -4550.76). Current reward after update: -1408.50, Optimal reward -1408.50
Iteration 42 took 2.03 seconds (mean sampled reward: -4985.68). Current reward after update: -1302.57, Optimal reward -1302.57
Iteration 43 took 1.91 seconds (mean sampled reward: -5655.12). Current reward after update: -1181.74, Optimal reward -1181.74
Iteration 44 took 1.94 seconds (mean sampled reward: -6201.02). Current reward after update: -1842.47, Optimal reward -1181.74
Iteration 45 took 1.99 seconds (mean sampled reward: -4096.35). Current reward after update: -1112.41, Optimal reward -1112.41
Iteration 46 took 1.99 seconds (mean sampled reward: -4826.58). Current reward after update: -1134.75, Optimal reward -1112.41
Iteration 47 took 1.90 seconds (mean sampled reward: -5411.72). Current reward after update: -1794.49, Optimal reward -1112.41
Iteration 48 took 2.02 seconds (mean sampled reward: -4236.82). Current reward after update: -1153.34, Optimal reward -1112.41
Iteration 49 took 2.08 seconds (mean sampled reward: -3962.84). Current reward after update: -1015.77, Optimal reward -1015.77
Iteration 50 took 2.04 seconds (mean sampled reward: -3779.71). Current reward after update: -931.27, Optimal reward -931.27
Iteration 51 took 2.13 seconds (mean sampled reward: -2919.23). Current reward after update: -908.77, Optimal reward -908.77
Iteration 52 took 2.13 seconds (mean sampled reward: -3984.78). Current reward after update: -1255.28, Optimal reward -908.77
Iteration 53 took 2.04 seconds (mean sampled reward: -3462.03). Current reward after update: -1125.37, Optimal reward -908.77
Iteration 54 took 2.34 seconds (mean sampled reward: -3705.84). Current reward after update: -912.44, Optimal reward -908.77
Iteration 55 took 1.99 seconds (mean sampled reward: -3879.37). Current reward after update: -836.66, Optimal reward -836.66
Iteration 56 took 2.15 seconds (mean sampled reward: -4021.70). Current reward after update: -904.25, Optimal reward -836.66
Iteration 57 took 2.21 seconds (mean sampled reward: -4517.71). Current reward after update: -984.08, Optimal reward -836.66
Iteration 58 took 1.98 seconds (mean sampled reward: -4608.70). Current reward after update: -1017.82, Optimal reward -836.66
Iteration 59 took 2.08 seconds (mean sampled reward: -4508.30). Current reward after update: -909.18, Optimal reward -836.66
Iteration 60 took 2.01 seconds (mean sampled reward: -4437.14). Current reward after update: -902.59, Optimal reward -836.66
Iteration 61 took 2.10 seconds (mean sampled reward: -4116.80). Current reward after update: -908.25, Optimal reward -836.66
Iteration 62 took 2.02 seconds (mean sampled reward: -3661.39). Current reward after update: -889.33, Optimal reward -836.66
Iteration 63 took 1.95 seconds (mean sampled reward: -5235.03). Current reward after update: -847.41, Optimal reward -836.66
Iteration 64 took 1.96 seconds (mean sampled reward: -5102.99). Current reward after update: -822.50, Optimal reward -822.50
Iteration 65 took 2.09 seconds (mean sampled reward: -5440.67). Current reward after update: -933.83, Optimal reward -822.50
Iteration 66 took 2.09 seconds (mean sampled reward: -4831.12). Current reward after update: -865.98, Optimal reward -822.50
Iteration 67 took 2.07 seconds (mean sampled reward: -3334.11). Current reward after update: -978.83, Optimal reward -822.50
Iteration 68 took 2.13 seconds (mean sampled reward: -2665.98). Current reward after update: -1098.53, Optimal reward -822.50
Iteration 69 took 2.11 seconds (mean sampled reward: -3136.33). Current reward after update: -985.42, Optimal reward -822.50
Iteration 70 took 2.11 seconds (mean sampled reward: -2864.30). Current reward after update: -949.37, Optimal reward -822.50
Iteration 71 took 2.10 seconds (mean sampled reward: -2272.25). Current reward after update: -1126.66, Optimal reward -822.50
Iteration 72 took 2.00 seconds (mean sampled reward: -3864.45). Current reward after update: -872.47, Optimal reward -822.50
Iteration 73 took 2.12 seconds (mean sampled reward: -3832.38). Current reward after update: -943.67, Optimal reward -822.50
Iteration 74 took 2.19 seconds (mean sampled reward: -3944.76). Current reward after update: -1015.63, Optimal reward -822.50
Iteration 75 took 2.04 seconds (mean sampled reward: -3511.84). Current reward after update: -881.71, Optimal reward -822.50
Iteration 76 took 2.09 seconds (mean sampled reward: -3781.97). Current reward after update: -842.24, Optimal reward -822.50
Iteration 77 took 2.03 seconds (mean sampled reward: -4534.15). Current reward after update: -883.65, Optimal reward -822.50
Iteration 78 took 1.91 seconds (mean sampled reward: -4816.80). Current reward after update: -953.85, Optimal reward -822.50
Iteration 79 took 2.01 seconds (mean sampled reward: -3875.64). Current reward after update: -816.24, Optimal reward -816.24
Iteration 80 took 1.95 seconds (mean sampled reward: -4791.69). Current reward after update: -816.55, Optimal reward -816.24
Iteration 81 took 1.95 seconds (mean sampled reward: -4826.01). Current reward after update: -984.04, Optimal reward -816.24
Iteration 82 took 2.01 seconds (mean sampled reward: -3825.26). Current reward after update: -889.70, Optimal reward -816.24
Iteration 83 took 2.07 seconds (mean sampled reward: -3515.75). Current reward after update: -1555.92, Optimal reward -816.24
Iteration 84 took 2.00 seconds (mean sampled reward: -4277.99). Current reward after update: -993.02, Optimal reward -816.24
Iteration 85 took 1.95 seconds (mean sampled reward: -4239.14). Current reward after update: -859.55, Optimal reward -816.24
Iteration 86 took 1.97 seconds (mean sampled reward: -3741.20). Current reward after update: -856.43, Optimal reward -816.24
Iteration 87 took 2.00 seconds (mean sampled reward: -4579.94). Current reward after update: -839.67, Optimal reward -816.24
Iteration 88 took 1.99 seconds (mean sampled reward: -4424.31). Current reward after update: -1324.31, Optimal reward -816.24
Iteration 89 took 2.01 seconds (mean sampled reward: -3225.87). Current reward after update: -797.23, Optimal reward -797.23
Iteration 90 took 2.01 seconds (mean sampled reward: -3598.73). Current reward after update: -682.54, Optimal reward -682.54
Iteration 91 took 1.99 seconds (mean sampled reward: -3062.71). Current reward after update: -925.36, Optimal reward -682.54
Iteration 92 took 2.02 seconds (mean sampled reward: -2780.74). Current reward after update: -755.23, Optimal reward -682.54
Iteration 93 took 1.98 seconds (mean sampled reward: -3854.99). Current reward after update: -777.80, Optimal reward -682.54
Iteration 94 took 2.01 seconds (mean sampled reward: -4028.46). Current reward after update: -771.79, Optimal reward -682.54
Iteration 95 took 2.05 seconds (mean sampled reward: -3088.97). Current reward after update: -790.22, Optimal reward -682.54
Iteration 96 took 2.02 seconds (mean sampled reward: -3110.64). Current reward after update: -746.13, Optimal reward -682.54
Iteration 97 took 2.06 seconds (mean sampled reward: -4046.11). Current reward after update: -766.62, Optimal reward -682.54
Iteration 98 took 2.17 seconds (mean sampled reward: -3111.81). Current reward after update: -1224.80, Optimal reward -682.54
Iteration 99 took 2.09 seconds (mean sampled reward: -3852.65). Current reward after update: -867.19, Optimal reward -682.54
Iteration 100 took 2.04 seconds (mean sampled reward: -3424.75). Current reward after update: -820.77, Optimal reward -682.54
Iteration 101 took 2.11 seconds (mean sampled reward: -2424.66). Current reward after update: -2177.27, Optimal reward -682.54
Iteration 102 took 2.13 seconds (mean sampled reward: -2778.42). Current reward after update: -886.24, Optimal reward -682.54
Iteration 103 took 2.09 seconds (mean sampled reward: -4298.12). Current reward after update: -802.19, Optimal reward -682.54
Iteration 104 took 2.10 seconds (mean sampled reward: -2935.82). Current reward after update: -1039.51, Optimal reward -682.54
Iteration 105 took 2.26 seconds (mean sampled reward: -3153.93). Current reward after update: -913.00, Optimal reward -682.54
Iteration 106 took 2.04 seconds (mean sampled reward: -5427.07). Current reward after update: -926.09, Optimal reward -682.54
Iteration 107 took 2.26 seconds (mean sampled reward: -4086.26). Current reward after update: -1377.58, Optimal reward -682.54
Iteration 108 took 2.07 seconds (mean sampled reward: -4185.54). Current reward after update: -886.80, Optimal reward -682.54
Iteration 109 took 1.96 seconds (mean sampled reward: -4073.46). Current reward after update: -896.06, Optimal reward -682.54
Iteration 110 took 2.00 seconds (mean sampled reward: -4397.01). Current reward after update: -1366.01, Optimal reward -682.54
Iteration 111 took 2.03 seconds (mean sampled reward: -5580.99). Current reward after update: -838.20, Optimal reward -682.54
Iteration 112 took 2.04 seconds (mean sampled reward: -4065.94). Current reward after update: -824.59, Optimal reward -682.54
Iteration 113 took 2.10 seconds (mean sampled reward: -4311.30). Current reward after update: -818.44, Optimal reward -682.54
Iteration 114 took 2.17 seconds (mean sampled reward: -4082.69). Current reward after update: -814.83, Optimal reward -682.54
Iteration 115 took 2.07 seconds (mean sampled reward: -4612.76). Current reward after update: -921.12, Optimal reward -682.54
Iteration 116 took 2.05 seconds (mean sampled reward: -5027.13). Current reward after update: -797.12, Optimal reward -682.54
Iteration 117 took 2.20 seconds (mean sampled reward: -4949.67). Current reward after update: -733.26, Optimal reward -682.54
Iteration 118 took 2.14 seconds (mean sampled reward: -4288.04). Current reward after update: -615.98, Optimal reward -615.98
Iteration 119 took 2.07 seconds (mean sampled reward: -5448.75). Current reward after update: -731.24, Optimal reward -615.98
Iteration 120 took 2.08 seconds (mean sampled reward: -5729.46). Current reward after update: -738.22, Optimal reward -615.98
Iteration 121 took 2.15 seconds (mean sampled reward: -4730.34). Current reward after update: -688.07, Optimal reward -615.98
Iteration 122 took 2.02 seconds (mean sampled reward: -5369.27). Current reward after update: -809.63, Optimal reward -615.98
Iteration 123 took 1.97 seconds (mean sampled reward: -4743.37). Current reward after update: -797.90, Optimal reward -615.98
Iteration 124 took 2.08 seconds (mean sampled reward: -4679.43). Current reward after update: -947.78, Optimal reward -615.98
Iteration 125 took 2.13 seconds (mean sampled reward: -5716.47). Current reward after update: -851.65, Optimal reward -615.98
Iteration 126 took 2.08 seconds (mean sampled reward: -5298.78). Current reward after update: -817.01, Optimal reward -615.98
Iteration 127 took 2.14 seconds (mean sampled reward: -4989.80). Current reward after update: -795.86, Optimal reward -615.98
Iteration 128 took 2.11 seconds (mean sampled reward: -5081.62). Current reward after update: -5446.61, Optimal reward -615.98
Iteration 129 took 2.17 seconds (mean sampled reward: -3480.03). Current reward after update: -755.09, Optimal reward -615.98
Iteration 130 took 2.10 seconds (mean sampled reward: -4443.81). Current reward after update: -658.35, Optimal reward -615.98
Iteration 131 took 2.13 seconds (mean sampled reward: -3798.79). Current reward after update: -744.24, Optimal reward -615.98
Iteration 132 took 2.17 seconds (mean sampled reward: -5194.42). Current reward after update: -763.84, Optimal reward -615.98
Iteration 133 took 2.00 seconds (mean sampled reward: -4990.75). Current reward after update: -1062.91, Optimal reward -615.98
Iteration 134 took 2.11 seconds (mean sampled reward: -5752.84). Current reward after update: -874.05, Optimal reward -615.98
Iteration 135 took 2.01 seconds (mean sampled reward: -5367.26). Current reward after update: -1233.98, Optimal reward -615.98
Iteration 136 took 2.00 seconds (mean sampled reward: -5832.27). Current reward after update: -818.81, Optimal reward -615.98
Iteration 137 took 1.97 seconds (mean sampled reward: -5412.09). Current reward after update: -862.90, Optimal reward -615.98
Iteration 138 took 2.09 seconds (mean sampled reward: -4890.42). Current reward after update: -871.89, Optimal reward -615.98
Iteration 139 took 1.94 seconds (mean sampled reward: -5808.16). Current reward after update: -1822.56, Optimal reward -615.98
Iteration 140 took 1.96 seconds (mean sampled reward: -5676.55). Current reward after update: -962.37, Optimal reward -615.98
Iteration 141 took 2.00 seconds (mean sampled reward: -5440.06). Current reward after update: -867.61, Optimal reward -615.98
Iteration 142 took 2.04 seconds (mean sampled reward: -5291.00). Current reward after update: -932.37, Optimal reward -615.98
Iteration 143 took 1.96 seconds (mean sampled reward: -5465.60). Current reward after update: -894.21, Optimal reward -615.98
Iteration 144 took 1.90 seconds (mean sampled reward: -5694.58). Current reward after update: -922.22, Optimal reward -615.98
Iteration 145 took 1.95 seconds (mean sampled reward: -5905.61). Current reward after update: -907.88, Optimal reward -615.98
Iteration 146 took 2.01 seconds (mean sampled reward: -5028.92). Current reward after update: -899.90, Optimal reward -615.98
Iteration 147 took 2.05 seconds (mean sampled reward: -4286.20). Current reward after update: -898.95, Optimal reward -615.98
Iteration 148 took 2.08 seconds (mean sampled reward: -3830.82). Current reward after update: -827.44, Optimal reward -615.98
Iteration 149 took 2.11 seconds (mean sampled reward: -4033.28). Current reward after update: -873.33, Optimal reward -615.98
Iteration 150 took 2.16 seconds (mean sampled reward: -3782.12). Current reward after update: -829.15, Optimal reward -615.98
Iteration 151 took 2.12 seconds (mean sampled reward: -3861.59). Current reward after update: -820.47, Optimal reward -615.98
Iteration 152 took 2.11 seconds (mean sampled reward: -3730.75). Current reward after update: -840.98, Optimal reward -615.98
Iteration 153 took 2.11 seconds (mean sampled reward: -3637.83). Current reward after update: -1385.44, Optimal reward -615.98
Iteration 154 took 2.14 seconds (mean sampled reward: -3746.17). Current reward after update: -944.11, Optimal reward -615.98
Iteration 155 took 2.28 seconds (mean sampled reward: -2734.67). Current reward after update: -799.34, Optimal reward -615.98
Iteration 156 took 2.32 seconds (mean sampled reward: -2868.11). Current reward after update: -805.03, Optimal reward -615.98
Iteration 157 took 2.30 seconds (mean sampled reward: -2767.91). Current reward after update: -671.89, Optimal reward -615.98
Iteration 158 took 2.19 seconds (mean sampled reward: -2947.36). Current reward after update: -714.18, Optimal reward -615.98
Iteration 159 took 2.19 seconds (mean sampled reward: -2613.89). Current reward after update: -801.11, Optimal reward -615.98
Iteration 160 took 2.26 seconds (mean sampled reward: -2829.25). Current reward after update: -693.43, Optimal reward -615.98
Iteration 161 took 2.21 seconds (mean sampled reward: -2922.12). Current reward after update: -1156.53, Optimal reward -615.98
Iteration 162 took 2.15 seconds (mean sampled reward: -2928.41). Current reward after update: -778.34, Optimal reward -615.98
Iteration 163 took 2.19 seconds (mean sampled reward: -2626.86). Current reward after update: -764.67, Optimal reward -615.98
Iteration 164 took 2.13 seconds (mean sampled reward: -3135.34). Current reward after update: -780.78, Optimal reward -615.98
Iteration 165 took 2.16 seconds (mean sampled reward: -2939.65). Current reward after update: -1049.58, Optimal reward -615.98
Iteration 166 took 2.21 seconds (mean sampled reward: -2969.21). Current reward after update: -1001.14, Optimal reward -615.98
Iteration 167 took 2.17 seconds (mean sampled reward: -2869.55). Current reward after update: -1018.14, Optimal reward -615.98
Iteration 168 took 2.12 seconds (mean sampled reward: -3230.50). Current reward after update: -882.14, Optimal reward -615.98
Iteration 169 took 2.10 seconds (mean sampled reward: -3245.56). Current reward after update: -991.26, Optimal reward -615.98
Iteration 170 took 2.20 seconds (mean sampled reward: -2930.31). Current reward after update: -2719.13, Optimal reward -615.98
Iteration 171 took 2.14 seconds (mean sampled reward: -4121.39). Current reward after update: -937.93, Optimal reward -615.98
Iteration 172 took 1.99 seconds (mean sampled reward: -5094.21). Current reward after update: -934.47, Optimal reward -615.98
Iteration 173 took 2.09 seconds (mean sampled reward: -4994.38). Current reward after update: -1015.42, Optimal reward -615.98
Iteration 174 took 2.18 seconds (mean sampled reward: -4116.87). Current reward after update: -937.97, Optimal reward -615.98
Iteration 175 took 2.21 seconds (mean sampled reward: -2787.95). Current reward after update: -908.47, Optimal reward -615.98
Iteration 176 took 2.10 seconds (mean sampled reward: -3627.63). Current reward after update: -939.14, Optimal reward -615.98
Iteration 177 took 2.04 seconds (mean sampled reward: -4666.62). Current reward after update: -959.24, Optimal reward -615.98
Iteration 178 took 2.07 seconds (mean sampled reward: -3702.65). Current reward after update: -1154.06, Optimal reward -615.98
Iteration 179 took 1.97 seconds (mean sampled reward: -5625.20). Current reward after update: -836.35, Optimal reward -615.98
Iteration 180 took 1.99 seconds (mean sampled reward: -6030.12). Current reward after update: -866.82, Optimal reward -615.98
Iteration 181 took 2.08 seconds (mean sampled reward: -4188.72). Current reward after update: -813.93, Optimal reward -615.98
Iteration 182 took 2.11 seconds (mean sampled reward: -3172.68). Current reward after update: -801.97, Optimal reward -615.98
Iteration 183 took 2.12 seconds (mean sampled reward: -3220.27). Current reward after update: -1765.12, Optimal reward -615.98
Iteration 184 took 2.04 seconds (mean sampled reward: -3599.68). Current reward after update: -1230.03, Optimal reward -615.98
Iteration 185 took 2.07 seconds (mean sampled reward: -4340.36). Current reward after update: -2605.10, Optimal reward -615.98
Iteration 186 took 1.94 seconds (mean sampled reward: -4989.64). Current reward after update: -929.07, Optimal reward -615.98
Iteration 187 took 2.04 seconds (mean sampled reward: -3581.68). Current reward after update: -867.69, Optimal reward -615.98
Iteration 188 took 2.18 seconds (mean sampled reward: -2781.66). Current reward after update: -764.74, Optimal reward -615.98
Iteration 189 took 2.09 seconds (mean sampled reward: -3509.50). Current reward after update: -1028.39, Optimal reward -615.98
Iteration 190 took 2.07 seconds (mean sampled reward: -2913.63). Current reward after update: -1588.67, Optimal reward -615.98
Iteration 191 took 2.06 seconds (mean sampled reward: -3160.62). Current reward after update: -883.21, Optimal reward -615.98
Iteration 192 took 2.06 seconds (mean sampled reward: -3475.22). Current reward after update: -1350.15, Optimal reward -615.98
Iteration 193 took 2.10 seconds (mean sampled reward: -3028.54). Current reward after update: -1561.76, Optimal reward -615.98
Iteration 194 took 2.17 seconds (mean sampled reward: -2835.65). Current reward after update: -778.72, Optimal reward -615.98
Iteration 195 took 2.13 seconds (mean sampled reward: -3040.29). Current reward after update: -798.03, Optimal reward -615.98
Iteration 196 took 2.18 seconds (mean sampled reward: -3168.19). Current reward after update: -783.78, Optimal reward -615.98
Iteration 197 took 2.17 seconds (mean sampled reward: -2725.13). Current reward after update: -663.70, Optimal reward -615.98
Iteration 198 took 2.08 seconds (mean sampled reward: -2722.75). Current reward after update: -703.50, Optimal reward -615.98
Iteration 199 took 2.15 seconds (mean sampled reward: -3531.97). Current reward after update: -830.96, Optimal reward -615.98
Iteration 200 took 2.10 seconds (mean sampled reward: -3197.54). Current reward after update: -1329.66, Optimal reward -615.98
Iteration 1 took 2.10 seconds (mean sampled reward: -7551.86). Current reward after update: -6940.70, Optimal reward -6940.70
Iteration 2 took 2.14 seconds (mean sampled reward: -7383.34). Current reward after update: -4892.28, Optimal reward -4892.28
Iteration 3 took 2.06 seconds (mean sampled reward: -6715.34). Current reward after update: -4278.50, Optimal reward -4278.50
Iteration 4 took 2.25 seconds (mean sampled reward: -6269.51). Current reward after update: -4790.75, Optimal reward -4278.50
Iteration 5 took 2.09 seconds (mean sampled reward: -5876.43). Current reward after update: -3686.70, Optimal reward -3686.70
Iteration 6 took 2.24 seconds (mean sampled reward: -5397.32). Current reward after update: -3358.51, Optimal reward -3358.51
Iteration 7 took 2.28 seconds (mean sampled reward: -5146.11). Current reward after update: -3089.21, Optimal reward -3089.21
Iteration 8 took 2.10 seconds (mean sampled reward: -5432.41). Current reward after update: -2838.08, Optimal reward -2838.08
Iteration 9 took 2.28 seconds (mean sampled reward: -5754.06). Current reward after update: -3088.97, Optimal reward -2838.08
Iteration 10 took 2.09 seconds (mean sampled reward: -5346.28). Current reward after update: -3265.23, Optimal reward -2838.08
Iteration 11 took 2.42 seconds (mean sampled reward: -4924.18). Current reward after update: -3280.91, Optimal reward -2838.08
Iteration 12 took 2.26 seconds (mean sampled reward: -4894.12). Current reward after update: -3194.66, Optimal reward -2838.08
Iteration 13 took 2.29 seconds (mean sampled reward: -4641.67). Current reward after update: -3185.08, Optimal reward -2838.08
Iteration 14 took 2.23 seconds (mean sampled reward: -5350.29). Current reward after update: -3274.03, Optimal reward -2838.08
Iteration 15 took 2.12 seconds (mean sampled reward: -5275.05). Current reward after update: -3303.49, Optimal reward -2838.08
Iteration 16 took 2.27 seconds (mean sampled reward: -5359.33). Current reward after update: -3383.51, Optimal reward -2838.08
Iteration 17 took 2.11 seconds (mean sampled reward: -4612.76). Current reward after update: -3861.56, Optimal reward -2838.08
Iteration 18 took 2.30 seconds (mean sampled reward: -4797.51). Current reward after update: -3082.85, Optimal reward -2838.08
Iteration 19 took 2.22 seconds (mean sampled reward: -5046.46). Current reward after update: -3103.43, Optimal reward -2838.08
Iteration 20 took 2.21 seconds (mean sampled reward: -5149.58). Current reward after update: -3499.58, Optimal reward -2838.08
Iteration 21 took 2.17 seconds (mean sampled reward: -5635.20). Current reward after update: -3363.46, Optimal reward -2838.08
Iteration 22 took 2.23 seconds (mean sampled reward: -4827.11). Current reward after update: -3015.29, Optimal reward -2838.08
Iteration 23 took 2.10 seconds (mean sampled reward: -4952.36). Current reward after update: -3088.51, Optimal reward -2838.08
Iteration 24 took 2.12 seconds (mean sampled reward: -4623.30). Current reward after update: -2918.79, Optimal reward -2838.08
Iteration 25 took 2.13 seconds (mean sampled reward: -4685.87). Current reward after update: -3141.41, Optimal reward -2838.08
Iteration 26 took 2.12 seconds (mean sampled reward: -4298.39). Current reward after update: -2872.83, Optimal reward -2838.08
Iteration 27 took 2.28 seconds (mean sampled reward: -4372.79). Current reward after update: -2858.28, Optimal reward -2838.08
Iteration 28 took 2.22 seconds (mean sampled reward: -4010.03). Current reward after update: -3416.15, Optimal reward -2838.08
Iteration 29 took 2.45 seconds (mean sampled reward: -4548.02). Current reward after update: -2526.15, Optimal reward -2526.15
Iteration 30 took 2.16 seconds (mean sampled reward: -4695.15). Current reward after update: -2674.53, Optimal reward -2526.15
Iteration 31 took 2.43 seconds (mean sampled reward: -4301.12). Current reward after update: -2934.21, Optimal reward -2526.15
Iteration 32 took 2.15 seconds (mean sampled reward: -4577.58). Current reward after update: -2452.20, Optimal reward -2452.20
Iteration 33 took 2.25 seconds (mean sampled reward: -4088.86). Current reward after update: -2311.00, Optimal reward -2311.00
Iteration 34 took 2.16 seconds (mean sampled reward: -5157.74). Current reward after update: -2289.90, Optimal reward -2289.90
Iteration 35 took 2.21 seconds (mean sampled reward: -3879.07). Current reward after update: -2182.21, Optimal reward -2182.21
Iteration 36 took 2.14 seconds (mean sampled reward: -5435.72). Current reward after update: -2260.39, Optimal reward -2182.21
Iteration 37 took 2.10 seconds (mean sampled reward: -5559.44). Current reward after update: -2280.08, Optimal reward -2182.21
Iteration 38 took 2.19 seconds (mean sampled reward: -4144.72). Current reward after update: -2731.14, Optimal reward -2182.21
Iteration 39 took 2.16 seconds (mean sampled reward: -4884.08). Current reward after update: -2359.21, Optimal reward -2182.21
Iteration 40 took 2.12 seconds (mean sampled reward: -5557.47). Current reward after update: -2164.08, Optimal reward -2164.08
Iteration 41 took 2.10 seconds (mean sampled reward: -5644.57). Current reward after update: -2309.87, Optimal reward -2164.08
Iteration 42 took 2.17 seconds (mean sampled reward: -4603.55). Current reward after update: -2216.48, Optimal reward -2164.08
Iteration 43 took 2.09 seconds (mean sampled reward: -5194.12). Current reward after update: -2127.58, Optimal reward -2127.58
Iteration 44 took 2.10 seconds (mean sampled reward: -3810.19). Current reward after update: -2152.53, Optimal reward -2127.58
Iteration 45 took 2.15 seconds (mean sampled reward: -3837.60). Current reward after update: -2086.43, Optimal reward -2086.43
Iteration 46 took 2.14 seconds (mean sampled reward: -3356.79). Current reward after update: -2202.71, Optimal reward -2086.43
Iteration 47 took 2.16 seconds (mean sampled reward: -3817.29). Current reward after update: -2163.44, Optimal reward -2086.43
Iteration 48 took 2.12 seconds (mean sampled reward: -3872.13). Current reward after update: -2119.08, Optimal reward -2086.43
Iteration 49 took 2.15 seconds (mean sampled reward: -3651.06). Current reward after update: -2163.73, Optimal reward -2086.43
Iteration 50 took 2.13 seconds (mean sampled reward: -3483.01). Current reward after update: -2190.54, Optimal reward -2086.43
Iteration 51 took 2.21 seconds (mean sampled reward: -3848.45). Current reward after update: -2233.06, Optimal reward -2086.43
Iteration 52 took 2.17 seconds (mean sampled reward: -3758.40). Current reward after update: -2161.19, Optimal reward -2086.43
Iteration 53 took 2.18 seconds (mean sampled reward: -5152.07). Current reward after update: -2838.57, Optimal reward -2086.43
Iteration 54 took 2.13 seconds (mean sampled reward: -4463.24). Current reward after update: -2176.70, Optimal reward -2086.43
Iteration 55 took 2.10 seconds (mean sampled reward: -4360.74). Current reward after update: -2100.57, Optimal reward -2086.43
Iteration 56 took 2.43 seconds (mean sampled reward: -3880.02). Current reward after update: -1992.60, Optimal reward -1992.60
Iteration 57 took 2.30 seconds (mean sampled reward: -3448.52). Current reward after update: -2509.72, Optimal reward -1992.60
Iteration 58 took 2.28 seconds (mean sampled reward: -3211.26). Current reward after update: -2056.44, Optimal reward -1992.60
Iteration 59 took 2.30 seconds (mean sampled reward: -4188.97). Current reward after update: -2210.63, Optimal reward -1992.60
Iteration 60 took 2.30 seconds (mean sampled reward: -3187.25). Current reward after update: -2069.58, Optimal reward -1992.60
Iteration 61 took 2.21 seconds (mean sampled reward: -3717.54). Current reward after update: -2086.23, Optimal reward -1992.60
Iteration 62 took 2.18 seconds (mean sampled reward: -3385.65). Current reward after update: -2010.31, Optimal reward -1992.60
Iteration 63 took 2.13 seconds (mean sampled reward: -3245.02). Current reward after update: -2048.59, Optimal reward -1992.60
Iteration 64 took 2.08 seconds (mean sampled reward: -3626.97). Current reward after update: -2091.45, Optimal reward -1992.60
Iteration 65 took 2.13 seconds (mean sampled reward: -4504.17). Current reward after update: -2037.97, Optimal reward -1992.60
Iteration 66 took 2.25 seconds (mean sampled reward: -3837.83). Current reward after update: -2240.92, Optimal reward -1992.60
Iteration 67 took 2.12 seconds (mean sampled reward: -4569.47). Current reward after update: -1989.35, Optimal reward -1989.35
Iteration 68 took 2.14 seconds (mean sampled reward: -4108.53). Current reward after update: -2009.41, Optimal reward -1989.35
Iteration 69 took 2.10 seconds (mean sampled reward: -5025.49). Current reward after update: -1962.57, Optimal reward -1962.57
Iteration 70 took 2.18 seconds (mean sampled reward: -4054.65). Current reward after update: -3814.59, Optimal reward -1962.57
Iteration 71 took 2.09 seconds (mean sampled reward: -3583.21). Current reward after update: -2478.66, Optimal reward -1962.57
Iteration 72 took 2.18 seconds (mean sampled reward: -3888.66). Current reward after update: -2027.32, Optimal reward -1962.57
Iteration 73 took 2.14 seconds (mean sampled reward: -3815.79). Current reward after update: -2224.33, Optimal reward -1962.57
Iteration 74 took 2.17 seconds (mean sampled reward: -3613.80). Current reward after update: -3008.35, Optimal reward -1962.57
Iteration 75 took 2.10 seconds (mean sampled reward: -4960.62). Current reward after update: -2053.81, Optimal reward -1962.57
Iteration 76 took 2.10 seconds (mean sampled reward: -4070.77). Current reward after update: -1965.95, Optimal reward -1962.57
Iteration 77 took 2.13 seconds (mean sampled reward: -4513.21). Current reward after update: -2142.20, Optimal reward -1962.57
Iteration 78 took 2.13 seconds (mean sampled reward: -3612.40). Current reward after update: -2061.22, Optimal reward -1962.57
Iteration 79 took 2.18 seconds (mean sampled reward: -3567.65). Current reward after update: -2051.48, Optimal reward -1962.57
Iteration 80 took 2.15 seconds (mean sampled reward: -3150.83). Current reward after update: -2101.88, Optimal reward -1962.57
Iteration 81 took 2.13 seconds (mean sampled reward: -3198.57). Current reward after update: -1967.07, Optimal reward -1962.57
Iteration 82 took 2.17 seconds (mean sampled reward: -3204.36). Current reward after update: -1941.95, Optimal reward -1941.95
Iteration 83 took 2.12 seconds (mean sampled reward: -3398.79). Current reward after update: -1960.43, Optimal reward -1941.95
Iteration 84 took 2.04 seconds (mean sampled reward: -4239.69). Current reward after update: -1923.08, Optimal reward -1923.08
Iteration 85 took 2.15 seconds (mean sampled reward: -3719.74). Current reward after update: -1918.90, Optimal reward -1918.90
Iteration 86 took 2.09 seconds (mean sampled reward: -4183.87). Current reward after update: -2022.13, Optimal reward -1918.90
Iteration 87 took 2.13 seconds (mean sampled reward: -3669.46). Current reward after update: -1927.79, Optimal reward -1918.90
Iteration 88 took 2.06 seconds (mean sampled reward: -4309.92). Current reward after update: -1870.50, Optimal reward -1870.50
Iteration 89 took 2.13 seconds (mean sampled reward: -2764.21). Current reward after update: -1868.27, Optimal reward -1868.27
Iteration 90 took 2.12 seconds (mean sampled reward: -2784.47). Current reward after update: -1918.50, Optimal reward -1868.27
Iteration 91 took 2.10 seconds (mean sampled reward: -3647.88). Current reward after update: -1908.49, Optimal reward -1868.27
Iteration 92 took 2.09 seconds (mean sampled reward: -3132.28). Current reward after update: -1870.89, Optimal reward -1868.27
Iteration 93 took 2.12 seconds (mean sampled reward: -3358.83). Current reward after update: -1975.83, Optimal reward -1868.27
Iteration 94 took 2.09 seconds (mean sampled reward: -3379.71). Current reward after update: -2002.64, Optimal reward -1868.27
Iteration 95 took 2.23 seconds (mean sampled reward: -3147.87). Current reward after update: -3123.26, Optimal reward -1868.27
Iteration 96 took 2.16 seconds (mean sampled reward: -3731.41). Current reward after update: -1961.58, Optimal reward -1868.27
Iteration 97 took 2.19 seconds (mean sampled reward: -5092.46). Current reward after update: -2045.44, Optimal reward -1868.27
Iteration 98 took 2.07 seconds (mean sampled reward: -4644.51). Current reward after update: -2025.89, Optimal reward -1868.27
Iteration 99 took 2.12 seconds (mean sampled reward: -3556.86). Current reward after update: -3560.41, Optimal reward -1868.27
Iteration 100 took 2.05 seconds (mean sampled reward: -3800.54). Current reward after update: -2654.53, Optimal reward -1868.27
Iteration 101 took 2.04 seconds (mean sampled reward: -4366.53). Current reward after update: -2751.45, Optimal reward -1868.27
Iteration 102 took 2.05 seconds (mean sampled reward: -4292.71). Current reward after update: -1972.03, Optimal reward -1868.27
Iteration 103 took 2.13 seconds (mean sampled reward: -3552.82). Current reward after update: -1936.46, Optimal reward -1868.27
Iteration 104 took 2.10 seconds (mean sampled reward: -3689.18). Current reward after update: -1882.30, Optimal reward -1868.27
Iteration 105 took 2.08 seconds (mean sampled reward: -3603.14). Current reward after update: -1899.74, Optimal reward -1868.27
Iteration 106 took 2.15 seconds (mean sampled reward: -4508.02). Current reward after update: -2064.96, Optimal reward -1868.27
Iteration 107 took 2.13 seconds (mean sampled reward: -3716.77). Current reward after update: -1969.12, Optimal reward -1868.27
Iteration 108 took 2.44 seconds (mean sampled reward: -3132.62). Current reward after update: -1986.87, Optimal reward -1868.27
Iteration 109 took 2.34 seconds (mean sampled reward: -3246.82). Current reward after update: -1882.13, Optimal reward -1868.27
Iteration 110 took 2.22 seconds (mean sampled reward: -3254.42). Current reward after update: -1855.67, Optimal reward -1855.67
Iteration 111 took 2.25 seconds (mean sampled reward: -3501.18). Current reward after update: -1880.48, Optimal reward -1855.67
Iteration 112 took 2.20 seconds (mean sampled reward: -3550.98). Current reward after update: -1887.35, Optimal reward -1855.67
Iteration 113 took 2.36 seconds (mean sampled reward: -3104.30). Current reward after update: -1769.91, Optimal reward -1769.91
Iteration 114 took 2.16 seconds (mean sampled reward: -3085.66). Current reward after update: -1755.90, Optimal reward -1755.90
Iteration 115 took 2.28 seconds (mean sampled reward: -2960.77). Current reward after update: -2004.07, Optimal reward -1755.90
Iteration 116 took 2.20 seconds (mean sampled reward: -3322.56). Current reward after update: -1849.59, Optimal reward -1755.90
Iteration 117 took 2.27 seconds (mean sampled reward: -3366.70). Current reward after update: -1884.95, Optimal reward -1755.90
Iteration 118 took 2.26 seconds (mean sampled reward: -2970.06). Current reward after update: -1832.98, Optimal reward -1755.90
Iteration 119 took 2.22 seconds (mean sampled reward: -3695.25). Current reward after update: -1900.24, Optimal reward -1755.90
Iteration 120 took 2.25 seconds (mean sampled reward: -4313.61). Current reward after update: -2008.71, Optimal reward -1755.90
Iteration 121 took 2.44 seconds (mean sampled reward: -3107.88). Current reward after update: -2478.94, Optimal reward -1755.90
Iteration 122 took 2.26 seconds (mean sampled reward: -2824.30). Current reward after update: -1844.14, Optimal reward -1755.90
Iteration 123 took 2.32 seconds (mean sampled reward: -2914.14). Current reward after update: -1843.50, Optimal reward -1755.90
Iteration 124 took 2.22 seconds (mean sampled reward: -2680.24). Current reward after update: -1857.61, Optimal reward -1755.90
Iteration 125 took 2.31 seconds (mean sampled reward: -2911.27). Current reward after update: -3110.03, Optimal reward -1755.90
Iteration 126 took 2.26 seconds (mean sampled reward: -2714.21). Current reward after update: -1780.59, Optimal reward -1755.90
Iteration 127 took 2.27 seconds (mean sampled reward: -3181.11). Current reward after update: -1820.45, Optimal reward -1755.90
Iteration 128 took 2.24 seconds (mean sampled reward: -2783.89). Current reward after update: -1885.38, Optimal reward -1755.90
Iteration 129 took 2.29 seconds (mean sampled reward: -4097.92). Current reward after update: -1904.03, Optimal reward -1755.90
Iteration 130 took 2.30 seconds (mean sampled reward: -4287.51). Current reward after update: -2119.45, Optimal reward -1755.90
Iteration 131 took 2.27 seconds (mean sampled reward: -2983.69). Current reward after update: -1886.03, Optimal reward -1755.90
Iteration 132 took 2.27 seconds (mean sampled reward: -3680.45). Current reward after update: -1757.34, Optimal reward -1755.90
Iteration 133 took 2.24 seconds (mean sampled reward: -2901.23). Current reward after update: -2540.67, Optimal reward -1755.90
Iteration 134 took 2.22 seconds (mean sampled reward: -3272.88). Current reward after update: -1803.15, Optimal reward -1755.90
Iteration 135 took 2.26 seconds (mean sampled reward: -2791.93). Current reward after update: -1803.05, Optimal reward -1755.90
Iteration 136 took 2.26 seconds (mean sampled reward: -2781.02). Current reward after update: -1669.31, Optimal reward -1669.31
Iteration 137 took 2.25 seconds (mean sampled reward: -2502.03). Current reward after update: -1766.82, Optimal reward -1669.31
Iteration 138 took 2.27 seconds (mean sampled reward: -2558.29). Current reward after update: -1792.71, Optimal reward -1669.31
Iteration 139 took 2.25 seconds (mean sampled reward: -2465.52). Current reward after update: -1961.24, Optimal reward -1669.31
Iteration 140 took 2.28 seconds (mean sampled reward: -3187.76). Current reward after update: -2517.87, Optimal reward -1669.31
Iteration 141 took 2.27 seconds (mean sampled reward: -3269.28). Current reward after update: -1771.94, Optimal reward -1669.31
Iteration 142 took 2.27 seconds (mean sampled reward: -3094.93). Current reward after update: -1806.75, Optimal reward -1669.31
Iteration 143 took 2.25 seconds (mean sampled reward: -3139.15). Current reward after update: -1709.47, Optimal reward -1669.31
Iteration 144 took 2.24 seconds (mean sampled reward: -2936.27). Current reward after update: -1642.51, Optimal reward -1642.51
Iteration 145 took 2.19 seconds (mean sampled reward: -2627.98). Current reward after update: -1868.36, Optimal reward -1642.51
Iteration 146 took 2.23 seconds (mean sampled reward: -2437.88). Current reward after update: -1548.48, Optimal reward -1548.48
Iteration 147 took 2.17 seconds (mean sampled reward: -2449.58). Current reward after update: -1634.78, Optimal reward -1548.48
Iteration 148 took 2.17 seconds (mean sampled reward: -2436.83). Current reward after update: -1525.39, Optimal reward -1525.39
Iteration 149 took 2.25 seconds (mean sampled reward: -2641.91). Current reward after update: -1581.16, Optimal reward -1525.39
Iteration 150 took 2.18 seconds (mean sampled reward: -2262.03). Current reward after update: -1456.73, Optimal reward -1456.73
Iteration 151 took 2.21 seconds (mean sampled reward: -2402.73). Current reward after update: -1363.64, Optimal reward -1363.64
Iteration 152 took 2.11 seconds (mean sampled reward: -2504.09). Current reward after update: -1178.09, Optimal reward -1178.09
Iteration 153 took 2.25 seconds (mean sampled reward: -2683.10). Current reward after update: -1363.67, Optimal reward -1178.09
Iteration 154 took 2.21 seconds (mean sampled reward: -2272.38). Current reward after update: -1167.56, Optimal reward -1167.56
Iteration 155 took 2.18 seconds (mean sampled reward: -2364.38). Current reward after update: -1169.05, Optimal reward -1167.56
Iteration 156 took 2.24 seconds (mean sampled reward: -2178.88). Current reward after update: -1003.63, Optimal reward -1003.63
Iteration 157 took 2.34 seconds (mean sampled reward: -2183.17). Current reward after update: -970.94, Optimal reward -970.94
Iteration 158 took 2.31 seconds (mean sampled reward: -2073.51). Current reward after update: -1468.96, Optimal reward -970.94
Iteration 159 took 2.26 seconds (mean sampled reward: -1993.56). Current reward after update: -969.01, Optimal reward -969.01
Iteration 160 took 2.29 seconds (mean sampled reward: -2174.52). Current reward after update: -959.27, Optimal reward -959.27
Iteration 161 took 2.25 seconds (mean sampled reward: -2224.39). Current reward after update: -1146.24, Optimal reward -959.27
Iteration 162 took 2.24 seconds (mean sampled reward: -2395.65). Current reward after update: -2200.95, Optimal reward -959.27
Iteration 163 took 2.25 seconds (mean sampled reward: -2502.94). Current reward after update: -1070.80, Optimal reward -959.27
Iteration 164 took 2.21 seconds (mean sampled reward: -2573.91). Current reward after update: -1099.12, Optimal reward -959.27
Iteration 165 took 2.27 seconds (mean sampled reward: -2603.64). Current reward after update: -935.28, Optimal reward -935.28
Iteration 166 took 2.29 seconds (mean sampled reward: -2210.10). Current reward after update: -991.06, Optimal reward -935.28
Iteration 167 took 2.42 seconds (mean sampled reward: -2212.15). Current reward after update: -963.03, Optimal reward -935.28
Iteration 168 took 2.36 seconds (mean sampled reward: -2173.30). Current reward after update: -999.50, Optimal reward -935.28
Iteration 169 took 2.31 seconds (mean sampled reward: -1956.00). Current reward after update: -924.77, Optimal reward -924.77
Iteration 170 took 2.29 seconds (mean sampled reward: -1856.37). Current reward after update: -870.80, Optimal reward -870.80
Iteration 171 took 2.28 seconds (mean sampled reward: -2753.58). Current reward after update: -948.05, Optimal reward -870.80
Iteration 172 took 2.26 seconds (mean sampled reward: -2201.44). Current reward after update: -1606.62, Optimal reward -870.80
Iteration 173 took 2.20 seconds (mean sampled reward: -3532.97). Current reward after update: -888.69, Optimal reward -870.80
Iteration 174 took 2.20 seconds (mean sampled reward: -4064.23). Current reward after update: -1658.62, Optimal reward -870.80
Iteration 175 took 2.27 seconds (mean sampled reward: -2149.87). Current reward after update: -2473.16, Optimal reward -870.80
Iteration 176 took 2.24 seconds (mean sampled reward: -1894.69). Current reward after update: -998.98, Optimal reward -870.80
Iteration 177 took 2.23 seconds (mean sampled reward: -1990.74). Current reward after update: -1679.78, Optimal reward -870.80
Iteration 178 took 2.23 seconds (mean sampled reward: -2355.97). Current reward after update: -1266.32, Optimal reward -870.80
Iteration 179 took 2.24 seconds (mean sampled reward: -2250.40). Current reward after update: -990.44, Optimal reward -870.80
Iteration 180 took 2.24 seconds (mean sampled reward: -2006.49). Current reward after update: -1066.79, Optimal reward -870.80
Iteration 181 took 2.26 seconds (mean sampled reward: -1931.90). Current reward after update: -2364.51, Optimal reward -870.80
Iteration 182 took 2.28 seconds (mean sampled reward: -2252.35). Current reward after update: -1055.63, Optimal reward -870.80
Iteration 183 took 2.27 seconds (mean sampled reward: -2222.88). Current reward after update: -1196.95, Optimal reward -870.80
Iteration 184 took 2.29 seconds (mean sampled reward: -2179.63). Current reward after update: -995.43, Optimal reward -870.80
Iteration 185 took 2.27 seconds (mean sampled reward: -2202.18). Current reward after update: -1037.27, Optimal reward -870.80
Iteration 186 took 2.28 seconds (mean sampled reward: -2522.45). Current reward after update: -1050.74, Optimal reward -870.80
Iteration 187 took 2.22 seconds (mean sampled reward: -2403.37). Current reward after update: -1087.25, Optimal reward -870.80
Iteration 188 took 2.25 seconds (mean sampled reward: -3061.52). Current reward after update: -1045.67, Optimal reward -870.80
Iteration 189 took 2.20 seconds (mean sampled reward: -3722.43). Current reward after update: -976.54, Optimal reward -870.80
Iteration 190 took 2.22 seconds (mean sampled reward: -2766.87). Current reward after update: -996.03, Optimal reward -870.80
Iteration 191 took 2.25 seconds (mean sampled reward: -2019.71). Current reward after update: -1852.14, Optimal reward -870.80
Iteration 192 took 2.18 seconds (mean sampled reward: -2966.82). Current reward after update: -927.28, Optimal reward -870.80
Iteration 193 took 2.23 seconds (mean sampled reward: -2370.62). Current reward after update: -957.96, Optimal reward -870.80
Iteration 194 took 2.02 seconds (mean sampled reward: -4960.68). Current reward after update: -887.31, Optimal reward -870.80
Iteration 195 took 2.03 seconds (mean sampled reward: -4897.99). Current reward after update: -909.54, Optimal reward -870.80
Iteration 196 took 2.21 seconds (mean sampled reward: -4039.29). Current reward after update: -975.56, Optimal reward -870.80
Iteration 197 took 2.20 seconds (mean sampled reward: -2420.31). Current reward after update: -891.57, Optimal reward -870.80
Iteration 198 took 2.11 seconds (mean sampled reward: -4215.58). Current reward after update: -997.17, Optimal reward -870.80
Iteration 199 took 2.13 seconds (mean sampled reward: -3526.71). Current reward after update: -917.68, Optimal reward -870.80
Iteration 200 took 2.02 seconds (mean sampled reward: -5094.01). Current reward after update: -970.05, Optimal reward -870.80
Max force: 10 Sigma: 0.4 mean rewards: -707.7147331953962, best rewards:-615.9797855681525

Iteration 1 took 2.24 seconds (mean sampled reward: -7466.39). Current reward after update: -5858.72, Optimal reward -5858.72
Iteration 2 took 2.27 seconds (mean sampled reward: -6989.06). Current reward after update: -5108.35, Optimal reward -5108.35
Iteration 3 took 2.10 seconds (mean sampled reward: -7037.54). Current reward after update: -4394.13, Optimal reward -4394.13
Iteration 4 took 2.19 seconds (mean sampled reward: -7029.51). Current reward after update: -3654.70, Optimal reward -3654.70
Iteration 5 took 2.22 seconds (mean sampled reward: -6713.27). Current reward after update: -3324.76, Optimal reward -3324.76
Iteration 6 took 2.20 seconds (mean sampled reward: -6338.96). Current reward after update: -3197.29, Optimal reward -3197.29
Iteration 7 took 2.02 seconds (mean sampled reward: -6334.66). Current reward after update: -2973.12, Optimal reward -2973.12
Iteration 8 took 2.01 seconds (mean sampled reward: -6104.64). Current reward after update: -3118.44, Optimal reward -2973.12
Iteration 9 took 2.17 seconds (mean sampled reward: -6137.79). Current reward after update: -2963.85, Optimal reward -2963.85
Iteration 10 took 1.99 seconds (mean sampled reward: -5843.21). Current reward after update: -2791.65, Optimal reward -2791.65
Iteration 11 took 2.16 seconds (mean sampled reward: -6922.81). Current reward after update: -2816.49, Optimal reward -2791.65
Iteration 12 took 2.18 seconds (mean sampled reward: -6605.38). Current reward after update: -2396.26, Optimal reward -2396.26
Iteration 13 took 2.16 seconds (mean sampled reward: -5968.21). Current reward after update: -2516.87, Optimal reward -2396.26
Iteration 14 took 2.07 seconds (mean sampled reward: -5714.24). Current reward after update: -2348.71, Optimal reward -2348.71
Iteration 15 took 2.01 seconds (mean sampled reward: -6147.78). Current reward after update: -2159.62, Optimal reward -2159.62
Iteration 16 took 2.05 seconds (mean sampled reward: -6404.01). Current reward after update: -2179.37, Optimal reward -2159.62
Iteration 17 took 1.98 seconds (mean sampled reward: -4942.83). Current reward after update: -1925.05, Optimal reward -1925.05
Iteration 18 took 1.93 seconds (mean sampled reward: -4991.74). Current reward after update: -1840.33, Optimal reward -1840.33
Iteration 19 took 2.01 seconds (mean sampled reward: -4593.98). Current reward after update: -1534.12, Optimal reward -1534.12
Iteration 20 took 1.94 seconds (mean sampled reward: -5279.58). Current reward after update: -1683.44, Optimal reward -1534.12
Iteration 21 took 2.17 seconds (mean sampled reward: -4038.84). Current reward after update: -3404.92, Optimal reward -1534.12
Iteration 22 took 2.14 seconds (mean sampled reward: -3917.39). Current reward after update: -1486.93, Optimal reward -1486.93
Iteration 23 took 1.96 seconds (mean sampled reward: -3830.75). Current reward after update: -1363.28, Optimal reward -1363.28
Iteration 24 took 1.87 seconds (mean sampled reward: -4973.02). Current reward after update: -1280.58, Optimal reward -1280.58
Iteration 25 took 1.97 seconds (mean sampled reward: -4237.55). Current reward after update: -1418.60, Optimal reward -1280.58
Iteration 26 took 1.98 seconds (mean sampled reward: -3772.63). Current reward after update: -1302.44, Optimal reward -1280.58
Iteration 27 took 1.94 seconds (mean sampled reward: -4708.69). Current reward after update: -1464.02, Optimal reward -1280.58
Iteration 28 took 2.06 seconds (mean sampled reward: -4450.30). Current reward after update: -1496.95, Optimal reward -1280.58
Iteration 29 took 2.10 seconds (mean sampled reward: -5219.21). Current reward after update: -1564.53, Optimal reward -1280.58
Iteration 30 took 1.96 seconds (mean sampled reward: -5285.52). Current reward after update: -1130.85, Optimal reward -1130.85
Iteration 31 took 2.05 seconds (mean sampled reward: -5878.88). Current reward after update: -1625.41, Optimal reward -1130.85
Iteration 32 took 2.00 seconds (mean sampled reward: -5693.08). Current reward after update: -1654.64, Optimal reward -1130.85
Iteration 33 took 1.94 seconds (mean sampled reward: -4246.78). Current reward after update: -1410.13, Optimal reward -1130.85
Iteration 34 took 1.98 seconds (mean sampled reward: -5371.87). Current reward after update: -1476.09, Optimal reward -1130.85
Iteration 35 took 2.06 seconds (mean sampled reward: -4943.14). Current reward after update: -1632.66, Optimal reward -1130.85
Iteration 36 took 1.98 seconds (mean sampled reward: -4317.93). Current reward after update: -1835.91, Optimal reward -1130.85
Iteration 37 took 2.01 seconds (mean sampled reward: -4037.93). Current reward after update: -1844.55, Optimal reward -1130.85
Iteration 38 took 2.00 seconds (mean sampled reward: -5005.10). Current reward after update: -1667.97, Optimal reward -1130.85
Iteration 39 took 2.08 seconds (mean sampled reward: -4820.60). Current reward after update: -3201.18, Optimal reward -1130.85
Iteration 40 took 1.93 seconds (mean sampled reward: -4833.89). Current reward after update: -1595.17, Optimal reward -1130.85
Iteration 41 took 1.90 seconds (mean sampled reward: -5051.86). Current reward after update: -1507.28, Optimal reward -1130.85
Iteration 42 took 1.97 seconds (mean sampled reward: -3528.45). Current reward after update: -1479.23, Optimal reward -1130.85
Iteration 43 took 1.96 seconds (mean sampled reward: -3983.75). Current reward after update: -1413.64, Optimal reward -1130.85
Iteration 44 took 1.89 seconds (mean sampled reward: -5731.58). Current reward after update: -1178.13, Optimal reward -1130.85
Iteration 45 took 1.91 seconds (mean sampled reward: -5776.69). Current reward after update: -1460.27, Optimal reward -1130.85
Iteration 46 took 1.88 seconds (mean sampled reward: -5032.28). Current reward after update: -1597.59, Optimal reward -1130.85
Iteration 47 took 1.93 seconds (mean sampled reward: -4906.21). Current reward after update: -1582.45, Optimal reward -1130.85
Iteration 48 took 1.89 seconds (mean sampled reward: -5898.82). Current reward after update: -1989.63, Optimal reward -1130.85
Iteration 49 took 1.94 seconds (mean sampled reward: -4937.25). Current reward after update: -1428.09, Optimal reward -1130.85
Iteration 50 took 1.86 seconds (mean sampled reward: -5519.33). Current reward after update: -1732.46, Optimal reward -1130.85
Iteration 51 took 1.92 seconds (mean sampled reward: -4187.94). Current reward after update: -1463.37, Optimal reward -1130.85
Iteration 52 took 2.00 seconds (mean sampled reward: -3159.14). Current reward after update: -1635.59, Optimal reward -1130.85
Iteration 53 took 2.13 seconds (mean sampled reward: -3205.86). Current reward after update: -2633.97, Optimal reward -1130.85
Iteration 54 took 1.96 seconds (mean sampled reward: -3453.54). Current reward after update: -1549.00, Optimal reward -1130.85
Iteration 55 took 2.17 seconds (mean sampled reward: -4454.29). Current reward after update: -1647.12, Optimal reward -1130.85
Iteration 56 took 2.08 seconds (mean sampled reward: -4984.09). Current reward after update: -1911.16, Optimal reward -1130.85
Iteration 57 took 2.12 seconds (mean sampled reward: -3900.47). Current reward after update: -1754.61, Optimal reward -1130.85
Iteration 58 took 2.02 seconds (mean sampled reward: -3621.12). Current reward after update: -1679.17, Optimal reward -1130.85
Iteration 59 took 1.96 seconds (mean sampled reward: -4153.27). Current reward after update: -1519.23, Optimal reward -1130.85
Iteration 60 took 1.98 seconds (mean sampled reward: -3581.66). Current reward after update: -1631.96, Optimal reward -1130.85
Iteration 61 took 1.99 seconds (mean sampled reward: -4317.65). Current reward after update: -1643.90, Optimal reward -1130.85
Iteration 62 took 1.99 seconds (mean sampled reward: -4025.28). Current reward after update: -2098.26, Optimal reward -1130.85
Iteration 63 took 1.96 seconds (mean sampled reward: -4379.01). Current reward after update: -1824.12, Optimal reward -1130.85
Iteration 64 took 2.06 seconds (mean sampled reward: -3504.28). Current reward after update: -1679.88, Optimal reward -1130.85
Iteration 65 took 2.02 seconds (mean sampled reward: -3769.11). Current reward after update: -2047.69, Optimal reward -1130.85
Iteration 66 took 2.04 seconds (mean sampled reward: -3312.42). Current reward after update: -1609.19, Optimal reward -1130.85
Iteration 67 took 2.02 seconds (mean sampled reward: -3531.08). Current reward after update: -1733.12, Optimal reward -1130.85
Iteration 68 took 2.04 seconds (mean sampled reward: -4068.91). Current reward after update: -2796.14, Optimal reward -1130.85
Iteration 69 took 2.04 seconds (mean sampled reward: -3644.23). Current reward after update: -5914.38, Optimal reward -1130.85
Iteration 70 took 2.01 seconds (mean sampled reward: -3982.49). Current reward after update: -1461.22, Optimal reward -1130.85
Iteration 71 took 2.03 seconds (mean sampled reward: -4332.89). Current reward after update: -1550.17, Optimal reward -1130.85
Iteration 72 took 2.00 seconds (mean sampled reward: -3177.26). Current reward after update: -1715.75, Optimal reward -1130.85
Iteration 73 took 2.01 seconds (mean sampled reward: -3205.50). Current reward after update: -1577.21, Optimal reward -1130.85
Iteration 74 took 2.08 seconds (mean sampled reward: -3351.65). Current reward after update: -1739.15, Optimal reward -1130.85
Iteration 75 took 1.98 seconds (mean sampled reward: -3363.73). Current reward after update: -1495.80, Optimal reward -1130.85
Iteration 76 took 2.04 seconds (mean sampled reward: -3267.03). Current reward after update: -2688.23, Optimal reward -1130.85
Iteration 77 took 1.99 seconds (mean sampled reward: -3519.80). Current reward after update: -1623.14, Optimal reward -1130.85
Iteration 78 took 2.06 seconds (mean sampled reward: -3321.05). Current reward after update: -1591.98, Optimal reward -1130.85
Iteration 79 took 2.04 seconds (mean sampled reward: -2976.94). Current reward after update: -2054.90, Optimal reward -1130.85
Iteration 80 took 1.92 seconds (mean sampled reward: -4285.53). Current reward after update: -1680.28, Optimal reward -1130.85
Iteration 81 took 2.05 seconds (mean sampled reward: -2777.48). Current reward after update: -1662.17, Optimal reward -1130.85
Iteration 82 took 1.98 seconds (mean sampled reward: -2850.73). Current reward after update: -1974.31, Optimal reward -1130.85
Iteration 83 took 2.01 seconds (mean sampled reward: -3612.76). Current reward after update: -1860.39, Optimal reward -1130.85
Iteration 84 took 2.04 seconds (mean sampled reward: -3531.56). Current reward after update: -5891.33, Optimal reward -1130.85
Iteration 85 took 2.05 seconds (mean sampled reward: -3444.72). Current reward after update: -1468.09, Optimal reward -1130.85
Iteration 86 took 2.04 seconds (mean sampled reward: -2799.65). Current reward after update: -6072.21, Optimal reward -1130.85
Iteration 87 took 2.01 seconds (mean sampled reward: -3045.67). Current reward after update: -5203.03, Optimal reward -1130.85
Iteration 88 took 2.00 seconds (mean sampled reward: -3300.38). Current reward after update: -1570.36, Optimal reward -1130.85
Iteration 89 took 2.00 seconds (mean sampled reward: -3150.39). Current reward after update: -1437.68, Optimal reward -1130.85
Iteration 90 took 2.02 seconds (mean sampled reward: -3007.52). Current reward after update: -1454.16, Optimal reward -1130.85
Iteration 91 took 2.03 seconds (mean sampled reward: -2784.63). Current reward after update: -1488.60, Optimal reward -1130.85
Iteration 92 took 1.96 seconds (mean sampled reward: -4106.51). Current reward after update: -1546.03, Optimal reward -1130.85
Iteration 93 took 1.94 seconds (mean sampled reward: -4809.27). Current reward after update: -1697.10, Optimal reward -1130.85
Iteration 94 took 1.92 seconds (mean sampled reward: -4522.74). Current reward after update: -1514.02, Optimal reward -1130.85
Iteration 95 took 1.86 seconds (mean sampled reward: -5386.40). Current reward after update: -1824.30, Optimal reward -1130.85
Iteration 96 took 1.91 seconds (mean sampled reward: -4580.15). Current reward after update: -1768.89, Optimal reward -1130.85
Iteration 97 took 2.06 seconds (mean sampled reward: -3886.54). Current reward after update: -1773.83, Optimal reward -1130.85
Iteration 98 took 1.96 seconds (mean sampled reward: -4926.18). Current reward after update: -1846.83, Optimal reward -1130.85
Iteration 99 took 1.89 seconds (mean sampled reward: -5200.83). Current reward after update: -1799.49, Optimal reward -1130.85
Iteration 100 took 1.97 seconds (mean sampled reward: -4341.43). Current reward after update: -1757.21, Optimal reward -1130.85
Iteration 101 took 2.04 seconds (mean sampled reward: -3714.26). Current reward after update: -1749.87, Optimal reward -1130.85
Iteration 102 took 2.01 seconds (mean sampled reward: -3854.87). Current reward after update: -2101.24, Optimal reward -1130.85
Iteration 103 took 1.91 seconds (mean sampled reward: -5505.68). Current reward after update: -1878.23, Optimal reward -1130.85
Iteration 104 took 1.99 seconds (mean sampled reward: -5243.17). Current reward after update: -1850.69, Optimal reward -1130.85
Iteration 105 took 1.97 seconds (mean sampled reward: -4160.72). Current reward after update: -1664.84, Optimal reward -1130.85
Iteration 106 took 2.22 seconds (mean sampled reward: -3054.53). Current reward after update: -1676.31, Optimal reward -1130.85
Iteration 107 took 2.20 seconds (mean sampled reward: -4230.59). Current reward after update: -1643.49, Optimal reward -1130.85
Iteration 108 took 2.06 seconds (mean sampled reward: -4720.27). Current reward after update: -1802.62, Optimal reward -1130.85
Iteration 109 took 2.18 seconds (mean sampled reward: -4268.44). Current reward after update: -1590.60, Optimal reward -1130.85
Iteration 110 took 2.21 seconds (mean sampled reward: -3269.93). Current reward after update: -2295.03, Optimal reward -1130.85
Iteration 111 took 2.16 seconds (mean sampled reward: -3775.16). Current reward after update: -1952.40, Optimal reward -1130.85
Iteration 112 took 2.23 seconds (mean sampled reward: -3173.46). Current reward after update: -1735.72, Optimal reward -1130.85
Iteration 113 took 2.11 seconds (mean sampled reward: -3331.20). Current reward after update: -2378.13, Optimal reward -1130.85
Iteration 114 took 2.12 seconds (mean sampled reward: -2902.41). Current reward after update: -1639.97, Optimal reward -1130.85
Iteration 115 took 2.30 seconds (mean sampled reward: -2695.28). Current reward after update: -1663.68, Optimal reward -1130.85
Iteration 116 took 2.06 seconds (mean sampled reward: -2936.03). Current reward after update: -2404.46, Optimal reward -1130.85
Iteration 117 took 2.11 seconds (mean sampled reward: -3147.32). Current reward after update: -1765.80, Optimal reward -1130.85
Iteration 118 took 2.13 seconds (mean sampled reward: -2957.78). Current reward after update: -1774.45, Optimal reward -1130.85
Iteration 119 took 2.13 seconds (mean sampled reward: -2983.78). Current reward after update: -1898.50, Optimal reward -1130.85
Iteration 120 took 2.15 seconds (mean sampled reward: -3035.88). Current reward after update: -2558.36, Optimal reward -1130.85
Iteration 121 took 2.15 seconds (mean sampled reward: -3322.06). Current reward after update: -1983.53, Optimal reward -1130.85
Iteration 122 took 2.16 seconds (mean sampled reward: -3189.28). Current reward after update: -1757.10, Optimal reward -1130.85
Iteration 123 took 2.14 seconds (mean sampled reward: -3233.91). Current reward after update: -1676.56, Optimal reward -1130.85
Iteration 124 took 2.16 seconds (mean sampled reward: -3031.93). Current reward after update: -1832.74, Optimal reward -1130.85
Iteration 125 took 2.12 seconds (mean sampled reward: -3765.98). Current reward after update: -2307.29, Optimal reward -1130.85
Iteration 126 took 2.09 seconds (mean sampled reward: -3595.03). Current reward after update: -1787.55, Optimal reward -1130.85
Iteration 127 took 2.06 seconds (mean sampled reward: -3280.65). Current reward after update: -1971.25, Optimal reward -1130.85
Iteration 128 took 2.05 seconds (mean sampled reward: -3882.72). Current reward after update: -1573.63, Optimal reward -1130.85
Iteration 129 took 2.01 seconds (mean sampled reward: -3700.83). Current reward after update: -2013.12, Optimal reward -1130.85
Iteration 130 took 2.04 seconds (mean sampled reward: -3815.44). Current reward after update: -1783.19, Optimal reward -1130.85
Iteration 131 took 2.06 seconds (mean sampled reward: -3897.84). Current reward after update: -2183.90, Optimal reward -1130.85
Iteration 132 took 2.07 seconds (mean sampled reward: -3769.34). Current reward after update: -2402.13, Optimal reward -1130.85
Iteration 133 took 2.14 seconds (mean sampled reward: -2978.96). Current reward after update: -1607.90, Optimal reward -1130.85
Iteration 134 took 2.15 seconds (mean sampled reward: -3317.81). Current reward after update: -1708.65, Optimal reward -1130.85
Iteration 135 took 2.05 seconds (mean sampled reward: -4257.65). Current reward after update: -1666.69, Optimal reward -1130.85
Iteration 136 took 2.08 seconds (mean sampled reward: -4739.39). Current reward after update: -1630.19, Optimal reward -1130.85
Iteration 137 took 1.95 seconds (mean sampled reward: -5113.02). Current reward after update: -1441.14, Optimal reward -1130.85
Iteration 138 took 1.95 seconds (mean sampled reward: -5389.25). Current reward after update: -1610.83, Optimal reward -1130.85
Iteration 139 took 2.05 seconds (mean sampled reward: -4625.03). Current reward after update: -1860.53, Optimal reward -1130.85
Iteration 140 took 2.04 seconds (mean sampled reward: -4369.56). Current reward after update: -1586.66, Optimal reward -1130.85
Iteration 141 took 1.99 seconds (mean sampled reward: -4756.71). Current reward after update: -1460.97, Optimal reward -1130.85
Iteration 142 took 1.90 seconds (mean sampled reward: -5404.35). Current reward after update: -1336.54, Optimal reward -1130.85
Iteration 143 took 2.00 seconds (mean sampled reward: -5293.31). Current reward after update: -1458.28, Optimal reward -1130.85
Iteration 144 took 1.92 seconds (mean sampled reward: -5363.15). Current reward after update: -1552.19, Optimal reward -1130.85
Iteration 145 took 2.12 seconds (mean sampled reward: -3458.41). Current reward after update: -1561.45, Optimal reward -1130.85
Iteration 146 took 1.99 seconds (mean sampled reward: -4342.91). Current reward after update: -1611.87, Optimal reward -1130.85
Iteration 147 took 1.99 seconds (mean sampled reward: -4609.22). Current reward after update: -3010.32, Optimal reward -1130.85
Iteration 148 took 2.10 seconds (mean sampled reward: -3571.11). Current reward after update: -1421.73, Optimal reward -1130.85
Iteration 149 took 2.14 seconds (mean sampled reward: -3703.93). Current reward after update: -1501.53, Optimal reward -1130.85
Iteration 150 took 2.05 seconds (mean sampled reward: -3977.70). Current reward after update: -1756.28, Optimal reward -1130.85
Iteration 151 took 2.00 seconds (mean sampled reward: -5266.80). Current reward after update: -1254.32, Optimal reward -1130.85
Iteration 152 took 1.94 seconds (mean sampled reward: -4838.40). Current reward after update: -1550.68, Optimal reward -1130.85
Iteration 153 took 2.02 seconds (mean sampled reward: -4101.86). Current reward after update: -1399.47, Optimal reward -1130.85
Iteration 154 took 2.01 seconds (mean sampled reward: -4681.97). Current reward after update: -1422.80, Optimal reward -1130.85
Iteration 155 took 2.22 seconds (mean sampled reward: -3908.30). Current reward after update: -1548.64, Optimal reward -1130.85
Iteration 156 took 2.14 seconds (mean sampled reward: -3775.58). Current reward after update: -1569.39, Optimal reward -1130.85
Iteration 157 took 2.02 seconds (mean sampled reward: -4176.98). Current reward after update: -1261.82, Optimal reward -1130.85
Iteration 158 took 2.04 seconds (mean sampled reward: -4387.88). Current reward after update: -1555.99, Optimal reward -1130.85
Iteration 159 took 1.97 seconds (mean sampled reward: -4927.71). Current reward after update: -1312.20, Optimal reward -1130.85
Iteration 160 took 1.96 seconds (mean sampled reward: -4905.47). Current reward after update: -1551.76, Optimal reward -1130.85
Iteration 161 took 1.92 seconds (mean sampled reward: -5627.36). Current reward after update: -1317.87, Optimal reward -1130.85
Iteration 162 took 1.89 seconds (mean sampled reward: -5982.57). Current reward after update: -1409.33, Optimal reward -1130.85
Iteration 163 took 1.93 seconds (mean sampled reward: -5865.16). Current reward after update: -1277.46, Optimal reward -1130.85
Iteration 164 took 1.94 seconds (mean sampled reward: -5779.45). Current reward after update: -2894.39, Optimal reward -1130.85
Iteration 165 took 1.92 seconds (mean sampled reward: -5540.73). Current reward after update: -1909.61, Optimal reward -1130.85
Iteration 166 took 2.02 seconds (mean sampled reward: -4500.30). Current reward after update: -1296.45, Optimal reward -1130.85
Iteration 167 took 2.02 seconds (mean sampled reward: -3954.12). Current reward after update: -1276.35, Optimal reward -1130.85
Iteration 168 took 2.07 seconds (mean sampled reward: -4679.69). Current reward after update: -1309.72, Optimal reward -1130.85
Iteration 169 took 1.89 seconds (mean sampled reward: -5339.29). Current reward after update: -1257.47, Optimal reward -1130.85
Iteration 170 took 1.81 seconds (mean sampled reward: -6356.57). Current reward after update: -1349.30, Optimal reward -1130.85
Iteration 171 took 1.90 seconds (mean sampled reward: -5139.89). Current reward after update: -2183.87, Optimal reward -1130.85
Iteration 172 took 1.98 seconds (mean sampled reward: -4813.29). Current reward after update: -1238.30, Optimal reward -1130.85
Iteration 173 took 2.02 seconds (mean sampled reward: -4944.89). Current reward after update: -2592.67, Optimal reward -1130.85
Iteration 174 took 1.99 seconds (mean sampled reward: -4641.74). Current reward after update: -1323.93, Optimal reward -1130.85
Iteration 175 took 2.03 seconds (mean sampled reward: -3883.42). Current reward after update: -1443.57, Optimal reward -1130.85
Iteration 176 took 2.08 seconds (mean sampled reward: -4441.74). Current reward after update: -1253.23, Optimal reward -1130.85
Iteration 177 took 1.93 seconds (mean sampled reward: -4554.58). Current reward after update: -1637.64, Optimal reward -1130.85
Iteration 178 took 2.01 seconds (mean sampled reward: -4626.43). Current reward after update: -1319.65, Optimal reward -1130.85
Iteration 179 took 2.04 seconds (mean sampled reward: -4196.51). Current reward after update: -5771.80, Optimal reward -1130.85
Iteration 180 took 2.01 seconds (mean sampled reward: -4066.65). Current reward after update: -1231.14, Optimal reward -1130.85
Iteration 181 took 1.95 seconds (mean sampled reward: -4358.79). Current reward after update: -1289.25, Optimal reward -1130.85
Iteration 182 took 2.02 seconds (mean sampled reward: -3879.12). Current reward after update: -1213.70, Optimal reward -1130.85
Iteration 183 took 1.98 seconds (mean sampled reward: -4416.58). Current reward after update: -1626.86, Optimal reward -1130.85
Iteration 184 took 1.93 seconds (mean sampled reward: -4580.24). Current reward after update: -2520.77, Optimal reward -1130.85
Iteration 185 took 1.98 seconds (mean sampled reward: -4199.07). Current reward after update: -1288.10, Optimal reward -1130.85
Iteration 186 took 2.03 seconds (mean sampled reward: -4026.11). Current reward after update: -1273.83, Optimal reward -1130.85
Iteration 187 took 2.01 seconds (mean sampled reward: -4245.98). Current reward after update: -1336.45, Optimal reward -1130.85
Iteration 188 took 2.03 seconds (mean sampled reward: -3755.46). Current reward after update: -1262.23, Optimal reward -1130.85
Iteration 189 took 2.04 seconds (mean sampled reward: -3801.91). Current reward after update: -2529.19, Optimal reward -1130.85
Iteration 190 took 2.10 seconds (mean sampled reward: -3568.02). Current reward after update: -1380.73, Optimal reward -1130.85
Iteration 191 took 1.99 seconds (mean sampled reward: -4095.34). Current reward after update: -2826.88, Optimal reward -1130.85
Iteration 192 took 2.06 seconds (mean sampled reward: -3595.92). Current reward after update: -1272.31, Optimal reward -1130.85
Iteration 193 took 2.08 seconds (mean sampled reward: -3399.98). Current reward after update: -1361.58, Optimal reward -1130.85
Iteration 194 took 2.09 seconds (mean sampled reward: -3558.55). Current reward after update: -1263.89, Optimal reward -1130.85
Iteration 195 took 1.98 seconds (mean sampled reward: -4253.05). Current reward after update: -2705.00, Optimal reward -1130.85
Iteration 196 took 2.06 seconds (mean sampled reward: -3531.49). Current reward after update: -2144.39, Optimal reward -1130.85
Iteration 197 took 2.05 seconds (mean sampled reward: -3594.79). Current reward after update: -2449.19, Optimal reward -1130.85
Iteration 198 took 2.16 seconds (mean sampled reward: -3040.25). Current reward after update: -1237.03, Optimal reward -1130.85
Iteration 199 took 2.16 seconds (mean sampled reward: -2484.41). Current reward after update: -1272.00, Optimal reward -1130.85
Iteration 200 took 2.09 seconds (mean sampled reward: -2544.06). Current reward after update: -2492.13, Optimal reward -1130.85
Iteration 1 took 2.29 seconds (mean sampled reward: -7503.34). Current reward after update: -5391.20, Optimal reward -5391.20
Iteration 2 took 2.22 seconds (mean sampled reward: -6947.13). Current reward after update: -3554.03, Optimal reward -3554.03
Iteration 3 took 2.21 seconds (mean sampled reward: -6364.71). Current reward after update: -3415.07, Optimal reward -3415.07
Iteration 4 took 2.12 seconds (mean sampled reward: -6255.97). Current reward after update: -3393.78, Optimal reward -3393.78
Iteration 5 took 2.31 seconds (mean sampled reward: -6100.88). Current reward after update: -3350.43, Optimal reward -3350.43
Iteration 6 took 2.22 seconds (mean sampled reward: -6098.02). Current reward after update: -3231.63, Optimal reward -3231.63
Iteration 7 took 2.01 seconds (mean sampled reward: -5177.81). Current reward after update: -2525.18, Optimal reward -2525.18
Iteration 8 took 2.10 seconds (mean sampled reward: -5543.96). Current reward after update: -2134.59, Optimal reward -2134.59
Iteration 9 took 1.99 seconds (mean sampled reward: -5927.72). Current reward after update: -2306.28, Optimal reward -2134.59
Iteration 10 took 2.17 seconds (mean sampled reward: -5316.47). Current reward after update: -2959.47, Optimal reward -2134.59
Iteration 11 took 2.28 seconds (mean sampled reward: -4376.13). Current reward after update: -2363.04, Optimal reward -2134.59
Iteration 12 took 2.19 seconds (mean sampled reward: -4731.81). Current reward after update: -2222.22, Optimal reward -2134.59
Iteration 13 took 2.15 seconds (mean sampled reward: -5478.65). Current reward after update: -2346.80, Optimal reward -2134.59
Iteration 14 took 2.22 seconds (mean sampled reward: -5222.36). Current reward after update: -2226.09, Optimal reward -2134.59
Iteration 15 took 2.13 seconds (mean sampled reward: -5419.51). Current reward after update: -2028.79, Optimal reward -2028.79
Iteration 16 took 2.29 seconds (mean sampled reward: -3941.89). Current reward after update: -2054.44, Optimal reward -2028.79
Iteration 17 took 2.09 seconds (mean sampled reward: -4700.57). Current reward after update: -2891.88, Optimal reward -2028.79
Iteration 18 took 2.11 seconds (mean sampled reward: -4543.90). Current reward after update: -2057.87, Optimal reward -2028.79
Iteration 19 took 2.05 seconds (mean sampled reward: -4890.47). Current reward after update: -2177.42, Optimal reward -2028.79
Iteration 20 took 2.11 seconds (mean sampled reward: -4774.27). Current reward after update: -1983.96, Optimal reward -1983.96
Iteration 21 took 2.19 seconds (mean sampled reward: -5028.04). Current reward after update: -2146.10, Optimal reward -1983.96
Iteration 22 took 2.16 seconds (mean sampled reward: -4689.05). Current reward after update: -2197.54, Optimal reward -1983.96
Iteration 23 took 2.16 seconds (mean sampled reward: -4649.52). Current reward after update: -1886.12, Optimal reward -1886.12
Iteration 24 took 1.95 seconds (mean sampled reward: -5570.51). Current reward after update: -1741.68, Optimal reward -1741.68
Iteration 25 took 2.06 seconds (mean sampled reward: -5641.73). Current reward after update: -1628.99, Optimal reward -1628.99
Iteration 26 took 1.86 seconds (mean sampled reward: -6023.44). Current reward after update: -1630.18, Optimal reward -1628.99
Iteration 27 took 1.95 seconds (mean sampled reward: -5633.65). Current reward after update: -1662.55, Optimal reward -1628.99
Iteration 28 took 2.17 seconds (mean sampled reward: -4892.44). Current reward after update: -3762.00, Optimal reward -1628.99
Iteration 29 took 2.10 seconds (mean sampled reward: -3941.16). Current reward after update: -1542.02, Optimal reward -1542.02
Iteration 30 took 2.09 seconds (mean sampled reward: -3870.63). Current reward after update: -1356.28, Optimal reward -1356.28
Iteration 31 took 2.20 seconds (mean sampled reward: -4415.19). Current reward after update: -1588.92, Optimal reward -1356.28
Iteration 32 took 2.03 seconds (mean sampled reward: -4194.04). Current reward after update: -1549.10, Optimal reward -1356.28
Iteration 33 took 2.06 seconds (mean sampled reward: -3385.24). Current reward after update: -1407.71, Optimal reward -1356.28
Iteration 34 took 2.02 seconds (mean sampled reward: -4027.88). Current reward after update: -1369.78, Optimal reward -1356.28
Iteration 35 took 2.10 seconds (mean sampled reward: -3195.42). Current reward after update: -1377.48, Optimal reward -1356.28
Iteration 36 took 2.01 seconds (mean sampled reward: -4986.67). Current reward after update: -1589.91, Optimal reward -1356.28
Iteration 37 took 2.00 seconds (mean sampled reward: -3235.40). Current reward after update: -1480.49, Optimal reward -1356.28
Iteration 38 took 1.92 seconds (mean sampled reward: -4046.56). Current reward after update: -1470.32, Optimal reward -1356.28
Iteration 39 took 1.95 seconds (mean sampled reward: -4090.48). Current reward after update: -1427.76, Optimal reward -1356.28
Iteration 40 took 2.07 seconds (mean sampled reward: -4070.93). Current reward after update: -1255.53, Optimal reward -1255.53
Iteration 41 took 2.03 seconds (mean sampled reward: -4292.58). Current reward after update: -6599.99, Optimal reward -1255.53
Iteration 42 took 2.03 seconds (mean sampled reward: -4227.14). Current reward after update: -1376.66, Optimal reward -1255.53
Iteration 43 took 2.09 seconds (mean sampled reward: -3949.47). Current reward after update: -1303.66, Optimal reward -1255.53
Iteration 44 took 2.08 seconds (mean sampled reward: -3811.51). Current reward after update: -1259.43, Optimal reward -1255.53
Iteration 45 took 2.03 seconds (mean sampled reward: -3298.17). Current reward after update: -1255.82, Optimal reward -1255.53
Iteration 46 took 2.05 seconds (mean sampled reward: -3399.73). Current reward after update: -1412.69, Optimal reward -1255.53
Iteration 47 took 2.00 seconds (mean sampled reward: -3584.83). Current reward after update: -1417.87, Optimal reward -1255.53
Iteration 48 took 1.98 seconds (mean sampled reward: -3660.29). Current reward after update: -1230.91, Optimal reward -1230.91
Iteration 49 took 2.04 seconds (mean sampled reward: -4537.29). Current reward after update: -1408.54, Optimal reward -1230.91
Iteration 50 took 2.05 seconds (mean sampled reward: -4579.16). Current reward after update: -1440.65, Optimal reward -1230.91
Iteration 51 took 2.05 seconds (mean sampled reward: -4334.65). Current reward after update: -1609.86, Optimal reward -1230.91
Iteration 52 took 2.02 seconds (mean sampled reward: -4053.44). Current reward after update: -2221.51, Optimal reward -1230.91
Iteration 53 took 2.00 seconds (mean sampled reward: -3550.17). Current reward after update: -1501.69, Optimal reward -1230.91
Iteration 54 took 1.95 seconds (mean sampled reward: -3952.80). Current reward after update: -3012.99, Optimal reward -1230.91
Iteration 55 took 2.23 seconds (mean sampled reward: -4639.73). Current reward after update: -1407.09, Optimal reward -1230.91
Iteration 56 took 1.95 seconds (mean sampled reward: -4002.45). Current reward after update: -1353.31, Optimal reward -1230.91
Iteration 57 took 1.91 seconds (mean sampled reward: -4559.81). Current reward after update: -1297.17, Optimal reward -1230.91
Iteration 58 took 1.95 seconds (mean sampled reward: -4736.56). Current reward after update: -1393.12, Optimal reward -1230.91
Iteration 59 took 1.98 seconds (mean sampled reward: -3459.28). Current reward after update: -1240.18, Optimal reward -1230.91
Iteration 60 took 2.02 seconds (mean sampled reward: -3600.54). Current reward after update: -1330.44, Optimal reward -1230.91
Iteration 61 took 2.07 seconds (mean sampled reward: -4624.76). Current reward after update: -2634.25, Optimal reward -1230.91
Iteration 62 took 2.07 seconds (mean sampled reward: -4455.74). Current reward after update: -1392.61, Optimal reward -1230.91
Iteration 63 took 1.96 seconds (mean sampled reward: -4619.75). Current reward after update: -1429.00, Optimal reward -1230.91
Iteration 64 took 1.91 seconds (mean sampled reward: -4827.41). Current reward after update: -1252.69, Optimal reward -1230.91
Iteration 65 took 1.91 seconds (mean sampled reward: -5614.80). Current reward after update: -1386.39, Optimal reward -1230.91
Iteration 66 took 1.93 seconds (mean sampled reward: -5393.79). Current reward after update: -1354.78, Optimal reward -1230.91
Iteration 67 took 1.89 seconds (mean sampled reward: -5097.30). Current reward after update: -1412.52, Optimal reward -1230.91
Iteration 68 took 2.07 seconds (mean sampled reward: -4703.04). Current reward after update: -1366.08, Optimal reward -1230.91
Iteration 69 took 1.88 seconds (mean sampled reward: -5105.11). Current reward after update: -1382.96, Optimal reward -1230.91
Iteration 70 took 1.86 seconds (mean sampled reward: -5533.58). Current reward after update: -1467.31, Optimal reward -1230.91
Iteration 71 took 1.90 seconds (mean sampled reward: -5302.46). Current reward after update: -1386.97, Optimal reward -1230.91
Iteration 72 took 1.93 seconds (mean sampled reward: -4927.94). Current reward after update: -1324.49, Optimal reward -1230.91
Iteration 73 took 1.91 seconds (mean sampled reward: -5619.77). Current reward after update: -1285.16, Optimal reward -1230.91
Iteration 74 took 1.91 seconds (mean sampled reward: -5870.70). Current reward after update: -1288.80, Optimal reward -1230.91
Iteration 75 took 1.86 seconds (mean sampled reward: -5291.34). Current reward after update: -1223.89, Optimal reward -1223.89
Iteration 76 took 1.86 seconds (mean sampled reward: -5535.56). Current reward after update: -1514.56, Optimal reward -1223.89
Iteration 77 took 1.87 seconds (mean sampled reward: -5205.85). Current reward after update: -6538.28, Optimal reward -1223.89
Iteration 78 took 1.86 seconds (mean sampled reward: -5727.46). Current reward after update: -1644.42, Optimal reward -1223.89
Iteration 79 took 2.00 seconds (mean sampled reward: -4397.58). Current reward after update: -1305.00, Optimal reward -1223.89
Iteration 80 took 1.89 seconds (mean sampled reward: -5542.52). Current reward after update: -1449.74, Optimal reward -1223.89
Iteration 81 took 1.94 seconds (mean sampled reward: -4429.33). Current reward after update: -1371.33, Optimal reward -1223.89
Iteration 82 took 1.98 seconds (mean sampled reward: -4158.39). Current reward after update: -1361.06, Optimal reward -1223.89
Iteration 83 took 1.96 seconds (mean sampled reward: -4223.48). Current reward after update: -1943.88, Optimal reward -1223.89
Iteration 84 took 1.97 seconds (mean sampled reward: -4578.77). Current reward after update: -6507.05, Optimal reward -1223.89
Iteration 85 took 1.94 seconds (mean sampled reward: -4683.02). Current reward after update: -1320.64, Optimal reward -1223.89
Iteration 86 took 1.96 seconds (mean sampled reward: -5136.87). Current reward after update: -1406.31, Optimal reward -1223.89
Iteration 87 took 1.95 seconds (mean sampled reward: -4612.98). Current reward after update: -1285.59, Optimal reward -1223.89
Iteration 88 took 1.91 seconds (mean sampled reward: -4940.83). Current reward after update: -1307.14, Optimal reward -1223.89
Iteration 89 took 1.93 seconds (mean sampled reward: -5012.83). Current reward after update: -1358.68, Optimal reward -1223.89
Iteration 90 took 1.94 seconds (mean sampled reward: -5016.10). Current reward after update: -1271.47, Optimal reward -1223.89
Iteration 91 took 1.92 seconds (mean sampled reward: -5266.72). Current reward after update: -1438.45, Optimal reward -1223.89
Iteration 92 took 1.95 seconds (mean sampled reward: -4983.67). Current reward after update: -1262.40, Optimal reward -1223.89
Iteration 93 took 1.99 seconds (mean sampled reward: -5006.06). Current reward after update: -1658.11, Optimal reward -1223.89
Iteration 94 took 2.01 seconds (mean sampled reward: -4768.58). Current reward after update: -3583.69, Optimal reward -1223.89
Iteration 95 took 2.01 seconds (mean sampled reward: -4323.07). Current reward after update: -1864.74, Optimal reward -1223.89
Iteration 96 took 2.03 seconds (mean sampled reward: -5314.78). Current reward after update: -1195.76, Optimal reward -1195.76
Iteration 97 took 1.98 seconds (mean sampled reward: -5645.43). Current reward after update: -1224.07, Optimal reward -1195.76
Iteration 98 took 2.06 seconds (mean sampled reward: -4808.66). Current reward after update: -1251.81, Optimal reward -1195.76
Iteration 99 took 1.99 seconds (mean sampled reward: -5098.77). Current reward after update: -1219.49, Optimal reward -1195.76
Iteration 100 took 2.04 seconds (mean sampled reward: -3477.72). Current reward after update: -2101.74, Optimal reward -1195.76
Iteration 101 took 1.95 seconds (mean sampled reward: -3840.39). Current reward after update: -2700.82, Optimal reward -1195.76
Iteration 102 took 2.14 seconds (mean sampled reward: -2900.93). Current reward after update: -1138.70, Optimal reward -1138.70
Iteration 103 took 2.14 seconds (mean sampled reward: -3215.34). Current reward after update: -1211.12, Optimal reward -1138.70
Iteration 104 took 2.04 seconds (mean sampled reward: -3772.15). Current reward after update: -2403.17, Optimal reward -1138.70
Iteration 105 took 2.02 seconds (mean sampled reward: -3678.53). Current reward after update: -1125.06, Optimal reward -1125.06
Iteration 106 took 2.11 seconds (mean sampled reward: -3765.71). Current reward after update: -1223.31, Optimal reward -1125.06
Iteration 107 took 2.25 seconds (mean sampled reward: -3853.51). Current reward after update: -2794.89, Optimal reward -1125.06
Iteration 108 took 2.14 seconds (mean sampled reward: -3081.11). Current reward after update: -1173.40, Optimal reward -1125.06
Iteration 109 took 2.24 seconds (mean sampled reward: -3802.68). Current reward after update: -1193.59, Optimal reward -1125.06
Iteration 110 took 2.25 seconds (mean sampled reward: -3392.81). Current reward after update: -1242.92, Optimal reward -1125.06
Iteration 111 took 2.20 seconds (mean sampled reward: -3353.47). Current reward after update: -1161.54, Optimal reward -1125.06
Iteration 112 took 2.06 seconds (mean sampled reward: -3500.96). Current reward after update: -1904.55, Optimal reward -1125.06
Iteration 113 took 2.00 seconds (mean sampled reward: -4983.95). Current reward after update: -1299.83, Optimal reward -1125.06
Iteration 114 took 2.00 seconds (mean sampled reward: -3585.97). Current reward after update: -1072.23, Optimal reward -1072.23
Iteration 115 took 1.98 seconds (mean sampled reward: -3827.89). Current reward after update: -1103.43, Optimal reward -1072.23
Iteration 116 took 2.05 seconds (mean sampled reward: -3224.17). Current reward after update: -6490.03, Optimal reward -1072.23
Iteration 117 took 2.04 seconds (mean sampled reward: -4914.21). Current reward after update: -1137.98, Optimal reward -1072.23
Iteration 118 took 1.99 seconds (mean sampled reward: -4388.88). Current reward after update: -1075.31, Optimal reward -1072.23
Iteration 119 took 2.00 seconds (mean sampled reward: -5135.27). Current reward after update: -1159.98, Optimal reward -1072.23
Iteration 120 took 1.92 seconds (mean sampled reward: -5566.67). Current reward after update: -1506.86, Optimal reward -1072.23
Iteration 121 took 1.91 seconds (mean sampled reward: -5280.77). Current reward after update: -1192.70, Optimal reward -1072.23
Iteration 122 took 1.90 seconds (mean sampled reward: -4916.24). Current reward after update: -1262.04, Optimal reward -1072.23
Iteration 123 took 2.01 seconds (mean sampled reward: -3768.63). Current reward after update: -1245.32, Optimal reward -1072.23
Iteration 124 took 2.00 seconds (mean sampled reward: -4065.78). Current reward after update: -1091.86, Optimal reward -1072.23
Iteration 125 took 1.90 seconds (mean sampled reward: -4863.52). Current reward after update: -1129.23, Optimal reward -1072.23
Iteration 126 took 1.98 seconds (mean sampled reward: -4418.68). Current reward after update: -1130.42, Optimal reward -1072.23
Iteration 127 took 1.94 seconds (mean sampled reward: -4681.22). Current reward after update: -1121.67, Optimal reward -1072.23
Iteration 128 took 1.86 seconds (mean sampled reward: -5142.06). Current reward after update: -1032.40, Optimal reward -1032.40
Iteration 129 took 1.92 seconds (mean sampled reward: -3951.97). Current reward after update: -3107.56, Optimal reward -1032.40
Iteration 130 took 1.92 seconds (mean sampled reward: -3536.25). Current reward after update: -1050.64, Optimal reward -1032.40
Iteration 131 took 1.98 seconds (mean sampled reward: -3443.35). Current reward after update: -1109.14, Optimal reward -1032.40
Iteration 132 took 1.86 seconds (mean sampled reward: -4557.78). Current reward after update: -1086.88, Optimal reward -1032.40
Iteration 133 took 1.99 seconds (mean sampled reward: -3039.88). Current reward after update: -1033.73, Optimal reward -1032.40
Iteration 134 took 1.95 seconds (mean sampled reward: -4471.03). Current reward after update: -1153.75, Optimal reward -1032.40
Iteration 135 took 1.90 seconds (mean sampled reward: -4811.93). Current reward after update: -1079.15, Optimal reward -1032.40
Iteration 136 took 1.99 seconds (mean sampled reward: -3058.80). Current reward after update: -1038.36, Optimal reward -1032.40
Iteration 137 took 1.98 seconds (mean sampled reward: -3470.29). Current reward after update: -1402.96, Optimal reward -1032.40
Iteration 138 took 1.97 seconds (mean sampled reward: -3416.98). Current reward after update: -1069.93, Optimal reward -1032.40
Iteration 139 took 1.90 seconds (mean sampled reward: -4730.09). Current reward after update: -6528.94, Optimal reward -1032.40
Iteration 140 took 1.93 seconds (mean sampled reward: -3974.13). Current reward after update: -1057.75, Optimal reward -1032.40
Iteration 141 took 2.02 seconds (mean sampled reward: -2966.12). Current reward after update: -1027.75, Optimal reward -1027.75
Iteration 142 took 2.10 seconds (mean sampled reward: -4438.03). Current reward after update: -1087.67, Optimal reward -1027.75
Iteration 143 took 1.96 seconds (mean sampled reward: -3942.61). Current reward after update: -1112.96, Optimal reward -1027.75
Iteration 144 took 2.02 seconds (mean sampled reward: -3958.76). Current reward after update: -1094.35, Optimal reward -1027.75
Iteration 145 took 1.95 seconds (mean sampled reward: -5103.72). Current reward after update: -1211.06, Optimal reward -1027.75
Iteration 146 took 1.94 seconds (mean sampled reward: -4685.08). Current reward after update: -1058.50, Optimal reward -1027.75
Iteration 147 took 1.91 seconds (mean sampled reward: -5282.40). Current reward after update: -984.99, Optimal reward -984.99
Iteration 148 took 1.94 seconds (mean sampled reward: -4581.78). Current reward after update: -1032.34, Optimal reward -984.99
Iteration 149 took 1.95 seconds (mean sampled reward: -4548.63). Current reward after update: -1016.64, Optimal reward -984.99
Iteration 150 took 1.97 seconds (mean sampled reward: -3375.10). Current reward after update: -1394.02, Optimal reward -984.99
Iteration 151 took 1.96 seconds (mean sampled reward: -3570.29). Current reward after update: -893.12, Optimal reward -893.12
Iteration 152 took 1.98 seconds (mean sampled reward: -4452.84). Current reward after update: -1072.13, Optimal reward -893.12
Iteration 153 took 1.92 seconds (mean sampled reward: -4171.95). Current reward after update: -1020.25, Optimal reward -893.12
Iteration 154 took 2.01 seconds (mean sampled reward: -3232.39). Current reward after update: -1620.32, Optimal reward -893.12
Iteration 155 took 2.13 seconds (mean sampled reward: -2676.63). Current reward after update: -2833.31, Optimal reward -893.12
Iteration 156 took 2.08 seconds (mean sampled reward: -3837.31). Current reward after update: -1507.77, Optimal reward -893.12
Iteration 157 took 1.98 seconds (mean sampled reward: -3387.36). Current reward after update: -1057.43, Optimal reward -893.12
Iteration 158 took 2.14 seconds (mean sampled reward: -3569.67). Current reward after update: -1075.84, Optimal reward -893.12
Iteration 159 took 2.01 seconds (mean sampled reward: -3683.47). Current reward after update: -994.37, Optimal reward -893.12
Iteration 160 took 1.90 seconds (mean sampled reward: -4924.51). Current reward after update: -1029.05, Optimal reward -893.12
Iteration 161 took 2.01 seconds (mean sampled reward: -3864.19). Current reward after update: -1043.36, Optimal reward -893.12
Iteration 162 took 1.99 seconds (mean sampled reward: -3120.39). Current reward after update: -2037.51, Optimal reward -893.12
Iteration 163 took 1.99 seconds (mean sampled reward: -3480.46). Current reward after update: -1054.22, Optimal reward -893.12
Iteration 164 took 1.92 seconds (mean sampled reward: -4044.86). Current reward after update: -1086.45, Optimal reward -893.12
Iteration 165 took 1.97 seconds (mean sampled reward: -3740.98). Current reward after update: -2098.56, Optimal reward -893.12
Iteration 166 took 1.95 seconds (mean sampled reward: -4641.62). Current reward after update: -999.90, Optimal reward -893.12
Iteration 167 took 2.01 seconds (mean sampled reward: -3328.81). Current reward after update: -971.55, Optimal reward -893.12
Iteration 168 took 2.04 seconds (mean sampled reward: -2784.51). Current reward after update: -1057.09, Optimal reward -893.12
Iteration 169 took 2.11 seconds (mean sampled reward: -2906.35). Current reward after update: -1087.82, Optimal reward -893.12
Iteration 170 took 1.97 seconds (mean sampled reward: -4507.61). Current reward after update: -1203.52, Optimal reward -893.12
Iteration 171 took 2.01 seconds (mean sampled reward: -3768.47). Current reward after update: -1056.12, Optimal reward -893.12
Iteration 172 took 1.94 seconds (mean sampled reward: -4332.59). Current reward after update: -1060.98, Optimal reward -893.12
Iteration 173 took 1.95 seconds (mean sampled reward: -4733.25). Current reward after update: -1791.97, Optimal reward -893.12
Iteration 174 took 1.99 seconds (mean sampled reward: -4197.87). Current reward after update: -1189.67, Optimal reward -893.12
Iteration 175 took 1.99 seconds (mean sampled reward: -4275.31). Current reward after update: -3128.41, Optimal reward -893.12
Iteration 176 took 1.94 seconds (mean sampled reward: -5084.71). Current reward after update: -1201.80, Optimal reward -893.12
Iteration 177 took 2.05 seconds (mean sampled reward: -2804.06). Current reward after update: -1005.63, Optimal reward -893.12
Iteration 178 took 2.10 seconds (mean sampled reward: -3894.68). Current reward after update: -1115.63, Optimal reward -893.12
Iteration 179 took 2.07 seconds (mean sampled reward: -2802.91). Current reward after update: -1909.96, Optimal reward -893.12
Iteration 180 took 2.02 seconds (mean sampled reward: -2778.50). Current reward after update: -1088.38, Optimal reward -893.12
Iteration 181 took 2.04 seconds (mean sampled reward: -2662.05). Current reward after update: -1076.89, Optimal reward -893.12
Iteration 182 took 2.00 seconds (mean sampled reward: -2604.99). Current reward after update: -3116.25, Optimal reward -893.12
Iteration 183 took 1.96 seconds (mean sampled reward: -3304.82). Current reward after update: -1104.56, Optimal reward -893.12
Iteration 184 took 1.99 seconds (mean sampled reward: -3003.33). Current reward after update: -982.07, Optimal reward -893.12
Iteration 185 took 2.03 seconds (mean sampled reward: -2883.36). Current reward after update: -1380.87, Optimal reward -893.12
Iteration 186 took 2.02 seconds (mean sampled reward: -2886.96). Current reward after update: -1092.95, Optimal reward -893.12
Iteration 187 took 2.03 seconds (mean sampled reward: -3387.57). Current reward after update: -3245.01, Optimal reward -893.12
Iteration 188 took 1.93 seconds (mean sampled reward: -3614.80). Current reward after update: -1109.43, Optimal reward -893.12
Iteration 189 took 2.02 seconds (mean sampled reward: -3841.05). Current reward after update: -1098.24, Optimal reward -893.12
Iteration 190 took 1.92 seconds (mean sampled reward: -4968.64). Current reward after update: -1760.00, Optimal reward -893.12
Iteration 191 took 1.97 seconds (mean sampled reward: -4272.56). Current reward after update: -1123.11, Optimal reward -893.12
Iteration 192 took 1.97 seconds (mean sampled reward: -4347.48). Current reward after update: -1161.77, Optimal reward -893.12
Iteration 193 took 1.96 seconds (mean sampled reward: -3979.00). Current reward after update: -1127.00, Optimal reward -893.12
Iteration 194 took 2.03 seconds (mean sampled reward: -3499.57). Current reward after update: -1644.58, Optimal reward -893.12
Iteration 195 took 1.94 seconds (mean sampled reward: -4498.32). Current reward after update: -2083.55, Optimal reward -893.12
Iteration 196 took 1.94 seconds (mean sampled reward: -4152.92). Current reward after update: -1139.13, Optimal reward -893.12
Iteration 197 took 1.98 seconds (mean sampled reward: -3023.44). Current reward after update: -1106.30, Optimal reward -893.12
Iteration 198 took 1.89 seconds (mean sampled reward: -4696.53). Current reward after update: -2418.38, Optimal reward -893.12
Iteration 199 took 1.97 seconds (mean sampled reward: -4561.61). Current reward after update: -1268.38, Optimal reward -893.12
Iteration 200 took 1.95 seconds (mean sampled reward: -4855.63). Current reward after update: -1251.96, Optimal reward -893.12
Iteration 1 took 2.19 seconds (mean sampled reward: -7482.68). Current reward after update: -5478.43, Optimal reward -5478.43
Iteration 2 took 2.18 seconds (mean sampled reward: -7058.52). Current reward after update: -4515.61, Optimal reward -4515.61
Iteration 3 took 2.21 seconds (mean sampled reward: -6467.93). Current reward after update: -3905.42, Optimal reward -3905.42
Iteration 4 took 2.15 seconds (mean sampled reward: -6146.06). Current reward after update: -2969.37, Optimal reward -2969.37
Iteration 5 took 2.09 seconds (mean sampled reward: -5836.45). Current reward after update: -2733.66, Optimal reward -2733.66
Iteration 6 took 1.99 seconds (mean sampled reward: -6125.66). Current reward after update: -2989.69, Optimal reward -2733.66
Iteration 7 took 1.96 seconds (mean sampled reward: -6664.42). Current reward after update: -3045.52, Optimal reward -2733.66
Iteration 8 took 2.07 seconds (mean sampled reward: -6289.50). Current reward after update: -2901.71, Optimal reward -2733.66
Iteration 9 took 2.05 seconds (mean sampled reward: -6425.36). Current reward after update: -2793.93, Optimal reward -2733.66
Iteration 10 took 1.99 seconds (mean sampled reward: -6204.37). Current reward after update: -2042.74, Optimal reward -2042.74
Iteration 11 took 2.11 seconds (mean sampled reward: -6507.10). Current reward after update: -2229.73, Optimal reward -2042.74
Iteration 12 took 1.91 seconds (mean sampled reward: -6457.15). Current reward after update: -1685.01, Optimal reward -1685.01
Iteration 13 took 1.99 seconds (mean sampled reward: -5924.89). Current reward after update: -1417.75, Optimal reward -1417.75
Iteration 14 took 2.12 seconds (mean sampled reward: -6013.73). Current reward after update: -1987.89, Optimal reward -1417.75
Iteration 15 took 2.06 seconds (mean sampled reward: -5861.42). Current reward after update: -2027.72, Optimal reward -1417.75
Iteration 16 took 2.06 seconds (mean sampled reward: -5387.79). Current reward after update: -1713.78, Optimal reward -1417.75
Iteration 17 took 2.07 seconds (mean sampled reward: -6922.02). Current reward after update: -2291.40, Optimal reward -1417.75
Iteration 18 took 2.04 seconds (mean sampled reward: -6058.48). Current reward after update: -1768.25, Optimal reward -1417.75
Iteration 19 took 1.95 seconds (mean sampled reward: -6561.21). Current reward after update: -1520.46, Optimal reward -1417.75
Iteration 20 took 1.94 seconds (mean sampled reward: -6284.61). Current reward after update: -1293.09, Optimal reward -1293.09
Iteration 21 took 1.94 seconds (mean sampled reward: -5493.92). Current reward after update: -1245.76, Optimal reward -1245.76
Iteration 22 took 1.94 seconds (mean sampled reward: -6112.15). Current reward after update: -1391.18, Optimal reward -1245.76
Iteration 23 took 1.82 seconds (mean sampled reward: -6664.76). Current reward after update: -1569.37, Optimal reward -1245.76
Iteration 24 took 1.97 seconds (mean sampled reward: -6388.36). Current reward after update: -1285.14, Optimal reward -1245.76
Iteration 25 took 1.96 seconds (mean sampled reward: -6345.16). Current reward after update: -1189.48, Optimal reward -1189.48
Iteration 26 took 1.89 seconds (mean sampled reward: -5845.41). Current reward after update: -1278.50, Optimal reward -1189.48
Iteration 27 took 2.05 seconds (mean sampled reward: -4786.28). Current reward after update: -1374.19, Optimal reward -1189.48
Iteration 28 took 2.09 seconds (mean sampled reward: -4390.11). Current reward after update: -1069.43, Optimal reward -1069.43
Iteration 29 took 2.09 seconds (mean sampled reward: -3929.87). Current reward after update: -1276.73, Optimal reward -1069.43
Iteration 30 took 2.21 seconds (mean sampled reward: -4292.91). Current reward after update: -1242.44, Optimal reward -1069.43
Iteration 31 took 2.13 seconds (mean sampled reward: -3478.50). Current reward after update: -1143.64, Optimal reward -1069.43
Iteration 32 took 2.23 seconds (mean sampled reward: -3414.52). Current reward after update: -1350.04, Optimal reward -1069.43
Iteration 33 took 2.15 seconds (mean sampled reward: -3127.36). Current reward after update: -1884.59, Optimal reward -1069.43
Iteration 34 took 2.16 seconds (mean sampled reward: -3037.51). Current reward after update: -1356.09, Optimal reward -1069.43
Iteration 35 took 2.12 seconds (mean sampled reward: -3212.45). Current reward after update: -1128.35, Optimal reward -1069.43
Iteration 36 took 2.08 seconds (mean sampled reward: -3810.81). Current reward after update: -1129.89, Optimal reward -1069.43
Iteration 37 took 2.05 seconds (mean sampled reward: -3212.71). Current reward after update: -1174.37, Optimal reward -1069.43
Iteration 38 took 2.10 seconds (mean sampled reward: -3553.65). Current reward after update: -1960.72, Optimal reward -1069.43
Iteration 39 took 2.12 seconds (mean sampled reward: -3289.87). Current reward after update: -1283.94, Optimal reward -1069.43
Iteration 40 took 2.09 seconds (mean sampled reward: -3003.12). Current reward after update: -1106.55, Optimal reward -1069.43
Iteration 41 took 2.05 seconds (mean sampled reward: -3535.30). Current reward after update: -1114.71, Optimal reward -1069.43
Iteration 42 took 2.06 seconds (mean sampled reward: -3930.34). Current reward after update: -1305.12, Optimal reward -1069.43
Iteration 43 took 2.07 seconds (mean sampled reward: -4411.84). Current reward after update: -1214.12, Optimal reward -1069.43
Iteration 44 took 2.08 seconds (mean sampled reward: -4878.12). Current reward after update: -1184.44, Optimal reward -1069.43
Iteration 45 took 1.96 seconds (mean sampled reward: -4574.70). Current reward after update: -1206.56, Optimal reward -1069.43
Iteration 46 took 1.94 seconds (mean sampled reward: -4166.94). Current reward after update: -1228.05, Optimal reward -1069.43
Iteration 47 took 2.02 seconds (mean sampled reward: -4029.66). Current reward after update: -1460.69, Optimal reward -1069.43
Iteration 48 took 2.20 seconds (mean sampled reward: -4723.38). Current reward after update: -1106.70, Optimal reward -1069.43
Iteration 49 took 2.03 seconds (mean sampled reward: -3911.28). Current reward after update: -1153.40, Optimal reward -1069.43
Iteration 50 took 2.00 seconds (mean sampled reward: -4892.79). Current reward after update: -1336.09, Optimal reward -1069.43
Iteration 51 took 1.93 seconds (mean sampled reward: -5094.23). Current reward after update: -1099.49, Optimal reward -1069.43
Iteration 52 took 1.97 seconds (mean sampled reward: -4985.59). Current reward after update: -1184.25, Optimal reward -1069.43
Iteration 53 took 1.93 seconds (mean sampled reward: -5198.20). Current reward after update: -1285.16, Optimal reward -1069.43
Iteration 54 took 2.14 seconds (mean sampled reward: -5335.90). Current reward after update: -1101.09, Optimal reward -1069.43
Iteration 55 took 2.02 seconds (mean sampled reward: -5907.42). Current reward after update: -1589.79, Optimal reward -1069.43
Iteration 56 took 2.40 seconds (mean sampled reward: -5192.27). Current reward after update: -1316.68, Optimal reward -1069.43
Iteration 57 took 1.90 seconds (mean sampled reward: -4348.31). Current reward after update: -1305.79, Optimal reward -1069.43
Iteration 58 took 2.08 seconds (mean sampled reward: -4724.90). Current reward after update: -1381.50, Optimal reward -1069.43
Iteration 59 took 1.97 seconds (mean sampled reward: -4504.69). Current reward after update: -1003.55, Optimal reward -1003.55
Iteration 60 took 2.07 seconds (mean sampled reward: -4311.04). Current reward after update: -1207.48, Optimal reward -1003.55
Iteration 61 took 2.01 seconds (mean sampled reward: -4738.19). Current reward after update: -1201.03, Optimal reward -1003.55
Iteration 62 took 1.98 seconds (mean sampled reward: -3991.58). Current reward after update: -1209.39, Optimal reward -1003.55
Iteration 63 took 2.14 seconds (mean sampled reward: -4099.62). Current reward after update: -1172.72, Optimal reward -1003.55
Iteration 64 took 1.92 seconds (mean sampled reward: -4946.91). Current reward after update: -2439.86, Optimal reward -1003.55
Iteration 65 took 1.94 seconds (mean sampled reward: -5546.65). Current reward after update: -1341.87, Optimal reward -1003.55
Iteration 66 took 1.89 seconds (mean sampled reward: -5698.70). Current reward after update: -1302.66, Optimal reward -1003.55
Iteration 67 took 2.15 seconds (mean sampled reward: -3518.30). Current reward after update: -984.32, Optimal reward -984.32
Iteration 68 took 2.25 seconds (mean sampled reward: -3534.14). Current reward after update: -1146.93, Optimal reward -984.32
Iteration 69 took 2.09 seconds (mean sampled reward: -3847.43). Current reward after update: -1148.88, Optimal reward -984.32
Iteration 70 took 1.99 seconds (mean sampled reward: -4759.82). Current reward after update: -1302.89, Optimal reward -984.32
Iteration 71 took 1.96 seconds (mean sampled reward: -4308.91). Current reward after update: -1983.46, Optimal reward -984.32
Iteration 72 took 1.93 seconds (mean sampled reward: -5180.40). Current reward after update: -1202.28, Optimal reward -984.32
Iteration 73 took 2.05 seconds (mean sampled reward: -3553.48). Current reward after update: -769.31, Optimal reward -769.31
Iteration 74 took 1.86 seconds (mean sampled reward: -5505.81). Current reward after update: -1327.33, Optimal reward -769.31
Iteration 75 took 2.14 seconds (mean sampled reward: -4327.72). Current reward after update: -1205.83, Optimal reward -769.31
Iteration 76 took 2.04 seconds (mean sampled reward: -4076.29). Current reward after update: -1037.54, Optimal reward -769.31
Iteration 77 took 2.15 seconds (mean sampled reward: -4219.33). Current reward after update: -1144.64, Optimal reward -769.31
Iteration 78 took 2.00 seconds (mean sampled reward: -4880.57). Current reward after update: -1319.76, Optimal reward -769.31
Iteration 79 took 2.02 seconds (mean sampled reward: -4000.73). Current reward after update: -1279.22, Optimal reward -769.31
Iteration 80 took 2.03 seconds (mean sampled reward: -3972.96). Current reward after update: -1349.02, Optimal reward -769.31
Iteration 81 took 2.03 seconds (mean sampled reward: -3698.84). Current reward after update: -1265.60, Optimal reward -769.31
Iteration 82 took 1.97 seconds (mean sampled reward: -3734.31). Current reward after update: -1113.65, Optimal reward -769.31
Iteration 83 took 2.01 seconds (mean sampled reward: -3819.92). Current reward after update: -1131.81, Optimal reward -769.31
Iteration 84 took 1.96 seconds (mean sampled reward: -4537.58). Current reward after update: -1202.14, Optimal reward -769.31
Iteration 85 took 1.94 seconds (mean sampled reward: -4719.65). Current reward after update: -1147.05, Optimal reward -769.31
Iteration 86 took 1.99 seconds (mean sampled reward: -4085.03). Current reward after update: -1010.01, Optimal reward -769.31
Iteration 87 took 1.99 seconds (mean sampled reward: -4266.53). Current reward after update: -1276.94, Optimal reward -769.31
Iteration 88 took 1.91 seconds (mean sampled reward: -5993.72). Current reward after update: -1113.86, Optimal reward -769.31
Iteration 89 took 2.03 seconds (mean sampled reward: -4487.76). Current reward after update: -2144.47, Optimal reward -769.31
Iteration 90 took 1.89 seconds (mean sampled reward: -5987.84). Current reward after update: -1163.49, Optimal reward -769.31
Iteration 91 took 1.83 seconds (mean sampled reward: -6269.94). Current reward after update: -1395.98, Optimal reward -769.31
Iteration 92 took 1.93 seconds (mean sampled reward: -5055.29). Current reward after update: -1494.60, Optimal reward -769.31
Iteration 93 took 1.99 seconds (mean sampled reward: -3530.25). Current reward after update: -968.83, Optimal reward -769.31
Iteration 94 took 1.97 seconds (mean sampled reward: -4682.40). Current reward after update: -803.89, Optimal reward -769.31
Iteration 95 took 1.92 seconds (mean sampled reward: -4515.32). Current reward after update: -873.44, Optimal reward -769.31
Iteration 96 took 1.95 seconds (mean sampled reward: -5036.66). Current reward after update: -795.73, Optimal reward -769.31
Iteration 97 took 1.88 seconds (mean sampled reward: -5558.02). Current reward after update: -1023.79, Optimal reward -769.31
Iteration 98 took 1.88 seconds (mean sampled reward: -5010.36). Current reward after update: -788.06, Optimal reward -769.31
Iteration 99 took 1.99 seconds (mean sampled reward: -5036.38). Current reward after update: -849.56, Optimal reward -769.31
Iteration 100 took 1.95 seconds (mean sampled reward: -4646.86). Current reward after update: -1728.59, Optimal reward -769.31
Iteration 101 took 2.07 seconds (mean sampled reward: -3764.54). Current reward after update: -963.14, Optimal reward -769.31
Iteration 102 took 2.17 seconds (mean sampled reward: -3563.09). Current reward after update: -780.91, Optimal reward -769.31
Iteration 103 took 2.10 seconds (mean sampled reward: -4128.72). Current reward after update: -857.08, Optimal reward -769.31
Iteration 104 took 2.20 seconds (mean sampled reward: -3286.51). Current reward after update: -862.14, Optimal reward -769.31
Iteration 105 took 2.19 seconds (mean sampled reward: -3442.70). Current reward after update: -880.06, Optimal reward -769.31
Iteration 106 took 2.25 seconds (mean sampled reward: -3014.62). Current reward after update: -996.35, Optimal reward -769.31
Iteration 107 took 2.38 seconds (mean sampled reward: -3661.36). Current reward after update: -726.74, Optimal reward -726.74
Iteration 108 took 2.25 seconds (mean sampled reward: -3937.92). Current reward after update: -971.45, Optimal reward -726.74
Iteration 109 took 2.18 seconds (mean sampled reward: -5302.60). Current reward after update: -723.58, Optimal reward -723.58
Iteration 110 took 2.32 seconds (mean sampled reward: -5299.74). Current reward after update: -1063.65, Optimal reward -723.58
Iteration 111 took 2.28 seconds (mean sampled reward: -5583.14). Current reward after update: -1396.80, Optimal reward -723.58
Iteration 112 took 2.22 seconds (mean sampled reward: -3592.51). Current reward after update: -1330.16, Optimal reward -723.58
Iteration 113 took 2.21 seconds (mean sampled reward: -4230.27). Current reward after update: -1435.31, Optimal reward -723.58
Iteration 114 took 2.11 seconds (mean sampled reward: -3501.22). Current reward after update: -1205.59, Optimal reward -723.58
Iteration 115 took 2.06 seconds (mean sampled reward: -3706.29). Current reward after update: -1074.49, Optimal reward -723.58
Iteration 116 took 2.04 seconds (mean sampled reward: -3629.31). Current reward after update: -1999.74, Optimal reward -723.58
Iteration 117 took 2.05 seconds (mean sampled reward: -3453.55). Current reward after update: -1004.85, Optimal reward -723.58
Iteration 118 took 2.03 seconds (mean sampled reward: -3619.09). Current reward after update: -1261.68, Optimal reward -723.58
Iteration 119 took 2.04 seconds (mean sampled reward: -2953.77). Current reward after update: -1126.53, Optimal reward -723.58
Iteration 120 took 2.05 seconds (mean sampled reward: -3547.75). Current reward after update: -1110.91, Optimal reward -723.58
Iteration 121 took 2.11 seconds (mean sampled reward: -3109.86). Current reward after update: -959.34, Optimal reward -723.58
Iteration 122 took 2.07 seconds (mean sampled reward: -2502.67). Current reward after update: -1030.10, Optimal reward -723.58
Iteration 123 took 2.12 seconds (mean sampled reward: -2609.67). Current reward after update: -3719.90, Optimal reward -723.58
Iteration 124 took 2.14 seconds (mean sampled reward: -3013.74). Current reward after update: -902.40, Optimal reward -723.58
Iteration 125 took 2.05 seconds (mean sampled reward: -4026.07). Current reward after update: -906.23, Optimal reward -723.58
Iteration 126 took 2.06 seconds (mean sampled reward: -3007.08). Current reward after update: -963.49, Optimal reward -723.58
Iteration 127 took 2.05 seconds (mean sampled reward: -3120.88). Current reward after update: -924.32, Optimal reward -723.58
Iteration 128 took 2.04 seconds (mean sampled reward: -2972.90). Current reward after update: -1004.86, Optimal reward -723.58
Iteration 129 took 2.15 seconds (mean sampled reward: -2445.98). Current reward after update: -946.08, Optimal reward -723.58
Iteration 130 took 2.05 seconds (mean sampled reward: -2987.03). Current reward after update: -935.32, Optimal reward -723.58
Iteration 131 took 2.14 seconds (mean sampled reward: -2263.03). Current reward after update: -887.04, Optimal reward -723.58
Iteration 132 took 2.01 seconds (mean sampled reward: -4142.60). Current reward after update: -959.59, Optimal reward -723.58
Iteration 133 took 1.98 seconds (mean sampled reward: -3494.90). Current reward after update: -1288.65, Optimal reward -723.58
Iteration 134 took 2.05 seconds (mean sampled reward: -3694.19). Current reward after update: -946.82, Optimal reward -723.58
Iteration 135 took 2.11 seconds (mean sampled reward: -3491.94). Current reward after update: -1386.02, Optimal reward -723.58
Iteration 136 took 2.11 seconds (mean sampled reward: -2942.51). Current reward after update: -895.23, Optimal reward -723.58
Iteration 137 took 2.05 seconds (mean sampled reward: -3576.51). Current reward after update: -2669.03, Optimal reward -723.58
Iteration 138 took 2.08 seconds (mean sampled reward: -2953.84). Current reward after update: -842.50, Optimal reward -723.58
Iteration 139 took 2.11 seconds (mean sampled reward: -3656.90). Current reward after update: -1005.44, Optimal reward -723.58
Iteration 140 took 2.14 seconds (mean sampled reward: -2967.28). Current reward after update: -951.50, Optimal reward -723.58
Iteration 141 took 2.10 seconds (mean sampled reward: -3304.09). Current reward after update: -841.42, Optimal reward -723.58
Iteration 142 took 2.24 seconds (mean sampled reward: -3509.33). Current reward after update: -1146.30, Optimal reward -723.58
Iteration 143 took 2.06 seconds (mean sampled reward: -3034.04). Current reward after update: -962.03, Optimal reward -723.58
Iteration 144 took 2.03 seconds (mean sampled reward: -3394.77). Current reward after update: -890.05, Optimal reward -723.58
Iteration 145 took 2.01 seconds (mean sampled reward: -3528.09). Current reward after update: -832.10, Optimal reward -723.58
Iteration 146 took 2.02 seconds (mean sampled reward: -3410.26). Current reward after update: -1454.74, Optimal reward -723.58
Iteration 147 took 2.03 seconds (mean sampled reward: -3263.47). Current reward after update: -916.58, Optimal reward -723.58
Iteration 148 took 2.07 seconds (mean sampled reward: -3500.82). Current reward after update: -812.89, Optimal reward -723.58
Iteration 149 took 2.02 seconds (mean sampled reward: -3253.79). Current reward after update: -941.67, Optimal reward -723.58
Iteration 150 took 2.08 seconds (mean sampled reward: -2443.00). Current reward after update: -817.42, Optimal reward -723.58
Iteration 151 took 2.03 seconds (mean sampled reward: -2524.48). Current reward after update: -820.19, Optimal reward -723.58
Iteration 152 took 2.05 seconds (mean sampled reward: -2613.60). Current reward after update: -804.63, Optimal reward -723.58
Iteration 153 took 2.08 seconds (mean sampled reward: -2757.29). Current reward after update: -751.94, Optimal reward -723.58
Iteration 154 took 2.06 seconds (mean sampled reward: -2776.41). Current reward after update: -865.83, Optimal reward -723.58
Iteration 155 took 2.08 seconds (mean sampled reward: -3072.86). Current reward after update: -1800.57, Optimal reward -723.58
Iteration 156 took 2.04 seconds (mean sampled reward: -2845.74). Current reward after update: -978.50, Optimal reward -723.58
Iteration 157 took 2.08 seconds (mean sampled reward: -2997.87). Current reward after update: -852.87, Optimal reward -723.58
Iteration 158 took 2.13 seconds (mean sampled reward: -3046.39). Current reward after update: -895.20, Optimal reward -723.58
Iteration 159 took 2.02 seconds (mean sampled reward: -2990.68). Current reward after update: -768.69, Optimal reward -723.58
Iteration 160 took 2.01 seconds (mean sampled reward: -3064.28). Current reward after update: -845.18, Optimal reward -723.58
Iteration 161 took 2.01 seconds (mean sampled reward: -3333.49). Current reward after update: -2431.77, Optimal reward -723.58
Iteration 162 took 2.03 seconds (mean sampled reward: -3100.47). Current reward after update: -773.45, Optimal reward -723.58
Iteration 163 took 2.09 seconds (mean sampled reward: -2617.37). Current reward after update: -874.50, Optimal reward -723.58
Iteration 164 took 2.04 seconds (mean sampled reward: -2899.00). Current reward after update: -2776.18, Optimal reward -723.58
Iteration 165 took 1.99 seconds (mean sampled reward: -3833.08). Current reward after update: -983.93, Optimal reward -723.58
Iteration 166 took 2.07 seconds (mean sampled reward: -2758.09). Current reward after update: -862.44, Optimal reward -723.58
Iteration 167 took 2.01 seconds (mean sampled reward: -2860.63). Current reward after update: -774.49, Optimal reward -723.58
Iteration 168 took 2.05 seconds (mean sampled reward: -2239.84). Current reward after update: -783.51, Optimal reward -723.58
Iteration 169 took 2.04 seconds (mean sampled reward: -2577.83). Current reward after update: -822.13, Optimal reward -723.58
Iteration 170 took 2.05 seconds (mean sampled reward: -2786.26). Current reward after update: -742.10, Optimal reward -723.58
Iteration 171 took 2.08 seconds (mean sampled reward: -2706.20). Current reward after update: -656.17, Optimal reward -656.17
Iteration 172 took 2.06 seconds (mean sampled reward: -2826.14). Current reward after update: -834.60, Optimal reward -656.17
Iteration 173 took 2.11 seconds (mean sampled reward: -2767.29). Current reward after update: -699.67, Optimal reward -656.17
Iteration 174 took 2.04 seconds (mean sampled reward: -3199.43). Current reward after update: -1086.78, Optimal reward -656.17
Iteration 175 took 2.02 seconds (mean sampled reward: -3003.95). Current reward after update: -1077.16, Optimal reward -656.17
Iteration 176 took 2.08 seconds (mean sampled reward: -3333.92). Current reward after update: -1018.08, Optimal reward -656.17
Iteration 177 took 1.97 seconds (mean sampled reward: -4336.35). Current reward after update: -900.02, Optimal reward -656.17
Iteration 178 took 2.05 seconds (mean sampled reward: -3022.04). Current reward after update: -909.74, Optimal reward -656.17
Iteration 179 took 2.02 seconds (mean sampled reward: -4262.25). Current reward after update: -1048.54, Optimal reward -656.17
Iteration 180 took 1.98 seconds (mean sampled reward: -4454.40). Current reward after update: -862.99, Optimal reward -656.17
Iteration 181 took 1.94 seconds (mean sampled reward: -4681.93). Current reward after update: -819.68, Optimal reward -656.17
Iteration 182 took 1.96 seconds (mean sampled reward: -4500.43). Current reward after update: -917.64, Optimal reward -656.17
Iteration 183 took 1.94 seconds (mean sampled reward: -4878.60). Current reward after update: -786.70, Optimal reward -656.17
Iteration 184 took 1.91 seconds (mean sampled reward: -4534.58). Current reward after update: -923.94, Optimal reward -656.17
Iteration 185 took 1.97 seconds (mean sampled reward: -3411.84). Current reward after update: -1008.90, Optimal reward -656.17
Iteration 186 took 1.95 seconds (mean sampled reward: -4006.78). Current reward after update: -961.30, Optimal reward -656.17
Iteration 187 took 1.97 seconds (mean sampled reward: -3679.45). Current reward after update: -765.81, Optimal reward -656.17
Iteration 188 took 1.96 seconds (mean sampled reward: -4623.89). Current reward after update: -791.48, Optimal reward -656.17
Iteration 189 took 2.00 seconds (mean sampled reward: -4448.95). Current reward after update: -892.38, Optimal reward -656.17
Iteration 190 took 2.06 seconds (mean sampled reward: -3347.77). Current reward after update: -904.83, Optimal reward -656.17
Iteration 191 took 1.92 seconds (mean sampled reward: -4764.24). Current reward after update: -1023.42, Optimal reward -656.17
Iteration 192 took 1.90 seconds (mean sampled reward: -4980.49). Current reward after update: -1082.49, Optimal reward -656.17
Iteration 193 took 1.91 seconds (mean sampled reward: -4610.14). Current reward after update: -800.87, Optimal reward -656.17
Iteration 194 took 1.88 seconds (mean sampled reward: -5086.35). Current reward after update: -788.41, Optimal reward -656.17
Iteration 195 took 1.92 seconds (mean sampled reward: -4599.72). Current reward after update: -849.47, Optimal reward -656.17
Iteration 196 took 1.98 seconds (mean sampled reward: -3798.18). Current reward after update: -727.39, Optimal reward -656.17
Iteration 197 took 1.95 seconds (mean sampled reward: -3739.95). Current reward after update: -886.60, Optimal reward -656.17
Iteration 198 took 1.96 seconds (mean sampled reward: -3863.10). Current reward after update: -1236.73, Optimal reward -656.17
Iteration 199 took 1.99 seconds (mean sampled reward: -3974.34). Current reward after update: -762.16, Optimal reward -656.17
Iteration 200 took 2.03 seconds (mean sampled reward: -2452.91). Current reward after update: -746.28, Optimal reward -656.17
Max force: 10 Sigma: 0.8 mean rewards: -893.3804057026008, best rewards:-656.1703553773041

Iteration 1 took 2.20 seconds (mean sampled reward: -7501.20). Current reward after update: -7216.22, Optimal reward -7216.22
Iteration 2 took 2.15 seconds (mean sampled reward: -7475.82). Current reward after update: -7203.42, Optimal reward -7203.42
Iteration 3 took 2.21 seconds (mean sampled reward: -7441.84). Current reward after update: -7170.58, Optimal reward -7170.58
Iteration 4 took 2.38 seconds (mean sampled reward: -7308.78). Current reward after update: -7110.37, Optimal reward -7110.37
Iteration 5 took 2.35 seconds (mean sampled reward: -7341.83). Current reward after update: -7093.05, Optimal reward -7093.05
Iteration 6 took 2.35 seconds (mean sampled reward: -7364.66). Current reward after update: -7051.66, Optimal reward -7051.66
Iteration 7 took 2.29 seconds (mean sampled reward: -7343.92). Current reward after update: -6926.70, Optimal reward -6926.70
Iteration 8 took 2.47 seconds (mean sampled reward: -7422.98). Current reward after update: -6601.69, Optimal reward -6601.69
Iteration 9 took 2.43 seconds (mean sampled reward: -7135.17). Current reward after update: -6316.73, Optimal reward -6316.73
Iteration 10 took 2.39 seconds (mean sampled reward: -6999.22). Current reward after update: -5960.31, Optimal reward -5960.31
Iteration 11 took 2.54 seconds (mean sampled reward: -6619.28). Current reward after update: -5833.38, Optimal reward -5833.38
Iteration 12 took 2.49 seconds (mean sampled reward: -6560.17). Current reward after update: -5741.70, Optimal reward -5741.70
Iteration 13 took 2.48 seconds (mean sampled reward: -6408.52). Current reward after update: -5427.89, Optimal reward -5427.89
Iteration 14 took 2.47 seconds (mean sampled reward: -6061.34). Current reward after update: -4999.83, Optimal reward -4999.83
Iteration 15 took 2.57 seconds (mean sampled reward: -5993.67). Current reward after update: -4634.59, Optimal reward -4634.59
Iteration 16 took 2.47 seconds (mean sampled reward: -5938.06). Current reward after update: -4652.01, Optimal reward -4634.59
Iteration 17 took 2.42 seconds (mean sampled reward: -6262.61). Current reward after update: -4641.39, Optimal reward -4634.59
Iteration 18 took 2.53 seconds (mean sampled reward: -6116.18). Current reward after update: -4270.02, Optimal reward -4270.02
Iteration 19 took 2.44 seconds (mean sampled reward: -5590.68). Current reward after update: -4307.75, Optimal reward -4270.02
Iteration 20 took 2.45 seconds (mean sampled reward: -6212.39). Current reward after update: -4226.95, Optimal reward -4226.95
Iteration 21 took 2.47 seconds (mean sampled reward: -6097.99). Current reward after update: -4232.53, Optimal reward -4226.95
Iteration 22 took 2.44 seconds (mean sampled reward: -6047.27). Current reward after update: -4231.79, Optimal reward -4226.95
Iteration 23 took 2.56 seconds (mean sampled reward: -5588.97). Current reward after update: -3312.03, Optimal reward -3312.03
Iteration 24 took 2.51 seconds (mean sampled reward: -5258.82). Current reward after update: -3401.21, Optimal reward -3312.03
Iteration 25 took 2.48 seconds (mean sampled reward: -5758.31). Current reward after update: -3826.73, Optimal reward -3312.03
Iteration 26 took 2.45 seconds (mean sampled reward: -5868.87). Current reward after update: -3495.90, Optimal reward -3312.03
Iteration 27 took 2.60 seconds (mean sampled reward: -5762.71). Current reward after update: -3068.54, Optimal reward -3068.54
Iteration 28 took 2.71 seconds (mean sampled reward: -5419.74). Current reward after update: -3694.07, Optimal reward -3068.54
Iteration 29 took 2.68 seconds (mean sampled reward: -5485.22). Current reward after update: -3225.06, Optimal reward -3068.54
Iteration 30 took 2.81 seconds (mean sampled reward: -5392.80). Current reward after update: -2872.95, Optimal reward -2872.95
Iteration 31 took 2.48 seconds (mean sampled reward: -5735.59). Current reward after update: -2989.52, Optimal reward -2872.95
Iteration 32 took 2.37 seconds (mean sampled reward: -5052.84). Current reward after update: -5459.78, Optimal reward -2872.95
Iteration 33 took 2.54 seconds (mean sampled reward: -5069.92). Current reward after update: -2742.64, Optimal reward -2742.64
Iteration 34 took 2.54 seconds (mean sampled reward: -5184.89). Current reward after update: -2869.90, Optimal reward -2742.64
Iteration 35 took 2.36 seconds (mean sampled reward: -5046.45). Current reward after update: -2818.74, Optimal reward -2742.64
Iteration 36 took 2.62 seconds (mean sampled reward: -5231.39). Current reward after update: -2736.32, Optimal reward -2736.32
Iteration 37 took 2.33 seconds (mean sampled reward: -5417.81). Current reward after update: -2906.38, Optimal reward -2736.32
Iteration 38 took 2.28 seconds (mean sampled reward: -4503.96). Current reward after update: -5637.36, Optimal reward -2736.32
Iteration 39 took 2.31 seconds (mean sampled reward: -4855.02). Current reward after update: -2696.31, Optimal reward -2696.31
Iteration 40 took 2.27 seconds (mean sampled reward: -4335.46). Current reward after update: -2651.70, Optimal reward -2651.70
Iteration 41 took 2.22 seconds (mean sampled reward: -4188.89). Current reward after update: -2581.70, Optimal reward -2581.70
Iteration 42 took 2.19 seconds (mean sampled reward: -4186.52). Current reward after update: -2504.37, Optimal reward -2504.37
Iteration 43 took 2.27 seconds (mean sampled reward: -4691.60). Current reward after update: -3282.47, Optimal reward -2504.37
Iteration 44 took 2.29 seconds (mean sampled reward: -4745.45). Current reward after update: -2606.75, Optimal reward -2504.37
Iteration 45 took 2.29 seconds (mean sampled reward: -4609.43). Current reward after update: -3077.57, Optimal reward -2504.37
Iteration 46 took 2.39 seconds (mean sampled reward: -5052.21). Current reward after update: -2573.21, Optimal reward -2504.37
Iteration 47 took 2.20 seconds (mean sampled reward: -4453.66). Current reward after update: -5553.03, Optimal reward -2504.37
Iteration 48 took 2.45 seconds (mean sampled reward: -4792.92). Current reward after update: -2508.16, Optimal reward -2504.37
Iteration 49 took 2.19 seconds (mean sampled reward: -4355.63). Current reward after update: -5458.98, Optimal reward -2504.37
Iteration 50 took 2.19 seconds (mean sampled reward: -4313.59). Current reward after update: -4446.75, Optimal reward -2504.37
Iteration 51 took 2.18 seconds (mean sampled reward: -4315.01). Current reward after update: -2469.36, Optimal reward -2469.36
Iteration 52 took 2.19 seconds (mean sampled reward: -4310.97). Current reward after update: -2299.44, Optimal reward -2299.44
Iteration 53 took 2.19 seconds (mean sampled reward: -3900.30). Current reward after update: -2041.37, Optimal reward -2041.37
Iteration 54 took 2.38 seconds (mean sampled reward: -3661.65). Current reward after update: -3668.07, Optimal reward -2041.37
Iteration 55 took 2.46 seconds (mean sampled reward: -4321.02). Current reward after update: -1987.04, Optimal reward -1987.04
Iteration 56 took 2.33 seconds (mean sampled reward: -4506.13). Current reward after update: -2035.90, Optimal reward -1987.04
Iteration 57 took 2.50 seconds (mean sampled reward: -4754.15). Current reward after update: -1938.94, Optimal reward -1938.94
Iteration 58 took 2.45 seconds (mean sampled reward: -4353.31). Current reward after update: -1883.04, Optimal reward -1883.04
Iteration 59 took 2.42 seconds (mean sampled reward: -4391.56). Current reward after update: -1948.73, Optimal reward -1883.04
Iteration 60 took 2.24 seconds (mean sampled reward: -4824.89). Current reward after update: -2013.11, Optimal reward -1883.04
Iteration 61 took 2.16 seconds (mean sampled reward: -4929.59). Current reward after update: -2012.84, Optimal reward -1883.04
Iteration 62 took 2.26 seconds (mean sampled reward: -4050.53). Current reward after update: -1957.24, Optimal reward -1883.04
Iteration 63 took 2.33 seconds (mean sampled reward: -4012.11). Current reward after update: -2252.97, Optimal reward -1883.04
Iteration 64 took 2.14 seconds (mean sampled reward: -4333.92). Current reward after update: -4233.33, Optimal reward -1883.04
Iteration 65 took 2.25 seconds (mean sampled reward: -4826.21). Current reward after update: -2013.25, Optimal reward -1883.04
Iteration 66 took 2.35 seconds (mean sampled reward: -4450.13). Current reward after update: -2021.20, Optimal reward -1883.04
Iteration 67 took 2.28 seconds (mean sampled reward: -4744.89). Current reward after update: -2019.36, Optimal reward -1883.04
Iteration 68 took 2.23 seconds (mean sampled reward: -4574.59). Current reward after update: -2212.27, Optimal reward -1883.04
Iteration 69 took 2.21 seconds (mean sampled reward: -4768.37). Current reward after update: -1989.61, Optimal reward -1883.04
Iteration 70 took 2.22 seconds (mean sampled reward: -4846.70). Current reward after update: -2079.33, Optimal reward -1883.04
Iteration 71 took 2.20 seconds (mean sampled reward: -4460.93). Current reward after update: -1950.80, Optimal reward -1883.04
Iteration 72 took 2.19 seconds (mean sampled reward: -4384.54). Current reward after update: -2065.46, Optimal reward -1883.04
Iteration 73 took 2.20 seconds (mean sampled reward: -4415.48). Current reward after update: -2017.20, Optimal reward -1883.04
Iteration 74 took 2.20 seconds (mean sampled reward: -4402.31). Current reward after update: -1794.74, Optimal reward -1794.74
Iteration 75 took 2.21 seconds (mean sampled reward: -4716.60). Current reward after update: -2241.96, Optimal reward -1794.74
Iteration 76 took 2.21 seconds (mean sampled reward: -4420.81). Current reward after update: -2039.85, Optimal reward -1794.74
Iteration 77 took 2.17 seconds (mean sampled reward: -4284.47). Current reward after update: -4815.51, Optimal reward -1794.74
Iteration 78 took 2.22 seconds (mean sampled reward: -4199.74). Current reward after update: -2302.65, Optimal reward -1794.74
Iteration 79 took 2.26 seconds (mean sampled reward: -3918.29). Current reward after update: -2055.22, Optimal reward -1794.74
Iteration 80 took 2.20 seconds (mean sampled reward: -4581.30). Current reward after update: -1609.55, Optimal reward -1609.55
Iteration 81 took 2.25 seconds (mean sampled reward: -4685.47). Current reward after update: -1902.05, Optimal reward -1609.55
Iteration 82 took 2.20 seconds (mean sampled reward: -4361.60). Current reward after update: -1392.18, Optimal reward -1392.18
Iteration 83 took 2.19 seconds (mean sampled reward: -3944.85). Current reward after update: -1432.23, Optimal reward -1392.18
Iteration 84 took 2.24 seconds (mean sampled reward: -3920.49). Current reward after update: -1379.43, Optimal reward -1379.43
Iteration 85 took 2.28 seconds (mean sampled reward: -3715.23). Current reward after update: -1333.31, Optimal reward -1333.31
Iteration 86 took 2.21 seconds (mean sampled reward: -3703.55). Current reward after update: -1388.75, Optimal reward -1333.31
Iteration 87 took 2.23 seconds (mean sampled reward: -3517.42). Current reward after update: -1861.78, Optimal reward -1333.31
Iteration 88 took 2.14 seconds (mean sampled reward: -4978.02). Current reward after update: -1476.17, Optimal reward -1333.31
Iteration 89 took 2.34 seconds (mean sampled reward: -3497.76). Current reward after update: -3058.84, Optimal reward -1333.31
Iteration 90 took 2.24 seconds (mean sampled reward: -4294.59). Current reward after update: -1359.34, Optimal reward -1333.31
Iteration 91 took 2.23 seconds (mean sampled reward: -4525.18). Current reward after update: -1454.07, Optimal reward -1333.31
Iteration 92 took 2.15 seconds (mean sampled reward: -4879.46). Current reward after update: -1340.07, Optimal reward -1333.31
Iteration 93 took 2.14 seconds (mean sampled reward: -4389.55). Current reward after update: -1549.48, Optimal reward -1333.31
Iteration 94 took 2.22 seconds (mean sampled reward: -4362.14). Current reward after update: -1907.78, Optimal reward -1333.31
Iteration 95 took 2.26 seconds (mean sampled reward: -4625.24). Current reward after update: -1507.69, Optimal reward -1333.31
Iteration 96 took 2.23 seconds (mean sampled reward: -4159.69). Current reward after update: -1766.03, Optimal reward -1333.31
Iteration 97 took 2.10 seconds (mean sampled reward: -4883.74). Current reward after update: -1473.61, Optimal reward -1333.31
Iteration 98 took 2.12 seconds (mean sampled reward: -5401.32). Current reward after update: -2201.12, Optimal reward -1333.31
Iteration 99 took 2.06 seconds (mean sampled reward: -5256.67). Current reward after update: -6768.58, Optimal reward -1333.31
Iteration 100 took 2.12 seconds (mean sampled reward: -5082.46). Current reward after update: -1862.14, Optimal reward -1333.31
Iteration 101 took 2.17 seconds (mean sampled reward: -5430.66). Current reward after update: -1527.24, Optimal reward -1333.31
Iteration 102 took 2.11 seconds (mean sampled reward: -4990.02). Current reward after update: -2242.13, Optimal reward -1333.31
Iteration 103 took 2.19 seconds (mean sampled reward: -5126.09). Current reward after update: -1521.45, Optimal reward -1333.31
Iteration 104 took 2.09 seconds (mean sampled reward: -5444.14). Current reward after update: -1442.99, Optimal reward -1333.31
Iteration 105 took 2.22 seconds (mean sampled reward: -4957.59). Current reward after update: -1570.12, Optimal reward -1333.31
Iteration 106 took 2.24 seconds (mean sampled reward: -4911.19). Current reward after update: -1831.99, Optimal reward -1333.31
Iteration 107 took 2.31 seconds (mean sampled reward: -5607.33). Current reward after update: -1549.69, Optimal reward -1333.31
Iteration 108 took 2.27 seconds (mean sampled reward: -5736.22). Current reward after update: -1418.42, Optimal reward -1333.31
Iteration 109 took 2.15 seconds (mean sampled reward: -5613.41). Current reward after update: -1442.47, Optimal reward -1333.31
Iteration 110 took 2.26 seconds (mean sampled reward: -5197.00). Current reward after update: -1391.17, Optimal reward -1333.31
Iteration 111 took 2.28 seconds (mean sampled reward: -5006.67). Current reward after update: -1394.76, Optimal reward -1333.31
Iteration 112 took 2.29 seconds (mean sampled reward: -5620.58). Current reward after update: -1246.31, Optimal reward -1246.31
Iteration 113 took 2.19 seconds (mean sampled reward: -4614.37). Current reward after update: -1388.23, Optimal reward -1246.31
Iteration 114 took 2.22 seconds (mean sampled reward: -4987.55). Current reward after update: -1387.54, Optimal reward -1246.31
Iteration 115 took 2.27 seconds (mean sampled reward: -5468.59). Current reward after update: -1347.41, Optimal reward -1246.31
Iteration 116 took 2.14 seconds (mean sampled reward: -6281.91). Current reward after update: -1405.40, Optimal reward -1246.31
Iteration 117 took 2.16 seconds (mean sampled reward: -4707.09). Current reward after update: -1290.05, Optimal reward -1246.31
Iteration 118 took 2.27 seconds (mean sampled reward: -4173.54). Current reward after update: -1284.37, Optimal reward -1246.31
Iteration 119 took 2.30 seconds (mean sampled reward: -3992.80). Current reward after update: -1332.50, Optimal reward -1246.31
Iteration 120 took 2.19 seconds (mean sampled reward: -5111.68). Current reward after update: -1336.41, Optimal reward -1246.31
Iteration 121 took 2.09 seconds (mean sampled reward: -5601.82). Current reward after update: -6923.87, Optimal reward -1246.31
Iteration 122 took 2.05 seconds (mean sampled reward: -6086.18). Current reward after update: -1342.54, Optimal reward -1246.31
Iteration 123 took 2.18 seconds (mean sampled reward: -5727.46). Current reward after update: -1394.62, Optimal reward -1246.31
Iteration 124 took 2.23 seconds (mean sampled reward: -5411.44). Current reward after update: -1361.22, Optimal reward -1246.31
Iteration 125 took 2.17 seconds (mean sampled reward: -5392.22). Current reward after update: -1407.05, Optimal reward -1246.31
Iteration 126 took 2.18 seconds (mean sampled reward: -4674.79). Current reward after update: -7161.25, Optimal reward -1246.31
Iteration 127 took 2.40 seconds (mean sampled reward: -5863.55). Current reward after update: -1316.52, Optimal reward -1246.31
Iteration 128 took 2.24 seconds (mean sampled reward: -5132.09). Current reward after update: -1313.69, Optimal reward -1246.31
Iteration 129 took 2.31 seconds (mean sampled reward: -4636.29). Current reward after update: -1306.83, Optimal reward -1246.31
Iteration 130 took 2.28 seconds (mean sampled reward: -5188.01). Current reward after update: -1329.71, Optimal reward -1246.31
Iteration 131 took 2.23 seconds (mean sampled reward: -5047.63). Current reward after update: -1363.74, Optimal reward -1246.31
Iteration 132 took 2.30 seconds (mean sampled reward: -4989.39). Current reward after update: -6832.28, Optimal reward -1246.31
Iteration 133 took 2.16 seconds (mean sampled reward: -4777.07). Current reward after update: -1314.09, Optimal reward -1246.31
Iteration 134 took 2.09 seconds (mean sampled reward: -4482.99). Current reward after update: -1504.33, Optimal reward -1246.31
Iteration 135 took 2.08 seconds (mean sampled reward: -5024.98). Current reward after update: -1319.88, Optimal reward -1246.31
Iteration 136 took 2.24 seconds (mean sampled reward: -4890.56). Current reward after update: -1303.69, Optimal reward -1246.31
Iteration 137 took 2.19 seconds (mean sampled reward: -5125.92). Current reward after update: -1363.88, Optimal reward -1246.31
Iteration 138 took 2.18 seconds (mean sampled reward: -4677.50). Current reward after update: -1287.12, Optimal reward -1246.31
Iteration 139 took 2.09 seconds (mean sampled reward: -5313.19). Current reward after update: -1248.77, Optimal reward -1246.31
Iteration 140 took 2.03 seconds (mean sampled reward: -6111.36). Current reward after update: -6696.15, Optimal reward -1246.31
Iteration 141 took 2.04 seconds (mean sampled reward: -5054.22). Current reward after update: -1248.36, Optimal reward -1246.31
Iteration 142 took 2.04 seconds (mean sampled reward: -4948.79). Current reward after update: -1202.06, Optimal reward -1202.06
Iteration 143 took 2.08 seconds (mean sampled reward: -4887.71). Current reward after update: -1217.71, Optimal reward -1202.06
Iteration 144 took 2.10 seconds (mean sampled reward: -4260.77). Current reward after update: -1183.37, Optimal reward -1183.37
Iteration 145 took 2.08 seconds (mean sampled reward: -4154.26). Current reward after update: -1238.84, Optimal reward -1183.37
Iteration 146 took 2.14 seconds (mean sampled reward: -4226.47). Current reward after update: -1636.33, Optimal reward -1183.37
Iteration 147 took 2.16 seconds (mean sampled reward: -3915.87). Current reward after update: -1214.88, Optimal reward -1183.37
Iteration 148 took 2.13 seconds (mean sampled reward: -4505.78). Current reward after update: -1688.45, Optimal reward -1183.37
Iteration 149 took 2.21 seconds (mean sampled reward: -3749.80). Current reward after update: -2287.62, Optimal reward -1183.37
Iteration 150 took 2.17 seconds (mean sampled reward: -3922.30). Current reward after update: -1191.27, Optimal reward -1183.37
Iteration 151 took 2.14 seconds (mean sampled reward: -3897.70). Current reward after update: -6746.63, Optimal reward -1183.37
Iteration 152 took 2.17 seconds (mean sampled reward: -3987.65). Current reward after update: -1463.25, Optimal reward -1183.37
Iteration 153 took 2.11 seconds (mean sampled reward: -4412.21). Current reward after update: -1456.40, Optimal reward -1183.37
Iteration 154 took 2.18 seconds (mean sampled reward: -4309.86). Current reward after update: -1136.45, Optimal reward -1136.45
Iteration 155 took 2.16 seconds (mean sampled reward: -4270.24). Current reward after update: -1419.65, Optimal reward -1136.45
Iteration 156 took 2.10 seconds (mean sampled reward: -4013.73). Current reward after update: -1170.32, Optimal reward -1136.45
Iteration 157 took 2.16 seconds (mean sampled reward: -3404.18). Current reward after update: -1111.62, Optimal reward -1111.62
Iteration 158 took 2.13 seconds (mean sampled reward: -3543.38). Current reward after update: -1148.73, Optimal reward -1111.62
Iteration 159 took 2.18 seconds (mean sampled reward: -3345.23). Current reward after update: -1074.60, Optimal reward -1074.60
Iteration 160 took 2.22 seconds (mean sampled reward: -4407.99). Current reward after update: -1241.18, Optimal reward -1074.60
Iteration 161 took 2.10 seconds (mean sampled reward: -4839.69). Current reward after update: -1324.30, Optimal reward -1074.60
Iteration 162 took 2.14 seconds (mean sampled reward: -5492.07). Current reward after update: -1022.08, Optimal reward -1022.08
Iteration 163 took 2.15 seconds (mean sampled reward: -5346.76). Current reward after update: -1762.43, Optimal reward -1022.08
Iteration 164 took 2.14 seconds (mean sampled reward: -4669.66). Current reward after update: -1138.91, Optimal reward -1022.08
Iteration 165 took 2.20 seconds (mean sampled reward: -4712.79). Current reward after update: -6805.13, Optimal reward -1022.08
Iteration 166 took 2.24 seconds (mean sampled reward: -4504.51). Current reward after update: -1047.66, Optimal reward -1022.08
Iteration 167 took 2.21 seconds (mean sampled reward: -5456.33). Current reward after update: -1071.78, Optimal reward -1022.08
Iteration 168 took 2.12 seconds (mean sampled reward: -5523.69). Current reward after update: -1111.91, Optimal reward -1022.08
Iteration 169 took 2.24 seconds (mean sampled reward: -5144.90). Current reward after update: -1170.18, Optimal reward -1022.08
Iteration 170 took 2.18 seconds (mean sampled reward: -5194.39). Current reward after update: -1253.10, Optimal reward -1022.08
Iteration 171 took 2.11 seconds (mean sampled reward: -5333.76). Current reward after update: -1073.65, Optimal reward -1022.08
Iteration 172 took 2.19 seconds (mean sampled reward: -5835.51). Current reward after update: -1137.56, Optimal reward -1022.08
Iteration 173 took 2.24 seconds (mean sampled reward: -5512.19). Current reward after update: -1066.78, Optimal reward -1022.08
Iteration 174 took 2.27 seconds (mean sampled reward: -6064.38). Current reward after update: -1416.40, Optimal reward -1022.08
Iteration 175 took 2.29 seconds (mean sampled reward: -5733.64). Current reward after update: -1110.10, Optimal reward -1022.08
Iteration 176 took 2.30 seconds (mean sampled reward: -6021.99). Current reward after update: -1175.14, Optimal reward -1022.08
Iteration 177 took 2.38 seconds (mean sampled reward: -5960.78). Current reward after update: -1240.34, Optimal reward -1022.08
Iteration 178 took 2.31 seconds (mean sampled reward: -6256.55). Current reward after update: -1173.00, Optimal reward -1022.08
Iteration 179 took 2.22 seconds (mean sampled reward: -6121.14). Current reward after update: -1844.71, Optimal reward -1022.08
Iteration 180 took 2.20 seconds (mean sampled reward: -6109.77). Current reward after update: -1295.14, Optimal reward -1022.08
Iteration 181 took 2.19 seconds (mean sampled reward: -6179.06). Current reward after update: -1186.24, Optimal reward -1022.08
Iteration 182 took 2.15 seconds (mean sampled reward: -5799.01). Current reward after update: -1219.85, Optimal reward -1022.08
Iteration 183 took 2.15 seconds (mean sampled reward: -5677.89). Current reward after update: -1108.35, Optimal reward -1022.08
Iteration 184 took 2.15 seconds (mean sampled reward: -5090.98). Current reward after update: -1209.47, Optimal reward -1022.08
Iteration 185 took 2.27 seconds (mean sampled reward: -5207.87). Current reward after update: -1157.32, Optimal reward -1022.08
Iteration 186 took 2.14 seconds (mean sampled reward: -4752.17). Current reward after update: -1166.42, Optimal reward -1022.08
Iteration 187 took 2.12 seconds (mean sampled reward: -4618.70). Current reward after update: -2227.10, Optimal reward -1022.08
Iteration 188 took 2.15 seconds (mean sampled reward: -4359.10). Current reward after update: -4176.54, Optimal reward -1022.08
Iteration 189 took 2.21 seconds (mean sampled reward: -4441.70). Current reward after update: -1085.51, Optimal reward -1022.08
Iteration 190 took 2.11 seconds (mean sampled reward: -4379.68). Current reward after update: -1082.50, Optimal reward -1022.08
Iteration 191 took 2.11 seconds (mean sampled reward: -4021.04). Current reward after update: -1199.24, Optimal reward -1022.08
Iteration 192 took 2.20 seconds (mean sampled reward: -3789.60). Current reward after update: -1114.59, Optimal reward -1022.08
Iteration 193 took 2.10 seconds (mean sampled reward: -4528.98). Current reward after update: -6696.13, Optimal reward -1022.08
Iteration 194 took 2.09 seconds (mean sampled reward: -4804.15). Current reward after update: -1136.03, Optimal reward -1022.08
Iteration 195 took 2.00 seconds (mean sampled reward: -5189.43). Current reward after update: -1090.23, Optimal reward -1022.08
Iteration 196 took 2.05 seconds (mean sampled reward: -4663.75). Current reward after update: -6757.79, Optimal reward -1022.08
Iteration 197 took 2.10 seconds (mean sampled reward: -4381.20). Current reward after update: -1124.03, Optimal reward -1022.08
Iteration 198 took 2.11 seconds (mean sampled reward: -4072.33). Current reward after update: -1162.88, Optimal reward -1022.08
Iteration 199 took 2.10 seconds (mean sampled reward: -4654.15). Current reward after update: -6657.25, Optimal reward -1022.08
Iteration 200 took 2.23 seconds (mean sampled reward: -4014.34). Current reward after update: -1150.74, Optimal reward -1022.08
Iteration 1 took 2.20 seconds (mean sampled reward: -7512.14). Current reward after update: -7221.45, Optimal reward -7221.45
Iteration 2 took 2.39 seconds (mean sampled reward: -7424.70). Current reward after update: -7313.68, Optimal reward -7221.45
Iteration 3 took 1.96 seconds (mean sampled reward: -7397.82). Current reward after update: -7170.03, Optimal reward -7170.03
Iteration 4 took 1.99 seconds (mean sampled reward: -7323.14). Current reward after update: -7032.84, Optimal reward -7032.84
Iteration 5 took 1.92 seconds (mean sampled reward: -7302.76). Current reward after update: -6813.30, Optimal reward -6813.30
Iteration 6 took 2.17 seconds (mean sampled reward: -7287.46). Current reward after update: -6227.73, Optimal reward -6227.73
Iteration 7 took 2.22 seconds (mean sampled reward: -7253.35). Current reward after update: -5944.48, Optimal reward -5944.48
Iteration 8 took 2.47 seconds (mean sampled reward: -6996.09). Current reward after update: -5356.73, Optimal reward -5356.73
Iteration 9 took 2.44 seconds (mean sampled reward: -6747.98). Current reward after update: -5132.50, Optimal reward -5132.50
Iteration 10 took 2.51 seconds (mean sampled reward: -6736.68). Current reward after update: -5067.71, Optimal reward -5067.71
Iteration 11 took 2.31 seconds (mean sampled reward: -7034.69). Current reward after update: -5223.90, Optimal reward -5067.71
Iteration 12 took 2.19 seconds (mean sampled reward: -6233.66). Current reward after update: -4595.51, Optimal reward -4595.51
Iteration 13 took 2.25 seconds (mean sampled reward: -5536.87). Current reward after update: -4512.22, Optimal reward -4512.22
Iteration 14 took 2.21 seconds (mean sampled reward: -5890.32). Current reward after update: -4641.62, Optimal reward -4512.22
Iteration 15 took 2.19 seconds (mean sampled reward: -5874.25). Current reward after update: -4537.06, Optimal reward -4512.22
Iteration 16 took 2.31 seconds (mean sampled reward: -6135.50). Current reward after update: -4517.28, Optimal reward -4512.22
Iteration 17 took 2.37 seconds (mean sampled reward: -5293.32). Current reward after update: -4256.42, Optimal reward -4256.42
Iteration 18 took 2.24 seconds (mean sampled reward: -4850.54). Current reward after update: -3940.81, Optimal reward -3940.81
Iteration 19 took 2.27 seconds (mean sampled reward: -4778.37). Current reward after update: -3821.09, Optimal reward -3821.09
Iteration 20 took 2.56 seconds (mean sampled reward: -4750.56). Current reward after update: -4245.36, Optimal reward -3821.09
Iteration 21 took 2.35 seconds (mean sampled reward: -5100.54). Current reward after update: -3820.93, Optimal reward -3820.93
Iteration 22 took 2.37 seconds (mean sampled reward: -4890.16). Current reward after update: -3732.90, Optimal reward -3732.90
Iteration 23 took 2.35 seconds (mean sampled reward: -4966.76). Current reward after update: -3749.77, Optimal reward -3732.90
Iteration 24 took 2.40 seconds (mean sampled reward: -4653.27). Current reward after update: -3687.45, Optimal reward -3687.45
Iteration 25 took 2.31 seconds (mean sampled reward: -4536.42). Current reward after update: -3738.37, Optimal reward -3687.45
Iteration 26 took 2.38 seconds (mean sampled reward: -4465.45). Current reward after update: -3344.94, Optimal reward -3344.94
Iteration 27 took 2.46 seconds (mean sampled reward: -4914.11). Current reward after update: -3045.01, Optimal reward -3045.01
Iteration 28 took 2.58 seconds (mean sampled reward: -4673.56). Current reward after update: -3061.51, Optimal reward -3045.01
Iteration 29 took 2.48 seconds (mean sampled reward: -4939.94). Current reward after update: -3019.25, Optimal reward -3019.25
Iteration 30 took 2.59 seconds (mean sampled reward: -5310.80). Current reward after update: -2841.99, Optimal reward -2841.99
Iteration 31 took 2.42 seconds (mean sampled reward: -5881.69). Current reward after update: -3056.72, Optimal reward -2841.99
Iteration 32 took 2.40 seconds (mean sampled reward: -4915.79). Current reward after update: -2898.18, Optimal reward -2841.99
Iteration 33 took 2.27 seconds (mean sampled reward: -4519.85). Current reward after update: -2498.69, Optimal reward -2498.69
Iteration 34 took 2.48 seconds (mean sampled reward: -4834.30). Current reward after update: -2650.75, Optimal reward -2498.69
Iteration 35 took 2.30 seconds (mean sampled reward: -4495.22). Current reward after update: -2605.47, Optimal reward -2498.69
Iteration 36 took 2.48 seconds (mean sampled reward: -4901.98). Current reward after update: -1717.52, Optimal reward -1717.52
Iteration 37 took 2.18 seconds (mean sampled reward: -5269.90). Current reward after update: -2624.36, Optimal reward -1717.52
Iteration 38 took 2.21 seconds (mean sampled reward: -5485.61). Current reward after update: -2439.23, Optimal reward -1717.52
Iteration 39 took 2.33 seconds (mean sampled reward: -4822.77). Current reward after update: -2521.14, Optimal reward -1717.52
Iteration 40 took 2.33 seconds (mean sampled reward: -5455.86). Current reward after update: -2579.72, Optimal reward -1717.52
Iteration 41 took 2.23 seconds (mean sampled reward: -5078.81). Current reward after update: -2483.98, Optimal reward -1717.52
Iteration 42 took 2.20 seconds (mean sampled reward: -4817.49). Current reward after update: -2529.15, Optimal reward -1717.52
Iteration 43 took 2.31 seconds (mean sampled reward: -5085.77). Current reward after update: -2275.26, Optimal reward -1717.52
Iteration 44 took 2.25 seconds (mean sampled reward: -4045.17). Current reward after update: -2076.62, Optimal reward -1717.52
Iteration 45 took 2.25 seconds (mean sampled reward: -5020.49). Current reward after update: -1835.15, Optimal reward -1717.52
Iteration 46 took 2.20 seconds (mean sampled reward: -5629.91). Current reward after update: -2032.67, Optimal reward -1717.52
Iteration 47 took 2.22 seconds (mean sampled reward: -4568.42). Current reward after update: -1869.46, Optimal reward -1717.52
Iteration 48 took 2.32 seconds (mean sampled reward: -3780.38). Current reward after update: -1827.62, Optimal reward -1717.52
Iteration 49 took 2.27 seconds (mean sampled reward: -3042.62). Current reward after update: -1674.89, Optimal reward -1674.89
Iteration 50 took 2.23 seconds (mean sampled reward: -3495.77). Current reward after update: -1492.82, Optimal reward -1492.82
Iteration 51 took 2.27 seconds (mean sampled reward: -2986.42). Current reward after update: -1455.94, Optimal reward -1455.94
Iteration 52 took 2.19 seconds (mean sampled reward: -3377.27). Current reward after update: -1554.21, Optimal reward -1455.94
Iteration 53 took 2.30 seconds (mean sampled reward: -3587.97). Current reward after update: -1993.31, Optimal reward -1455.94
Iteration 54 took 2.33 seconds (mean sampled reward: -3614.98). Current reward after update: -1473.16, Optimal reward -1455.94
Iteration 55 took 2.28 seconds (mean sampled reward: -3130.19). Current reward after update: -2067.28, Optimal reward -1455.94
Iteration 56 took 2.18 seconds (mean sampled reward: -2494.45). Current reward after update: -1548.64, Optimal reward -1455.94
Iteration 57 took 2.26 seconds (mean sampled reward: -2432.16). Current reward after update: -1418.05, Optimal reward -1418.05
Iteration 58 took 2.31 seconds (mean sampled reward: -3065.12). Current reward after update: -2333.15, Optimal reward -1418.05
Iteration 59 took 2.46 seconds (mean sampled reward: -2392.00). Current reward after update: -2169.73, Optimal reward -1418.05
Iteration 60 took 2.29 seconds (mean sampled reward: -2172.72). Current reward after update: -1445.10, Optimal reward -1418.05
Iteration 61 took 2.33 seconds (mean sampled reward: -2440.16). Current reward after update: -1442.87, Optimal reward -1418.05
Iteration 62 took 2.23 seconds (mean sampled reward: -2488.58). Current reward after update: -1493.34, Optimal reward -1418.05
Iteration 63 took 2.22 seconds (mean sampled reward: -3088.98). Current reward after update: -1394.86, Optimal reward -1394.86
Iteration 64 took 2.20 seconds (mean sampled reward: -2663.78). Current reward after update: -1942.51, Optimal reward -1394.86
Iteration 65 took 2.21 seconds (mean sampled reward: -2797.50). Current reward after update: -1431.45, Optimal reward -1394.86
Iteration 66 took 2.29 seconds (mean sampled reward: -2649.98). Current reward after update: -1404.11, Optimal reward -1394.86
Iteration 67 took 2.15 seconds (mean sampled reward: -2452.45). Current reward after update: -1944.62, Optimal reward -1394.86
Iteration 68 took 2.28 seconds (mean sampled reward: -3775.53). Current reward after update: -1506.37, Optimal reward -1394.86
Iteration 69 took 2.21 seconds (mean sampled reward: -2383.22). Current reward after update: -1661.17, Optimal reward -1394.86
Iteration 70 took 2.15 seconds (mean sampled reward: -2124.23). Current reward after update: -1366.48, Optimal reward -1366.48
Iteration 71 took 2.15 seconds (mean sampled reward: -2322.44). Current reward after update: -1385.32, Optimal reward -1366.48
Iteration 72 took 2.17 seconds (mean sampled reward: -2038.55). Current reward after update: -1387.48, Optimal reward -1366.48
Iteration 73 took 2.12 seconds (mean sampled reward: -1903.95). Current reward after update: -1396.96, Optimal reward -1366.48
Iteration 74 took 2.21 seconds (mean sampled reward: -2078.14). Current reward after update: -2156.90, Optimal reward -1366.48
Iteration 75 took 2.28 seconds (mean sampled reward: -2096.33). Current reward after update: -1533.69, Optimal reward -1366.48
Iteration 76 took 2.16 seconds (mean sampled reward: -2064.86). Current reward after update: -1399.50, Optimal reward -1366.48
Iteration 77 took 2.10 seconds (mean sampled reward: -2111.83). Current reward after update: -1412.66, Optimal reward -1366.48
Iteration 78 took 2.12 seconds (mean sampled reward: -2610.48). Current reward after update: -1383.67, Optimal reward -1366.48
Iteration 79 took 2.22 seconds (mean sampled reward: -2258.87). Current reward after update: -1358.96, Optimal reward -1358.96
Iteration 80 took 2.19 seconds (mean sampled reward: -2019.18). Current reward after update: -1347.93, Optimal reward -1347.93
Iteration 81 took 2.22 seconds (mean sampled reward: -2182.93). Current reward after update: -1367.76, Optimal reward -1347.93
Iteration 82 took 2.15 seconds (mean sampled reward: -2535.43). Current reward after update: -3597.01, Optimal reward -1347.93
Iteration 83 took 2.24 seconds (mean sampled reward: -3239.55). Current reward after update: -2229.54, Optimal reward -1347.93
Iteration 84 took 2.22 seconds (mean sampled reward: -3184.20). Current reward after update: -1391.48, Optimal reward -1347.93
Iteration 85 took 2.31 seconds (mean sampled reward: -3647.19). Current reward after update: -1280.65, Optimal reward -1280.65
Iteration 86 took 2.26 seconds (mean sampled reward: -3389.96). Current reward after update: -1406.83, Optimal reward -1280.65
Iteration 87 took 2.24 seconds (mean sampled reward: -3436.81). Current reward after update: -1316.85, Optimal reward -1280.65
Iteration 88 took 2.20 seconds (mean sampled reward: -2574.16). Current reward after update: -1336.62, Optimal reward -1280.65
Iteration 89 took 2.19 seconds (mean sampled reward: -3357.29). Current reward after update: -2208.10, Optimal reward -1280.65
Iteration 90 took 2.21 seconds (mean sampled reward: -3044.03). Current reward after update: -1715.60, Optimal reward -1280.65
Iteration 91 took 2.25 seconds (mean sampled reward: -3663.94). Current reward after update: -1313.92, Optimal reward -1280.65
Iteration 92 took 2.17 seconds (mean sampled reward: -3582.57). Current reward after update: -1259.67, Optimal reward -1259.67
Iteration 93 took 2.12 seconds (mean sampled reward: -2369.29). Current reward after update: -1303.07, Optimal reward -1259.67
Iteration 94 took 2.14 seconds (mean sampled reward: -2540.42). Current reward after update: -1387.50, Optimal reward -1259.67
Iteration 95 took 2.25 seconds (mean sampled reward: -2282.35). Current reward after update: -1463.16, Optimal reward -1259.67
Iteration 96 took 2.18 seconds (mean sampled reward: -1963.38). Current reward after update: -1302.05, Optimal reward -1259.67
Iteration 97 took 2.17 seconds (mean sampled reward: -2235.96). Current reward after update: -1993.25, Optimal reward -1259.67
Iteration 98 took 2.16 seconds (mean sampled reward: -2076.74). Current reward after update: -1742.41, Optimal reward -1259.67
Iteration 99 took 2.19 seconds (mean sampled reward: -1994.19). Current reward after update: -1296.71, Optimal reward -1259.67
Iteration 100 took 2.15 seconds (mean sampled reward: -1907.81). Current reward after update: -1252.30, Optimal reward -1252.30
Iteration 101 took 2.20 seconds (mean sampled reward: -2166.20). Current reward after update: -2108.42, Optimal reward -1252.30
Iteration 102 took 2.16 seconds (mean sampled reward: -2206.54). Current reward after update: -1379.45, Optimal reward -1252.30
Iteration 103 took 2.18 seconds (mean sampled reward: -2039.47). Current reward after update: -1319.45, Optimal reward -1252.30
Iteration 104 took 2.17 seconds (mean sampled reward: -1737.66). Current reward after update: -1220.79, Optimal reward -1220.79
Iteration 105 took 2.18 seconds (mean sampled reward: -1835.24). Current reward after update: -1146.30, Optimal reward -1146.30
Iteration 106 took 2.20 seconds (mean sampled reward: -1813.28). Current reward after update: -1117.46, Optimal reward -1117.46
Iteration 107 took 2.30 seconds (mean sampled reward: -2038.58). Current reward after update: -1139.20, Optimal reward -1117.46
Iteration 108 took 2.44 seconds (mean sampled reward: -3392.27). Current reward after update: -1157.43, Optimal reward -1117.46
Iteration 109 took 2.31 seconds (mean sampled reward: -2456.52). Current reward after update: -1176.90, Optimal reward -1117.46
Iteration 110 took 2.14 seconds (mean sampled reward: -1759.31). Current reward after update: -1178.16, Optimal reward -1117.46
Iteration 111 took 2.28 seconds (mean sampled reward: -1690.91). Current reward after update: -1273.89, Optimal reward -1117.46
Iteration 112 took 2.17 seconds (mean sampled reward: -1686.54). Current reward after update: -1013.54, Optimal reward -1013.54
Iteration 113 took 2.14 seconds (mean sampled reward: -2685.45). Current reward after update: -1028.56, Optimal reward -1013.54
Iteration 114 took 2.22 seconds (mean sampled reward: -3093.77). Current reward after update: -1621.96, Optimal reward -1013.54
Iteration 115 took 2.24 seconds (mean sampled reward: -2191.18). Current reward after update: -1196.83, Optimal reward -1013.54
Iteration 116 took 2.22 seconds (mean sampled reward: -2685.04). Current reward after update: -1018.80, Optimal reward -1013.54
Iteration 117 took 2.19 seconds (mean sampled reward: -2775.91). Current reward after update: -1065.30, Optimal reward -1013.54
Iteration 118 took 2.15 seconds (mean sampled reward: -2279.93). Current reward after update: -1024.51, Optimal reward -1013.54
Iteration 119 took 2.16 seconds (mean sampled reward: -1560.08). Current reward after update: -958.60, Optimal reward -958.60
Iteration 120 took 2.28 seconds (mean sampled reward: -1811.34). Current reward after update: -995.68, Optimal reward -958.60
Iteration 121 took 2.15 seconds (mean sampled reward: -1651.86). Current reward after update: -1076.44, Optimal reward -958.60
Iteration 122 took 2.17 seconds (mean sampled reward: -1655.64). Current reward after update: -1931.29, Optimal reward -958.60
Iteration 123 took 2.16 seconds (mean sampled reward: -1693.50). Current reward after update: -1546.83, Optimal reward -958.60
Iteration 124 took 2.12 seconds (mean sampled reward: -1837.51). Current reward after update: -1305.60, Optimal reward -958.60
Iteration 125 took 2.26 seconds (mean sampled reward: -1761.27). Current reward after update: -908.25, Optimal reward -908.25
Iteration 126 took 2.27 seconds (mean sampled reward: -1619.25). Current reward after update: -1919.19, Optimal reward -908.25
Iteration 127 took 2.24 seconds (mean sampled reward: -1647.45). Current reward after update: -965.43, Optimal reward -908.25
Iteration 128 took 2.16 seconds (mean sampled reward: -1671.96). Current reward after update: -938.67, Optimal reward -908.25
Iteration 129 took 2.23 seconds (mean sampled reward: -1700.62). Current reward after update: -915.67, Optimal reward -908.25
Iteration 130 took 2.20 seconds (mean sampled reward: -1660.75). Current reward after update: -1690.24, Optimal reward -908.25
Iteration 131 took 2.18 seconds (mean sampled reward: -1659.97). Current reward after update: -987.23, Optimal reward -908.25
Iteration 132 took 2.17 seconds (mean sampled reward: -1697.99). Current reward after update: -1611.88, Optimal reward -908.25
Iteration 133 took 2.17 seconds (mean sampled reward: -1649.71). Current reward after update: -1015.63, Optimal reward -908.25
Iteration 134 took 2.19 seconds (mean sampled reward: -1580.39). Current reward after update: -945.93, Optimal reward -908.25
Iteration 135 took 2.23 seconds (mean sampled reward: -1643.26). Current reward after update: -880.74, Optimal reward -880.74
Iteration 136 took 2.19 seconds (mean sampled reward: -1651.73). Current reward after update: -939.76, Optimal reward -880.74
Iteration 137 took 2.16 seconds (mean sampled reward: -1708.64). Current reward after update: -938.24, Optimal reward -880.74
Iteration 138 took 2.12 seconds (mean sampled reward: -1978.37). Current reward after update: -1509.16, Optimal reward -880.74
Iteration 139 took 2.13 seconds (mean sampled reward: -2048.74). Current reward after update: -947.56, Optimal reward -880.74
Iteration 140 took 2.10 seconds (mean sampled reward: -1651.16). Current reward after update: -1012.36, Optimal reward -880.74
Iteration 141 took 2.14 seconds (mean sampled reward: -1596.80). Current reward after update: -927.59, Optimal reward -880.74
Iteration 142 took 2.13 seconds (mean sampled reward: -1550.09). Current reward after update: -1125.85, Optimal reward -880.74
Iteration 143 took 2.12 seconds (mean sampled reward: -1463.76). Current reward after update: -934.37, Optimal reward -880.74
Iteration 144 took 2.14 seconds (mean sampled reward: -1511.81). Current reward after update: -933.74, Optimal reward -880.74
Iteration 145 took 2.15 seconds (mean sampled reward: -1522.52). Current reward after update: -1296.18, Optimal reward -880.74
Iteration 146 took 2.12 seconds (mean sampled reward: -1631.94). Current reward after update: -1198.95, Optimal reward -880.74
Iteration 147 took 2.14 seconds (mean sampled reward: -1700.75). Current reward after update: -968.78, Optimal reward -880.74
Iteration 148 took 2.15 seconds (mean sampled reward: -1660.10). Current reward after update: -958.76, Optimal reward -880.74
Iteration 149 took 2.14 seconds (mean sampled reward: -2104.10). Current reward after update: -953.96, Optimal reward -880.74
Iteration 150 took 2.12 seconds (mean sampled reward: -2072.26). Current reward after update: -954.45, Optimal reward -880.74
Iteration 151 took 2.19 seconds (mean sampled reward: -1859.41). Current reward after update: -1910.95, Optimal reward -880.74
Iteration 152 took 2.20 seconds (mean sampled reward: -2072.54). Current reward after update: -896.84, Optimal reward -880.74
Iteration 153 took 2.16 seconds (mean sampled reward: -1934.35). Current reward after update: -959.41, Optimal reward -880.74
Iteration 154 took 2.15 seconds (mean sampled reward: -2101.29). Current reward after update: -1976.90, Optimal reward -880.74
Iteration 155 took 2.15 seconds (mean sampled reward: -1922.27). Current reward after update: -1274.88, Optimal reward -880.74
Iteration 156 took 2.18 seconds (mean sampled reward: -1939.94). Current reward after update: -934.96, Optimal reward -880.74
Iteration 157 took 2.14 seconds (mean sampled reward: -2278.08). Current reward after update: -1012.44, Optimal reward -880.74
Iteration 158 took 2.09 seconds (mean sampled reward: -1856.69). Current reward after update: -1003.78, Optimal reward -880.74
Iteration 159 took 2.09 seconds (mean sampled reward: -1908.96). Current reward after update: -1497.60, Optimal reward -880.74
Iteration 160 took 2.14 seconds (mean sampled reward: -2073.24). Current reward after update: -931.03, Optimal reward -880.74
Iteration 161 took 2.15 seconds (mean sampled reward: -1883.71). Current reward after update: -1134.79, Optimal reward -880.74
Iteration 162 took 2.18 seconds (mean sampled reward: -1599.91). Current reward after update: -1244.03, Optimal reward -880.74
Iteration 163 took 2.17 seconds (mean sampled reward: -1791.23). Current reward after update: -1169.45, Optimal reward -880.74
Iteration 164 took 2.16 seconds (mean sampled reward: -1794.74). Current reward after update: -943.10, Optimal reward -880.74
Iteration 165 took 2.14 seconds (mean sampled reward: -1518.16). Current reward after update: -962.84, Optimal reward -880.74
Iteration 166 took 2.23 seconds (mean sampled reward: -1608.93). Current reward after update: -1447.76, Optimal reward -880.74
Iteration 167 took 2.21 seconds (mean sampled reward: -1574.36). Current reward after update: -1010.30, Optimal reward -880.74
Iteration 168 took 2.23 seconds (mean sampled reward: -2198.49). Current reward after update: -1839.58, Optimal reward -880.74
Iteration 169 took 2.18 seconds (mean sampled reward: -1992.38). Current reward after update: -918.83, Optimal reward -880.74
Iteration 170 took 2.23 seconds (mean sampled reward: -1695.82). Current reward after update: -924.29, Optimal reward -880.74
Iteration 171 took 2.19 seconds (mean sampled reward: -1669.01). Current reward after update: -1847.92, Optimal reward -880.74
Iteration 172 took 2.22 seconds (mean sampled reward: -2060.42). Current reward after update: -878.61, Optimal reward -878.61
Iteration 173 took 2.24 seconds (mean sampled reward: -1644.91). Current reward after update: -1727.10, Optimal reward -878.61
Iteration 174 took 2.15 seconds (mean sampled reward: -1922.74). Current reward after update: -995.55, Optimal reward -878.61
Iteration 175 took 2.21 seconds (mean sampled reward: -1448.15). Current reward after update: -1019.79, Optimal reward -878.61
Iteration 176 took 2.26 seconds (mean sampled reward: -1385.54). Current reward after update: -846.00, Optimal reward -846.00
Iteration 177 took 2.20 seconds (mean sampled reward: -1379.68). Current reward after update: -844.88, Optimal reward -844.88
Iteration 178 took 2.25 seconds (mean sampled reward: -1314.18). Current reward after update: -880.87, Optimal reward -844.88
Iteration 179 took 2.22 seconds (mean sampled reward: -1392.83). Current reward after update: -886.06, Optimal reward -844.88
Iteration 180 took 2.26 seconds (mean sampled reward: -1544.68). Current reward after update: -1707.42, Optimal reward -844.88
Iteration 181 took 2.26 seconds (mean sampled reward: -1571.49). Current reward after update: -1583.80, Optimal reward -844.88
Iteration 182 took 2.25 seconds (mean sampled reward: -1741.61). Current reward after update: -1676.41, Optimal reward -844.88
Iteration 183 took 2.19 seconds (mean sampled reward: -1477.65). Current reward after update: -802.14, Optimal reward -802.14
Iteration 184 took 2.19 seconds (mean sampled reward: -1656.87). Current reward after update: -804.56, Optimal reward -802.14
Iteration 185 took 2.16 seconds (mean sampled reward: -1689.84). Current reward after update: -840.65, Optimal reward -802.14
Iteration 186 took 2.16 seconds (mean sampled reward: -1659.83). Current reward after update: -868.79, Optimal reward -802.14
Iteration 187 took 2.17 seconds (mean sampled reward: -1466.49). Current reward after update: -820.73, Optimal reward -802.14
Iteration 188 took 2.21 seconds (mean sampled reward: -1524.37). Current reward after update: -844.97, Optimal reward -802.14
Iteration 189 took 2.16 seconds (mean sampled reward: -1732.96). Current reward after update: -801.78, Optimal reward -801.78
Iteration 190 took 2.15 seconds (mean sampled reward: -1470.74). Current reward after update: -784.05, Optimal reward -784.05
Iteration 191 took 2.18 seconds (mean sampled reward: -2072.39). Current reward after update: -1542.60, Optimal reward -784.05
Iteration 192 took 2.12 seconds (mean sampled reward: -2915.80). Current reward after update: -1032.70, Optimal reward -784.05
Iteration 193 took 2.18 seconds (mean sampled reward: -2720.11). Current reward after update: -953.37, Optimal reward -784.05
Iteration 194 took 2.09 seconds (mean sampled reward: -2514.30). Current reward after update: -1753.79, Optimal reward -784.05
Iteration 195 took 2.20 seconds (mean sampled reward: -2082.95). Current reward after update: -1440.44, Optimal reward -784.05
Iteration 196 took 2.20 seconds (mean sampled reward: -2728.30). Current reward after update: -1160.38, Optimal reward -784.05
Iteration 197 took 2.18 seconds (mean sampled reward: -1625.54). Current reward after update: -863.40, Optimal reward -784.05
Iteration 198 took 2.17 seconds (mean sampled reward: -1521.03). Current reward after update: -1430.34, Optimal reward -784.05
Iteration 199 took 2.16 seconds (mean sampled reward: -1707.16). Current reward after update: -770.38, Optimal reward -770.38
Iteration 200 took 2.24 seconds (mean sampled reward: -1453.31). Current reward after update: -832.30, Optimal reward -770.38
Iteration 1 took 2.17 seconds (mean sampled reward: -7505.11). Current reward after update: -7209.03, Optimal reward -7209.03
Iteration 2 took 2.25 seconds (mean sampled reward: -7436.38). Current reward after update: -7204.54, Optimal reward -7204.54
Iteration 3 took 2.14 seconds (mean sampled reward: -7344.00). Current reward after update: -7152.87, Optimal reward -7152.87
Iteration 4 took 2.05 seconds (mean sampled reward: -7355.95). Current reward after update: -7147.33, Optimal reward -7147.33
Iteration 5 took 2.07 seconds (mean sampled reward: -7318.78). Current reward after update: -7129.44, Optimal reward -7129.44
Iteration 6 took 2.09 seconds (mean sampled reward: -7296.62). Current reward after update: -7215.59, Optimal reward -7129.44
Iteration 7 took 1.90 seconds (mean sampled reward: -7257.23). Current reward after update: -7051.92, Optimal reward -7051.92
Iteration 8 took 2.09 seconds (mean sampled reward: -7217.40). Current reward after update: -7185.23, Optimal reward -7051.92
Iteration 9 took 2.08 seconds (mean sampled reward: -7209.15). Current reward after update: -7053.49, Optimal reward -7051.92
Iteration 10 took 2.12 seconds (mean sampled reward: -7277.56). Current reward after update: -7032.42, Optimal reward -7032.42
Iteration 11 took 2.14 seconds (mean sampled reward: -7259.86). Current reward after update: -7017.37, Optimal reward -7017.37
Iteration 12 took 2.16 seconds (mean sampled reward: -7182.81). Current reward after update: -6958.05, Optimal reward -6958.05
Iteration 13 took 2.11 seconds (mean sampled reward: -7167.06). Current reward after update: -6713.35, Optimal reward -6713.35
Iteration 14 took 2.07 seconds (mean sampled reward: -6997.29). Current reward after update: -6458.59, Optimal reward -6458.59
Iteration 15 took 2.10 seconds (mean sampled reward: -6690.56). Current reward after update: -6024.27, Optimal reward -6024.27
Iteration 16 took 2.19 seconds (mean sampled reward: -6581.73). Current reward after update: -5533.83, Optimal reward -5533.83
Iteration 17 took 2.32 seconds (mean sampled reward: -6052.68). Current reward after update: -5271.85, Optimal reward -5271.85
Iteration 18 took 2.25 seconds (mean sampled reward: -5946.28). Current reward after update: -5196.98, Optimal reward -5196.98
Iteration 19 took 2.27 seconds (mean sampled reward: -5992.26). Current reward after update: -5150.36, Optimal reward -5150.36
Iteration 20 took 2.17 seconds (mean sampled reward: -5689.84). Current reward after update: -5051.09, Optimal reward -5051.09
Iteration 21 took 2.09 seconds (mean sampled reward: -5696.55). Current reward after update: -4647.01, Optimal reward -4647.01
Iteration 22 took 2.12 seconds (mean sampled reward: -5655.77). Current reward after update: -4005.65, Optimal reward -4005.65
Iteration 23 took 2.22 seconds (mean sampled reward: -5535.57). Current reward after update: -3071.97, Optimal reward -3071.97
Iteration 24 took 2.17 seconds (mean sampled reward: -5401.10). Current reward after update: -3088.99, Optimal reward -3071.97
Iteration 25 took 2.20 seconds (mean sampled reward: -4818.41). Current reward after update: -2799.73, Optimal reward -2799.73
Iteration 26 took 2.32 seconds (mean sampled reward: -4376.96). Current reward after update: -2718.40, Optimal reward -2718.40
Iteration 27 took 2.27 seconds (mean sampled reward: -4516.61). Current reward after update: -2639.85, Optimal reward -2639.85
Iteration 28 took 2.18 seconds (mean sampled reward: -4548.02). Current reward after update: -2418.92, Optimal reward -2418.92
Iteration 29 took 2.40 seconds (mean sampled reward: -3694.45). Current reward after update: -3521.81, Optimal reward -2418.92
Iteration 30 took 2.06 seconds (mean sampled reward: -5134.52). Current reward after update: -2303.06, Optimal reward -2303.06
Iteration 31 took 2.02 seconds (mean sampled reward: -5899.87). Current reward after update: -2388.10, Optimal reward -2303.06
Iteration 32 took 2.12 seconds (mean sampled reward: -4187.99). Current reward after update: -2160.01, Optimal reward -2160.01
Iteration 33 took 1.97 seconds (mean sampled reward: -5379.65). Current reward after update: -6711.34, Optimal reward -2160.01
Iteration 34 took 2.14 seconds (mean sampled reward: -4433.04). Current reward after update: -2226.60, Optimal reward -2160.01
Iteration 35 took 2.04 seconds (mean sampled reward: -4062.62). Current reward after update: -2135.12, Optimal reward -2135.12
Iteration 36 took 2.10 seconds (mean sampled reward: -4793.51). Current reward after update: -6684.54, Optimal reward -2135.12
Iteration 37 took 2.04 seconds (mean sampled reward: -4738.73). Current reward after update: -2189.62, Optimal reward -2135.12
Iteration 38 took 2.15 seconds (mean sampled reward: -5356.16). Current reward after update: -2103.90, Optimal reward -2103.90
Iteration 39 took 2.02 seconds (mean sampled reward: -4559.08). Current reward after update: -2041.11, Optimal reward -2041.11
Iteration 40 took 2.01 seconds (mean sampled reward: -5081.90). Current reward after update: -2085.00, Optimal reward -2041.11
Iteration 41 took 1.97 seconds (mean sampled reward: -6503.20). Current reward after update: -2260.78, Optimal reward -2041.11
Iteration 42 took 2.02 seconds (mean sampled reward: -4747.21). Current reward after update: -2476.55, Optimal reward -2041.11
Iteration 43 took 1.91 seconds (mean sampled reward: -5555.78). Current reward after update: -2015.76, Optimal reward -2015.76
Iteration 44 took 1.90 seconds (mean sampled reward: -5792.59). Current reward after update: -2128.64, Optimal reward -2015.76
Iteration 45 took 1.99 seconds (mean sampled reward: -5448.18). Current reward after update: -2134.43, Optimal reward -2015.76
Iteration 46 took 1.86 seconds (mean sampled reward: -6317.97). Current reward after update: -2051.84, Optimal reward -2015.76
Iteration 47 took 1.92 seconds (mean sampled reward: -6513.71). Current reward after update: -2072.93, Optimal reward -2015.76
Iteration 48 took 1.91 seconds (mean sampled reward: -6182.03). Current reward after update: -2040.94, Optimal reward -2015.76
Iteration 49 took 2.07 seconds (mean sampled reward: -4543.40). Current reward after update: -1887.09, Optimal reward -1887.09
Iteration 50 took 1.99 seconds (mean sampled reward: -5574.11). Current reward after update: -1970.31, Optimal reward -1887.09
Iteration 51 took 1.95 seconds (mean sampled reward: -5826.27). Current reward after update: -1914.15, Optimal reward -1887.09
Iteration 52 took 1.98 seconds (mean sampled reward: -5938.67). Current reward after update: -2011.89, Optimal reward -1887.09
Iteration 53 took 1.91 seconds (mean sampled reward: -6057.25). Current reward after update: -2025.58, Optimal reward -1887.09
Iteration 54 took 1.93 seconds (mean sampled reward: -5802.03). Current reward after update: -2011.03, Optimal reward -1887.09
Iteration 55 took 2.05 seconds (mean sampled reward: -5819.86). Current reward after update: -1980.26, Optimal reward -1887.09
Iteration 56 took 2.05 seconds (mean sampled reward: -4265.64). Current reward after update: -1714.80, Optimal reward -1714.80
Iteration 57 took 2.21 seconds (mean sampled reward: -4260.49). Current reward after update: -1467.65, Optimal reward -1467.65
Iteration 58 took 2.12 seconds (mean sampled reward: -4336.95). Current reward after update: -1990.37, Optimal reward -1467.65
Iteration 59 took 2.40 seconds (mean sampled reward: -4051.33). Current reward after update: -1186.62, Optimal reward -1186.62
Iteration 60 took 2.11 seconds (mean sampled reward: -3970.48). Current reward after update: -1304.06, Optimal reward -1186.62
Iteration 61 took 2.28 seconds (mean sampled reward: -3569.36). Current reward after update: -1215.53, Optimal reward -1186.62
Iteration 62 took 2.20 seconds (mean sampled reward: -3357.24). Current reward after update: -1349.47, Optimal reward -1186.62
Iteration 63 took 2.28 seconds (mean sampled reward: -2952.39). Current reward after update: -1299.65, Optimal reward -1186.62
Iteration 64 took 2.19 seconds (mean sampled reward: -3363.93). Current reward after update: -1346.56, Optimal reward -1186.62
Iteration 65 took 2.14 seconds (mean sampled reward: -3341.98). Current reward after update: -1686.53, Optimal reward -1186.62
Iteration 66 took 2.11 seconds (mean sampled reward: -3683.71). Current reward after update: -1238.52, Optimal reward -1186.62
Iteration 67 took 2.21 seconds (mean sampled reward: -3546.01). Current reward after update: -1571.05, Optimal reward -1186.62
Iteration 68 took 2.20 seconds (mean sampled reward: -4013.03). Current reward after update: -1716.74, Optimal reward -1186.62
Iteration 69 took 2.23 seconds (mean sampled reward: -3638.88). Current reward after update: -1200.11, Optimal reward -1186.62
Iteration 70 took 2.10 seconds (mean sampled reward: -4102.32). Current reward after update: -1837.09, Optimal reward -1186.62
Iteration 71 took 2.12 seconds (mean sampled reward: -3191.15). Current reward after update: -1179.02, Optimal reward -1179.02
Iteration 72 took 2.25 seconds (mean sampled reward: -3239.11). Current reward after update: -1196.08, Optimal reward -1179.02
Iteration 73 took 2.25 seconds (mean sampled reward: -2610.06). Current reward after update: -1103.80, Optimal reward -1103.80
Iteration 74 took 2.16 seconds (mean sampled reward: -2115.92). Current reward after update: -1133.49, Optimal reward -1103.80
Iteration 75 took 2.18 seconds (mean sampled reward: -1966.07). Current reward after update: -1067.00, Optimal reward -1067.00
Iteration 76 took 2.14 seconds (mean sampled reward: -1809.15). Current reward after update: -1902.24, Optimal reward -1067.00
Iteration 77 took 2.17 seconds (mean sampled reward: -3830.55). Current reward after update: -1073.79, Optimal reward -1067.00
Iteration 78 took 2.21 seconds (mean sampled reward: -2079.08). Current reward after update: -1163.27, Optimal reward -1067.00
Iteration 79 took 2.16 seconds (mean sampled reward: -3421.77). Current reward after update: -1031.51, Optimal reward -1031.51
Iteration 80 took 2.16 seconds (mean sampled reward: -2698.67). Current reward after update: -1049.44, Optimal reward -1031.51
Iteration 81 took 2.12 seconds (mean sampled reward: -2427.16). Current reward after update: -1723.47, Optimal reward -1031.51
Iteration 82 took 2.15 seconds (mean sampled reward: -2627.93). Current reward after update: -1170.88, Optimal reward -1031.51
Iteration 83 took 2.19 seconds (mean sampled reward: -2394.76). Current reward after update: -1792.33, Optimal reward -1031.51
Iteration 84 took 2.21 seconds (mean sampled reward: -2173.55). Current reward after update: -1121.63, Optimal reward -1031.51
Iteration 85 took 2.11 seconds (mean sampled reward: -3213.20). Current reward after update: -1077.33, Optimal reward -1031.51
Iteration 86 took 2.14 seconds (mean sampled reward: -4072.62). Current reward after update: -1049.69, Optimal reward -1031.51
Iteration 87 took 2.35 seconds (mean sampled reward: -3220.22). Current reward after update: -1068.39, Optimal reward -1031.51
Iteration 88 took 2.40 seconds (mean sampled reward: -3875.22). Current reward after update: -1070.94, Optimal reward -1031.51
Iteration 89 took 2.12 seconds (mean sampled reward: -3694.07). Current reward after update: -1029.99, Optimal reward -1029.99
Iteration 90 took 2.12 seconds (mean sampled reward: -4128.94). Current reward after update: -1702.79, Optimal reward -1029.99
Iteration 91 took 2.11 seconds (mean sampled reward: -3853.97). Current reward after update: -1113.08, Optimal reward -1029.99
Iteration 92 took 2.05 seconds (mean sampled reward: -3831.40). Current reward after update: -1059.04, Optimal reward -1029.99
Iteration 93 took 2.08 seconds (mean sampled reward: -3702.29). Current reward after update: -1604.07, Optimal reward -1029.99
Iteration 94 took 2.12 seconds (mean sampled reward: -4293.08). Current reward after update: -1013.13, Optimal reward -1013.13
Iteration 95 took 2.14 seconds (mean sampled reward: -3451.91). Current reward after update: -1246.01, Optimal reward -1013.13
Iteration 96 took 2.27 seconds (mean sampled reward: -2364.02). Current reward after update: -2410.79, Optimal reward -1013.13
Iteration 97 took 2.28 seconds (mean sampled reward: -2856.06). Current reward after update: -1054.94, Optimal reward -1013.13
Iteration 98 took 2.25 seconds (mean sampled reward: -1963.68). Current reward after update: -1310.15, Optimal reward -1013.13
Iteration 99 took 2.22 seconds (mean sampled reward: -2161.23). Current reward after update: -1557.58, Optimal reward -1013.13
Iteration 100 took 2.22 seconds (mean sampled reward: -3668.35). Current reward after update: -974.40, Optimal reward -974.40
Iteration 101 took 2.16 seconds (mean sampled reward: -4113.84). Current reward after update: -1243.17, Optimal reward -974.40
Iteration 102 took 2.30 seconds (mean sampled reward: -3166.34). Current reward after update: -1022.78, Optimal reward -974.40
Iteration 103 took 2.23 seconds (mean sampled reward: -3934.46). Current reward after update: -1037.62, Optimal reward -974.40
Iteration 104 took 2.16 seconds (mean sampled reward: -3258.98). Current reward after update: -988.41, Optimal reward -974.40
Iteration 105 took 2.16 seconds (mean sampled reward: -2751.24). Current reward after update: -1337.07, Optimal reward -974.40
Iteration 106 took 2.30 seconds (mean sampled reward: -3495.20). Current reward after update: -1051.68, Optimal reward -974.40
Iteration 107 took 2.37 seconds (mean sampled reward: -4076.64). Current reward after update: -1091.29, Optimal reward -974.40
Iteration 108 took 2.23 seconds (mean sampled reward: -4383.14). Current reward after update: -1065.87, Optimal reward -974.40
Iteration 109 took 2.16 seconds (mean sampled reward: -3708.47). Current reward after update: -1187.81, Optimal reward -974.40
Iteration 110 took 2.21 seconds (mean sampled reward: -3252.13). Current reward after update: -1466.09, Optimal reward -974.40
Iteration 111 took 2.23 seconds (mean sampled reward: -2782.67). Current reward after update: -1039.69, Optimal reward -974.40
Iteration 112 took 2.40 seconds (mean sampled reward: -2899.82). Current reward after update: -1104.29, Optimal reward -974.40
Iteration 113 took 2.20 seconds (mean sampled reward: -2890.88). Current reward after update: -1419.21, Optimal reward -974.40
Iteration 114 took 2.26 seconds (mean sampled reward: -3125.31). Current reward after update: -968.10, Optimal reward -968.10
Iteration 115 took 2.19 seconds (mean sampled reward: -3203.48). Current reward after update: -992.19, Optimal reward -968.10
Iteration 116 took 2.19 seconds (mean sampled reward: -3073.22). Current reward after update: -1299.50, Optimal reward -968.10
Iteration 117 took 2.26 seconds (mean sampled reward: -2391.10). Current reward after update: -932.72, Optimal reward -932.72
Iteration 118 took 2.23 seconds (mean sampled reward: -2632.37). Current reward after update: -1694.41, Optimal reward -932.72
Iteration 119 took 2.35 seconds (mean sampled reward: -2621.51). Current reward after update: -1690.10, Optimal reward -932.72
Iteration 120 took 2.18 seconds (mean sampled reward: -3409.61). Current reward after update: -1682.44, Optimal reward -932.72
Iteration 121 took 2.31 seconds (mean sampled reward: -3535.19). Current reward after update: -1043.74, Optimal reward -932.72
Iteration 122 took 2.20 seconds (mean sampled reward: -3439.54). Current reward after update: -981.17, Optimal reward -932.72
Iteration 123 took 2.28 seconds (mean sampled reward: -3408.38). Current reward after update: -980.91, Optimal reward -932.72
Iteration 124 took 2.17 seconds (mean sampled reward: -3151.56). Current reward after update: -1311.10, Optimal reward -932.72
Iteration 125 took 2.16 seconds (mean sampled reward: -3084.25). Current reward after update: -1056.13, Optimal reward -932.72
Iteration 126 took 2.22 seconds (mean sampled reward: -3039.87). Current reward after update: -1080.67, Optimal reward -932.72
Iteration 127 took 2.15 seconds (mean sampled reward: -3865.38). Current reward after update: -1378.23, Optimal reward -932.72
Iteration 128 took 2.19 seconds (mean sampled reward: -3535.70). Current reward after update: -1019.92, Optimal reward -932.72
Iteration 129 took 2.25 seconds (mean sampled reward: -3295.18). Current reward after update: -984.29, Optimal reward -932.72
Iteration 130 took 2.18 seconds (mean sampled reward: -2802.29). Current reward after update: -1000.89, Optimal reward -932.72
Iteration 131 took 2.19 seconds (mean sampled reward: -3250.96). Current reward after update: -1197.73, Optimal reward -932.72
Iteration 132 took 2.17 seconds (mean sampled reward: -3345.47). Current reward after update: -986.53, Optimal reward -932.72
Iteration 133 took 2.15 seconds (mean sampled reward: -3929.86). Current reward after update: -1001.56, Optimal reward -932.72
Iteration 134 took 2.19 seconds (mean sampled reward: -3344.40). Current reward after update: -919.83, Optimal reward -919.83
Iteration 135 took 2.17 seconds (mean sampled reward: -2750.72). Current reward after update: -901.31, Optimal reward -901.31
Iteration 136 took 2.18 seconds (mean sampled reward: -3050.22). Current reward after update: -2740.39, Optimal reward -901.31
Iteration 137 took 2.18 seconds (mean sampled reward: -3379.93). Current reward after update: -1093.28, Optimal reward -901.31
Iteration 138 took 2.19 seconds (mean sampled reward: -3328.55). Current reward after update: -1876.44, Optimal reward -901.31
Iteration 139 took 2.14 seconds (mean sampled reward: -3853.10). Current reward after update: -928.19, Optimal reward -901.31
Iteration 140 took 2.23 seconds (mean sampled reward: -4289.84). Current reward after update: -1225.21, Optimal reward -901.31
Iteration 141 took 2.31 seconds (mean sampled reward: -4271.03). Current reward after update: -973.08, Optimal reward -901.31
Iteration 142 took 2.16 seconds (mean sampled reward: -3886.55). Current reward after update: -920.63, Optimal reward -901.31
Iteration 143 took 2.12 seconds (mean sampled reward: -3756.06). Current reward after update: -958.34, Optimal reward -901.31
Iteration 144 took 2.18 seconds (mean sampled reward: -4055.48). Current reward after update: -887.12, Optimal reward -887.12
Iteration 145 took 2.16 seconds (mean sampled reward: -4423.02). Current reward after update: -998.80, Optimal reward -887.12
Iteration 146 took 2.12 seconds (mean sampled reward: -3809.26). Current reward after update: -1953.45, Optimal reward -887.12
Iteration 147 took 2.17 seconds (mean sampled reward: -3949.01). Current reward after update: -1496.22, Optimal reward -887.12
Iteration 148 took 2.16 seconds (mean sampled reward: -5146.10). Current reward after update: -919.58, Optimal reward -887.12
Iteration 149 took 2.11 seconds (mean sampled reward: -4688.88). Current reward after update: -1309.38, Optimal reward -887.12
Iteration 150 took 2.10 seconds (mean sampled reward: -5199.04). Current reward after update: -6403.58, Optimal reward -887.12
Iteration 151 took 2.17 seconds (mean sampled reward: -4506.18). Current reward after update: -1232.13, Optimal reward -887.12
Iteration 152 took 2.11 seconds (mean sampled reward: -4002.29). Current reward after update: -954.42, Optimal reward -887.12
Iteration 153 took 2.18 seconds (mean sampled reward: -2765.51). Current reward after update: -1163.28, Optimal reward -887.12
Iteration 154 took 2.15 seconds (mean sampled reward: -3181.78). Current reward after update: -985.87, Optimal reward -887.12
Iteration 155 took 2.11 seconds (mean sampled reward: -4643.26). Current reward after update: -877.48, Optimal reward -877.48
Iteration 156 took 2.17 seconds (mean sampled reward: -4736.15). Current reward after update: -950.06, Optimal reward -877.48
Iteration 157 took 2.15 seconds (mean sampled reward: -4718.61). Current reward after update: -1029.15, Optimal reward -877.48
Iteration 158 took 2.16 seconds (mean sampled reward: -4111.81). Current reward after update: -1135.37, Optimal reward -877.48
Iteration 159 took 2.12 seconds (mean sampled reward: -4550.18). Current reward after update: -993.56, Optimal reward -877.48
Iteration 160 took 2.11 seconds (mean sampled reward: -4460.12). Current reward after update: -1024.29, Optimal reward -877.48
Iteration 161 took 2.17 seconds (mean sampled reward: -5018.60). Current reward after update: -1061.23, Optimal reward -877.48
Iteration 162 took 2.16 seconds (mean sampled reward: -4318.94). Current reward after update: -1184.23, Optimal reward -877.48
Iteration 163 took 2.12 seconds (mean sampled reward: -4551.15). Current reward after update: -6406.94, Optimal reward -877.48
Iteration 164 took 2.17 seconds (mean sampled reward: -4350.86). Current reward after update: -1013.90, Optimal reward -877.48
Iteration 165 took 2.14 seconds (mean sampled reward: -4160.79). Current reward after update: -1252.20, Optimal reward -877.48
Iteration 166 took 2.18 seconds (mean sampled reward: -3720.88). Current reward after update: -1251.39, Optimal reward -877.48
Iteration 167 took 2.16 seconds (mean sampled reward: -3257.87). Current reward after update: -1164.87, Optimal reward -877.48
Iteration 168 took 2.19 seconds (mean sampled reward: -3631.96). Current reward after update: -1100.14, Optimal reward -877.48
Iteration 169 took 2.24 seconds (mean sampled reward: -3128.30). Current reward after update: -2109.66, Optimal reward -877.48
Iteration 170 took 2.17 seconds (mean sampled reward: -3361.80). Current reward after update: -6372.11, Optimal reward -877.48
Iteration 171 took 2.17 seconds (mean sampled reward: -4076.05). Current reward after update: -1066.69, Optimal reward -877.48
Iteration 172 took 2.15 seconds (mean sampled reward: -2446.56). Current reward after update: -1044.00, Optimal reward -877.48
Iteration 173 took 2.14 seconds (mean sampled reward: -2747.37). Current reward after update: -969.90, Optimal reward -877.48
Iteration 174 took 2.13 seconds (mean sampled reward: -4548.22). Current reward after update: -1050.63, Optimal reward -877.48
Iteration 175 took 2.19 seconds (mean sampled reward: -3815.85). Current reward after update: -1009.05, Optimal reward -877.48
Iteration 176 took 2.11 seconds (mean sampled reward: -5115.08). Current reward after update: -1024.65, Optimal reward -877.48
Iteration 177 took 2.09 seconds (mean sampled reward: -5556.89). Current reward after update: -1690.80, Optimal reward -877.48
Iteration 178 took 2.13 seconds (mean sampled reward: -4951.29). Current reward after update: -1174.06, Optimal reward -877.48
Iteration 179 took 2.22 seconds (mean sampled reward: -5242.97). Current reward after update: -1070.41, Optimal reward -877.48
Iteration 180 took 2.12 seconds (mean sampled reward: -5697.05). Current reward after update: -1103.32, Optimal reward -877.48
Iteration 181 took 2.07 seconds (mean sampled reward: -5795.87). Current reward after update: -1158.81, Optimal reward -877.48
Iteration 182 took 2.14 seconds (mean sampled reward: -5856.55). Current reward after update: -1075.90, Optimal reward -877.48
Iteration 183 took 2.09 seconds (mean sampled reward: -5713.13). Current reward after update: -1012.81, Optimal reward -877.48
Iteration 184 took 2.14 seconds (mean sampled reward: -5792.80). Current reward after update: -1107.06, Optimal reward -877.48
Iteration 185 took 2.15 seconds (mean sampled reward: -4997.99). Current reward after update: -1738.62, Optimal reward -877.48
Iteration 186 took 2.18 seconds (mean sampled reward: -4638.14). Current reward after update: -1644.68, Optimal reward -877.48
Iteration 187 took 2.16 seconds (mean sampled reward: -4750.31). Current reward after update: -1015.98, Optimal reward -877.48
Iteration 188 took 2.17 seconds (mean sampled reward: -4515.19). Current reward after update: -1026.10, Optimal reward -877.48
Iteration 189 took 2.18 seconds (mean sampled reward: -4582.84). Current reward after update: -6433.96, Optimal reward -877.48
Iteration 190 took 2.18 seconds (mean sampled reward: -5176.58). Current reward after update: -1071.69, Optimal reward -877.48
Iteration 191 took 2.18 seconds (mean sampled reward: -4917.80). Current reward after update: -1043.55, Optimal reward -877.48
Iteration 192 took 2.19 seconds (mean sampled reward: -4962.42). Current reward after update: -1108.43, Optimal reward -877.48
Iteration 193 took 2.25 seconds (mean sampled reward: -5182.42). Current reward after update: -1046.59, Optimal reward -877.48
Iteration 194 took 2.18 seconds (mean sampled reward: -5876.59). Current reward after update: -1098.83, Optimal reward -877.48
Iteration 195 took 2.25 seconds (mean sampled reward: -5607.81). Current reward after update: -6402.01, Optimal reward -877.48
Iteration 196 took 2.15 seconds (mean sampled reward: -5775.53). Current reward after update: -1106.32, Optimal reward -877.48
Iteration 197 took 2.16 seconds (mean sampled reward: -5875.85). Current reward after update: -1461.84, Optimal reward -877.48
Iteration 198 took 2.18 seconds (mean sampled reward: -5879.93). Current reward after update: -1112.42, Optimal reward -877.48
Iteration 199 took 2.16 seconds (mean sampled reward: -5919.73). Current reward after update: -1230.44, Optimal reward -877.48
Iteration 200 took 2.18 seconds (mean sampled reward: -6205.48). Current reward after update: -1209.70, Optimal reward -877.48
Max force: 20 Sigma: 0.1 mean rewards: -889.9788545167162, best rewards:-770.3808830234127

Iteration 1 took 2.17 seconds (mean sampled reward: -7505.45). Current reward after update: -7188.86, Optimal reward -7188.86
Iteration 2 took 2.08 seconds (mean sampled reward: -7487.28). Current reward after update: -7173.90, Optimal reward -7173.90
Iteration 3 took 2.19 seconds (mean sampled reward: -7435.82). Current reward after update: -6986.75, Optimal reward -6986.75
Iteration 4 took 2.23 seconds (mean sampled reward: -7440.17). Current reward after update: -6075.14, Optimal reward -6075.14
Iteration 5 took 2.33 seconds (mean sampled reward: -7276.63). Current reward after update: -5837.39, Optimal reward -5837.39
Iteration 6 took 2.29 seconds (mean sampled reward: -6980.50). Current reward after update: -5754.10, Optimal reward -5754.10
Iteration 7 took 2.13 seconds (mean sampled reward: -7004.91). Current reward after update: -5548.93, Optimal reward -5548.93
Iteration 8 took 2.46 seconds (mean sampled reward: -6817.97). Current reward after update: -5363.05, Optimal reward -5363.05
Iteration 9 took 2.50 seconds (mean sampled reward: -6767.30). Current reward after update: -5324.11, Optimal reward -5324.11
Iteration 10 took 2.37 seconds (mean sampled reward: -6563.62). Current reward after update: -5045.35, Optimal reward -5045.35
Iteration 11 took 2.37 seconds (mean sampled reward: -6326.88). Current reward after update: -4817.23, Optimal reward -4817.23
Iteration 12 took 2.56 seconds (mean sampled reward: -5895.31). Current reward after update: -4472.61, Optimal reward -4472.61
Iteration 13 took 2.72 seconds (mean sampled reward: -6084.77). Current reward after update: -4442.44, Optimal reward -4442.44
Iteration 14 took 2.34 seconds (mean sampled reward: -6205.52). Current reward after update: -4526.14, Optimal reward -4442.44
Iteration 15 took 2.34 seconds (mean sampled reward: -6396.19). Current reward after update: -4558.93, Optimal reward -4442.44
Iteration 16 took 2.63 seconds (mean sampled reward: -6505.04). Current reward after update: -4586.72, Optimal reward -4442.44
Iteration 17 took 2.55 seconds (mean sampled reward: -6249.70). Current reward after update: -4417.93, Optimal reward -4417.93
Iteration 18 took 2.52 seconds (mean sampled reward: -6585.35). Current reward after update: -4415.69, Optimal reward -4415.69
Iteration 19 took 2.43 seconds (mean sampled reward: -6104.60). Current reward after update: -4623.16, Optimal reward -4415.69
Iteration 20 took 2.43 seconds (mean sampled reward: -6107.89). Current reward after update: -4429.06, Optimal reward -4415.69
Iteration 21 took 2.49 seconds (mean sampled reward: -6589.26). Current reward after update: -4413.71, Optimal reward -4413.71
Iteration 22 took 2.37 seconds (mean sampled reward: -6583.65). Current reward after update: -4948.30, Optimal reward -4413.71
Iteration 23 took 2.25 seconds (mean sampled reward: -6391.86). Current reward after update: -4159.79, Optimal reward -4159.79
Iteration 24 took 2.29 seconds (mean sampled reward: -6619.24). Current reward after update: -4285.29, Optimal reward -4159.79
Iteration 25 took 2.22 seconds (mean sampled reward: -5819.37). Current reward after update: -3802.05, Optimal reward -3802.05
Iteration 26 took 2.40 seconds (mean sampled reward: -5679.23). Current reward after update: -3874.57, Optimal reward -3802.05
Iteration 27 took 2.26 seconds (mean sampled reward: -6453.87). Current reward after update: -4068.16, Optimal reward -3802.05
Iteration 28 took 2.56 seconds (mean sampled reward: -6289.28). Current reward after update: -3995.36, Optimal reward -3802.05
Iteration 29 took 2.17 seconds (mean sampled reward: -5991.55). Current reward after update: -4256.20, Optimal reward -3802.05
Iteration 30 took 2.25 seconds (mean sampled reward: -6184.66). Current reward after update: -4070.68, Optimal reward -3802.05
Iteration 31 took 2.41 seconds (mean sampled reward: -6254.87). Current reward after update: -3758.96, Optimal reward -3758.96
Iteration 32 took 2.28 seconds (mean sampled reward: -6678.71). Current reward after update: -3551.92, Optimal reward -3551.92
Iteration 33 took 2.38 seconds (mean sampled reward: -5617.56). Current reward after update: -3832.53, Optimal reward -3551.92
Iteration 34 took 2.36 seconds (mean sampled reward: -5050.27). Current reward after update: -3466.58, Optimal reward -3466.58
Iteration 35 took 2.26 seconds (mean sampled reward: -5293.91). Current reward after update: -3372.76, Optimal reward -3372.76
Iteration 36 took 2.29 seconds (mean sampled reward: -5147.31). Current reward after update: -3424.54, Optimal reward -3372.76
Iteration 37 took 2.22 seconds (mean sampled reward: -6187.51). Current reward after update: -3282.07, Optimal reward -3282.07
Iteration 38 took 2.17 seconds (mean sampled reward: -5292.47). Current reward after update: -3298.99, Optimal reward -3282.07
Iteration 39 took 2.19 seconds (mean sampled reward: -4913.10). Current reward after update: -3874.89, Optimal reward -3282.07
Iteration 40 took 2.16 seconds (mean sampled reward: -4677.49). Current reward after update: -3234.16, Optimal reward -3234.16
Iteration 41 took 2.13 seconds (mean sampled reward: -5065.94). Current reward after update: -3106.14, Optimal reward -3106.14
Iteration 42 took 2.22 seconds (mean sampled reward: -4541.30). Current reward after update: -6906.40, Optimal reward -3106.14
Iteration 43 took 2.18 seconds (mean sampled reward: -4835.19). Current reward after update: -3955.94, Optimal reward -3106.14
Iteration 44 took 2.18 seconds (mean sampled reward: -5770.19). Current reward after update: -3106.80, Optimal reward -3106.14
Iteration 45 took 2.31 seconds (mean sampled reward: -5087.04). Current reward after update: -3021.82, Optimal reward -3021.82
Iteration 46 took 2.19 seconds (mean sampled reward: -4346.04). Current reward after update: -2711.07, Optimal reward -2711.07
Iteration 47 took 2.21 seconds (mean sampled reward: -4894.82). Current reward after update: -2633.40, Optimal reward -2633.40
Iteration 48 took 2.29 seconds (mean sampled reward: -5284.89). Current reward after update: -2459.31, Optimal reward -2459.31
Iteration 49 took 2.22 seconds (mean sampled reward: -5587.11). Current reward after update: -2491.68, Optimal reward -2459.31
Iteration 50 took 2.19 seconds (mean sampled reward: -4121.22). Current reward after update: -2559.18, Optimal reward -2459.31
Iteration 51 took 2.18 seconds (mean sampled reward: -4883.31). Current reward after update: -2121.81, Optimal reward -2121.81
Iteration 52 took 2.18 seconds (mean sampled reward: -6045.91). Current reward after update: -2312.62, Optimal reward -2121.81
Iteration 53 took 2.37 seconds (mean sampled reward: -5777.63). Current reward after update: -2159.84, Optimal reward -2121.81
Iteration 54 took 2.61 seconds (mean sampled reward: -6619.32). Current reward after update: -2473.12, Optimal reward -2121.81
Iteration 55 took 2.24 seconds (mean sampled reward: -5405.62). Current reward after update: -6026.89, Optimal reward -2121.81
Iteration 56 took 2.31 seconds (mean sampled reward: -5208.78). Current reward after update: -1914.63, Optimal reward -1914.63
Iteration 57 took 2.30 seconds (mean sampled reward: -4822.90). Current reward after update: -1677.28, Optimal reward -1677.28
Iteration 58 took 2.17 seconds (mean sampled reward: -4602.33). Current reward after update: -1751.33, Optimal reward -1677.28
Iteration 59 took 2.22 seconds (mean sampled reward: -4225.46). Current reward after update: -1643.11, Optimal reward -1643.11
Iteration 60 took 2.31 seconds (mean sampled reward: -4830.44). Current reward after update: -1469.67, Optimal reward -1469.67
Iteration 61 took 2.17 seconds (mean sampled reward: -5150.88). Current reward after update: -1791.35, Optimal reward -1469.67
Iteration 62 took 2.31 seconds (mean sampled reward: -5506.46). Current reward after update: -1505.59, Optimal reward -1469.67
Iteration 63 took 2.17 seconds (mean sampled reward: -5185.50). Current reward after update: -1954.10, Optimal reward -1469.67
Iteration 64 took 2.20 seconds (mean sampled reward: -4268.04). Current reward after update: -1376.33, Optimal reward -1376.33
Iteration 65 took 2.21 seconds (mean sampled reward: -5584.00). Current reward after update: -1914.91, Optimal reward -1376.33
Iteration 66 took 2.23 seconds (mean sampled reward: -3668.98). Current reward after update: -2136.08, Optimal reward -1376.33
Iteration 67 took 2.33 seconds (mean sampled reward: -3541.82). Current reward after update: -1462.86, Optimal reward -1376.33
Iteration 68 took 2.22 seconds (mean sampled reward: -4189.45). Current reward after update: -2475.76, Optimal reward -1376.33
Iteration 69 took 2.37 seconds (mean sampled reward: -3504.85). Current reward after update: -1492.38, Optimal reward -1376.33
Iteration 70 took 2.25 seconds (mean sampled reward: -4541.62). Current reward after update: -2097.78, Optimal reward -1376.33
Iteration 71 took 2.21 seconds (mean sampled reward: -4204.38). Current reward after update: -3147.39, Optimal reward -1376.33
Iteration 72 took 2.21 seconds (mean sampled reward: -4425.74). Current reward after update: -1510.42, Optimal reward -1376.33
Iteration 73 took 2.18 seconds (mean sampled reward: -5382.93). Current reward after update: -1960.16, Optimal reward -1376.33
Iteration 74 took 2.23 seconds (mean sampled reward: -4730.87). Current reward after update: -1727.94, Optimal reward -1376.33
Iteration 75 took 2.26 seconds (mean sampled reward: -3564.28). Current reward after update: -2246.26, Optimal reward -1376.33
Iteration 76 took 2.11 seconds (mean sampled reward: -3595.68). Current reward after update: -1438.25, Optimal reward -1376.33
Iteration 77 took 2.15 seconds (mean sampled reward: -3868.81). Current reward after update: -1788.84, Optimal reward -1376.33
Iteration 78 took 2.20 seconds (mean sampled reward: -4358.42). Current reward after update: -1581.44, Optimal reward -1376.33
Iteration 79 took 2.17 seconds (mean sampled reward: -4303.83). Current reward after update: -1365.38, Optimal reward -1365.38
Iteration 80 took 2.17 seconds (mean sampled reward: -4008.91). Current reward after update: -1632.35, Optimal reward -1365.38
Iteration 81 took 2.24 seconds (mean sampled reward: -3254.15). Current reward after update: -1441.45, Optimal reward -1365.38
Iteration 82 took 2.31 seconds (mean sampled reward: -3446.15). Current reward after update: -1438.37, Optimal reward -1365.38
Iteration 83 took 2.35 seconds (mean sampled reward: -4849.95). Current reward after update: -1557.14, Optimal reward -1365.38
Iteration 84 took 2.25 seconds (mean sampled reward: -4449.31). Current reward after update: -1493.46, Optimal reward -1365.38
Iteration 85 took 2.34 seconds (mean sampled reward: -4765.90). Current reward after update: -1502.96, Optimal reward -1365.38
Iteration 86 took 2.27 seconds (mean sampled reward: -4684.60). Current reward after update: -1489.13, Optimal reward -1365.38
Iteration 87 took 2.35 seconds (mean sampled reward: -4543.02). Current reward after update: -5651.30, Optimal reward -1365.38
Iteration 88 took 2.31 seconds (mean sampled reward: -4356.77). Current reward after update: -1334.65, Optimal reward -1334.65
Iteration 89 took 2.45 seconds (mean sampled reward: -4790.47). Current reward after update: -1419.44, Optimal reward -1334.65
Iteration 90 took 2.35 seconds (mean sampled reward: -4957.96). Current reward after update: -1303.94, Optimal reward -1303.94
Iteration 91 took 2.24 seconds (mean sampled reward: -4189.54). Current reward after update: -1321.03, Optimal reward -1303.94
Iteration 92 took 2.30 seconds (mean sampled reward: -4277.24). Current reward after update: -1282.01, Optimal reward -1282.01
Iteration 93 took 2.32 seconds (mean sampled reward: -4266.51). Current reward after update: -1411.94, Optimal reward -1282.01
Iteration 94 took 2.35 seconds (mean sampled reward: -4432.39). Current reward after update: -1290.73, Optimal reward -1282.01
Iteration 95 took 2.34 seconds (mean sampled reward: -4758.70). Current reward after update: -1604.51, Optimal reward -1282.01
Iteration 96 took 2.41 seconds (mean sampled reward: -5187.08). Current reward after update: -1280.28, Optimal reward -1280.28
Iteration 97 took 2.27 seconds (mean sampled reward: -5439.92). Current reward after update: -5638.02, Optimal reward -1280.28
Iteration 98 took 2.29 seconds (mean sampled reward: -5315.51). Current reward after update: -1305.11, Optimal reward -1280.28
Iteration 99 took 2.26 seconds (mean sampled reward: -5420.76). Current reward after update: -1357.25, Optimal reward -1280.28
Iteration 100 took 2.28 seconds (mean sampled reward: -5720.99). Current reward after update: -1396.26, Optimal reward -1280.28
Iteration 101 took 2.28 seconds (mean sampled reward: -5239.00). Current reward after update: -1431.69, Optimal reward -1280.28
Iteration 102 took 2.29 seconds (mean sampled reward: -5377.91). Current reward after update: -1408.67, Optimal reward -1280.28
Iteration 103 took 2.34 seconds (mean sampled reward: -4829.64). Current reward after update: -1365.82, Optimal reward -1280.28
Iteration 104 took 2.38 seconds (mean sampled reward: -4136.47). Current reward after update: -1329.92, Optimal reward -1280.28
Iteration 105 took 2.28 seconds (mean sampled reward: -3827.45). Current reward after update: -1331.98, Optimal reward -1280.28
Iteration 106 took 2.34 seconds (mean sampled reward: -4420.47). Current reward after update: -1641.88, Optimal reward -1280.28
Iteration 107 took 2.22 seconds (mean sampled reward: -4155.37). Current reward after update: -1524.72, Optimal reward -1280.28
Iteration 108 took 2.22 seconds (mean sampled reward: -4713.53). Current reward after update: -3101.29, Optimal reward -1280.28
Iteration 109 took 2.58 seconds (mean sampled reward: -4211.70). Current reward after update: -1415.28, Optimal reward -1280.28
Iteration 110 took 2.39 seconds (mean sampled reward: -5311.31). Current reward after update: -1002.84, Optimal reward -1002.84
Iteration 111 took 2.35 seconds (mean sampled reward: -5218.43). Current reward after update: -956.81, Optimal reward -956.81
Iteration 112 took 2.28 seconds (mean sampled reward: -4872.64). Current reward after update: -932.53, Optimal reward -932.53
Iteration 113 took 2.22 seconds (mean sampled reward: -4113.02). Current reward after update: -981.74, Optimal reward -932.53
Iteration 114 took 2.17 seconds (mean sampled reward: -4718.06). Current reward after update: -756.78, Optimal reward -756.78
Iteration 115 took 2.28 seconds (mean sampled reward: -4053.09). Current reward after update: -789.16, Optimal reward -756.78
Iteration 116 took 2.25 seconds (mean sampled reward: -3134.81). Current reward after update: -817.96, Optimal reward -756.78
Iteration 117 took 2.14 seconds (mean sampled reward: -3860.43). Current reward after update: -744.12, Optimal reward -744.12
Iteration 118 took 2.14 seconds (mean sampled reward: -3812.22). Current reward after update: -634.20, Optimal reward -634.20
Iteration 119 took 2.24 seconds (mean sampled reward: -4009.09). Current reward after update: -6021.81, Optimal reward -634.20
Iteration 120 took 2.13 seconds (mean sampled reward: -4539.84). Current reward after update: -640.15, Optimal reward -634.20
Iteration 121 took 2.26 seconds (mean sampled reward: -3453.97). Current reward after update: -611.69, Optimal reward -611.69
Iteration 122 took 2.15 seconds (mean sampled reward: -4184.53). Current reward after update: -537.42, Optimal reward -537.42
Iteration 123 took 2.20 seconds (mean sampled reward: -3003.66). Current reward after update: -528.12, Optimal reward -528.12
Iteration 124 took 2.22 seconds (mean sampled reward: -3000.96). Current reward after update: -538.72, Optimal reward -528.12
Iteration 125 took 2.20 seconds (mean sampled reward: -3503.90). Current reward after update: -604.35, Optimal reward -528.12
Iteration 126 took 2.19 seconds (mean sampled reward: -3505.56). Current reward after update: -640.25, Optimal reward -528.12
Iteration 127 took 2.17 seconds (mean sampled reward: -3910.88). Current reward after update: -490.90, Optimal reward -490.90
Iteration 128 took 2.18 seconds (mean sampled reward: -3842.79). Current reward after update: -1260.88, Optimal reward -490.90
Iteration 129 took 2.16 seconds (mean sampled reward: -4837.62). Current reward after update: -676.96, Optimal reward -490.90
Iteration 130 took 2.20 seconds (mean sampled reward: -5121.43). Current reward after update: -641.86, Optimal reward -490.90
Iteration 131 took 2.12 seconds (mean sampled reward: -3337.58). Current reward after update: -667.53, Optimal reward -490.90
Iteration 132 took 2.16 seconds (mean sampled reward: -2919.29). Current reward after update: -542.88, Optimal reward -490.90
Iteration 133 took 2.13 seconds (mean sampled reward: -2839.58). Current reward after update: -579.22, Optimal reward -490.90
Iteration 134 took 2.12 seconds (mean sampled reward: -4544.17). Current reward after update: -598.68, Optimal reward -490.90
Iteration 135 took 2.07 seconds (mean sampled reward: -4562.40). Current reward after update: -658.76, Optimal reward -490.90
Iteration 136 took 2.20 seconds (mean sampled reward: -4051.74). Current reward after update: -626.71, Optimal reward -490.90
Iteration 137 took 2.11 seconds (mean sampled reward: -2280.23). Current reward after update: -1231.07, Optimal reward -490.90
Iteration 138 took 2.10 seconds (mean sampled reward: -2008.26). Current reward after update: -649.83, Optimal reward -490.90
Iteration 139 took 2.10 seconds (mean sampled reward: -2318.47). Current reward after update: -779.43, Optimal reward -490.90
Iteration 140 took 2.12 seconds (mean sampled reward: -2457.16). Current reward after update: -604.34, Optimal reward -490.90
Iteration 141 took 2.10 seconds (mean sampled reward: -2385.07). Current reward after update: -846.54, Optimal reward -490.90
Iteration 142 took 2.08 seconds (mean sampled reward: -2456.61). Current reward after update: -3300.01, Optimal reward -490.90
Iteration 143 took 2.15 seconds (mean sampled reward: -1951.99). Current reward after update: -1362.55, Optimal reward -490.90
Iteration 144 took 2.13 seconds (mean sampled reward: -2911.20). Current reward after update: -848.75, Optimal reward -490.90
Iteration 145 took 2.10 seconds (mean sampled reward: -4263.95). Current reward after update: -605.88, Optimal reward -490.90
Iteration 146 took 2.08 seconds (mean sampled reward: -4732.81). Current reward after update: -571.74, Optimal reward -490.90
Iteration 147 took 2.16 seconds (mean sampled reward: -3577.31). Current reward after update: -539.66, Optimal reward -490.90
Iteration 148 took 2.11 seconds (mean sampled reward: -3366.24). Current reward after update: -1839.42, Optimal reward -490.90
Iteration 149 took 2.04 seconds (mean sampled reward: -4275.07). Current reward after update: -517.03, Optimal reward -490.90
Iteration 150 took 2.06 seconds (mean sampled reward: -3755.11). Current reward after update: -497.42, Optimal reward -490.90
Iteration 151 took 2.11 seconds (mean sampled reward: -4100.94). Current reward after update: -499.51, Optimal reward -490.90
Iteration 152 took 2.14 seconds (mean sampled reward: -3299.67). Current reward after update: -1037.53, Optimal reward -490.90
Iteration 153 took 2.17 seconds (mean sampled reward: -2666.27). Current reward after update: -1584.53, Optimal reward -490.90
Iteration 154 took 2.12 seconds (mean sampled reward: -3345.01). Current reward after update: -966.85, Optimal reward -490.90
Iteration 155 took 2.19 seconds (mean sampled reward: -2501.34). Current reward after update: -594.12, Optimal reward -490.90
Iteration 156 took 2.20 seconds (mean sampled reward: -3275.54). Current reward after update: -572.88, Optimal reward -490.90
Iteration 157 took 2.20 seconds (mean sampled reward: -3431.83). Current reward after update: -685.60, Optimal reward -490.90
Iteration 158 took 2.18 seconds (mean sampled reward: -4051.64). Current reward after update: -616.95, Optimal reward -490.90
Iteration 159 took 2.16 seconds (mean sampled reward: -3659.59). Current reward after update: -731.90, Optimal reward -490.90
Iteration 160 took 2.15 seconds (mean sampled reward: -2592.51). Current reward after update: -654.49, Optimal reward -490.90
Iteration 161 took 2.14 seconds (mean sampled reward: -2328.07). Current reward after update: -658.16, Optimal reward -490.90
Iteration 162 took 2.17 seconds (mean sampled reward: -2529.72). Current reward after update: -608.43, Optimal reward -490.90
Iteration 163 took 2.16 seconds (mean sampled reward: -2290.80). Current reward after update: -534.10, Optimal reward -490.90
Iteration 164 took 2.16 seconds (mean sampled reward: -2230.94). Current reward after update: -533.82, Optimal reward -490.90
Iteration 165 took 2.21 seconds (mean sampled reward: -3311.56). Current reward after update: -756.87, Optimal reward -490.90
Iteration 166 took 2.14 seconds (mean sampled reward: -3022.49). Current reward after update: -1104.97, Optimal reward -490.90
Iteration 167 took 2.15 seconds (mean sampled reward: -4018.25). Current reward after update: -607.49, Optimal reward -490.90
Iteration 168 took 2.19 seconds (mean sampled reward: -4987.31). Current reward after update: -661.55, Optimal reward -490.90
Iteration 169 took 2.18 seconds (mean sampled reward: -4666.32). Current reward after update: -635.83, Optimal reward -490.90
Iteration 170 took 2.17 seconds (mean sampled reward: -4717.47). Current reward after update: -585.04, Optimal reward -490.90
Iteration 171 took 2.18 seconds (mean sampled reward: -4977.82). Current reward after update: -832.40, Optimal reward -490.90
Iteration 172 took 2.17 seconds (mean sampled reward: -3575.96). Current reward after update: -676.93, Optimal reward -490.90
Iteration 173 took 2.26 seconds (mean sampled reward: -3054.70). Current reward after update: -576.78, Optimal reward -490.90
Iteration 174 took 2.24 seconds (mean sampled reward: -4263.88). Current reward after update: -6093.31, Optimal reward -490.90
Iteration 175 took 2.16 seconds (mean sampled reward: -4516.87). Current reward after update: -538.58, Optimal reward -490.90
Iteration 176 took 2.20 seconds (mean sampled reward: -4428.52). Current reward after update: -649.88, Optimal reward -490.90
Iteration 177 took 2.21 seconds (mean sampled reward: -4062.37). Current reward after update: -632.71, Optimal reward -490.90
Iteration 178 took 2.17 seconds (mean sampled reward: -3581.60). Current reward after update: -591.16, Optimal reward -490.90
Iteration 179 took 2.20 seconds (mean sampled reward: -4433.27). Current reward after update: -740.26, Optimal reward -490.90
Iteration 180 took 2.18 seconds (mean sampled reward: -4874.32). Current reward after update: -705.50, Optimal reward -490.90
Iteration 181 took 2.19 seconds (mean sampled reward: -4965.75). Current reward after update: -624.53, Optimal reward -490.90
Iteration 182 took 2.19 seconds (mean sampled reward: -3852.17). Current reward after update: -768.54, Optimal reward -490.90
Iteration 183 took 2.15 seconds (mean sampled reward: -2685.61). Current reward after update: -665.75, Optimal reward -490.90
Iteration 184 took 2.24 seconds (mean sampled reward: -3349.06). Current reward after update: -622.57, Optimal reward -490.90
Iteration 185 took 2.18 seconds (mean sampled reward: -3853.75). Current reward after update: -867.22, Optimal reward -490.90
Iteration 186 took 2.19 seconds (mean sampled reward: -2375.46). Current reward after update: -625.66, Optimal reward -490.90
Iteration 187 took 2.27 seconds (mean sampled reward: -3435.75). Current reward after update: -683.14, Optimal reward -490.90
Iteration 188 took 2.22 seconds (mean sampled reward: -3701.95). Current reward after update: -1005.14, Optimal reward -490.90
Iteration 189 took 2.23 seconds (mean sampled reward: -5071.44). Current reward after update: -689.72, Optimal reward -490.90
Iteration 190 took 2.18 seconds (mean sampled reward: -5332.28). Current reward after update: -627.55, Optimal reward -490.90
Iteration 191 took 2.20 seconds (mean sampled reward: -4097.50). Current reward after update: -646.64, Optimal reward -490.90
Iteration 192 took 2.16 seconds (mean sampled reward: -3546.97). Current reward after update: -1231.03, Optimal reward -490.90
Iteration 193 took 2.10 seconds (mean sampled reward: -5435.82). Current reward after update: -583.94, Optimal reward -490.90
Iteration 194 took 2.12 seconds (mean sampled reward: -5677.24). Current reward after update: -606.62, Optimal reward -490.90
Iteration 195 took 2.17 seconds (mean sampled reward: -4933.19). Current reward after update: -546.23, Optimal reward -490.90
Iteration 196 took 2.15 seconds (mean sampled reward: -3457.81). Current reward after update: -619.40, Optimal reward -490.90
Iteration 197 took 2.27 seconds (mean sampled reward: -4154.77). Current reward after update: -498.23, Optimal reward -490.90
Iteration 198 took 2.10 seconds (mean sampled reward: -4429.68). Current reward after update: -6877.08, Optimal reward -490.90
Iteration 199 took 2.19 seconds (mean sampled reward: -4479.80). Current reward after update: -727.47, Optimal reward -490.90
Iteration 200 took 2.25 seconds (mean sampled reward: -5371.53). Current reward after update: -663.95, Optimal reward -490.90
Iteration 1 took 2.26 seconds (mean sampled reward: -7510.21). Current reward after update: -7151.42, Optimal reward -7151.42
Iteration 2 took 2.13 seconds (mean sampled reward: -7487.03). Current reward after update: -7209.09, Optimal reward -7151.42
Iteration 3 took 2.11 seconds (mean sampled reward: -7462.97). Current reward after update: -7034.41, Optimal reward -7034.41
Iteration 4 took 2.08 seconds (mean sampled reward: -7399.22). Current reward after update: -6480.40, Optimal reward -6480.40
Iteration 5 took 2.23 seconds (mean sampled reward: -7083.34). Current reward after update: -5715.26, Optimal reward -5715.26
Iteration 6 took 2.18 seconds (mean sampled reward: -6363.27). Current reward after update: -4508.42, Optimal reward -4508.42
Iteration 7 took 2.16 seconds (mean sampled reward: -6416.69). Current reward after update: -4371.72, Optimal reward -4371.72
Iteration 8 took 2.24 seconds (mean sampled reward: -5691.64). Current reward after update: -3270.82, Optimal reward -3270.82
Iteration 9 took 2.19 seconds (mean sampled reward: -6320.30). Current reward after update: -3856.64, Optimal reward -3270.82
Iteration 10 took 2.15 seconds (mean sampled reward: -5589.48). Current reward after update: -3680.80, Optimal reward -3270.82
Iteration 11 took 2.08 seconds (mean sampled reward: -5982.57). Current reward after update: -2872.57, Optimal reward -2872.57
Iteration 12 took 2.06 seconds (mean sampled reward: -5736.20). Current reward after update: -3200.90, Optimal reward -2872.57
Iteration 13 took 2.12 seconds (mean sampled reward: -5111.37). Current reward after update: -2429.08, Optimal reward -2429.08
Iteration 14 took 2.26 seconds (mean sampled reward: -5101.51). Current reward after update: -2553.87, Optimal reward -2429.08
Iteration 15 took 2.16 seconds (mean sampled reward: -4782.00). Current reward after update: -1986.71, Optimal reward -1986.71
Iteration 16 took 2.11 seconds (mean sampled reward: -4477.74). Current reward after update: -2033.49, Optimal reward -1986.71
Iteration 17 took 2.13 seconds (mean sampled reward: -3839.58). Current reward after update: -5814.85, Optimal reward -1986.71
Iteration 18 took 2.17 seconds (mean sampled reward: -4579.94). Current reward after update: -1724.94, Optimal reward -1724.94
Iteration 19 took 2.13 seconds (mean sampled reward: -4868.05). Current reward after update: -1798.28, Optimal reward -1724.94
Iteration 20 took 2.15 seconds (mean sampled reward: -4066.89). Current reward after update: -1936.16, Optimal reward -1724.94
Iteration 21 took 2.15 seconds (mean sampled reward: -3917.11). Current reward after update: -1681.66, Optimal reward -1681.66
Iteration 22 took 2.14 seconds (mean sampled reward: -3817.16). Current reward after update: -2289.48, Optimal reward -1681.66
Iteration 23 took 2.21 seconds (mean sampled reward: -4150.17). Current reward after update: -1720.64, Optimal reward -1681.66
Iteration 24 took 2.25 seconds (mean sampled reward: -3223.90). Current reward after update: -2084.33, Optimal reward -1681.66
Iteration 25 took 2.25 seconds (mean sampled reward: -3659.41). Current reward after update: -1648.23, Optimal reward -1648.23
Iteration 26 took 2.21 seconds (mean sampled reward: -3221.79). Current reward after update: -2463.41, Optimal reward -1648.23
Iteration 27 took 2.15 seconds (mean sampled reward: -3498.38). Current reward after update: -1713.56, Optimal reward -1648.23
Iteration 28 took 2.28 seconds (mean sampled reward: -3476.86). Current reward after update: -1820.78, Optimal reward -1648.23
Iteration 29 took 2.29 seconds (mean sampled reward: -3150.63). Current reward after update: -1676.59, Optimal reward -1648.23
Iteration 30 took 2.22 seconds (mean sampled reward: -4020.89). Current reward after update: -1658.36, Optimal reward -1648.23
Iteration 31 took 2.13 seconds (mean sampled reward: -4366.49). Current reward after update: -1666.49, Optimal reward -1648.23
Iteration 32 took 2.36 seconds (mean sampled reward: -4185.68). Current reward after update: -1567.37, Optimal reward -1567.37
Iteration 33 took 2.24 seconds (mean sampled reward: -3356.97). Current reward after update: -1668.53, Optimal reward -1567.37
Iteration 34 took 2.24 seconds (mean sampled reward: -3373.20). Current reward after update: -1561.01, Optimal reward -1561.01
Iteration 35 took 2.16 seconds (mean sampled reward: -3695.97). Current reward after update: -1690.45, Optimal reward -1561.01
Iteration 36 took 2.23 seconds (mean sampled reward: -3830.93). Current reward after update: -2236.75, Optimal reward -1561.01
Iteration 37 took 2.11 seconds (mean sampled reward: -3990.05). Current reward after update: -2313.57, Optimal reward -1561.01
Iteration 38 took 2.36 seconds (mean sampled reward: -4873.06). Current reward after update: -2132.43, Optimal reward -1561.01
Iteration 39 took 2.10 seconds (mean sampled reward: -4587.75). Current reward after update: -1759.38, Optimal reward -1561.01
Iteration 40 took 2.10 seconds (mean sampled reward: -5120.42). Current reward after update: -1873.95, Optimal reward -1561.01
Iteration 41 took 2.12 seconds (mean sampled reward: -4867.68). Current reward after update: -1876.81, Optimal reward -1561.01
Iteration 42 took 2.14 seconds (mean sampled reward: -5086.34). Current reward after update: -1910.03, Optimal reward -1561.01
Iteration 43 took 2.13 seconds (mean sampled reward: -5365.78). Current reward after update: -1981.38, Optimal reward -1561.01
Iteration 44 took 2.14 seconds (mean sampled reward: -5644.25). Current reward after update: -1879.00, Optimal reward -1561.01
Iteration 45 took 2.33 seconds (mean sampled reward: -4302.18). Current reward after update: -5780.60, Optimal reward -1561.01
Iteration 46 took 2.28 seconds (mean sampled reward: -3859.42). Current reward after update: -1796.46, Optimal reward -1561.01
Iteration 47 took 2.10 seconds (mean sampled reward: -4098.75). Current reward after update: -1832.05, Optimal reward -1561.01
Iteration 48 took 2.11 seconds (mean sampled reward: -4493.48). Current reward after update: -1859.41, Optimal reward -1561.01
Iteration 49 took 2.05 seconds (mean sampled reward: -5134.05). Current reward after update: -1888.07, Optimal reward -1561.01
Iteration 50 took 2.14 seconds (mean sampled reward: -5117.96). Current reward after update: -1926.50, Optimal reward -1561.01
Iteration 51 took 2.11 seconds (mean sampled reward: -4782.74). Current reward after update: -1974.65, Optimal reward -1561.01
Iteration 52 took 2.06 seconds (mean sampled reward: -4841.55). Current reward after update: -1870.70, Optimal reward -1561.01
Iteration 53 took 2.22 seconds (mean sampled reward: -4439.66). Current reward after update: -1537.53, Optimal reward -1537.53
Iteration 54 took 2.12 seconds (mean sampled reward: -4121.36). Current reward after update: -1535.75, Optimal reward -1535.75
Iteration 55 took 2.34 seconds (mean sampled reward: -3312.37). Current reward after update: -1500.33, Optimal reward -1500.33
Iteration 56 took 2.05 seconds (mean sampled reward: -4327.33). Current reward after update: -1528.58, Optimal reward -1500.33
Iteration 57 took 2.03 seconds (mean sampled reward: -4186.03). Current reward after update: -1566.21, Optimal reward -1500.33
Iteration 58 took 2.23 seconds (mean sampled reward: -3891.66). Current reward after update: -1477.20, Optimal reward -1477.20
Iteration 59 took 2.02 seconds (mean sampled reward: -3407.65). Current reward after update: -1425.55, Optimal reward -1425.55
Iteration 60 took 2.06 seconds (mean sampled reward: -3535.23). Current reward after update: -1491.27, Optimal reward -1425.55
Iteration 61 took 2.09 seconds (mean sampled reward: -4120.29). Current reward after update: -2026.84, Optimal reward -1425.55
Iteration 62 took 2.01 seconds (mean sampled reward: -3492.20). Current reward after update: -2024.25, Optimal reward -1425.55
Iteration 63 took 2.05 seconds (mean sampled reward: -2696.19). Current reward after update: -1418.65, Optimal reward -1418.65
Iteration 64 took 2.05 seconds (mean sampled reward: -2999.49). Current reward after update: -1552.42, Optimal reward -1418.65
Iteration 65 took 2.14 seconds (mean sampled reward: -2185.24). Current reward after update: -1341.10, Optimal reward -1341.10
Iteration 66 took 2.17 seconds (mean sampled reward: -3799.76). Current reward after update: -1395.13, Optimal reward -1341.10
Iteration 67 took 2.18 seconds (mean sampled reward: -2067.56). Current reward after update: -1407.59, Optimal reward -1341.10
Iteration 68 took 2.30 seconds (mean sampled reward: -1893.58). Current reward after update: -1307.17, Optimal reward -1307.17
Iteration 69 took 2.15 seconds (mean sampled reward: -2207.02). Current reward after update: -1705.82, Optimal reward -1307.17
Iteration 70 took 2.16 seconds (mean sampled reward: -2479.67). Current reward after update: -1344.63, Optimal reward -1307.17
Iteration 71 took 2.15 seconds (mean sampled reward: -2338.92). Current reward after update: -1496.52, Optimal reward -1307.17
Iteration 72 took 2.19 seconds (mean sampled reward: -2310.58). Current reward after update: -1552.26, Optimal reward -1307.17
Iteration 73 took 2.16 seconds (mean sampled reward: -2942.59). Current reward after update: -1315.10, Optimal reward -1307.17
Iteration 74 took 2.12 seconds (mean sampled reward: -2557.48). Current reward after update: -1336.02, Optimal reward -1307.17
Iteration 75 took 2.17 seconds (mean sampled reward: -2474.55). Current reward after update: -1338.43, Optimal reward -1307.17
Iteration 76 took 2.25 seconds (mean sampled reward: -2086.20). Current reward after update: -1290.31, Optimal reward -1290.31
Iteration 77 took 2.13 seconds (mean sampled reward: -2155.25). Current reward after update: -1317.87, Optimal reward -1290.31
Iteration 78 took 2.13 seconds (mean sampled reward: -2185.70). Current reward after update: -1287.09, Optimal reward -1287.09
Iteration 79 took 2.16 seconds (mean sampled reward: -2755.60). Current reward after update: -1215.66, Optimal reward -1215.66
Iteration 80 took 2.05 seconds (mean sampled reward: -5031.00). Current reward after update: -1414.13, Optimal reward -1215.66
Iteration 81 took 2.17 seconds (mean sampled reward: -3166.86). Current reward after update: -1941.48, Optimal reward -1215.66
Iteration 82 took 2.10 seconds (mean sampled reward: -4127.11). Current reward after update: -1254.48, Optimal reward -1215.66
Iteration 83 took 2.16 seconds (mean sampled reward: -2671.92). Current reward after update: -1553.24, Optimal reward -1215.66
Iteration 84 took 2.14 seconds (mean sampled reward: -2464.36). Current reward after update: -1918.91, Optimal reward -1215.66
Iteration 85 took 2.21 seconds (mean sampled reward: -3104.25). Current reward after update: -1203.11, Optimal reward -1203.11
Iteration 86 took 2.12 seconds (mean sampled reward: -3594.02). Current reward after update: -1242.07, Optimal reward -1203.11
Iteration 87 took 2.17 seconds (mean sampled reward: -3978.25). Current reward after update: -1134.27, Optimal reward -1134.27
Iteration 88 took 2.20 seconds (mean sampled reward: -3275.50). Current reward after update: -1347.66, Optimal reward -1134.27
Iteration 89 took 2.22 seconds (mean sampled reward: -3386.84). Current reward after update: -1292.81, Optimal reward -1134.27
Iteration 90 took 2.14 seconds (mean sampled reward: -4230.15). Current reward after update: -1416.39, Optimal reward -1134.27
Iteration 91 took 2.20 seconds (mean sampled reward: -3626.41). Current reward after update: -1224.68, Optimal reward -1134.27
Iteration 92 took 2.18 seconds (mean sampled reward: -3724.86). Current reward after update: -1269.59, Optimal reward -1134.27
Iteration 93 took 2.13 seconds (mean sampled reward: -3483.41). Current reward after update: -1450.65, Optimal reward -1134.27
Iteration 94 took 2.13 seconds (mean sampled reward: -3613.29). Current reward after update: -1292.44, Optimal reward -1134.27
Iteration 95 took 2.22 seconds (mean sampled reward: -2870.48). Current reward after update: -1230.82, Optimal reward -1134.27
Iteration 96 took 2.26 seconds (mean sampled reward: -2368.00). Current reward after update: -1403.14, Optimal reward -1134.27
Iteration 97 took 2.16 seconds (mean sampled reward: -3369.90). Current reward after update: -1211.51, Optimal reward -1134.27
Iteration 98 took 2.24 seconds (mean sampled reward: -3203.17). Current reward after update: -1252.84, Optimal reward -1134.27
Iteration 99 took 2.21 seconds (mean sampled reward: -3250.45). Current reward after update: -1260.52, Optimal reward -1134.27
Iteration 100 took 2.23 seconds (mean sampled reward: -4642.26). Current reward after update: -1236.75, Optimal reward -1134.27
Iteration 101 took 2.24 seconds (mean sampled reward: -3557.24). Current reward after update: -1278.84, Optimal reward -1134.27
Iteration 102 took 2.17 seconds (mean sampled reward: -4122.49). Current reward after update: -1341.01, Optimal reward -1134.27
Iteration 103 took 2.13 seconds (mean sampled reward: -4959.72). Current reward after update: -1239.37, Optimal reward -1134.27
Iteration 104 took 2.10 seconds (mean sampled reward: -4662.21). Current reward after update: -2560.37, Optimal reward -1134.27
Iteration 105 took 2.08 seconds (mean sampled reward: -4860.37). Current reward after update: -1220.10, Optimal reward -1134.27
Iteration 106 took 2.27 seconds (mean sampled reward: -3210.43). Current reward after update: -1180.33, Optimal reward -1134.27
Iteration 107 took 2.40 seconds (mean sampled reward: -4046.05). Current reward after update: -1207.45, Optimal reward -1134.27
Iteration 108 took 2.46 seconds (mean sampled reward: -2866.69). Current reward after update: -1189.61, Optimal reward -1134.27
Iteration 109 took 2.30 seconds (mean sampled reward: -3647.82). Current reward after update: -1104.17, Optimal reward -1104.17
Iteration 110 took 2.18 seconds (mean sampled reward: -3424.37). Current reward after update: -1111.66, Optimal reward -1104.17
Iteration 111 took 2.26 seconds (mean sampled reward: -3977.47). Current reward after update: -1181.27, Optimal reward -1104.17
Iteration 112 took 2.23 seconds (mean sampled reward: -3847.93). Current reward after update: -1154.17, Optimal reward -1104.17
Iteration 113 took 2.16 seconds (mean sampled reward: -3991.00). Current reward after update: -1235.13, Optimal reward -1104.17
Iteration 114 took 2.16 seconds (mean sampled reward: -3445.97). Current reward after update: -1118.41, Optimal reward -1104.17
Iteration 115 took 2.20 seconds (mean sampled reward: -3157.45). Current reward after update: -1097.26, Optimal reward -1097.26
Iteration 116 took 2.18 seconds (mean sampled reward: -3842.47). Current reward after update: -1140.88, Optimal reward -1097.26
Iteration 117 took 2.12 seconds (mean sampled reward: -4078.33). Current reward after update: -1034.86, Optimal reward -1034.86
Iteration 118 took 2.14 seconds (mean sampled reward: -3764.03). Current reward after update: -1343.57, Optimal reward -1034.86
Iteration 119 took 2.11 seconds (mean sampled reward: -3273.08). Current reward after update: -1052.01, Optimal reward -1034.86
Iteration 120 took 2.30 seconds (mean sampled reward: -3136.18). Current reward after update: -1056.37, Optimal reward -1034.86
Iteration 121 took 2.23 seconds (mean sampled reward: -4084.32). Current reward after update: -1547.22, Optimal reward -1034.86
Iteration 122 took 2.14 seconds (mean sampled reward: -4602.68). Current reward after update: -1567.51, Optimal reward -1034.86
Iteration 123 took 2.20 seconds (mean sampled reward: -4024.63). Current reward after update: -1337.65, Optimal reward -1034.86
Iteration 124 took 2.25 seconds (mean sampled reward: -3003.81). Current reward after update: -1261.27, Optimal reward -1034.86
Iteration 125 took 2.30 seconds (mean sampled reward: -2176.30). Current reward after update: -1049.56, Optimal reward -1034.86
Iteration 126 took 2.21 seconds (mean sampled reward: -2230.42). Current reward after update: -1066.38, Optimal reward -1034.86
Iteration 127 took 2.25 seconds (mean sampled reward: -2081.81). Current reward after update: -1051.97, Optimal reward -1034.86
Iteration 128 took 2.25 seconds (mean sampled reward: -2077.77). Current reward after update: -1406.57, Optimal reward -1034.86
Iteration 129 took 2.18 seconds (mean sampled reward: -2883.79). Current reward after update: -1168.86, Optimal reward -1034.86
Iteration 130 took 2.13 seconds (mean sampled reward: -2564.44). Current reward after update: -1875.09, Optimal reward -1034.86
Iteration 131 took 2.20 seconds (mean sampled reward: -2416.00). Current reward after update: -1042.23, Optimal reward -1034.86
Iteration 132 took 2.21 seconds (mean sampled reward: -2596.58). Current reward after update: -1043.14, Optimal reward -1034.86
Iteration 133 took 2.21 seconds (mean sampled reward: -2466.69). Current reward after update: -1202.83, Optimal reward -1034.86
Iteration 134 took 2.16 seconds (mean sampled reward: -2026.09). Current reward after update: -1166.75, Optimal reward -1034.86
Iteration 135 took 2.17 seconds (mean sampled reward: -2688.73). Current reward after update: -903.01, Optimal reward -903.01
Iteration 136 took 2.17 seconds (mean sampled reward: -4250.70). Current reward after update: -1115.58, Optimal reward -903.01
Iteration 137 took 2.19 seconds (mean sampled reward: -2276.95). Current reward after update: -1418.06, Optimal reward -903.01
Iteration 138 took 2.17 seconds (mean sampled reward: -1993.98). Current reward after update: -947.14, Optimal reward -903.01
Iteration 139 took 2.12 seconds (mean sampled reward: -4187.73). Current reward after update: -903.75, Optimal reward -903.01
Iteration 140 took 2.16 seconds (mean sampled reward: -4259.50). Current reward after update: -826.19, Optimal reward -826.19
Iteration 141 took 2.23 seconds (mean sampled reward: -2879.34). Current reward after update: -1025.42, Optimal reward -826.19
Iteration 142 took 2.15 seconds (mean sampled reward: -3147.16). Current reward after update: -930.95, Optimal reward -826.19
Iteration 143 took 2.36 seconds (mean sampled reward: -2016.63). Current reward after update: -977.16, Optimal reward -826.19
Iteration 144 took 2.25 seconds (mean sampled reward: -2785.06). Current reward after update: -1398.06, Optimal reward -826.19
Iteration 145 took 2.16 seconds (mean sampled reward: -4212.74). Current reward after update: -862.72, Optimal reward -826.19
Iteration 146 took 2.19 seconds (mean sampled reward: -2625.63). Current reward after update: -860.95, Optimal reward -826.19
Iteration 147 took 2.20 seconds (mean sampled reward: -1863.77). Current reward after update: -904.81, Optimal reward -826.19
Iteration 148 took 2.19 seconds (mean sampled reward: -2395.00). Current reward after update: -1432.58, Optimal reward -826.19
Iteration 149 took 2.17 seconds (mean sampled reward: -2424.77). Current reward after update: -881.72, Optimal reward -826.19
Iteration 150 took 2.17 seconds (mean sampled reward: -3488.57). Current reward after update: -851.53, Optimal reward -826.19
Iteration 151 took 2.18 seconds (mean sampled reward: -3328.44). Current reward after update: -1658.18, Optimal reward -826.19
Iteration 152 took 2.17 seconds (mean sampled reward: -5051.66). Current reward after update: -812.01, Optimal reward -812.01
Iteration 153 took 2.18 seconds (mean sampled reward: -4273.97). Current reward after update: -988.48, Optimal reward -812.01
Iteration 154 took 2.19 seconds (mean sampled reward: -4510.01). Current reward after update: -1244.73, Optimal reward -812.01
Iteration 155 took 2.21 seconds (mean sampled reward: -3607.70). Current reward after update: -1053.61, Optimal reward -812.01
Iteration 156 took 2.18 seconds (mean sampled reward: -4543.45). Current reward after update: -1302.26, Optimal reward -812.01
Iteration 157 took 2.18 seconds (mean sampled reward: -5449.04). Current reward after update: -802.54, Optimal reward -802.54
Iteration 158 took 2.22 seconds (mean sampled reward: -3378.30). Current reward after update: -765.67, Optimal reward -765.67
Iteration 159 took 2.21 seconds (mean sampled reward: -1903.13). Current reward after update: -1882.84, Optimal reward -765.67
Iteration 160 took 2.25 seconds (mean sampled reward: -1830.30). Current reward after update: -1037.38, Optimal reward -765.67
Iteration 161 took 2.24 seconds (mean sampled reward: -1740.54). Current reward after update: -859.26, Optimal reward -765.67
Iteration 162 took 2.30 seconds (mean sampled reward: -2229.91). Current reward after update: -966.82, Optimal reward -765.67
Iteration 163 took 2.28 seconds (mean sampled reward: -2120.70). Current reward after update: -905.15, Optimal reward -765.67
Iteration 164 took 2.20 seconds (mean sampled reward: -2076.54). Current reward after update: -1260.79, Optimal reward -765.67
Iteration 165 took 2.21 seconds (mean sampled reward: -2221.55). Current reward after update: -1325.87, Optimal reward -765.67
Iteration 166 took 2.24 seconds (mean sampled reward: -3354.29). Current reward after update: -903.94, Optimal reward -765.67
Iteration 167 took 2.22 seconds (mean sampled reward: -3457.72). Current reward after update: -1656.20, Optimal reward -765.67
Iteration 168 took 2.20 seconds (mean sampled reward: -2164.03). Current reward after update: -792.03, Optimal reward -765.67
Iteration 169 took 2.14 seconds (mean sampled reward: -3863.72). Current reward after update: -823.69, Optimal reward -765.67
Iteration 170 took 2.18 seconds (mean sampled reward: -3987.43). Current reward after update: -810.44, Optimal reward -765.67
Iteration 171 took 2.19 seconds (mean sampled reward: -4548.44). Current reward after update: -810.10, Optimal reward -765.67
Iteration 172 took 2.22 seconds (mean sampled reward: -4722.38). Current reward after update: -1662.63, Optimal reward -765.67
Iteration 173 took 2.24 seconds (mean sampled reward: -3565.36). Current reward after update: -1073.37, Optimal reward -765.67
Iteration 174 took 2.19 seconds (mean sampled reward: -3700.93). Current reward after update: -1263.08, Optimal reward -765.67
Iteration 175 took 2.28 seconds (mean sampled reward: -3534.53). Current reward after update: -1222.08, Optimal reward -765.67
Iteration 176 took 2.23 seconds (mean sampled reward: -3675.61). Current reward after update: -884.02, Optimal reward -765.67
Iteration 177 took 2.19 seconds (mean sampled reward: -3003.98). Current reward after update: -943.16, Optimal reward -765.67
Iteration 178 took 2.19 seconds (mean sampled reward: -4869.43). Current reward after update: -926.07, Optimal reward -765.67
Iteration 179 took 2.11 seconds (mean sampled reward: -4445.51). Current reward after update: -1084.33, Optimal reward -765.67
Iteration 180 took 2.04 seconds (mean sampled reward: -4745.30). Current reward after update: -1090.81, Optimal reward -765.67
Iteration 181 took 2.20 seconds (mean sampled reward: -3437.48). Current reward after update: -1828.40, Optimal reward -765.67
Iteration 182 took 2.14 seconds (mean sampled reward: -3971.66). Current reward after update: -1565.95, Optimal reward -765.67
Iteration 183 took 2.15 seconds (mean sampled reward: -4719.80). Current reward after update: -950.51, Optimal reward -765.67
Iteration 184 took 2.13 seconds (mean sampled reward: -4002.00). Current reward after update: -954.81, Optimal reward -765.67
Iteration 185 took 2.24 seconds (mean sampled reward: -2880.74). Current reward after update: -1360.35, Optimal reward -765.67
Iteration 186 took 2.22 seconds (mean sampled reward: -2327.93). Current reward after update: -873.73, Optimal reward -765.67
Iteration 187 took 2.18 seconds (mean sampled reward: -3205.58). Current reward after update: -902.28, Optimal reward -765.67
Iteration 188 took 2.14 seconds (mean sampled reward: -2461.46). Current reward after update: -1068.16, Optimal reward -765.67
Iteration 189 took 2.12 seconds (mean sampled reward: -2837.79). Current reward after update: -1321.28, Optimal reward -765.67
Iteration 190 took 2.24 seconds (mean sampled reward: -2137.32). Current reward after update: -964.94, Optimal reward -765.67
Iteration 191 took 2.28 seconds (mean sampled reward: -2127.57). Current reward after update: -936.42, Optimal reward -765.67
Iteration 192 took 2.08 seconds (mean sampled reward: -3269.21). Current reward after update: -927.81, Optimal reward -765.67
Iteration 193 took 2.15 seconds (mean sampled reward: -3256.98). Current reward after update: -893.90, Optimal reward -765.67
Iteration 194 took 2.20 seconds (mean sampled reward: -3405.25). Current reward after update: -927.11, Optimal reward -765.67
Iteration 195 took 2.26 seconds (mean sampled reward: -2369.09). Current reward after update: -1161.38, Optimal reward -765.67
Iteration 196 took 2.21 seconds (mean sampled reward: -3843.42). Current reward after update: -848.81, Optimal reward -765.67
Iteration 197 took 2.28 seconds (mean sampled reward: -3289.96). Current reward after update: -873.11, Optimal reward -765.67
Iteration 198 took 2.28 seconds (mean sampled reward: -2957.58). Current reward after update: -837.81, Optimal reward -765.67
Iteration 199 took 2.28 seconds (mean sampled reward: -2078.16). Current reward after update: -1080.00, Optimal reward -765.67
Iteration 200 took 2.28 seconds (mean sampled reward: -3564.07). Current reward after update: -1673.74, Optimal reward -765.67
Iteration 1 took 2.23 seconds (mean sampled reward: -7500.20). Current reward after update: -7221.30, Optimal reward -7221.30
Iteration 2 took 2.12 seconds (mean sampled reward: -7491.29). Current reward after update: -6235.43, Optimal reward -6235.43
Iteration 3 took 2.22 seconds (mean sampled reward: -7246.55). Current reward after update: -5636.81, Optimal reward -5636.81
Iteration 4 took 2.24 seconds (mean sampled reward: -6952.18). Current reward after update: -4106.13, Optimal reward -4106.13
Iteration 5 took 2.29 seconds (mean sampled reward: -6102.23). Current reward after update: -3854.57, Optimal reward -3854.57
Iteration 6 took 2.31 seconds (mean sampled reward: -6153.60). Current reward after update: -3369.96, Optimal reward -3369.96
Iteration 7 took 2.17 seconds (mean sampled reward: -6473.81). Current reward after update: -2937.71, Optimal reward -2937.71
Iteration 8 took 2.14 seconds (mean sampled reward: -6484.24). Current reward after update: -2592.44, Optimal reward -2592.44
Iteration 9 took 2.09 seconds (mean sampled reward: -6665.69). Current reward after update: -2541.91, Optimal reward -2541.91
Iteration 10 took 2.09 seconds (mean sampled reward: -6717.73). Current reward after update: -2010.12, Optimal reward -2010.12
Iteration 11 took 2.04 seconds (mean sampled reward: -6980.86). Current reward after update: -1519.48, Optimal reward -1519.48
Iteration 12 took 2.04 seconds (mean sampled reward: -6706.98). Current reward after update: -1805.80, Optimal reward -1519.48
Iteration 13 took 1.98 seconds (mean sampled reward: -6398.01). Current reward after update: -6375.45, Optimal reward -1519.48
Iteration 14 took 1.97 seconds (mean sampled reward: -5815.53). Current reward after update: -1746.37, Optimal reward -1519.48
Iteration 15 took 2.14 seconds (mean sampled reward: -5800.27). Current reward after update: -2047.46, Optimal reward -1519.48
Iteration 16 took 2.18 seconds (mean sampled reward: -6274.09). Current reward after update: -1739.59, Optimal reward -1519.48
Iteration 17 took 2.24 seconds (mean sampled reward: -6024.70). Current reward after update: -2059.58, Optimal reward -1519.48
Iteration 18 took 2.29 seconds (mean sampled reward: -6024.24). Current reward after update: -1758.37, Optimal reward -1519.48
Iteration 19 took 2.29 seconds (mean sampled reward: -5323.07). Current reward after update: -1726.42, Optimal reward -1519.48
Iteration 20 took 2.27 seconds (mean sampled reward: -4878.74). Current reward after update: -1511.76, Optimal reward -1511.76
Iteration 21 took 2.43 seconds (mean sampled reward: -5100.34). Current reward after update: -1747.97, Optimal reward -1511.76
Iteration 22 took 2.22 seconds (mean sampled reward: -4562.25). Current reward after update: -1476.18, Optimal reward -1476.18
Iteration 23 took 2.20 seconds (mean sampled reward: -4460.97). Current reward after update: -1682.34, Optimal reward -1476.18
Iteration 24 took 2.29 seconds (mean sampled reward: -4907.35). Current reward after update: -1575.39, Optimal reward -1476.18
Iteration 25 took 2.23 seconds (mean sampled reward: -4868.85). Current reward after update: -1798.39, Optimal reward -1476.18
Iteration 26 took 2.18 seconds (mean sampled reward: -4452.15). Current reward after update: -1550.84, Optimal reward -1476.18
Iteration 27 took 2.20 seconds (mean sampled reward: -5043.04). Current reward after update: -1644.34, Optimal reward -1476.18
Iteration 28 took 2.30 seconds (mean sampled reward: -4801.31). Current reward after update: -1506.07, Optimal reward -1476.18
Iteration 29 took 2.17 seconds (mean sampled reward: -4574.21). Current reward after update: -1773.66, Optimal reward -1476.18
Iteration 30 took 2.18 seconds (mean sampled reward: -4917.50). Current reward after update: -1576.41, Optimal reward -1476.18
Iteration 31 took 2.29 seconds (mean sampled reward: -5938.54). Current reward after update: -1255.86, Optimal reward -1255.86
Iteration 32 took 2.16 seconds (mean sampled reward: -5563.19). Current reward after update: -1412.94, Optimal reward -1255.86
Iteration 33 took 2.19 seconds (mean sampled reward: -5613.97). Current reward after update: -1859.49, Optimal reward -1255.86
Iteration 34 took 2.38 seconds (mean sampled reward: -5628.47). Current reward after update: -1962.23, Optimal reward -1255.86
Iteration 35 took 2.22 seconds (mean sampled reward: -6238.73). Current reward after update: -1160.65, Optimal reward -1160.65
Iteration 36 took 2.14 seconds (mean sampled reward: -6236.52). Current reward after update: -5324.94, Optimal reward -1160.65
Iteration 37 took 2.21 seconds (mean sampled reward: -6061.77). Current reward after update: -1393.56, Optimal reward -1160.65
Iteration 38 took 2.15 seconds (mean sampled reward: -5006.08). Current reward after update: -1656.19, Optimal reward -1160.65
Iteration 39 took 2.18 seconds (mean sampled reward: -5254.08). Current reward after update: -1352.33, Optimal reward -1160.65
Iteration 40 took 2.16 seconds (mean sampled reward: -4309.91). Current reward after update: -1542.47, Optimal reward -1160.65
Iteration 41 took 2.16 seconds (mean sampled reward: -4267.47). Current reward after update: -1476.06, Optimal reward -1160.65
Iteration 42 took 2.10 seconds (mean sampled reward: -5015.41). Current reward after update: -1365.12, Optimal reward -1160.65
Iteration 43 took 2.13 seconds (mean sampled reward: -4397.23). Current reward after update: -1271.42, Optimal reward -1160.65
Iteration 44 took 2.12 seconds (mean sampled reward: -4062.53). Current reward after update: -1177.70, Optimal reward -1160.65
Iteration 45 took 2.15 seconds (mean sampled reward: -4021.01). Current reward after update: -1269.93, Optimal reward -1160.65
Iteration 46 took 2.08 seconds (mean sampled reward: -5055.10). Current reward after update: -1250.61, Optimal reward -1160.65
Iteration 47 took 2.13 seconds (mean sampled reward: -5334.99). Current reward after update: -1479.04, Optimal reward -1160.65
Iteration 48 took 2.21 seconds (mean sampled reward: -5013.41). Current reward after update: -1435.13, Optimal reward -1160.65
Iteration 49 took 2.13 seconds (mean sampled reward: -5831.13). Current reward after update: -1319.10, Optimal reward -1160.65
Iteration 50 took 2.20 seconds (mean sampled reward: -4071.47). Current reward after update: -1459.50, Optimal reward -1160.65
Iteration 51 took 2.22 seconds (mean sampled reward: -4146.25). Current reward after update: -1274.82, Optimal reward -1160.65
Iteration 52 took 2.17 seconds (mean sampled reward: -4275.25). Current reward after update: -1328.27, Optimal reward -1160.65
Iteration 53 took 2.26 seconds (mean sampled reward: -4724.70). Current reward after update: -1356.22, Optimal reward -1160.65
Iteration 54 took 2.25 seconds (mean sampled reward: -5189.86). Current reward after update: -1362.02, Optimal reward -1160.65
Iteration 55 took 2.23 seconds (mean sampled reward: -5155.66). Current reward after update: -1785.48, Optimal reward -1160.65
Iteration 56 took 2.03 seconds (mean sampled reward: -5657.08). Current reward after update: -1502.26, Optimal reward -1160.65
Iteration 57 took 2.17 seconds (mean sampled reward: -3593.90). Current reward after update: -1338.69, Optimal reward -1160.65
Iteration 58 took 2.11 seconds (mean sampled reward: -3983.31). Current reward after update: -5295.60, Optimal reward -1160.65
Iteration 59 took 2.11 seconds (mean sampled reward: -4901.81). Current reward after update: -1286.57, Optimal reward -1160.65
Iteration 60 took 2.04 seconds (mean sampled reward: -6225.15). Current reward after update: -1352.74, Optimal reward -1160.65
Iteration 61 took 2.06 seconds (mean sampled reward: -6715.71). Current reward after update: -1321.96, Optimal reward -1160.65
Iteration 62 took 2.12 seconds (mean sampled reward: -7259.80). Current reward after update: -1525.17, Optimal reward -1160.65
Iteration 63 took 2.10 seconds (mean sampled reward: -6921.05). Current reward after update: -2073.86, Optimal reward -1160.65
Iteration 64 took 2.14 seconds (mean sampled reward: -7101.53). Current reward after update: -1567.67, Optimal reward -1160.65
Iteration 65 took 2.25 seconds (mean sampled reward: -6397.67). Current reward after update: -1444.20, Optimal reward -1160.65
Iteration 66 took 2.09 seconds (mean sampled reward: -6350.19). Current reward after update: -1298.12, Optimal reward -1160.65
Iteration 67 took 2.12 seconds (mean sampled reward: -6266.85). Current reward after update: -1493.28, Optimal reward -1160.65
Iteration 68 took 2.03 seconds (mean sampled reward: -6550.01). Current reward after update: -1604.19, Optimal reward -1160.65
Iteration 69 took 2.08 seconds (mean sampled reward: -5671.74). Current reward after update: -1546.53, Optimal reward -1160.65
Iteration 70 took 2.11 seconds (mean sampled reward: -5911.60). Current reward after update: -1275.53, Optimal reward -1160.65
Iteration 71 took 2.18 seconds (mean sampled reward: -6525.08). Current reward after update: -1454.93, Optimal reward -1160.65
Iteration 72 took 2.10 seconds (mean sampled reward: -6384.35). Current reward after update: -1157.82, Optimal reward -1157.82
Iteration 73 took 2.10 seconds (mean sampled reward: -6131.80). Current reward after update: -1304.60, Optimal reward -1157.82
Iteration 74 took 2.02 seconds (mean sampled reward: -6790.18). Current reward after update: -1160.12, Optimal reward -1157.82
Iteration 75 took 2.02 seconds (mean sampled reward: -6775.75). Current reward after update: -1432.97, Optimal reward -1157.82
Iteration 76 took 1.96 seconds (mean sampled reward: -6506.87). Current reward after update: -1315.35, Optimal reward -1157.82
Iteration 77 took 2.03 seconds (mean sampled reward: -5925.44). Current reward after update: -1158.23, Optimal reward -1157.82
Iteration 78 took 2.01 seconds (mean sampled reward: -6048.87). Current reward after update: -1220.88, Optimal reward -1157.82
Iteration 79 took 2.02 seconds (mean sampled reward: -6057.20). Current reward after update: -7268.69, Optimal reward -1157.82
Iteration 80 took 2.02 seconds (mean sampled reward: -5816.37). Current reward after update: -1292.83, Optimal reward -1157.82
Iteration 81 took 2.03 seconds (mean sampled reward: -5909.23). Current reward after update: -1281.89, Optimal reward -1157.82
Iteration 82 took 2.01 seconds (mean sampled reward: -6745.41). Current reward after update: -1361.99, Optimal reward -1157.82
Iteration 83 took 1.98 seconds (mean sampled reward: -6767.97). Current reward after update: -7388.66, Optimal reward -1157.82
Iteration 84 took 2.03 seconds (mean sampled reward: -6793.27). Current reward after update: -1387.19, Optimal reward -1157.82
Iteration 85 took 1.99 seconds (mean sampled reward: -6546.19). Current reward after update: -1188.19, Optimal reward -1157.82
Iteration 86 took 2.01 seconds (mean sampled reward: -6505.51). Current reward after update: -1472.54, Optimal reward -1157.82
Iteration 87 took 2.01 seconds (mean sampled reward: -5747.56). Current reward after update: -1180.31, Optimal reward -1157.82
Iteration 88 took 2.00 seconds (mean sampled reward: -6245.55). Current reward after update: -1354.52, Optimal reward -1157.82
Iteration 89 took 2.04 seconds (mean sampled reward: -5821.39). Current reward after update: -1286.39, Optimal reward -1157.82
Iteration 90 took 2.01 seconds (mean sampled reward: -5397.81). Current reward after update: -1125.25, Optimal reward -1125.25
Iteration 91 took 2.00 seconds (mean sampled reward: -6028.69). Current reward after update: -1261.03, Optimal reward -1125.25
Iteration 92 took 2.02 seconds (mean sampled reward: -5642.41). Current reward after update: -1410.66, Optimal reward -1125.25
Iteration 93 took 2.05 seconds (mean sampled reward: -5849.68). Current reward after update: -1298.47, Optimal reward -1125.25
Iteration 94 took 2.03 seconds (mean sampled reward: -5478.51). Current reward after update: -1192.73, Optimal reward -1125.25
Iteration 95 took 2.09 seconds (mean sampled reward: -5396.93). Current reward after update: -1116.50, Optimal reward -1116.50
Iteration 96 took 2.15 seconds (mean sampled reward: -4836.93). Current reward after update: -1308.66, Optimal reward -1116.50
Iteration 97 took 2.09 seconds (mean sampled reward: -5052.11). Current reward after update: -1379.25, Optimal reward -1116.50
Iteration 98 took 2.10 seconds (mean sampled reward: -5268.61). Current reward after update: -1429.24, Optimal reward -1116.50
Iteration 99 took 2.03 seconds (mean sampled reward: -5690.75). Current reward after update: -1183.97, Optimal reward -1116.50
Iteration 100 took 2.04 seconds (mean sampled reward: -4833.03). Current reward after update: -1243.40, Optimal reward -1116.50
Iteration 101 took 2.03 seconds (mean sampled reward: -5754.55). Current reward after update: -1023.44, Optimal reward -1023.44
Iteration 102 took 2.09 seconds (mean sampled reward: -5673.69). Current reward after update: -1317.85, Optimal reward -1023.44
Iteration 103 took 2.16 seconds (mean sampled reward: -6091.01). Current reward after update: -1373.21, Optimal reward -1023.44
Iteration 104 took 2.06 seconds (mean sampled reward: -5627.14). Current reward after update: -1324.19, Optimal reward -1023.44
Iteration 105 took 2.09 seconds (mean sampled reward: -5245.74). Current reward after update: -2972.71, Optimal reward -1023.44
Iteration 106 took 2.15 seconds (mean sampled reward: -4316.34). Current reward after update: -1379.57, Optimal reward -1023.44
Iteration 107 took 2.21 seconds (mean sampled reward: -4502.58). Current reward after update: -1284.48, Optimal reward -1023.44
Iteration 108 took 2.12 seconds (mean sampled reward: -5161.89). Current reward after update: -7042.38, Optimal reward -1023.44
Iteration 109 took 2.22 seconds (mean sampled reward: -4970.29). Current reward after update: -1275.36, Optimal reward -1023.44
Iteration 110 took 2.10 seconds (mean sampled reward: -3672.95). Current reward after update: -1155.92, Optimal reward -1023.44
Iteration 111 took 2.06 seconds (mean sampled reward: -4119.79). Current reward after update: -1235.46, Optimal reward -1023.44
Iteration 112 took 2.08 seconds (mean sampled reward: -4420.93). Current reward after update: -1160.17, Optimal reward -1023.44
Iteration 113 took 2.18 seconds (mean sampled reward: -5640.73). Current reward after update: -1187.76, Optimal reward -1023.44
Iteration 114 took 2.09 seconds (mean sampled reward: -5757.10). Current reward after update: -1441.00, Optimal reward -1023.44
Iteration 115 took 2.06 seconds (mean sampled reward: -5365.97). Current reward after update: -1442.57, Optimal reward -1023.44
Iteration 116 took 2.11 seconds (mean sampled reward: -5378.66). Current reward after update: -1836.28, Optimal reward -1023.44
Iteration 117 took 2.18 seconds (mean sampled reward: -5473.61). Current reward after update: -1270.75, Optimal reward -1023.44
Iteration 118 took 2.00 seconds (mean sampled reward: -5494.45). Current reward after update: -1733.96, Optimal reward -1023.44
Iteration 119 took 2.02 seconds (mean sampled reward: -5278.95). Current reward after update: -1234.88, Optimal reward -1023.44
Iteration 120 took 2.08 seconds (mean sampled reward: -5615.25). Current reward after update: -1534.01, Optimal reward -1023.44
Iteration 121 took 2.00 seconds (mean sampled reward: -5508.43). Current reward after update: -1864.13, Optimal reward -1023.44
Iteration 122 took 2.06 seconds (mean sampled reward: -5317.30). Current reward after update: -1603.87, Optimal reward -1023.44
Iteration 123 took 2.04 seconds (mean sampled reward: -5504.68). Current reward after update: -1480.48, Optimal reward -1023.44
Iteration 124 took 2.15 seconds (mean sampled reward: -5136.19). Current reward after update: -1772.45, Optimal reward -1023.44
Iteration 125 took 2.11 seconds (mean sampled reward: -4933.11). Current reward after update: -1515.65, Optimal reward -1023.44
Iteration 126 took 2.05 seconds (mean sampled reward: -5404.98). Current reward after update: -1506.24, Optimal reward -1023.44
Iteration 127 took 2.06 seconds (mean sampled reward: -5514.03). Current reward after update: -2212.33, Optimal reward -1023.44
Iteration 128 took 2.13 seconds (mean sampled reward: -5606.67). Current reward after update: -6859.49, Optimal reward -1023.44
Iteration 129 took 2.10 seconds (mean sampled reward: -5553.42). Current reward after update: -1969.24, Optimal reward -1023.44
Iteration 130 took 2.12 seconds (mean sampled reward: -5277.24). Current reward after update: -1781.31, Optimal reward -1023.44
Iteration 131 took 2.13 seconds (mean sampled reward: -5351.35). Current reward after update: -2717.33, Optimal reward -1023.44
Iteration 132 took 2.09 seconds (mean sampled reward: -5102.05). Current reward after update: -1896.51, Optimal reward -1023.44
Iteration 133 took 2.13 seconds (mean sampled reward: -5569.86). Current reward after update: -1506.18, Optimal reward -1023.44
Iteration 134 took 2.11 seconds (mean sampled reward: -5536.97). Current reward after update: -1737.60, Optimal reward -1023.44
Iteration 135 took 2.09 seconds (mean sampled reward: -5684.71). Current reward after update: -1969.69, Optimal reward -1023.44
Iteration 136 took 2.09 seconds (mean sampled reward: -4819.37). Current reward after update: -1676.07, Optimal reward -1023.44
Iteration 137 took 2.14 seconds (mean sampled reward: -4885.67). Current reward after update: -1558.86, Optimal reward -1023.44
Iteration 138 took 2.10 seconds (mean sampled reward: -4546.97). Current reward after update: -1476.06, Optimal reward -1023.44
Iteration 139 took 2.08 seconds (mean sampled reward: -4992.31). Current reward after update: -1457.42, Optimal reward -1023.44
Iteration 140 took 2.22 seconds (mean sampled reward: -4103.48). Current reward after update: -1648.68, Optimal reward -1023.44
Iteration 141 took 2.10 seconds (mean sampled reward: -4365.99). Current reward after update: -1414.65, Optimal reward -1023.44
Iteration 142 took 2.10 seconds (mean sampled reward: -4750.15). Current reward after update: -1389.97, Optimal reward -1023.44
Iteration 143 took 2.07 seconds (mean sampled reward: -4441.82). Current reward after update: -1343.29, Optimal reward -1023.44
Iteration 144 took 2.10 seconds (mean sampled reward: -5276.63). Current reward after update: -1501.69, Optimal reward -1023.44
Iteration 145 took 2.15 seconds (mean sampled reward: -4454.32). Current reward after update: -1296.47, Optimal reward -1023.44
Iteration 146 took 2.06 seconds (mean sampled reward: -4055.85). Current reward after update: -1466.17, Optimal reward -1023.44
Iteration 147 took 2.16 seconds (mean sampled reward: -3846.96). Current reward after update: -1282.24, Optimal reward -1023.44
Iteration 148 took 2.09 seconds (mean sampled reward: -4369.93). Current reward after update: -1277.12, Optimal reward -1023.44
Iteration 149 took 2.10 seconds (mean sampled reward: -4352.28). Current reward after update: -1244.49, Optimal reward -1023.44
Iteration 150 took 2.15 seconds (mean sampled reward: -4328.46). Current reward after update: -1315.59, Optimal reward -1023.44
Iteration 151 took 2.12 seconds (mean sampled reward: -4441.18). Current reward after update: -1210.94, Optimal reward -1023.44
Iteration 152 took 2.16 seconds (mean sampled reward: -4158.58). Current reward after update: -1535.73, Optimal reward -1023.44
Iteration 153 took 2.10 seconds (mean sampled reward: -4608.02). Current reward after update: -3548.85, Optimal reward -1023.44
Iteration 154 took 2.10 seconds (mean sampled reward: -4682.07). Current reward after update: -1706.40, Optimal reward -1023.44
Iteration 155 took 2.14 seconds (mean sampled reward: -4852.95). Current reward after update: -1568.33, Optimal reward -1023.44
Iteration 156 took 2.14 seconds (mean sampled reward: -4793.47). Current reward after update: -1547.37, Optimal reward -1023.44
Iteration 157 took 2.09 seconds (mean sampled reward: -5501.78). Current reward after update: -6953.85, Optimal reward -1023.44
Iteration 158 took 2.05 seconds (mean sampled reward: -5659.51). Current reward after update: -1280.15, Optimal reward -1023.44
Iteration 159 took 2.07 seconds (mean sampled reward: -5054.35). Current reward after update: -1672.89, Optimal reward -1023.44
Iteration 160 took 2.20 seconds (mean sampled reward: -4423.26). Current reward after update: -1660.22, Optimal reward -1023.44
Iteration 161 took 2.10 seconds (mean sampled reward: -4050.07). Current reward after update: -1525.56, Optimal reward -1023.44
Iteration 162 took 2.21 seconds (mean sampled reward: -4252.52). Current reward after update: -1367.40, Optimal reward -1023.44
Iteration 163 took 2.10 seconds (mean sampled reward: -4323.11). Current reward after update: -1488.90, Optimal reward -1023.44
Iteration 164 took 2.12 seconds (mean sampled reward: -4356.69). Current reward after update: -1366.51, Optimal reward -1023.44
Iteration 165 took 2.13 seconds (mean sampled reward: -4444.03). Current reward after update: -1266.71, Optimal reward -1023.44
Iteration 166 took 2.19 seconds (mean sampled reward: -4777.82). Current reward after update: -1540.87, Optimal reward -1023.44
Iteration 167 took 2.16 seconds (mean sampled reward: -4767.83). Current reward after update: -1348.22, Optimal reward -1023.44
Iteration 168 took 2.18 seconds (mean sampled reward: -4327.60). Current reward after update: -1227.26, Optimal reward -1023.44
Iteration 169 took 2.11 seconds (mean sampled reward: -4256.89). Current reward after update: -1227.49, Optimal reward -1023.44
Iteration 170 took 2.10 seconds (mean sampled reward: -3847.36). Current reward after update: -1227.83, Optimal reward -1023.44
Iteration 171 took 2.18 seconds (mean sampled reward: -4320.52). Current reward after update: -1215.25, Optimal reward -1023.44
Iteration 172 took 2.12 seconds (mean sampled reward: -4145.59). Current reward after update: -5612.02, Optimal reward -1023.44
Iteration 173 took 2.14 seconds (mean sampled reward: -3884.76). Current reward after update: -5379.94, Optimal reward -1023.44
Iteration 174 took 2.09 seconds (mean sampled reward: -3705.48). Current reward after update: -5601.10, Optimal reward -1023.44
Iteration 175 took 2.11 seconds (mean sampled reward: -3679.38). Current reward after update: -1431.45, Optimal reward -1023.44
Iteration 176 took 2.10 seconds (mean sampled reward: -4080.16). Current reward after update: -1314.45, Optimal reward -1023.44
Iteration 177 took 2.12 seconds (mean sampled reward: -4020.37). Current reward after update: -1416.42, Optimal reward -1023.44
Iteration 178 took 2.14 seconds (mean sampled reward: -3634.03). Current reward after update: -1378.65, Optimal reward -1023.44
Iteration 179 took 2.11 seconds (mean sampled reward: -3623.09). Current reward after update: -1307.40, Optimal reward -1023.44
Iteration 180 took 2.09 seconds (mean sampled reward: -3521.89). Current reward after update: -1368.70, Optimal reward -1023.44
Iteration 181 took 2.10 seconds (mean sampled reward: -4134.43). Current reward after update: -1394.15, Optimal reward -1023.44
Iteration 182 took 2.10 seconds (mean sampled reward: -3323.48). Current reward after update: -1709.02, Optimal reward -1023.44
Iteration 183 took 2.13 seconds (mean sampled reward: -3382.02). Current reward after update: -1311.70, Optimal reward -1023.44
Iteration 184 took 2.12 seconds (mean sampled reward: -4378.10). Current reward after update: -1864.91, Optimal reward -1023.44
Iteration 185 took 2.11 seconds (mean sampled reward: -3841.77). Current reward after update: -1603.66, Optimal reward -1023.44
Iteration 186 took 2.09 seconds (mean sampled reward: -3762.26). Current reward after update: -1660.14, Optimal reward -1023.44
Iteration 187 took 2.06 seconds (mean sampled reward: -3586.27). Current reward after update: -1332.41, Optimal reward -1023.44
Iteration 188 took 2.15 seconds (mean sampled reward: -4469.71). Current reward after update: -1319.81, Optimal reward -1023.44
Iteration 189 took 2.07 seconds (mean sampled reward: -4930.53). Current reward after update: -1276.46, Optimal reward -1023.44
Iteration 190 took 2.10 seconds (mean sampled reward: -4444.03). Current reward after update: -1406.07, Optimal reward -1023.44
Iteration 191 took 2.10 seconds (mean sampled reward: -4282.00). Current reward after update: -3827.02, Optimal reward -1023.44
Iteration 192 took 2.09 seconds (mean sampled reward: -4013.77). Current reward after update: -1287.41, Optimal reward -1023.44
Iteration 193 took 2.09 seconds (mean sampled reward: -4561.98). Current reward after update: -1411.50, Optimal reward -1023.44
Iteration 194 took 2.07 seconds (mean sampled reward: -5352.15). Current reward after update: -1307.32, Optimal reward -1023.44
Iteration 195 took 2.06 seconds (mean sampled reward: -4390.54). Current reward after update: -1357.07, Optimal reward -1023.44
Iteration 196 took 2.11 seconds (mean sampled reward: -4855.75). Current reward after update: -1315.11, Optimal reward -1023.44
Iteration 197 took 2.09 seconds (mean sampled reward: -4500.24). Current reward after update: -1464.49, Optimal reward -1023.44
Iteration 198 took 2.24 seconds (mean sampled reward: -4758.72). Current reward after update: -1386.58, Optimal reward -1023.44
Iteration 199 took 2.23 seconds (mean sampled reward: -4386.00). Current reward after update: -1278.46, Optimal reward -1023.44
Iteration 200 took 2.16 seconds (mean sampled reward: -4849.25). Current reward after update: -1428.28, Optimal reward -1023.44
Max force: 20 Sigma: 0.2 mean rewards: -760.0068631024119, best rewards:-490.90428492008704

Iteration 1 took 2.22 seconds (mean sampled reward: -7508.15). Current reward after update: -6218.92, Optimal reward -6218.92
Iteration 2 took 2.38 seconds (mean sampled reward: -7360.87). Current reward after update: -5676.23, Optimal reward -5676.23
Iteration 3 took 2.33 seconds (mean sampled reward: -7181.34). Current reward after update: -5280.51, Optimal reward -5280.51
Iteration 4 took 2.30 seconds (mean sampled reward: -6592.78). Current reward after update: -4649.41, Optimal reward -4649.41
Iteration 5 took 2.38 seconds (mean sampled reward: -6239.96). Current reward after update: -4113.12, Optimal reward -4113.12
Iteration 6 took 2.51 seconds (mean sampled reward: -6592.10). Current reward after update: -4058.76, Optimal reward -4058.76
Iteration 7 took 2.42 seconds (mean sampled reward: -7126.12). Current reward after update: -4101.00, Optimal reward -4058.76
Iteration 8 took 2.45 seconds (mean sampled reward: -7103.19). Current reward after update: -3955.10, Optimal reward -3955.10
Iteration 9 took 2.49 seconds (mean sampled reward: -6906.28). Current reward after update: -3902.95, Optimal reward -3902.95
Iteration 10 took 2.45 seconds (mean sampled reward: -6899.58). Current reward after update: -4016.96, Optimal reward -3902.95
Iteration 11 took 2.36 seconds (mean sampled reward: -7306.79). Current reward after update: -4313.11, Optimal reward -3902.95
Iteration 12 took 2.27 seconds (mean sampled reward: -7132.31). Current reward after update: -4273.94, Optimal reward -3902.95
Iteration 13 took 2.27 seconds (mean sampled reward: -6837.07). Current reward after update: -4036.67, Optimal reward -3902.95
Iteration 14 took 2.64 seconds (mean sampled reward: -7162.81). Current reward after update: -3953.62, Optimal reward -3902.95
Iteration 15 took 2.55 seconds (mean sampled reward: -7361.01). Current reward after update: -3669.86, Optimal reward -3669.86
Iteration 16 took 2.43 seconds (mean sampled reward: -7438.47). Current reward after update: -3693.16, Optimal reward -3669.86
Iteration 17 took 2.36 seconds (mean sampled reward: -7147.74). Current reward after update: -3645.94, Optimal reward -3645.94
Iteration 18 took 2.30 seconds (mean sampled reward: -6679.04). Current reward after update: -5982.15, Optimal reward -3645.94
Iteration 19 took 2.36 seconds (mean sampled reward: -6385.16). Current reward after update: -2989.37, Optimal reward -2989.37
Iteration 20 took 2.40 seconds (mean sampled reward: -6731.67). Current reward after update: -2951.03, Optimal reward -2951.03
Iteration 21 took 2.34 seconds (mean sampled reward: -6842.85). Current reward after update: -3280.66, Optimal reward -2951.03
Iteration 22 took 2.31 seconds (mean sampled reward: -6359.76). Current reward after update: -2850.62, Optimal reward -2850.62
Iteration 23 took 2.28 seconds (mean sampled reward: -5912.25). Current reward after update: -2718.04, Optimal reward -2718.04
Iteration 24 took 2.26 seconds (mean sampled reward: -5826.41). Current reward after update: -2606.24, Optimal reward -2606.24
Iteration 25 took 2.26 seconds (mean sampled reward: -5587.66). Current reward after update: -2680.40, Optimal reward -2606.24
Iteration 26 took 2.29 seconds (mean sampled reward: -6061.04). Current reward after update: -2689.34, Optimal reward -2606.24
Iteration 27 took 2.34 seconds (mean sampled reward: -6628.52). Current reward after update: -2814.86, Optimal reward -2606.24
Iteration 28 took 2.27 seconds (mean sampled reward: -6437.16). Current reward after update: -2297.07, Optimal reward -2297.07
Iteration 29 took 2.52 seconds (mean sampled reward: -5904.45). Current reward after update: -2195.08, Optimal reward -2195.08
Iteration 30 took 2.35 seconds (mean sampled reward: -5743.50). Current reward after update: -2258.62, Optimal reward -2195.08
Iteration 31 took 2.19 seconds (mean sampled reward: -5510.24). Current reward after update: -2150.22, Optimal reward -2150.22
Iteration 32 took 2.19 seconds (mean sampled reward: -4913.54). Current reward after update: -1971.02, Optimal reward -1971.02
Iteration 33 took 2.12 seconds (mean sampled reward: -5375.61). Current reward after update: -2147.32, Optimal reward -1971.02
Iteration 34 took 2.16 seconds (mean sampled reward: -5073.70). Current reward after update: -2128.63, Optimal reward -1971.02
Iteration 35 took 2.18 seconds (mean sampled reward: -5550.32). Current reward after update: -2317.07, Optimal reward -1971.02
Iteration 36 took 2.18 seconds (mean sampled reward: -5412.81). Current reward after update: -2133.17, Optimal reward -1971.02
Iteration 37 took 2.13 seconds (mean sampled reward: -5710.78). Current reward after update: -2339.59, Optimal reward -1971.02
Iteration 38 took 2.22 seconds (mean sampled reward: -5644.08). Current reward after update: -2098.69, Optimal reward -1971.02
Iteration 39 took 2.17 seconds (mean sampled reward: -5186.73). Current reward after update: -2028.72, Optimal reward -1971.02
Iteration 40 took 2.48 seconds (mean sampled reward: -5194.26). Current reward after update: -2040.01, Optimal reward -1971.02
Iteration 41 took 2.37 seconds (mean sampled reward: -6083.16). Current reward after update: -2096.23, Optimal reward -1971.02
Iteration 42 took 2.24 seconds (mean sampled reward: -4848.69). Current reward after update: -1955.07, Optimal reward -1955.07
Iteration 43 took 2.18 seconds (mean sampled reward: -4469.25). Current reward after update: -1813.32, Optimal reward -1813.32
Iteration 44 took 2.21 seconds (mean sampled reward: -5371.93). Current reward after update: -1881.66, Optimal reward -1813.32
Iteration 45 took 2.25 seconds (mean sampled reward: -3766.45). Current reward after update: -1714.89, Optimal reward -1714.89
Iteration 46 took 2.27 seconds (mean sampled reward: -4276.22). Current reward after update: -1727.56, Optimal reward -1714.89
Iteration 47 took 2.16 seconds (mean sampled reward: -5638.46). Current reward after update: -1693.51, Optimal reward -1693.51
Iteration 48 took 2.26 seconds (mean sampled reward: -5353.87). Current reward after update: -1768.46, Optimal reward -1693.51
Iteration 49 took 2.21 seconds (mean sampled reward: -5393.67). Current reward after update: -1755.16, Optimal reward -1693.51
Iteration 50 took 2.22 seconds (mean sampled reward: -4421.30). Current reward after update: -1698.44, Optimal reward -1693.51
Iteration 51 took 2.17 seconds (mean sampled reward: -4368.73). Current reward after update: -1674.07, Optimal reward -1674.07
Iteration 52 took 2.16 seconds (mean sampled reward: -5002.43). Current reward after update: -1620.44, Optimal reward -1620.44
Iteration 53 took 2.26 seconds (mean sampled reward: -5140.42). Current reward after update: -1657.24, Optimal reward -1620.44
Iteration 54 took 2.34 seconds (mean sampled reward: -4134.11). Current reward after update: -1578.99, Optimal reward -1578.99
Iteration 55 took 2.49 seconds (mean sampled reward: -4242.95). Current reward after update: -1837.31, Optimal reward -1578.99
Iteration 56 took 2.26 seconds (mean sampled reward: -4260.47). Current reward after update: -2327.12, Optimal reward -1578.99
Iteration 57 took 2.33 seconds (mean sampled reward: -3974.49). Current reward after update: -1571.22, Optimal reward -1571.22
Iteration 58 took 2.25 seconds (mean sampled reward: -4671.64). Current reward after update: -1615.07, Optimal reward -1571.22
Iteration 59 took 2.34 seconds (mean sampled reward: -5744.94). Current reward after update: -1513.94, Optimal reward -1513.94
Iteration 60 took 2.33 seconds (mean sampled reward: -4618.94). Current reward after update: -2542.46, Optimal reward -1513.94
Iteration 61 took 2.20 seconds (mean sampled reward: -4404.71). Current reward after update: -1879.51, Optimal reward -1513.94
Iteration 62 took 2.17 seconds (mean sampled reward: -3823.44). Current reward after update: -1533.18, Optimal reward -1513.94
Iteration 63 took 2.17 seconds (mean sampled reward: -3350.07). Current reward after update: -1567.33, Optimal reward -1513.94
Iteration 64 took 2.17 seconds (mean sampled reward: -4101.20). Current reward after update: -2660.25, Optimal reward -1513.94
Iteration 65 took 2.23 seconds (mean sampled reward: -3762.57). Current reward after update: -1517.00, Optimal reward -1513.94
Iteration 66 took 2.24 seconds (mean sampled reward: -3661.44). Current reward after update: -1435.73, Optimal reward -1435.73
Iteration 67 took 2.20 seconds (mean sampled reward: -5518.52). Current reward after update: -1545.73, Optimal reward -1435.73
Iteration 68 took 2.17 seconds (mean sampled reward: -5316.75). Current reward after update: -1572.65, Optimal reward -1435.73
Iteration 69 took 2.23 seconds (mean sampled reward: -4779.80). Current reward after update: -1540.52, Optimal reward -1435.73
Iteration 70 took 2.32 seconds (mean sampled reward: -4320.80). Current reward after update: -1630.79, Optimal reward -1435.73
Iteration 71 took 2.20 seconds (mean sampled reward: -4178.25). Current reward after update: -1484.14, Optimal reward -1435.73
Iteration 72 took 2.18 seconds (mean sampled reward: -2938.50). Current reward after update: -1862.92, Optimal reward -1435.73
Iteration 73 took 2.17 seconds (mean sampled reward: -3524.76). Current reward after update: -1590.45, Optimal reward -1435.73
Iteration 74 took 2.20 seconds (mean sampled reward: -3668.40). Current reward after update: -1575.87, Optimal reward -1435.73
Iteration 75 took 2.21 seconds (mean sampled reward: -4011.37). Current reward after update: -1544.91, Optimal reward -1435.73
Iteration 76 took 2.29 seconds (mean sampled reward: -4729.96). Current reward after update: -1680.12, Optimal reward -1435.73
Iteration 77 took 2.38 seconds (mean sampled reward: -4742.55). Current reward after update: -1892.97, Optimal reward -1435.73
Iteration 78 took 2.31 seconds (mean sampled reward: -3628.56). Current reward after update: -1686.71, Optimal reward -1435.73
Iteration 79 took 2.24 seconds (mean sampled reward: -4575.35). Current reward after update: -1622.72, Optimal reward -1435.73
Iteration 80 took 2.33 seconds (mean sampled reward: -5108.90). Current reward after update: -1574.34, Optimal reward -1435.73
Iteration 81 took 2.42 seconds (mean sampled reward: -5011.34). Current reward after update: -1578.88, Optimal reward -1435.73
Iteration 82 took 2.27 seconds (mean sampled reward: -4730.37). Current reward after update: -1615.43, Optimal reward -1435.73
Iteration 83 took 2.24 seconds (mean sampled reward: -4183.38). Current reward after update: -2067.32, Optimal reward -1435.73
Iteration 84 took 2.26 seconds (mean sampled reward: -4823.90). Current reward after update: -1544.12, Optimal reward -1435.73
Iteration 85 took 2.17 seconds (mean sampled reward: -3735.33). Current reward after update: -1517.35, Optimal reward -1435.73
Iteration 86 took 2.23 seconds (mean sampled reward: -3590.17). Current reward after update: -1449.57, Optimal reward -1435.73
Iteration 87 took 2.18 seconds (mean sampled reward: -3487.43). Current reward after update: -1441.71, Optimal reward -1435.73
Iteration 88 took 2.29 seconds (mean sampled reward: -4182.86). Current reward after update: -2342.06, Optimal reward -1435.73
Iteration 89 took 2.33 seconds (mean sampled reward: -4977.80). Current reward after update: -1544.91, Optimal reward -1435.73
Iteration 90 took 2.27 seconds (mean sampled reward: -4229.34). Current reward after update: -2417.48, Optimal reward -1435.73
Iteration 91 took 2.28 seconds (mean sampled reward: -3352.90). Current reward after update: -1521.68, Optimal reward -1435.73
Iteration 92 took 2.25 seconds (mean sampled reward: -3882.55). Current reward after update: -1553.39, Optimal reward -1435.73
Iteration 93 took 2.18 seconds (mean sampled reward: -4129.37). Current reward after update: -1575.86, Optimal reward -1435.73
Iteration 94 took 2.21 seconds (mean sampled reward: -4828.93). Current reward after update: -1572.26, Optimal reward -1435.73
Iteration 95 took 2.25 seconds (mean sampled reward: -5201.47). Current reward after update: -1545.77, Optimal reward -1435.73
Iteration 96 took 2.30 seconds (mean sampled reward: -5418.82). Current reward after update: -1579.94, Optimal reward -1435.73
Iteration 97 took 2.24 seconds (mean sampled reward: -5096.02). Current reward after update: -1964.71, Optimal reward -1435.73
Iteration 98 took 2.26 seconds (mean sampled reward: -5687.52). Current reward after update: -1684.15, Optimal reward -1435.73
Iteration 99 took 2.30 seconds (mean sampled reward: -5431.72). Current reward after update: -1618.91, Optimal reward -1435.73
Iteration 100 took 2.18 seconds (mean sampled reward: -5617.30). Current reward after update: -1640.58, Optimal reward -1435.73
Iteration 101 took 2.29 seconds (mean sampled reward: -4946.67). Current reward after update: -1720.86, Optimal reward -1435.73
Iteration 102 took 2.25 seconds (mean sampled reward: -5630.10). Current reward after update: -1560.95, Optimal reward -1435.73
Iteration 103 took 2.37 seconds (mean sampled reward: -4810.27). Current reward after update: -1498.09, Optimal reward -1435.73
Iteration 104 took 2.26 seconds (mean sampled reward: -5512.54). Current reward after update: -1870.24, Optimal reward -1435.73
Iteration 105 took 2.38 seconds (mean sampled reward: -6390.30). Current reward after update: -1930.41, Optimal reward -1435.73
Iteration 106 took 2.38 seconds (mean sampled reward: -6515.41). Current reward after update: -1506.86, Optimal reward -1435.73
Iteration 107 took 2.43 seconds (mean sampled reward: -4331.49). Current reward after update: -1497.12, Optimal reward -1435.73
Iteration 108 took 2.50 seconds (mean sampled reward: -4178.95). Current reward after update: -1420.00, Optimal reward -1420.00
Iteration 109 took 2.47 seconds (mean sampled reward: -3808.65). Current reward after update: -2121.94, Optimal reward -1420.00
Iteration 110 took 2.31 seconds (mean sampled reward: -3936.84). Current reward after update: -1503.37, Optimal reward -1420.00
Iteration 111 took 2.37 seconds (mean sampled reward: -4225.53). Current reward after update: -1478.55, Optimal reward -1420.00
Iteration 112 took 2.21 seconds (mean sampled reward: -5244.41). Current reward after update: -1510.94, Optimal reward -1420.00
Iteration 113 took 2.30 seconds (mean sampled reward: -4311.10). Current reward after update: -1536.04, Optimal reward -1420.00
Iteration 114 took 2.25 seconds (mean sampled reward: -5008.59). Current reward after update: -1464.05, Optimal reward -1420.00
Iteration 115 took 2.21 seconds (mean sampled reward: -4511.57). Current reward after update: -1475.88, Optimal reward -1420.00
Iteration 116 took 2.17 seconds (mean sampled reward: -4071.97). Current reward after update: -1854.19, Optimal reward -1420.00
Iteration 117 took 2.15 seconds (mean sampled reward: -5129.56). Current reward after update: -1580.52, Optimal reward -1420.00
Iteration 118 took 2.20 seconds (mean sampled reward: -5040.13). Current reward after update: -1652.61, Optimal reward -1420.00
Iteration 119 took 2.18 seconds (mean sampled reward: -4092.34). Current reward after update: -1477.98, Optimal reward -1420.00
Iteration 120 took 2.21 seconds (mean sampled reward: -4658.98). Current reward after update: -1519.48, Optimal reward -1420.00
Iteration 121 took 2.18 seconds (mean sampled reward: -4362.30). Current reward after update: -1723.64, Optimal reward -1420.00
Iteration 122 took 2.18 seconds (mean sampled reward: -4084.84). Current reward after update: -1505.19, Optimal reward -1420.00
Iteration 123 took 2.23 seconds (mean sampled reward: -4318.48). Current reward after update: -1644.21, Optimal reward -1420.00
Iteration 124 took 2.18 seconds (mean sampled reward: -3528.47). Current reward after update: -1523.70, Optimal reward -1420.00
Iteration 125 took 2.20 seconds (mean sampled reward: -3534.21). Current reward after update: -1521.92, Optimal reward -1420.00
Iteration 126 took 2.18 seconds (mean sampled reward: -4874.17). Current reward after update: -1577.99, Optimal reward -1420.00
Iteration 127 took 2.18 seconds (mean sampled reward: -5253.77). Current reward after update: -2519.72, Optimal reward -1420.00
Iteration 128 took 2.16 seconds (mean sampled reward: -4525.36). Current reward after update: -1588.38, Optimal reward -1420.00
Iteration 129 took 2.22 seconds (mean sampled reward: -3647.13). Current reward after update: -1522.82, Optimal reward -1420.00
Iteration 130 took 2.20 seconds (mean sampled reward: -4836.01). Current reward after update: -1512.55, Optimal reward -1420.00
Iteration 131 took 2.28 seconds (mean sampled reward: -3951.57). Current reward after update: -1546.99, Optimal reward -1420.00
Iteration 132 took 2.24 seconds (mean sampled reward: -3861.08). Current reward after update: -1501.32, Optimal reward -1420.00
Iteration 133 took 2.18 seconds (mean sampled reward: -4048.39). Current reward after update: -1447.54, Optimal reward -1420.00
Iteration 134 took 2.19 seconds (mean sampled reward: -3889.80). Current reward after update: -1435.83, Optimal reward -1420.00
Iteration 135 took 2.20 seconds (mean sampled reward: -3955.81). Current reward after update: -1637.84, Optimal reward -1420.00
Iteration 136 took 2.22 seconds (mean sampled reward: -3718.45). Current reward after update: -2424.22, Optimal reward -1420.00
Iteration 137 took 2.17 seconds (mean sampled reward: -3605.34). Current reward after update: -1436.05, Optimal reward -1420.00
Iteration 138 took 2.20 seconds (mean sampled reward: -4722.61). Current reward after update: -1420.79, Optimal reward -1420.00
Iteration 139 took 2.26 seconds (mean sampled reward: -4221.65). Current reward after update: -1397.97, Optimal reward -1397.97
Iteration 140 took 2.20 seconds (mean sampled reward: -3396.99). Current reward after update: -1416.00, Optimal reward -1397.97
Iteration 141 took 2.19 seconds (mean sampled reward: -3697.98). Current reward after update: -1429.09, Optimal reward -1397.97
Iteration 142 took 2.25 seconds (mean sampled reward: -3547.63). Current reward after update: -1452.56, Optimal reward -1397.97
Iteration 143 took 2.21 seconds (mean sampled reward: -3349.63). Current reward after update: -1406.07, Optimal reward -1397.97
Iteration 144 took 2.25 seconds (mean sampled reward: -4158.85). Current reward after update: -1485.62, Optimal reward -1397.97
Iteration 145 took 2.29 seconds (mean sampled reward: -3968.30). Current reward after update: -1485.31, Optimal reward -1397.97
Iteration 146 took 2.30 seconds (mean sampled reward: -4565.40). Current reward after update: -1467.87, Optimal reward -1397.97
Iteration 147 took 2.33 seconds (mean sampled reward: -5435.76). Current reward after update: -1516.28, Optimal reward -1397.97
Iteration 148 took 2.28 seconds (mean sampled reward: -5276.73). Current reward after update: -1387.36, Optimal reward -1387.36
Iteration 149 took 2.27 seconds (mean sampled reward: -4827.05). Current reward after update: -2291.95, Optimal reward -1387.36
Iteration 150 took 2.25 seconds (mean sampled reward: -5829.28). Current reward after update: -1488.16, Optimal reward -1387.36
Iteration 151 took 2.23 seconds (mean sampled reward: -6079.89). Current reward after update: -1821.72, Optimal reward -1387.36
Iteration 152 took 2.18 seconds (mean sampled reward: -3705.77). Current reward after update: -5402.66, Optimal reward -1387.36
Iteration 153 took 2.21 seconds (mean sampled reward: -3136.21). Current reward after update: -1507.49, Optimal reward -1387.36
Iteration 154 took 2.20 seconds (mean sampled reward: -3038.45). Current reward after update: -1473.31, Optimal reward -1387.36
Iteration 155 took 2.14 seconds (mean sampled reward: -3086.57). Current reward after update: -1816.60, Optimal reward -1387.36
Iteration 156 took 2.25 seconds (mean sampled reward: -2923.25). Current reward after update: -1563.75, Optimal reward -1387.36
Iteration 157 took 2.24 seconds (mean sampled reward: -3497.60). Current reward after update: -1546.16, Optimal reward -1387.36
Iteration 158 took 2.18 seconds (mean sampled reward: -2653.54). Current reward after update: -1490.91, Optimal reward -1387.36
Iteration 159 took 2.22 seconds (mean sampled reward: -3406.62). Current reward after update: -1333.74, Optimal reward -1333.74
Iteration 160 took 2.13 seconds (mean sampled reward: -5301.43). Current reward after update: -1413.87, Optimal reward -1333.74
Iteration 161 took 2.20 seconds (mean sampled reward: -4370.51). Current reward after update: -1556.80, Optimal reward -1333.74
Iteration 162 took 2.18 seconds (mean sampled reward: -4136.84). Current reward after update: -1476.94, Optimal reward -1333.74
Iteration 163 took 2.21 seconds (mean sampled reward: -3429.40). Current reward after update: -1426.86, Optimal reward -1333.74
Iteration 164 took 2.23 seconds (mean sampled reward: -4387.78). Current reward after update: -1445.13, Optimal reward -1333.74
Iteration 165 took 2.32 seconds (mean sampled reward: -4077.55). Current reward after update: -1533.59, Optimal reward -1333.74
Iteration 166 took 2.22 seconds (mean sampled reward: -4050.17). Current reward after update: -1708.80, Optimal reward -1333.74
Iteration 167 took 2.26 seconds (mean sampled reward: -4839.29). Current reward after update: -1454.43, Optimal reward -1333.74
Iteration 168 took 2.20 seconds (mean sampled reward: -2962.95). Current reward after update: -1427.52, Optimal reward -1333.74
Iteration 169 took 2.20 seconds (mean sampled reward: -2987.44). Current reward after update: -3332.60, Optimal reward -1333.74
Iteration 170 took 2.20 seconds (mean sampled reward: -3004.98). Current reward after update: -1461.66, Optimal reward -1333.74
Iteration 171 took 2.18 seconds (mean sampled reward: -2738.31). Current reward after update: -1436.43, Optimal reward -1333.74
Iteration 172 took 2.24 seconds (mean sampled reward: -2470.73). Current reward after update: -1443.20, Optimal reward -1333.74
Iteration 173 took 2.29 seconds (mean sampled reward: -2704.09). Current reward after update: -1418.49, Optimal reward -1333.74
Iteration 174 took 2.32 seconds (mean sampled reward: -2620.09). Current reward after update: -1483.83, Optimal reward -1333.74
Iteration 175 took 2.19 seconds (mean sampled reward: -2673.79). Current reward after update: -1533.48, Optimal reward -1333.74
Iteration 176 took 2.22 seconds (mean sampled reward: -2927.04). Current reward after update: -1503.99, Optimal reward -1333.74
Iteration 177 took 2.16 seconds (mean sampled reward: -2772.53). Current reward after update: -1387.36, Optimal reward -1333.74
Iteration 178 took 2.19 seconds (mean sampled reward: -2366.75). Current reward after update: -1399.65, Optimal reward -1333.74
Iteration 179 took 2.21 seconds (mean sampled reward: -2559.38). Current reward after update: -1417.52, Optimal reward -1333.74
Iteration 180 took 2.20 seconds (mean sampled reward: -2914.75). Current reward after update: -1433.70, Optimal reward -1333.74
Iteration 181 took 2.18 seconds (mean sampled reward: -2393.93). Current reward after update: -1402.66, Optimal reward -1333.74
Iteration 182 took 2.23 seconds (mean sampled reward: -2479.73). Current reward after update: -1396.25, Optimal reward -1333.74
Iteration 183 took 2.22 seconds (mean sampled reward: -3224.87). Current reward after update: -1717.85, Optimal reward -1333.74
Iteration 184 took 2.17 seconds (mean sampled reward: -2532.27). Current reward after update: -1461.35, Optimal reward -1333.74
Iteration 185 took 2.23 seconds (mean sampled reward: -2458.39). Current reward after update: -1389.63, Optimal reward -1333.74
Iteration 186 took 2.22 seconds (mean sampled reward: -2232.53). Current reward after update: -1460.92, Optimal reward -1333.74
Iteration 187 took 2.21 seconds (mean sampled reward: -2304.41). Current reward after update: -1418.02, Optimal reward -1333.74
Iteration 188 took 2.24 seconds (mean sampled reward: -3301.67). Current reward after update: -1433.25, Optimal reward -1333.74
Iteration 189 took 2.27 seconds (mean sampled reward: -3284.98). Current reward after update: -1429.59, Optimal reward -1333.74
Iteration 190 took 2.23 seconds (mean sampled reward: -3305.04). Current reward after update: -1345.03, Optimal reward -1333.74
Iteration 191 took 2.23 seconds (mean sampled reward: -3982.09). Current reward after update: -1366.91, Optimal reward -1333.74
Iteration 192 took 2.22 seconds (mean sampled reward: -3507.99). Current reward after update: -1908.65, Optimal reward -1333.74
Iteration 193 took 2.22 seconds (mean sampled reward: -2738.52). Current reward after update: -5417.33, Optimal reward -1333.74
Iteration 194 took 2.28 seconds (mean sampled reward: -3964.16). Current reward after update: -1389.58, Optimal reward -1333.74
Iteration 195 took 2.27 seconds (mean sampled reward: -3678.72). Current reward after update: -1375.16, Optimal reward -1333.74
Iteration 196 took 2.29 seconds (mean sampled reward: -3425.04). Current reward after update: -1365.72, Optimal reward -1333.74
Iteration 197 took 2.31 seconds (mean sampled reward: -5325.12). Current reward after update: -1430.86, Optimal reward -1333.74
Iteration 198 took 2.35 seconds (mean sampled reward: -4231.78). Current reward after update: -1450.35, Optimal reward -1333.74
Iteration 199 took 2.31 seconds (mean sampled reward: -4273.46). Current reward after update: -1405.26, Optimal reward -1333.74
Iteration 200 took 2.26 seconds (mean sampled reward: -3785.24). Current reward after update: -1382.31, Optimal reward -1333.74
Iteration 1 took 2.21 seconds (mean sampled reward: -7528.30). Current reward after update: -6734.48, Optimal reward -6734.48
Iteration 2 took 2.25 seconds (mean sampled reward: -7161.26). Current reward after update: -5782.80, Optimal reward -5782.80
Iteration 3 took 2.33 seconds (mean sampled reward: -7130.94). Current reward after update: -5468.34, Optimal reward -5468.34
Iteration 4 took 2.31 seconds (mean sampled reward: -6785.02). Current reward after update: -5327.46, Optimal reward -5327.46
Iteration 5 took 2.27 seconds (mean sampled reward: -6895.21). Current reward after update: -5091.93, Optimal reward -5091.93
Iteration 6 took 2.52 seconds (mean sampled reward: -7024.00). Current reward after update: -4768.81, Optimal reward -4768.81
Iteration 7 took 2.48 seconds (mean sampled reward: -6542.85). Current reward after update: -4919.76, Optimal reward -4768.81
Iteration 8 took 2.27 seconds (mean sampled reward: -5858.90). Current reward after update: -4314.51, Optimal reward -4314.51
Iteration 9 took 2.26 seconds (mean sampled reward: -6752.79). Current reward after update: -3244.53, Optimal reward -3244.53
Iteration 10 took 2.33 seconds (mean sampled reward: -6550.91). Current reward after update: -3082.88, Optimal reward -3082.88
Iteration 11 took 2.23 seconds (mean sampled reward: -6083.03). Current reward after update: -2981.36, Optimal reward -2981.36
Iteration 12 took 2.32 seconds (mean sampled reward: -5431.07). Current reward after update: -2450.37, Optimal reward -2450.37
Iteration 13 took 2.33 seconds (mean sampled reward: -5098.33). Current reward after update: -2338.08, Optimal reward -2338.08
Iteration 14 took 2.25 seconds (mean sampled reward: -5251.66). Current reward after update: -2477.29, Optimal reward -2338.08
Iteration 15 took 2.32 seconds (mean sampled reward: -3775.74). Current reward after update: -2209.91, Optimal reward -2209.91
Iteration 16 took 2.19 seconds (mean sampled reward: -3746.06). Current reward after update: -2005.60, Optimal reward -2005.60
Iteration 17 took 2.11 seconds (mean sampled reward: -4941.12). Current reward after update: -2408.02, Optimal reward -2005.60
Iteration 18 took 2.12 seconds (mean sampled reward: -4440.98). Current reward after update: -2400.02, Optimal reward -2005.60
Iteration 19 took 2.24 seconds (mean sampled reward: -5263.52). Current reward after update: -2212.04, Optimal reward -2005.60
Iteration 20 took 2.18 seconds (mean sampled reward: -5043.58). Current reward after update: -2360.62, Optimal reward -2005.60
Iteration 21 took 2.09 seconds (mean sampled reward: -5572.80). Current reward after update: -2930.60, Optimal reward -2005.60
Iteration 22 took 2.09 seconds (mean sampled reward: -5065.34). Current reward after update: -2121.06, Optimal reward -2005.60
Iteration 23 took 2.09 seconds (mean sampled reward: -5652.94). Current reward after update: -2491.25, Optimal reward -2005.60
Iteration 24 took 2.17 seconds (mean sampled reward: -6174.86). Current reward after update: -2481.12, Optimal reward -2005.60
Iteration 25 took 2.06 seconds (mean sampled reward: -5561.17). Current reward after update: -2192.24, Optimal reward -2005.60
Iteration 26 took 2.06 seconds (mean sampled reward: -5744.85). Current reward after update: -2196.56, Optimal reward -2005.60
Iteration 27 took 2.12 seconds (mean sampled reward: -5124.93). Current reward after update: -2530.45, Optimal reward -2005.60
Iteration 28 took 2.11 seconds (mean sampled reward: -6130.25). Current reward after update: -2468.64, Optimal reward -2005.60
Iteration 29 took 2.19 seconds (mean sampled reward: -5634.07). Current reward after update: -2487.81, Optimal reward -2005.60
Iteration 30 took 2.25 seconds (mean sampled reward: -5736.96). Current reward after update: -2998.25, Optimal reward -2005.60
Iteration 31 took 2.18 seconds (mean sampled reward: -4492.97). Current reward after update: -2039.94, Optimal reward -2005.60
Iteration 32 took 2.11 seconds (mean sampled reward: -4923.44). Current reward after update: -2171.03, Optimal reward -2005.60
Iteration 33 took 2.14 seconds (mean sampled reward: -4101.01). Current reward after update: -2742.49, Optimal reward -2005.60
Iteration 34 took 2.12 seconds (mean sampled reward: -3811.50). Current reward after update: -2187.49, Optimal reward -2005.60
Iteration 35 took 2.21 seconds (mean sampled reward: -3994.58). Current reward after update: -2155.92, Optimal reward -2005.60
Iteration 36 took 2.16 seconds (mean sampled reward: -4106.78). Current reward after update: -2263.08, Optimal reward -2005.60
Iteration 37 took 2.13 seconds (mean sampled reward: -3516.77). Current reward after update: -2291.50, Optimal reward -2005.60
Iteration 38 took 2.16 seconds (mean sampled reward: -3347.16). Current reward after update: -2202.59, Optimal reward -2005.60
Iteration 39 took 2.23 seconds (mean sampled reward: -4061.52). Current reward after update: -2264.29, Optimal reward -2005.60
Iteration 40 took 2.15 seconds (mean sampled reward: -3581.36). Current reward after update: -2113.78, Optimal reward -2005.60
Iteration 41 took 2.18 seconds (mean sampled reward: -3422.89). Current reward after update: -2111.31, Optimal reward -2005.60
Iteration 42 took 2.19 seconds (mean sampled reward: -3086.99). Current reward after update: -2169.31, Optimal reward -2005.60
Iteration 43 took 2.09 seconds (mean sampled reward: -3156.93). Current reward after update: -2517.14, Optimal reward -2005.60
Iteration 44 took 2.25 seconds (mean sampled reward: -3153.18). Current reward after update: -1927.19, Optimal reward -1927.19
Iteration 45 took 2.07 seconds (mean sampled reward: -3744.97). Current reward after update: -1926.89, Optimal reward -1926.89
Iteration 46 took 2.14 seconds (mean sampled reward: -2630.23). Current reward after update: -1776.04, Optimal reward -1776.04
Iteration 47 took 2.08 seconds (mean sampled reward: -2967.84). Current reward after update: -1708.80, Optimal reward -1708.80
Iteration 48 took 2.09 seconds (mean sampled reward: -2892.81). Current reward after update: -1706.83, Optimal reward -1706.83
Iteration 49 took 2.09 seconds (mean sampled reward: -3002.58). Current reward after update: -1668.30, Optimal reward -1668.30
Iteration 50 took 2.01 seconds (mean sampled reward: -2962.74). Current reward after update: -1688.40, Optimal reward -1668.30
Iteration 51 took 2.05 seconds (mean sampled reward: -2845.85). Current reward after update: -1668.15, Optimal reward -1668.15
Iteration 52 took 2.05 seconds (mean sampled reward: -2736.28). Current reward after update: -1806.67, Optimal reward -1668.15
Iteration 53 took 2.22 seconds (mean sampled reward: -2728.22). Current reward after update: -1920.48, Optimal reward -1668.15
Iteration 54 took 2.35 seconds (mean sampled reward: -2959.05). Current reward after update: -1685.66, Optimal reward -1668.15
Iteration 55 took 2.12 seconds (mean sampled reward: -3518.53). Current reward after update: -2066.44, Optimal reward -1668.15
Iteration 56 took 2.33 seconds (mean sampled reward: -4429.37). Current reward after update: -1767.40, Optimal reward -1668.15
Iteration 57 took 2.21 seconds (mean sampled reward: -6588.11). Current reward after update: -1843.52, Optimal reward -1668.15
Iteration 58 took 2.30 seconds (mean sampled reward: -5944.29). Current reward after update: -1655.87, Optimal reward -1655.87
Iteration 59 took 2.26 seconds (mean sampled reward: -3706.00). Current reward after update: -1839.07, Optimal reward -1655.87
Iteration 60 took 2.07 seconds (mean sampled reward: -4957.27). Current reward after update: -1636.99, Optimal reward -1636.99
Iteration 61 took 2.07 seconds (mean sampled reward: -4187.31). Current reward after update: -1650.47, Optimal reward -1636.99
Iteration 62 took 2.17 seconds (mean sampled reward: -3994.05). Current reward after update: -1655.31, Optimal reward -1636.99
Iteration 63 took 2.11 seconds (mean sampled reward: -4620.91). Current reward after update: -1690.30, Optimal reward -1636.99
Iteration 64 took 2.07 seconds (mean sampled reward: -4378.44). Current reward after update: -1655.00, Optimal reward -1636.99
Iteration 65 took 2.26 seconds (mean sampled reward: -3122.78). Current reward after update: -1623.68, Optimal reward -1623.68
Iteration 66 took 2.25 seconds (mean sampled reward: -3141.17). Current reward after update: -1645.09, Optimal reward -1623.68
Iteration 67 took 2.15 seconds (mean sampled reward: -2847.81). Current reward after update: -2456.07, Optimal reward -1623.68
Iteration 68 took 2.20 seconds (mean sampled reward: -3665.10). Current reward after update: -1750.10, Optimal reward -1623.68
Iteration 69 took 2.02 seconds (mean sampled reward: -5218.40). Current reward after update: -1696.75, Optimal reward -1623.68
Iteration 70 took 2.04 seconds (mean sampled reward: -5640.56). Current reward after update: -1643.37, Optimal reward -1623.68
Iteration 71 took 2.05 seconds (mean sampled reward: -5290.34). Current reward after update: -1668.37, Optimal reward -1623.68
Iteration 72 took 2.16 seconds (mean sampled reward: -4205.85). Current reward after update: -1651.07, Optimal reward -1623.68
Iteration 73 took 2.06 seconds (mean sampled reward: -4862.15). Current reward after update: -1619.41, Optimal reward -1619.41
Iteration 74 took 2.06 seconds (mean sampled reward: -4115.99). Current reward after update: -1800.17, Optimal reward -1619.41
Iteration 75 took 1.99 seconds (mean sampled reward: -5554.27). Current reward after update: -1849.86, Optimal reward -1619.41
Iteration 76 took 2.17 seconds (mean sampled reward: -4155.14). Current reward after update: -3150.73, Optimal reward -1619.41
Iteration 77 took 2.13 seconds (mean sampled reward: -3336.65). Current reward after update: -2475.08, Optimal reward -1619.41
Iteration 78 took 2.24 seconds (mean sampled reward: -3302.13). Current reward after update: -1697.59, Optimal reward -1619.41
Iteration 79 took 2.01 seconds (mean sampled reward: -4768.58). Current reward after update: -1644.74, Optimal reward -1619.41
Iteration 80 took 2.17 seconds (mean sampled reward: -3031.76). Current reward after update: -4834.99, Optimal reward -1619.41
Iteration 81 took 2.12 seconds (mean sampled reward: -3356.02). Current reward after update: -1674.77, Optimal reward -1619.41
Iteration 82 took 2.13 seconds (mean sampled reward: -3578.39). Current reward after update: -1683.70, Optimal reward -1619.41
Iteration 83 took 2.24 seconds (mean sampled reward: -3801.26). Current reward after update: -1639.29, Optimal reward -1619.41
Iteration 84 took 2.09 seconds (mean sampled reward: -3203.58). Current reward after update: -1706.50, Optimal reward -1619.41
Iteration 85 took 2.06 seconds (mean sampled reward: -2937.07). Current reward after update: -3024.14, Optimal reward -1619.41
Iteration 86 took 2.05 seconds (mean sampled reward: -3395.07). Current reward after update: -1697.73, Optimal reward -1619.41
Iteration 87 took 2.07 seconds (mean sampled reward: -3055.11). Current reward after update: -1747.74, Optimal reward -1619.41
Iteration 88 took 2.03 seconds (mean sampled reward: -3696.02). Current reward after update: -2675.10, Optimal reward -1619.41
Iteration 89 took 2.09 seconds (mean sampled reward: -3529.99). Current reward after update: -2225.14, Optimal reward -1619.41
Iteration 90 took 1.96 seconds (mean sampled reward: -5340.33). Current reward after update: -1682.10, Optimal reward -1619.41
Iteration 91 took 2.00 seconds (mean sampled reward: -4663.12). Current reward after update: -1931.69, Optimal reward -1619.41
Iteration 92 took 2.00 seconds (mean sampled reward: -3033.63). Current reward after update: -1723.98, Optimal reward -1619.41
Iteration 93 took 2.04 seconds (mean sampled reward: -2493.06). Current reward after update: -2335.61, Optimal reward -1619.41
Iteration 94 took 2.06 seconds (mean sampled reward: -2544.99). Current reward after update: -1642.68, Optimal reward -1619.41
Iteration 95 took 2.04 seconds (mean sampled reward: -3972.74). Current reward after update: -2024.95, Optimal reward -1619.41
Iteration 96 took 2.11 seconds (mean sampled reward: -3025.39). Current reward after update: -1690.13, Optimal reward -1619.41
Iteration 97 took 2.08 seconds (mean sampled reward: -2564.91). Current reward after update: -2093.88, Optimal reward -1619.41
Iteration 98 took 2.21 seconds (mean sampled reward: -2649.97). Current reward after update: -1596.88, Optimal reward -1596.88
Iteration 99 took 2.10 seconds (mean sampled reward: -2827.94). Current reward after update: -1634.19, Optimal reward -1596.88
Iteration 100 took 2.11 seconds (mean sampled reward: -3206.26). Current reward after update: -1604.85, Optimal reward -1596.88
Iteration 101 took 2.12 seconds (mean sampled reward: -3100.70). Current reward after update: -1666.31, Optimal reward -1596.88
Iteration 102 took 2.13 seconds (mean sampled reward: -2519.87). Current reward after update: -1640.04, Optimal reward -1596.88
Iteration 103 took 2.18 seconds (mean sampled reward: -2428.18). Current reward after update: -1665.48, Optimal reward -1596.88
Iteration 104 took 2.13 seconds (mean sampled reward: -2561.75). Current reward after update: -1752.15, Optimal reward -1596.88
Iteration 105 took 2.19 seconds (mean sampled reward: -2507.71). Current reward after update: -1700.07, Optimal reward -1596.88
Iteration 106 took 2.19 seconds (mean sampled reward: -2456.34). Current reward after update: -2353.55, Optimal reward -1596.88
Iteration 107 took 2.05 seconds (mean sampled reward: -3102.73). Current reward after update: -1795.92, Optimal reward -1596.88
Iteration 108 took 2.23 seconds (mean sampled reward: -2863.01). Current reward after update: -1607.56, Optimal reward -1596.88
Iteration 109 took 2.06 seconds (mean sampled reward: -2877.64). Current reward after update: -1770.62, Optimal reward -1596.88
Iteration 110 took 2.17 seconds (mean sampled reward: -2939.76). Current reward after update: -1700.37, Optimal reward -1596.88
Iteration 111 took 2.07 seconds (mean sampled reward: -4150.71). Current reward after update: -1620.20, Optimal reward -1596.88
Iteration 112 took 2.02 seconds (mean sampled reward: -2846.75). Current reward after update: -1725.65, Optimal reward -1596.88
Iteration 113 took 2.05 seconds (mean sampled reward: -2643.38). Current reward after update: -1624.81, Optimal reward -1596.88
Iteration 114 took 2.05 seconds (mean sampled reward: -2678.40). Current reward after update: -1600.20, Optimal reward -1596.88
Iteration 115 took 2.18 seconds (mean sampled reward: -2971.86). Current reward after update: -1695.91, Optimal reward -1596.88
Iteration 116 took 2.06 seconds (mean sampled reward: -2925.82). Current reward after update: -1642.84, Optimal reward -1596.88
Iteration 117 took 2.13 seconds (mean sampled reward: -2976.37). Current reward after update: -1624.47, Optimal reward -1596.88
Iteration 118 took 2.06 seconds (mean sampled reward: -2751.00). Current reward after update: -1872.09, Optimal reward -1596.88
Iteration 119 took 2.11 seconds (mean sampled reward: -2949.17). Current reward after update: -1579.00, Optimal reward -1579.00
Iteration 120 took 2.15 seconds (mean sampled reward: -3570.46). Current reward after update: -2406.19, Optimal reward -1579.00
Iteration 121 took 2.07 seconds (mean sampled reward: -2290.17). Current reward after update: -1642.58, Optimal reward -1579.00
Iteration 122 took 2.05 seconds (mean sampled reward: -2653.18). Current reward after update: -1649.94, Optimal reward -1579.00
Iteration 123 took 2.20 seconds (mean sampled reward: -2896.09). Current reward after update: -1627.91, Optimal reward -1579.00
Iteration 124 took 2.06 seconds (mean sampled reward: -2490.66). Current reward after update: -1953.75, Optimal reward -1579.00
Iteration 125 took 2.08 seconds (mean sampled reward: -2282.00). Current reward after update: -1681.53, Optimal reward -1579.00
Iteration 126 took 2.07 seconds (mean sampled reward: -2515.94). Current reward after update: -1934.94, Optimal reward -1579.00
Iteration 127 took 2.09 seconds (mean sampled reward: -2643.10). Current reward after update: -1627.95, Optimal reward -1579.00
Iteration 128 took 2.11 seconds (mean sampled reward: -2809.40). Current reward after update: -1639.52, Optimal reward -1579.00
Iteration 129 took 2.08 seconds (mean sampled reward: -3170.76). Current reward after update: -1784.17, Optimal reward -1579.00
Iteration 130 took 2.02 seconds (mean sampled reward: -2799.99). Current reward after update: -1703.21, Optimal reward -1579.00
Iteration 131 took 2.14 seconds (mean sampled reward: -3897.24). Current reward after update: -1555.95, Optimal reward -1555.95
Iteration 132 took 2.04 seconds (mean sampled reward: -2949.00). Current reward after update: -1559.94, Optimal reward -1555.95
Iteration 133 took 2.08 seconds (mean sampled reward: -3139.33). Current reward after update: -1589.23, Optimal reward -1555.95
Iteration 134 took 2.06 seconds (mean sampled reward: -2838.54). Current reward after update: -1729.32, Optimal reward -1555.95
Iteration 135 took 2.14 seconds (mean sampled reward: -2723.07). Current reward after update: -1836.35, Optimal reward -1555.95
Iteration 136 took 2.09 seconds (mean sampled reward: -2257.02). Current reward after update: -1551.24, Optimal reward -1551.24
Iteration 137 took 2.06 seconds (mean sampled reward: -2543.97). Current reward after update: -1706.63, Optimal reward -1551.24
Iteration 138 took 2.02 seconds (mean sampled reward: -3176.58). Current reward after update: -1966.21, Optimal reward -1551.24
Iteration 139 took 2.06 seconds (mean sampled reward: -3483.15). Current reward after update: -1700.23, Optimal reward -1551.24
Iteration 140 took 2.04 seconds (mean sampled reward: -3125.69). Current reward after update: -1605.05, Optimal reward -1551.24
Iteration 141 took 2.02 seconds (mean sampled reward: -3138.68). Current reward after update: -1622.42, Optimal reward -1551.24
Iteration 142 took 2.12 seconds (mean sampled reward: -2096.49). Current reward after update: -1682.49, Optimal reward -1551.24
Iteration 143 took 2.08 seconds (mean sampled reward: -2397.78). Current reward after update: -1586.67, Optimal reward -1551.24
Iteration 144 took 2.12 seconds (mean sampled reward: -2497.04). Current reward after update: -1655.28, Optimal reward -1551.24
Iteration 145 took 2.08 seconds (mean sampled reward: -2886.19). Current reward after update: -1577.74, Optimal reward -1551.24
Iteration 146 took 2.09 seconds (mean sampled reward: -3481.53). Current reward after update: -1624.02, Optimal reward -1551.24
Iteration 147 took 2.04 seconds (mean sampled reward: -3702.57). Current reward after update: -1626.85, Optimal reward -1551.24
Iteration 148 took 2.06 seconds (mean sampled reward: -2619.78). Current reward after update: -1751.71, Optimal reward -1551.24
Iteration 149 took 2.06 seconds (mean sampled reward: -2400.52). Current reward after update: -1557.07, Optimal reward -1551.24
Iteration 150 took 2.07 seconds (mean sampled reward: -2224.18). Current reward after update: -1767.31, Optimal reward -1551.24
Iteration 151 took 2.08 seconds (mean sampled reward: -2329.68). Current reward after update: -1597.43, Optimal reward -1551.24
Iteration 152 took 2.09 seconds (mean sampled reward: -2231.89). Current reward after update: -2177.92, Optimal reward -1551.24
Iteration 153 took 2.11 seconds (mean sampled reward: -2299.50). Current reward after update: -1630.12, Optimal reward -1551.24
Iteration 154 took 2.06 seconds (mean sampled reward: -2148.77). Current reward after update: -1525.19, Optimal reward -1525.19
Iteration 155 took 2.12 seconds (mean sampled reward: -2216.60). Current reward after update: -1621.65, Optimal reward -1525.19
Iteration 156 took 2.12 seconds (mean sampled reward: -2137.83). Current reward after update: -1849.13, Optimal reward -1525.19
Iteration 157 took 2.07 seconds (mean sampled reward: -2128.13). Current reward after update: -1596.29, Optimal reward -1525.19
Iteration 158 took 2.15 seconds (mean sampled reward: -2179.19). Current reward after update: -1573.10, Optimal reward -1525.19
Iteration 159 took 2.20 seconds (mean sampled reward: -2118.97). Current reward after update: -1744.00, Optimal reward -1525.19
Iteration 160 took 2.11 seconds (mean sampled reward: -2186.44). Current reward after update: -1766.86, Optimal reward -1525.19
Iteration 161 took 2.14 seconds (mean sampled reward: -2271.24). Current reward after update: -1685.76, Optimal reward -1525.19
Iteration 162 took 2.08 seconds (mean sampled reward: -2380.83). Current reward after update: -1807.15, Optimal reward -1525.19
Iteration 163 took 2.10 seconds (mean sampled reward: -2462.34). Current reward after update: -1631.06, Optimal reward -1525.19
Iteration 164 took 2.19 seconds (mean sampled reward: -2707.41). Current reward after update: -1618.21, Optimal reward -1525.19
Iteration 165 took 2.05 seconds (mean sampled reward: -2522.15). Current reward after update: -1611.15, Optimal reward -1525.19
Iteration 166 took 2.12 seconds (mean sampled reward: -3156.77). Current reward after update: -2333.37, Optimal reward -1525.19
Iteration 167 took 2.09 seconds (mean sampled reward: -2993.35). Current reward after update: -1566.24, Optimal reward -1525.19
Iteration 168 took 2.07 seconds (mean sampled reward: -3577.50). Current reward after update: -1646.36, Optimal reward -1525.19
Iteration 169 took 2.01 seconds (mean sampled reward: -3491.33). Current reward after update: -1661.48, Optimal reward -1525.19
Iteration 170 took 2.07 seconds (mean sampled reward: -3510.68). Current reward after update: -1611.31, Optimal reward -1525.19
Iteration 171 took 2.10 seconds (mean sampled reward: -2571.51). Current reward after update: -1569.41, Optimal reward -1525.19
Iteration 172 took 2.04 seconds (mean sampled reward: -3167.99). Current reward after update: -1746.70, Optimal reward -1525.19
Iteration 173 took 2.06 seconds (mean sampled reward: -3813.23). Current reward after update: -1666.90, Optimal reward -1525.19
Iteration 174 took 2.10 seconds (mean sampled reward: -3730.38). Current reward after update: -1568.99, Optimal reward -1525.19
Iteration 175 took 2.04 seconds (mean sampled reward: -3780.55). Current reward after update: -1798.38, Optimal reward -1525.19
Iteration 176 took 2.06 seconds (mean sampled reward: -2804.82). Current reward after update: -1607.69, Optimal reward -1525.19
Iteration 177 took 2.01 seconds (mean sampled reward: -3261.37). Current reward after update: -1617.53, Optimal reward -1525.19
Iteration 178 took 2.08 seconds (mean sampled reward: -2193.86). Current reward after update: -1552.07, Optimal reward -1525.19
Iteration 179 took 2.10 seconds (mean sampled reward: -2519.07). Current reward after update: -2215.00, Optimal reward -1525.19
Iteration 180 took 2.09 seconds (mean sampled reward: -2322.95). Current reward after update: -1838.95, Optimal reward -1525.19
Iteration 181 took 2.12 seconds (mean sampled reward: -2334.96). Current reward after update: -2082.22, Optimal reward -1525.19
Iteration 182 took 2.08 seconds (mean sampled reward: -2418.69). Current reward after update: -1553.98, Optimal reward -1525.19
Iteration 183 took 2.08 seconds (mean sampled reward: -2499.65). Current reward after update: -2054.84, Optimal reward -1525.19
Iteration 184 took 2.09 seconds (mean sampled reward: -2544.70). Current reward after update: -1549.38, Optimal reward -1525.19
Iteration 185 took 2.09 seconds (mean sampled reward: -2529.89). Current reward after update: -1536.29, Optimal reward -1525.19
Iteration 186 took 2.08 seconds (mean sampled reward: -2801.61). Current reward after update: -1547.39, Optimal reward -1525.19
Iteration 187 took 2.13 seconds (mean sampled reward: -2535.47). Current reward after update: -1579.32, Optimal reward -1525.19
Iteration 188 took 2.14 seconds (mean sampled reward: -2528.34). Current reward after update: -1675.52, Optimal reward -1525.19
Iteration 189 took 2.14 seconds (mean sampled reward: -2627.87). Current reward after update: -1695.68, Optimal reward -1525.19
Iteration 190 took 2.10 seconds (mean sampled reward: -2450.64). Current reward after update: -1566.12, Optimal reward -1525.19
Iteration 191 took 2.14 seconds (mean sampled reward: -2834.43). Current reward after update: -1965.44, Optimal reward -1525.19
Iteration 192 took 2.09 seconds (mean sampled reward: -2575.39). Current reward after update: -1561.10, Optimal reward -1525.19
Iteration 193 took 2.13 seconds (mean sampled reward: -2543.56). Current reward after update: -2110.13, Optimal reward -1525.19
Iteration 194 took 2.10 seconds (mean sampled reward: -2870.65). Current reward after update: -1594.43, Optimal reward -1525.19
Iteration 195 took 2.13 seconds (mean sampled reward: -2538.01). Current reward after update: -1652.10, Optimal reward -1525.19
Iteration 196 took 2.11 seconds (mean sampled reward: -2428.29). Current reward after update: -1606.83, Optimal reward -1525.19
Iteration 197 took 2.09 seconds (mean sampled reward: -2506.27). Current reward after update: -1593.29, Optimal reward -1525.19
Iteration 198 took 2.14 seconds (mean sampled reward: -2559.71). Current reward after update: -1583.33, Optimal reward -1525.19
Iteration 199 took 2.12 seconds (mean sampled reward: -2303.52). Current reward after update: -1526.79, Optimal reward -1525.19
Iteration 200 took 2.12 seconds (mean sampled reward: -2817.60). Current reward after update: -1625.37, Optimal reward -1525.19
Iteration 1 took 2.17 seconds (mean sampled reward: -7537.05). Current reward after update: -6696.42, Optimal reward -6696.42
Iteration 2 took 2.15 seconds (mean sampled reward: -7277.15). Current reward after update: -5563.31, Optimal reward -5563.31
Iteration 3 took 2.14 seconds (mean sampled reward: -7019.76). Current reward after update: -4713.49, Optimal reward -4713.49
Iteration 4 took 2.17 seconds (mean sampled reward: -6431.24). Current reward after update: -4088.33, Optimal reward -4088.33
Iteration 5 took 2.22 seconds (mean sampled reward: -6362.32). Current reward after update: -4092.73, Optimal reward -4088.33
Iteration 6 took 2.21 seconds (mean sampled reward: -5909.00). Current reward after update: -3928.35, Optimal reward -3928.35
Iteration 7 took 2.17 seconds (mean sampled reward: -6024.59). Current reward after update: -4704.60, Optimal reward -3928.35
Iteration 8 took 2.18 seconds (mean sampled reward: -5491.09). Current reward after update: -3094.66, Optimal reward -3094.66
Iteration 9 took 2.38 seconds (mean sampled reward: -5688.39). Current reward after update: -2619.40, Optimal reward -2619.40
Iteration 10 took 2.16 seconds (mean sampled reward: -5802.40). Current reward after update: -2545.80, Optimal reward -2545.80
Iteration 11 took 2.21 seconds (mean sampled reward: -5746.44). Current reward after update: -2345.29, Optimal reward -2345.29
Iteration 12 took 2.15 seconds (mean sampled reward: -5858.94). Current reward after update: -2219.31, Optimal reward -2219.31
Iteration 13 took 2.48 seconds (mean sampled reward: -4798.07). Current reward after update: -2074.55, Optimal reward -2074.55
Iteration 14 took 2.26 seconds (mean sampled reward: -5682.21). Current reward after update: -2031.91, Optimal reward -2031.91
Iteration 15 took 2.30 seconds (mean sampled reward: -6114.31). Current reward after update: -2039.57, Optimal reward -2031.91
Iteration 16 took 2.76 seconds (mean sampled reward: -6206.79). Current reward after update: -2230.40, Optimal reward -2031.91
Iteration 17 took 2.36 seconds (mean sampled reward: -5981.37). Current reward after update: -1892.03, Optimal reward -1892.03
Iteration 18 took 2.17 seconds (mean sampled reward: -5929.61). Current reward after update: -1762.13, Optimal reward -1762.13
Iteration 19 took 2.23 seconds (mean sampled reward: -5826.69). Current reward after update: -1842.17, Optimal reward -1762.13
Iteration 20 took 2.23 seconds (mean sampled reward: -6029.78). Current reward after update: -1823.00, Optimal reward -1762.13
Iteration 21 took 2.16 seconds (mean sampled reward: -5893.59). Current reward after update: -1627.79, Optimal reward -1627.79
Iteration 22 took 2.17 seconds (mean sampled reward: -6330.64). Current reward after update: -1856.93, Optimal reward -1627.79
Iteration 23 took 2.20 seconds (mean sampled reward: -6690.48). Current reward after update: -1861.53, Optimal reward -1627.79
Iteration 24 took 2.04 seconds (mean sampled reward: -6406.39). Current reward after update: -2125.84, Optimal reward -1627.79
Iteration 25 took 2.18 seconds (mean sampled reward: -6734.11). Current reward after update: -1821.82, Optimal reward -1627.79
Iteration 26 took 2.28 seconds (mean sampled reward: -5867.84). Current reward after update: -1887.87, Optimal reward -1627.79
Iteration 27 took 2.12 seconds (mean sampled reward: -5873.65). Current reward after update: -5486.25, Optimal reward -1627.79
Iteration 28 took 2.12 seconds (mean sampled reward: -5731.10). Current reward after update: -1531.07, Optimal reward -1531.07
Iteration 29 took 2.24 seconds (mean sampled reward: -5745.57). Current reward after update: -1515.52, Optimal reward -1515.52
Iteration 30 took 2.20 seconds (mean sampled reward: -6188.54). Current reward after update: -1543.54, Optimal reward -1515.52
Iteration 31 took 2.16 seconds (mean sampled reward: -6506.93). Current reward after update: -2171.15, Optimal reward -1515.52
Iteration 32 took 2.14 seconds (mean sampled reward: -6669.27). Current reward after update: -6191.48, Optimal reward -1515.52
Iteration 33 took 2.13 seconds (mean sampled reward: -6452.12). Current reward after update: -1733.03, Optimal reward -1515.52
Iteration 34 took 2.08 seconds (mean sampled reward: -6085.27). Current reward after update: -1630.43, Optimal reward -1515.52
Iteration 35 took 2.11 seconds (mean sampled reward: -5796.96). Current reward after update: -1347.27, Optimal reward -1347.27
Iteration 36 took 2.17 seconds (mean sampled reward: -5490.15). Current reward after update: -1376.75, Optimal reward -1347.27
Iteration 37 took 2.13 seconds (mean sampled reward: -5473.58). Current reward after update: -1590.35, Optimal reward -1347.27
Iteration 38 took 2.23 seconds (mean sampled reward: -5485.71). Current reward after update: -1378.52, Optimal reward -1347.27
Iteration 39 took 2.25 seconds (mean sampled reward: -4907.87). Current reward after update: -2898.22, Optimal reward -1347.27
Iteration 40 took 2.23 seconds (mean sampled reward: -4583.13). Current reward after update: -1265.55, Optimal reward -1265.55
Iteration 41 took 2.14 seconds (mean sampled reward: -5266.06). Current reward after update: -6279.96, Optimal reward -1265.55
Iteration 42 took 2.28 seconds (mean sampled reward: -4127.57). Current reward after update: -1430.73, Optimal reward -1265.55
Iteration 43 took 2.22 seconds (mean sampled reward: -4517.28). Current reward after update: -1428.67, Optimal reward -1265.55
Iteration 44 took 2.18 seconds (mean sampled reward: -4065.15). Current reward after update: -2347.52, Optimal reward -1265.55
Iteration 45 took 2.22 seconds (mean sampled reward: -5517.81). Current reward after update: -1543.05, Optimal reward -1265.55
Iteration 46 took 2.10 seconds (mean sampled reward: -4873.54). Current reward after update: -1654.18, Optimal reward -1265.55
Iteration 47 took 2.17 seconds (mean sampled reward: -5693.06). Current reward after update: -1601.86, Optimal reward -1265.55
Iteration 48 took 2.08 seconds (mean sampled reward: -5252.06). Current reward after update: -1720.54, Optimal reward -1265.55
Iteration 49 took 2.16 seconds (mean sampled reward: -3610.98). Current reward after update: -2484.86, Optimal reward -1265.55
Iteration 50 took 2.17 seconds (mean sampled reward: -3507.32). Current reward after update: -1917.30, Optimal reward -1265.55
Iteration 51 took 2.11 seconds (mean sampled reward: -4091.91). Current reward after update: -1441.54, Optimal reward -1265.55
Iteration 52 took 2.16 seconds (mean sampled reward: -4740.62). Current reward after update: -1382.50, Optimal reward -1265.55
Iteration 53 took 2.19 seconds (mean sampled reward: -3937.65). Current reward after update: -1616.92, Optimal reward -1265.55
Iteration 54 took 2.11 seconds (mean sampled reward: -4369.47). Current reward after update: -1740.18, Optimal reward -1265.55
Iteration 55 took 2.17 seconds (mean sampled reward: -4469.69). Current reward after update: -1739.46, Optimal reward -1265.55
Iteration 56 took 2.31 seconds (mean sampled reward: -4055.76). Current reward after update: -1450.71, Optimal reward -1265.55
Iteration 57 took 2.18 seconds (mean sampled reward: -5138.59). Current reward after update: -1401.59, Optimal reward -1265.55
Iteration 58 took 2.23 seconds (mean sampled reward: -4468.20). Current reward after update: -1315.40, Optimal reward -1265.55
Iteration 59 took 2.24 seconds (mean sampled reward: -3696.74). Current reward after update: -1303.95, Optimal reward -1265.55
Iteration 60 took 2.31 seconds (mean sampled reward: -4086.76). Current reward after update: -1039.99, Optimal reward -1039.99
Iteration 61 took 2.25 seconds (mean sampled reward: -3859.70). Current reward after update: -1277.39, Optimal reward -1039.99
Iteration 62 took 2.19 seconds (mean sampled reward: -5203.44). Current reward after update: -1079.69, Optimal reward -1039.99
Iteration 63 took 2.26 seconds (mean sampled reward: -5336.42). Current reward after update: -1135.63, Optimal reward -1039.99
Iteration 64 took 2.13 seconds (mean sampled reward: -5635.96). Current reward after update: -1352.42, Optimal reward -1039.99
Iteration 65 took 2.22 seconds (mean sampled reward: -4779.25). Current reward after update: -1382.12, Optimal reward -1039.99
Iteration 66 took 2.17 seconds (mean sampled reward: -5567.07). Current reward after update: -990.44, Optimal reward -990.44
Iteration 67 took 2.18 seconds (mean sampled reward: -4644.54). Current reward after update: -1218.08, Optimal reward -990.44
Iteration 68 took 2.29 seconds (mean sampled reward: -3700.92). Current reward after update: -1141.87, Optimal reward -990.44
Iteration 69 took 2.23 seconds (mean sampled reward: -4061.19). Current reward after update: -887.08, Optimal reward -887.08
Iteration 70 took 2.27 seconds (mean sampled reward: -4106.50). Current reward after update: -804.23, Optimal reward -804.23
Iteration 71 took 2.26 seconds (mean sampled reward: -4277.92). Current reward after update: -937.76, Optimal reward -804.23
Iteration 72 took 2.22 seconds (mean sampled reward: -4282.08). Current reward after update: -1350.89, Optimal reward -804.23
Iteration 73 took 2.23 seconds (mean sampled reward: -4170.70). Current reward after update: -1112.33, Optimal reward -804.23
Iteration 74 took 2.19 seconds (mean sampled reward: -4452.49). Current reward after update: -1411.44, Optimal reward -804.23
Iteration 75 took 2.18 seconds (mean sampled reward: -5634.62). Current reward after update: -865.42, Optimal reward -804.23
Iteration 76 took 2.17 seconds (mean sampled reward: -5216.01). Current reward after update: -769.53, Optimal reward -769.53
Iteration 77 took 2.13 seconds (mean sampled reward: -3883.47). Current reward after update: -2853.08, Optimal reward -769.53
Iteration 78 took 2.32 seconds (mean sampled reward: -3559.94). Current reward after update: -886.70, Optimal reward -769.53
Iteration 79 took 2.19 seconds (mean sampled reward: -4832.27). Current reward after update: -882.58, Optimal reward -769.53
Iteration 80 took 2.16 seconds (mean sampled reward: -4808.17). Current reward after update: -822.06, Optimal reward -769.53
Iteration 81 took 2.27 seconds (mean sampled reward: -4429.88). Current reward after update: -5073.86, Optimal reward -769.53
Iteration 82 took 2.14 seconds (mean sampled reward: -5435.12). Current reward after update: -742.71, Optimal reward -742.71
Iteration 83 took 2.22 seconds (mean sampled reward: -5247.83). Current reward after update: -816.17, Optimal reward -742.71
Iteration 84 took 2.14 seconds (mean sampled reward: -5662.64). Current reward after update: -627.56, Optimal reward -627.56
Iteration 85 took 2.11 seconds (mean sampled reward: -5417.88). Current reward after update: -862.25, Optimal reward -627.56
Iteration 86 took 2.24 seconds (mean sampled reward: -5295.64). Current reward after update: -858.55, Optimal reward -627.56
Iteration 87 took 2.18 seconds (mean sampled reward: -5637.38). Current reward after update: -1103.73, Optimal reward -627.56
Iteration 88 took 2.20 seconds (mean sampled reward: -4762.64). Current reward after update: -1108.21, Optimal reward -627.56
Iteration 89 took 2.24 seconds (mean sampled reward: -3920.27). Current reward after update: -914.85, Optimal reward -627.56
Iteration 90 took 2.21 seconds (mean sampled reward: -4925.84). Current reward after update: -956.70, Optimal reward -627.56
Iteration 91 took 2.17 seconds (mean sampled reward: -4561.74). Current reward after update: -1031.46, Optimal reward -627.56
Iteration 92 took 2.10 seconds (mean sampled reward: -5453.34). Current reward after update: -919.40, Optimal reward -627.56
Iteration 93 took 2.18 seconds (mean sampled reward: -4792.16). Current reward after update: -954.50, Optimal reward -627.56
Iteration 94 took 2.13 seconds (mean sampled reward: -4989.08). Current reward after update: -876.17, Optimal reward -627.56
Iteration 95 took 2.19 seconds (mean sampled reward: -5695.05). Current reward after update: -955.52, Optimal reward -627.56
Iteration 96 took 2.16 seconds (mean sampled reward: -6355.37). Current reward after update: -760.48, Optimal reward -627.56
Iteration 97 took 2.12 seconds (mean sampled reward: -6351.45). Current reward after update: -884.63, Optimal reward -627.56
Iteration 98 took 2.14 seconds (mean sampled reward: -5893.73). Current reward after update: -788.91, Optimal reward -627.56
Iteration 99 took 2.04 seconds (mean sampled reward: -6048.77). Current reward after update: -1334.30, Optimal reward -627.56
Iteration 100 took 2.12 seconds (mean sampled reward: -5487.54). Current reward after update: -1021.11, Optimal reward -627.56
Iteration 101 took 2.03 seconds (mean sampled reward: -5949.13). Current reward after update: -1077.81, Optimal reward -627.56
Iteration 102 took 2.09 seconds (mean sampled reward: -6406.33). Current reward after update: -1075.50, Optimal reward -627.56
Iteration 103 took 2.10 seconds (mean sampled reward: -6320.33). Current reward after update: -1214.81, Optimal reward -627.56
Iteration 104 took 2.18 seconds (mean sampled reward: -5635.12). Current reward after update: -1117.62, Optimal reward -627.56
Iteration 105 took 2.11 seconds (mean sampled reward: -5365.58). Current reward after update: -1111.58, Optimal reward -627.56
Iteration 106 took 2.26 seconds (mean sampled reward: -5598.63). Current reward after update: -1057.96, Optimal reward -627.56
Iteration 107 took 2.10 seconds (mean sampled reward: -5499.70). Current reward after update: -1119.57, Optimal reward -627.56
Iteration 108 took 2.15 seconds (mean sampled reward: -6404.97). Current reward after update: -1164.94, Optimal reward -627.56
Iteration 109 took 2.21 seconds (mean sampled reward: -6538.81). Current reward after update: -1348.16, Optimal reward -627.56
Iteration 110 took 2.16 seconds (mean sampled reward: -5792.78). Current reward after update: -1142.45, Optimal reward -627.56
Iteration 111 took 2.05 seconds (mean sampled reward: -6827.35). Current reward after update: -1214.95, Optimal reward -627.56
Iteration 112 took 2.31 seconds (mean sampled reward: -6810.35). Current reward after update: -1290.80, Optimal reward -627.56
Iteration 113 took 2.21 seconds (mean sampled reward: -6053.37). Current reward after update: -1143.65, Optimal reward -627.56
Iteration 114 took 2.28 seconds (mean sampled reward: -5691.84). Current reward after update: -1118.06, Optimal reward -627.56
Iteration 115 took 2.17 seconds (mean sampled reward: -5635.47). Current reward after update: -1127.47, Optimal reward -627.56
Iteration 116 took 2.27 seconds (mean sampled reward: -6303.22). Current reward after update: -1008.38, Optimal reward -627.56
Iteration 117 took 2.16 seconds (mean sampled reward: -5936.74). Current reward after update: -1135.74, Optimal reward -627.56
Iteration 118 took 2.34 seconds (mean sampled reward: -5954.76). Current reward after update: -1271.05, Optimal reward -627.56
Iteration 119 took 2.25 seconds (mean sampled reward: -5823.08). Current reward after update: -1176.89, Optimal reward -627.56
Iteration 120 took 2.10 seconds (mean sampled reward: -6678.61). Current reward after update: -1254.02, Optimal reward -627.56
Iteration 121 took 2.21 seconds (mean sampled reward: -6601.58). Current reward after update: -1263.28, Optimal reward -627.56
Iteration 122 took 2.12 seconds (mean sampled reward: -6644.77). Current reward after update: -1131.47, Optimal reward -627.56
Iteration 123 took 2.21 seconds (mean sampled reward: -6121.83). Current reward after update: -1075.99, Optimal reward -627.56
Iteration 124 took 2.15 seconds (mean sampled reward: -5960.03). Current reward after update: -1160.57, Optimal reward -627.56
Iteration 125 took 2.18 seconds (mean sampled reward: -5857.91). Current reward after update: -1148.73, Optimal reward -627.56
Iteration 126 took 2.12 seconds (mean sampled reward: -4872.15). Current reward after update: -1039.52, Optimal reward -627.56
Iteration 127 took 2.09 seconds (mean sampled reward: -5729.89). Current reward after update: -972.29, Optimal reward -627.56
Iteration 128 took 2.10 seconds (mean sampled reward: -5262.04). Current reward after update: -1047.30, Optimal reward -627.56
Iteration 129 took 2.12 seconds (mean sampled reward: -5703.09). Current reward after update: -1003.68, Optimal reward -627.56
Iteration 130 took 2.20 seconds (mean sampled reward: -4769.10). Current reward after update: -972.54, Optimal reward -627.56
Iteration 131 took 2.17 seconds (mean sampled reward: -5830.39). Current reward after update: -960.02, Optimal reward -627.56
Iteration 132 took 2.26 seconds (mean sampled reward: -6218.18). Current reward after update: -1016.06, Optimal reward -627.56
Iteration 133 took 2.22 seconds (mean sampled reward: -6505.37). Current reward after update: -944.38, Optimal reward -627.56
Iteration 134 took 2.14 seconds (mean sampled reward: -6000.63). Current reward after update: -977.69, Optimal reward -627.56
Iteration 135 took 2.22 seconds (mean sampled reward: -5263.29). Current reward after update: -949.56, Optimal reward -627.56
Iteration 136 took 2.16 seconds (mean sampled reward: -5567.94). Current reward after update: -942.32, Optimal reward -627.56
Iteration 137 took 2.14 seconds (mean sampled reward: -5932.80). Current reward after update: -1100.35, Optimal reward -627.56
Iteration 138 took 2.21 seconds (mean sampled reward: -5605.67). Current reward after update: -6083.77, Optimal reward -627.56
Iteration 139 took 2.25 seconds (mean sampled reward: -5565.40). Current reward after update: -1046.82, Optimal reward -627.56
Iteration 140 took 2.18 seconds (mean sampled reward: -5486.36). Current reward after update: -1104.51, Optimal reward -627.56
Iteration 141 took 2.27 seconds (mean sampled reward: -5540.90). Current reward after update: -1018.31, Optimal reward -627.56
Iteration 142 took 2.19 seconds (mean sampled reward: -5860.69). Current reward after update: -880.65, Optimal reward -627.56
Iteration 143 took 2.22 seconds (mean sampled reward: -6509.79). Current reward after update: -978.81, Optimal reward -627.56
Iteration 144 took 2.19 seconds (mean sampled reward: -6510.61). Current reward after update: -1071.79, Optimal reward -627.56
Iteration 145 took 2.18 seconds (mean sampled reward: -6549.52). Current reward after update: -963.15, Optimal reward -627.56
Iteration 146 took 2.22 seconds (mean sampled reward: -5687.62). Current reward after update: -1096.82, Optimal reward -627.56
Iteration 147 took 2.22 seconds (mean sampled reward: -5581.10). Current reward after update: -933.50, Optimal reward -627.56
Iteration 148 took 2.17 seconds (mean sampled reward: -5162.63). Current reward after update: -892.24, Optimal reward -627.56
Iteration 149 took 2.21 seconds (mean sampled reward: -5365.56). Current reward after update: -852.45, Optimal reward -627.56
Iteration 150 took 2.25 seconds (mean sampled reward: -5478.35). Current reward after update: -860.48, Optimal reward -627.56
Iteration 151 took 2.23 seconds (mean sampled reward: -5536.91). Current reward after update: -981.70, Optimal reward -627.56
Iteration 152 took 2.23 seconds (mean sampled reward: -5638.97). Current reward after update: -959.08, Optimal reward -627.56
Iteration 153 took 2.23 seconds (mean sampled reward: -5497.56). Current reward after update: -837.51, Optimal reward -627.56
Iteration 154 took 2.24 seconds (mean sampled reward: -5296.18). Current reward after update: -1027.28, Optimal reward -627.56
Iteration 155 took 2.21 seconds (mean sampled reward: -4613.77). Current reward after update: -876.36, Optimal reward -627.56
Iteration 156 took 2.22 seconds (mean sampled reward: -4560.19). Current reward after update: -6047.40, Optimal reward -627.56
Iteration 157 took 2.22 seconds (mean sampled reward: -4445.90). Current reward after update: -786.43, Optimal reward -627.56
Iteration 158 took 2.23 seconds (mean sampled reward: -4957.49). Current reward after update: -817.53, Optimal reward -627.56
Iteration 159 took 2.27 seconds (mean sampled reward: -5008.43). Current reward after update: -2134.76, Optimal reward -627.56
Iteration 160 took 2.30 seconds (mean sampled reward: -4286.32). Current reward after update: -786.20, Optimal reward -627.56
Iteration 161 took 2.20 seconds (mean sampled reward: -4570.22). Current reward after update: -886.20, Optimal reward -627.56
Iteration 162 took 2.29 seconds (mean sampled reward: -4162.34). Current reward after update: -914.19, Optimal reward -627.56
Iteration 163 took 2.32 seconds (mean sampled reward: -4212.87). Current reward after update: -749.46, Optimal reward -627.56
Iteration 164 took 2.28 seconds (mean sampled reward: -4380.94). Current reward after update: -724.38, Optimal reward -627.56
Iteration 165 took 2.29 seconds (mean sampled reward: -4081.88). Current reward after update: -2736.51, Optimal reward -627.56
Iteration 166 took 2.29 seconds (mean sampled reward: -4163.69). Current reward after update: -1225.73, Optimal reward -627.56
Iteration 167 took 2.22 seconds (mean sampled reward: -4227.00). Current reward after update: -717.94, Optimal reward -627.56
Iteration 168 took 2.24 seconds (mean sampled reward: -4406.55). Current reward after update: -744.91, Optimal reward -627.56
Iteration 169 took 2.29 seconds (mean sampled reward: -4064.40). Current reward after update: -3101.49, Optimal reward -627.56
Iteration 170 took 2.20 seconds (mean sampled reward: -4419.93). Current reward after update: -706.83, Optimal reward -627.56
Iteration 171 took 2.31 seconds (mean sampled reward: -4381.14). Current reward after update: -827.48, Optimal reward -627.56
Iteration 172 took 2.21 seconds (mean sampled reward: -4796.13). Current reward after update: -784.06, Optimal reward -627.56
Iteration 173 took 2.29 seconds (mean sampled reward: -3999.05). Current reward after update: -732.11, Optimal reward -627.56
Iteration 174 took 2.26 seconds (mean sampled reward: -3612.94). Current reward after update: -1111.47, Optimal reward -627.56
Iteration 175 took 2.21 seconds (mean sampled reward: -4015.56). Current reward after update: -759.23, Optimal reward -627.56
Iteration 176 took 2.17 seconds (mean sampled reward: -4207.98). Current reward after update: -821.75, Optimal reward -627.56
Iteration 177 took 2.25 seconds (mean sampled reward: -3805.60). Current reward after update: -928.46, Optimal reward -627.56
Iteration 178 took 2.19 seconds (mean sampled reward: -4022.12). Current reward after update: -5854.30, Optimal reward -627.56
Iteration 179 took 2.36 seconds (mean sampled reward: -4153.91). Current reward after update: -697.97, Optimal reward -627.56
Iteration 180 took 2.24 seconds (mean sampled reward: -3745.30). Current reward after update: -773.49, Optimal reward -627.56
Iteration 181 took 2.21 seconds (mean sampled reward: -4062.54). Current reward after update: -1153.31, Optimal reward -627.56
Iteration 182 took 2.18 seconds (mean sampled reward: -3859.72). Current reward after update: -720.28, Optimal reward -627.56
Iteration 183 took 2.35 seconds (mean sampled reward: -4764.15). Current reward after update: -699.25, Optimal reward -627.56
Iteration 184 took 2.27 seconds (mean sampled reward: -3651.90). Current reward after update: -1231.32, Optimal reward -627.56
Iteration 185 took 2.24 seconds (mean sampled reward: -3623.15). Current reward after update: -767.73, Optimal reward -627.56
Iteration 186 took 2.23 seconds (mean sampled reward: -3683.89). Current reward after update: -856.65, Optimal reward -627.56
Iteration 187 took 2.20 seconds (mean sampled reward: -3370.08). Current reward after update: -689.99, Optimal reward -627.56
Iteration 188 took 2.20 seconds (mean sampled reward: -4030.84). Current reward after update: -705.78, Optimal reward -627.56
Iteration 189 took 2.24 seconds (mean sampled reward: -4037.45). Current reward after update: -903.06, Optimal reward -627.56
Iteration 190 took 2.23 seconds (mean sampled reward: -3802.16). Current reward after update: -698.44, Optimal reward -627.56
Iteration 191 took 2.24 seconds (mean sampled reward: -3758.36). Current reward after update: -786.65, Optimal reward -627.56
Iteration 192 took 2.29 seconds (mean sampled reward: -3481.46). Current reward after update: -712.33, Optimal reward -627.56
Iteration 193 took 2.23 seconds (mean sampled reward: -3576.05). Current reward after update: -826.20, Optimal reward -627.56
Iteration 194 took 2.19 seconds (mean sampled reward: -4204.56). Current reward after update: -766.00, Optimal reward -627.56
Iteration 195 took 2.21 seconds (mean sampled reward: -3576.20). Current reward after update: -752.91, Optimal reward -627.56
Iteration 196 took 2.19 seconds (mean sampled reward: -3526.32). Current reward after update: -720.75, Optimal reward -627.56
Iteration 197 took 2.21 seconds (mean sampled reward: -3584.51). Current reward after update: -1655.50, Optimal reward -627.56
Iteration 198 took 2.19 seconds (mean sampled reward: -3551.80). Current reward after update: -6136.57, Optimal reward -627.56
Iteration 199 took 2.22 seconds (mean sampled reward: -3598.80). Current reward after update: -709.10, Optimal reward -627.56
Iteration 200 took 2.19 seconds (mean sampled reward: -4514.84). Current reward after update: -761.61, Optimal reward -627.56
Max force: 20 Sigma: 0.4 mean rewards: -1162.1611042785303, best rewards:-627.5572591914822

Iteration 1 took 2.26 seconds (mean sampled reward: -7467.76). Current reward after update: -5843.37, Optimal reward -5843.37
Iteration 2 took 2.21 seconds (mean sampled reward: -7166.10). Current reward after update: -4818.37, Optimal reward -4818.37
Iteration 3 took 2.25 seconds (mean sampled reward: -6416.44). Current reward after update: -2938.65, Optimal reward -2938.65
Iteration 4 took 2.19 seconds (mean sampled reward: -6029.94). Current reward after update: -2757.48, Optimal reward -2757.48
Iteration 5 took 2.26 seconds (mean sampled reward: -4713.01). Current reward after update: -1977.52, Optimal reward -1977.52
Iteration 6 took 2.28 seconds (mean sampled reward: -5086.39). Current reward after update: -1375.52, Optimal reward -1375.52
Iteration 7 took 2.20 seconds (mean sampled reward: -3876.76). Current reward after update: -1189.50, Optimal reward -1189.50
Iteration 8 took 2.23 seconds (mean sampled reward: -4410.48). Current reward after update: -1405.29, Optimal reward -1189.50
Iteration 9 took 2.23 seconds (mean sampled reward: -3722.60). Current reward after update: -1032.91, Optimal reward -1032.91
Iteration 10 took 2.16 seconds (mean sampled reward: -3204.73). Current reward after update: -1368.75, Optimal reward -1032.91
Iteration 11 took 2.14 seconds (mean sampled reward: -3555.23). Current reward after update: -1042.60, Optimal reward -1032.91
Iteration 12 took 2.16 seconds (mean sampled reward: -3831.10). Current reward after update: -1113.47, Optimal reward -1032.91
Iteration 13 took 2.21 seconds (mean sampled reward: -5571.43). Current reward after update: -1041.13, Optimal reward -1032.91
Iteration 14 took 2.43 seconds (mean sampled reward: -5009.82). Current reward after update: -1322.02, Optimal reward -1032.91
Iteration 15 took 2.19 seconds (mean sampled reward: -5597.50). Current reward after update: -1191.51, Optimal reward -1032.91
Iteration 16 took 2.27 seconds (mean sampled reward: -5367.30). Current reward after update: -946.66, Optimal reward -946.66
Iteration 17 took 2.19 seconds (mean sampled reward: -5032.86). Current reward after update: -1411.37, Optimal reward -946.66
Iteration 18 took 2.24 seconds (mean sampled reward: -5478.04). Current reward after update: -1386.86, Optimal reward -946.66
Iteration 19 took 2.12 seconds (mean sampled reward: -5495.28). Current reward after update: -955.28, Optimal reward -946.66
Iteration 20 took 2.16 seconds (mean sampled reward: -5471.26). Current reward after update: -1006.81, Optimal reward -946.66
Iteration 21 took 2.18 seconds (mean sampled reward: -3544.86). Current reward after update: -968.67, Optimal reward -946.66
Iteration 22 took 2.10 seconds (mean sampled reward: -3122.26). Current reward after update: -1073.06, Optimal reward -946.66
Iteration 23 took 2.11 seconds (mean sampled reward: -3554.83). Current reward after update: -909.00, Optimal reward -909.00
Iteration 24 took 2.15 seconds (mean sampled reward: -5098.66). Current reward after update: -1037.35, Optimal reward -909.00
Iteration 25 took 2.21 seconds (mean sampled reward: -2477.26). Current reward after update: -717.14, Optimal reward -717.14
Iteration 26 took 2.16 seconds (mean sampled reward: -2941.29). Current reward after update: -514.77, Optimal reward -514.77
Iteration 27 took 2.41 seconds (mean sampled reward: -3323.40). Current reward after update: -561.66, Optimal reward -514.77
Iteration 28 took 2.20 seconds (mean sampled reward: -3585.98). Current reward after update: -538.62, Optimal reward -514.77
Iteration 29 took 2.19 seconds (mean sampled reward: -2617.20). Current reward after update: -735.09, Optimal reward -514.77
Iteration 30 took 2.15 seconds (mean sampled reward: -3002.45). Current reward after update: -651.13, Optimal reward -514.77
Iteration 31 took 2.09 seconds (mean sampled reward: -4490.20). Current reward after update: -440.41, Optimal reward -440.41
Iteration 32 took 2.27 seconds (mean sampled reward: -3424.19). Current reward after update: -440.37, Optimal reward -440.37
Iteration 33 took 2.27 seconds (mean sampled reward: -3970.73). Current reward after update: -515.90, Optimal reward -440.37
Iteration 34 took 2.07 seconds (mean sampled reward: -4895.63). Current reward after update: -616.97, Optimal reward -440.37
Iteration 35 took 2.28 seconds (mean sampled reward: -3844.71). Current reward after update: -503.37, Optimal reward -440.37
Iteration 36 took 2.14 seconds (mean sampled reward: -4554.13). Current reward after update: -591.35, Optimal reward -440.37
Iteration 37 took 2.07 seconds (mean sampled reward: -4598.00). Current reward after update: -532.35, Optimal reward -440.37
Iteration 38 took 2.08 seconds (mean sampled reward: -4535.06). Current reward after update: -6844.89, Optimal reward -440.37
Iteration 39 took 2.11 seconds (mean sampled reward: -5354.69). Current reward after update: -556.57, Optimal reward -440.37
Iteration 40 took 2.06 seconds (mean sampled reward: -5455.66). Current reward after update: -741.61, Optimal reward -440.37
Iteration 41 took 2.14 seconds (mean sampled reward: -5787.45). Current reward after update: -5732.30, Optimal reward -440.37
Iteration 42 took 2.13 seconds (mean sampled reward: -6367.13). Current reward after update: -1511.89, Optimal reward -440.37
Iteration 43 took 2.00 seconds (mean sampled reward: -5922.19). Current reward after update: -629.96, Optimal reward -440.37
Iteration 44 took 2.06 seconds (mean sampled reward: -5455.51). Current reward after update: -562.20, Optimal reward -440.37
Iteration 45 took 2.09 seconds (mean sampled reward: -5732.76). Current reward after update: -608.70, Optimal reward -440.37
Iteration 46 took 2.05 seconds (mean sampled reward: -4624.61). Current reward after update: -491.15, Optimal reward -440.37
Iteration 47 took 2.09 seconds (mean sampled reward: -6031.23). Current reward after update: -1049.93, Optimal reward -440.37
Iteration 48 took 2.05 seconds (mean sampled reward: -4818.01). Current reward after update: -930.65, Optimal reward -440.37
Iteration 49 took 2.04 seconds (mean sampled reward: -5540.01). Current reward after update: -762.98, Optimal reward -440.37
Iteration 50 took 2.03 seconds (mean sampled reward: -5304.42). Current reward after update: -618.74, Optimal reward -440.37
Iteration 51 took 2.01 seconds (mean sampled reward: -5604.17). Current reward after update: -994.66, Optimal reward -440.37
Iteration 52 took 2.05 seconds (mean sampled reward: -4740.54). Current reward after update: -523.97, Optimal reward -440.37
Iteration 53 took 2.08 seconds (mean sampled reward: -5371.42). Current reward after update: -725.39, Optimal reward -440.37
Iteration 54 took 2.35 seconds (mean sampled reward: -4908.19). Current reward after update: -748.69, Optimal reward -440.37
Iteration 55 took 2.05 seconds (mean sampled reward: -5961.31). Current reward after update: -638.25, Optimal reward -440.37
Iteration 56 took 2.10 seconds (mean sampled reward: -6762.01). Current reward after update: -719.92, Optimal reward -440.37
Iteration 57 took 2.29 seconds (mean sampled reward: -6763.81). Current reward after update: -983.22, Optimal reward -440.37
Iteration 58 took 2.20 seconds (mean sampled reward: -6756.14). Current reward after update: -654.85, Optimal reward -440.37
Iteration 59 took 2.22 seconds (mean sampled reward: -6833.50). Current reward after update: -763.10, Optimal reward -440.37
Iteration 60 took 2.03 seconds (mean sampled reward: -6456.39). Current reward after update: -421.84, Optimal reward -421.84
Iteration 61 took 2.16 seconds (mean sampled reward: -6765.08). Current reward after update: -686.52, Optimal reward -421.84
Iteration 62 took 2.08 seconds (mean sampled reward: -6436.55). Current reward after update: -402.35, Optimal reward -402.35
Iteration 63 took 2.04 seconds (mean sampled reward: -7133.08). Current reward after update: -840.26, Optimal reward -402.35
Iteration 64 took 2.25 seconds (mean sampled reward: -7068.20). Current reward after update: -1200.66, Optimal reward -402.35
Iteration 65 took 2.18 seconds (mean sampled reward: -6421.80). Current reward after update: -1074.24, Optimal reward -402.35
Iteration 66 took 2.16 seconds (mean sampled reward: -6937.14). Current reward after update: -752.66, Optimal reward -402.35
Iteration 67 took 2.10 seconds (mean sampled reward: -6940.98). Current reward after update: -648.98, Optimal reward -402.35
Iteration 68 took 2.08 seconds (mean sampled reward: -5893.58). Current reward after update: -478.64, Optimal reward -402.35
Iteration 69 took 2.06 seconds (mean sampled reward: -6427.44). Current reward after update: -958.31, Optimal reward -402.35
Iteration 70 took 2.07 seconds (mean sampled reward: -6619.40). Current reward after update: -968.48, Optimal reward -402.35
Iteration 71 took 2.06 seconds (mean sampled reward: -6502.03). Current reward after update: -412.17, Optimal reward -402.35
Iteration 72 took 2.09 seconds (mean sampled reward: -6252.19). Current reward after update: -778.04, Optimal reward -402.35
Iteration 73 took 2.00 seconds (mean sampled reward: -6436.39). Current reward after update: -643.11, Optimal reward -402.35
Iteration 74 took 2.04 seconds (mean sampled reward: -6585.52). Current reward after update: -752.18, Optimal reward -402.35
Iteration 75 took 2.14 seconds (mean sampled reward: -6557.91). Current reward after update: -726.97, Optimal reward -402.35
Iteration 76 took 2.03 seconds (mean sampled reward: -6937.54). Current reward after update: -682.63, Optimal reward -402.35
Iteration 77 took 2.13 seconds (mean sampled reward: -6706.35). Current reward after update: -615.34, Optimal reward -402.35
Iteration 78 took 1.97 seconds (mean sampled reward: -6595.66). Current reward after update: -765.65, Optimal reward -402.35
Iteration 79 took 2.08 seconds (mean sampled reward: -5662.83). Current reward after update: -482.39, Optimal reward -402.35
Iteration 80 took 2.08 seconds (mean sampled reward: -5633.08). Current reward after update: -539.53, Optimal reward -402.35
Iteration 81 took 2.07 seconds (mean sampled reward: -6118.88). Current reward after update: -731.04, Optimal reward -402.35
Iteration 82 took 2.05 seconds (mean sampled reward: -5532.82). Current reward after update: -1077.96, Optimal reward -402.35
Iteration 83 took 2.20 seconds (mean sampled reward: -5807.56). Current reward after update: -1010.05, Optimal reward -402.35
Iteration 84 took 2.11 seconds (mean sampled reward: -5002.96). Current reward after update: -875.09, Optimal reward -402.35
Iteration 85 took 2.01 seconds (mean sampled reward: -6205.90). Current reward after update: -705.85, Optimal reward -402.35
Iteration 86 took 2.10 seconds (mean sampled reward: -6058.93). Current reward after update: -677.62, Optimal reward -402.35
Iteration 87 took 2.07 seconds (mean sampled reward: -5442.60). Current reward after update: -535.42, Optimal reward -402.35
Iteration 88 took 2.15 seconds (mean sampled reward: -5184.21). Current reward after update: -555.57, Optimal reward -402.35
Iteration 89 took 2.08 seconds (mean sampled reward: -5469.92). Current reward after update: -338.55, Optimal reward -338.55
Iteration 90 took 2.11 seconds (mean sampled reward: -5372.55). Current reward after update: -540.97, Optimal reward -338.55
Iteration 91 took 2.16 seconds (mean sampled reward: -5261.50). Current reward after update: -735.52, Optimal reward -338.55
Iteration 92 took 2.12 seconds (mean sampled reward: -4629.84). Current reward after update: -624.23, Optimal reward -338.55
Iteration 93 took 2.11 seconds (mean sampled reward: -4669.45). Current reward after update: -645.24, Optimal reward -338.55
Iteration 94 took 2.12 seconds (mean sampled reward: -4920.57). Current reward after update: -621.31, Optimal reward -338.55
Iteration 95 took 2.21 seconds (mean sampled reward: -4245.67). Current reward after update: -760.11, Optimal reward -338.55
Iteration 96 took 2.06 seconds (mean sampled reward: -4686.32). Current reward after update: -869.97, Optimal reward -338.55
Iteration 97 took 2.15 seconds (mean sampled reward: -4708.70). Current reward after update: -748.51, Optimal reward -338.55
Iteration 98 took 2.15 seconds (mean sampled reward: -5600.30). Current reward after update: -845.65, Optimal reward -338.55
Iteration 99 took 2.08 seconds (mean sampled reward: -5787.30). Current reward after update: -869.28, Optimal reward -338.55
Iteration 100 took 2.13 seconds (mean sampled reward: -6121.44). Current reward after update: -811.61, Optimal reward -338.55
Iteration 101 took 2.17 seconds (mean sampled reward: -5109.68). Current reward after update: -760.70, Optimal reward -338.55
Iteration 102 took 2.25 seconds (mean sampled reward: -3698.77). Current reward after update: -1549.05, Optimal reward -338.55
Iteration 103 took 2.26 seconds (mean sampled reward: -4182.46). Current reward after update: -846.41, Optimal reward -338.55
Iteration 104 took 2.17 seconds (mean sampled reward: -3870.01). Current reward after update: -560.98, Optimal reward -338.55
Iteration 105 took 2.18 seconds (mean sampled reward: -5091.19). Current reward after update: -1672.78, Optimal reward -338.55
Iteration 106 took 2.20 seconds (mean sampled reward: -5127.07). Current reward after update: -650.00, Optimal reward -338.55
Iteration 107 took 2.30 seconds (mean sampled reward: -4482.52). Current reward after update: -649.67, Optimal reward -338.55
Iteration 108 took 2.12 seconds (mean sampled reward: -5079.90). Current reward after update: -673.32, Optimal reward -338.55
Iteration 109 took 2.04 seconds (mean sampled reward: -6655.63). Current reward after update: -834.72, Optimal reward -338.55
Iteration 110 took 2.22 seconds (mean sampled reward: -5268.98). Current reward after update: -830.88, Optimal reward -338.55
Iteration 111 took 2.30 seconds (mean sampled reward: -5725.90). Current reward after update: -583.36, Optimal reward -338.55
Iteration 112 took 2.04 seconds (mean sampled reward: -6168.76). Current reward after update: -643.80, Optimal reward -338.55
Iteration 113 took 2.04 seconds (mean sampled reward: -6125.64). Current reward after update: -601.45, Optimal reward -338.55
Iteration 114 took 2.12 seconds (mean sampled reward: -6582.21). Current reward after update: -734.39, Optimal reward -338.55
Iteration 115 took 2.03 seconds (mean sampled reward: -6298.51). Current reward after update: -637.06, Optimal reward -338.55
Iteration 116 took 2.19 seconds (mean sampled reward: -6138.44). Current reward after update: -770.45, Optimal reward -338.55
Iteration 117 took 2.06 seconds (mean sampled reward: -5988.70). Current reward after update: -1490.07, Optimal reward -338.55
Iteration 118 took 2.23 seconds (mean sampled reward: -4451.15). Current reward after update: -873.64, Optimal reward -338.55
Iteration 119 took 2.22 seconds (mean sampled reward: -3376.40). Current reward after update: -512.41, Optimal reward -338.55
Iteration 120 took 2.25 seconds (mean sampled reward: -3587.28). Current reward after update: -543.66, Optimal reward -338.55
Iteration 121 took 2.21 seconds (mean sampled reward: -3300.10). Current reward after update: -528.82, Optimal reward -338.55
Iteration 122 took 2.16 seconds (mean sampled reward: -3439.34). Current reward after update: -468.17, Optimal reward -338.55
Iteration 123 took 2.25 seconds (mean sampled reward: -3113.94). Current reward after update: -798.58, Optimal reward -338.55
Iteration 124 took 2.24 seconds (mean sampled reward: -2884.96). Current reward after update: -731.50, Optimal reward -338.55
Iteration 125 took 2.10 seconds (mean sampled reward: -4099.74). Current reward after update: -870.16, Optimal reward -338.55
Iteration 126 took 2.14 seconds (mean sampled reward: -4110.73). Current reward after update: -800.59, Optimal reward -338.55
Iteration 127 took 2.21 seconds (mean sampled reward: -4376.42). Current reward after update: -969.01, Optimal reward -338.55
Iteration 128 took 2.18 seconds (mean sampled reward: -3506.30). Current reward after update: -908.33, Optimal reward -338.55
Iteration 129 took 2.17 seconds (mean sampled reward: -3224.25). Current reward after update: -846.22, Optimal reward -338.55
Iteration 130 took 2.22 seconds (mean sampled reward: -3358.67). Current reward after update: -1101.79, Optimal reward -338.55
Iteration 131 took 2.15 seconds (mean sampled reward: -3919.86). Current reward after update: -1326.21, Optimal reward -338.55
Iteration 132 took 2.12 seconds (mean sampled reward: -4694.78). Current reward after update: -1219.58, Optimal reward -338.55
Iteration 133 took 2.20 seconds (mean sampled reward: -3827.73). Current reward after update: -1237.04, Optimal reward -338.55
Iteration 134 took 2.17 seconds (mean sampled reward: -4118.23). Current reward after update: -1418.40, Optimal reward -338.55
Iteration 135 took 2.27 seconds (mean sampled reward: -4608.90). Current reward after update: -860.08, Optimal reward -338.55
Iteration 136 took 2.27 seconds (mean sampled reward: -3213.28). Current reward after update: -779.53, Optimal reward -338.55
Iteration 137 took 2.21 seconds (mean sampled reward: -3331.50). Current reward after update: -1957.07, Optimal reward -338.55
Iteration 138 took 2.14 seconds (mean sampled reward: -5344.15). Current reward after update: -875.18, Optimal reward -338.55
Iteration 139 took 2.22 seconds (mean sampled reward: -2894.36). Current reward after update: -2415.21, Optimal reward -338.55
Iteration 140 took 2.17 seconds (mean sampled reward: -3070.90). Current reward after update: -698.49, Optimal reward -338.55
Iteration 141 took 2.14 seconds (mean sampled reward: -2818.37). Current reward after update: -1135.38, Optimal reward -338.55
Iteration 142 took 2.20 seconds (mean sampled reward: -2608.96). Current reward after update: -889.01, Optimal reward -338.55
Iteration 143 took 2.28 seconds (mean sampled reward: -3119.11). Current reward after update: -921.48, Optimal reward -338.55
Iteration 144 took 2.26 seconds (mean sampled reward: -3586.95). Current reward after update: -894.07, Optimal reward -338.55
Iteration 145 took 2.19 seconds (mean sampled reward: -3793.77). Current reward after update: -866.26, Optimal reward -338.55
Iteration 146 took 2.12 seconds (mean sampled reward: -5281.11). Current reward after update: -763.45, Optimal reward -338.55
Iteration 147 took 2.17 seconds (mean sampled reward: -5038.92). Current reward after update: -738.88, Optimal reward -338.55
Iteration 148 took 2.12 seconds (mean sampled reward: -5631.38). Current reward after update: -7030.94, Optimal reward -338.55
Iteration 149 took 2.15 seconds (mean sampled reward: -5603.62). Current reward after update: -737.37, Optimal reward -338.55
Iteration 150 took 2.11 seconds (mean sampled reward: -6136.16). Current reward after update: -670.57, Optimal reward -338.55
Iteration 151 took 2.13 seconds (mean sampled reward: -4828.05). Current reward after update: -549.32, Optimal reward -338.55
Iteration 152 took 2.14 seconds (mean sampled reward: -4692.73). Current reward after update: -631.07, Optimal reward -338.55
Iteration 153 took 2.25 seconds (mean sampled reward: -3740.37). Current reward after update: -519.27, Optimal reward -338.55
Iteration 154 took 2.20 seconds (mean sampled reward: -3089.63). Current reward after update: -7013.83, Optimal reward -338.55
Iteration 155 took 2.14 seconds (mean sampled reward: -4203.05). Current reward after update: -1662.68, Optimal reward -338.55
Iteration 156 took 2.18 seconds (mean sampled reward: -3290.56). Current reward after update: -676.69, Optimal reward -338.55
Iteration 157 took 2.22 seconds (mean sampled reward: -4691.62). Current reward after update: -544.25, Optimal reward -338.55
Iteration 158 took 2.21 seconds (mean sampled reward: -4330.98). Current reward after update: -567.09, Optimal reward -338.55
Iteration 159 took 2.15 seconds (mean sampled reward: -3105.51). Current reward after update: -525.66, Optimal reward -338.55
Iteration 160 took 2.16 seconds (mean sampled reward: -2494.95). Current reward after update: -514.28, Optimal reward -338.55
Iteration 161 took 2.19 seconds (mean sampled reward: -2509.64). Current reward after update: -515.22, Optimal reward -338.55
Iteration 162 took 2.20 seconds (mean sampled reward: -2393.71). Current reward after update: -567.61, Optimal reward -338.55
Iteration 163 took 2.26 seconds (mean sampled reward: -2108.27). Current reward after update: -534.49, Optimal reward -338.55
Iteration 164 took 2.29 seconds (mean sampled reward: -2395.81). Current reward after update: -468.94, Optimal reward -338.55
Iteration 165 took 2.23 seconds (mean sampled reward: -2290.60). Current reward after update: -455.09, Optimal reward -338.55
Iteration 166 took 2.25 seconds (mean sampled reward: -2397.60). Current reward after update: -417.44, Optimal reward -338.55
Iteration 167 took 2.27 seconds (mean sampled reward: -2871.06). Current reward after update: -457.26, Optimal reward -338.55
Iteration 168 took 2.27 seconds (mean sampled reward: -2333.97). Current reward after update: -433.11, Optimal reward -338.55
Iteration 169 took 2.18 seconds (mean sampled reward: -2353.07). Current reward after update: -1061.05, Optimal reward -338.55
Iteration 170 took 2.31 seconds (mean sampled reward: -2298.32). Current reward after update: -412.11, Optimal reward -338.55
Iteration 171 took 2.23 seconds (mean sampled reward: -2529.81). Current reward after update: -489.29, Optimal reward -338.55
Iteration 172 took 2.25 seconds (mean sampled reward: -2561.16). Current reward after update: -513.16, Optimal reward -338.55
Iteration 173 took 2.25 seconds (mean sampled reward: -2326.76). Current reward after update: -1450.65, Optimal reward -338.55
Iteration 174 took 2.15 seconds (mean sampled reward: -3295.79). Current reward after update: -443.60, Optimal reward -338.55
Iteration 175 took 2.20 seconds (mean sampled reward: -2954.57). Current reward after update: -541.52, Optimal reward -338.55
Iteration 176 took 2.26 seconds (mean sampled reward: -3274.78). Current reward after update: -452.86, Optimal reward -338.55
Iteration 177 took 2.09 seconds (mean sampled reward: -5017.79). Current reward after update: -1910.84, Optimal reward -338.55
Iteration 178 took 2.15 seconds (mean sampled reward: -3872.85). Current reward after update: -518.52, Optimal reward -338.55
Iteration 179 took 2.17 seconds (mean sampled reward: -5931.43). Current reward after update: -765.72, Optimal reward -338.55
Iteration 180 took 2.07 seconds (mean sampled reward: -5474.22). Current reward after update: -575.86, Optimal reward -338.55
Iteration 181 took 2.14 seconds (mean sampled reward: -4231.29). Current reward after update: -509.20, Optimal reward -338.55
Iteration 182 took 2.11 seconds (mean sampled reward: -4985.03). Current reward after update: -530.73, Optimal reward -338.55
Iteration 183 took 2.20 seconds (mean sampled reward: -4237.41). Current reward after update: -5753.60, Optimal reward -338.55
Iteration 184 took 2.17 seconds (mean sampled reward: -4506.63). Current reward after update: -505.22, Optimal reward -338.55
Iteration 185 took 2.16 seconds (mean sampled reward: -2929.54). Current reward after update: -506.70, Optimal reward -338.55
Iteration 186 took 2.21 seconds (mean sampled reward: -3205.30). Current reward after update: -652.57, Optimal reward -338.55
Iteration 187 took 2.24 seconds (mean sampled reward: -3162.91). Current reward after update: -492.69, Optimal reward -338.55
Iteration 188 took 2.22 seconds (mean sampled reward: -3661.60). Current reward after update: -656.23, Optimal reward -338.55
Iteration 189 took 2.18 seconds (mean sampled reward: -2728.83). Current reward after update: -484.38, Optimal reward -338.55
Iteration 190 took 2.14 seconds (mean sampled reward: -5242.47). Current reward after update: -592.88, Optimal reward -338.55
Iteration 191 took 2.17 seconds (mean sampled reward: -3473.49). Current reward after update: -1226.70, Optimal reward -338.55
Iteration 192 took 2.11 seconds (mean sampled reward: -4835.92). Current reward after update: -576.74, Optimal reward -338.55
Iteration 193 took 2.16 seconds (mean sampled reward: -3336.03). Current reward after update: -564.01, Optimal reward -338.55
Iteration 194 took 2.18 seconds (mean sampled reward: -3740.16). Current reward after update: -600.60, Optimal reward -338.55
Iteration 195 took 2.24 seconds (mean sampled reward: -2343.92). Current reward after update: -1456.86, Optimal reward -338.55
Iteration 196 took 2.13 seconds (mean sampled reward: -5382.06). Current reward after update: -765.55, Optimal reward -338.55
Iteration 197 took 2.14 seconds (mean sampled reward: -4557.88). Current reward after update: -778.94, Optimal reward -338.55
Iteration 198 took 2.20 seconds (mean sampled reward: -3317.55). Current reward after update: -1030.22, Optimal reward -338.55
Iteration 199 took 2.21 seconds (mean sampled reward: -4112.66). Current reward after update: -855.06, Optimal reward -338.55
Iteration 200 took 2.22 seconds (mean sampled reward: -4750.40). Current reward after update: -782.27, Optimal reward -338.55
Iteration 1 took 2.25 seconds (mean sampled reward: -7431.12). Current reward after update: -5661.03, Optimal reward -5661.03
Iteration 2 took 2.25 seconds (mean sampled reward: -6829.32). Current reward after update: -4017.96, Optimal reward -4017.96
Iteration 3 took 2.19 seconds (mean sampled reward: -6447.23). Current reward after update: -2831.99, Optimal reward -2831.99
Iteration 4 took 2.21 seconds (mean sampled reward: -6420.49). Current reward after update: -2102.19, Optimal reward -2102.19
Iteration 5 took 2.26 seconds (mean sampled reward: -6376.75). Current reward after update: -2194.02, Optimal reward -2102.19
Iteration 6 took 2.34 seconds (mean sampled reward: -6158.09). Current reward after update: -2328.40, Optimal reward -2102.19
Iteration 7 took 2.23 seconds (mean sampled reward: -5977.54). Current reward after update: -1894.77, Optimal reward -1894.77
Iteration 8 took 2.25 seconds (mean sampled reward: -5321.22). Current reward after update: -1817.93, Optimal reward -1817.93
Iteration 9 took 2.29 seconds (mean sampled reward: -4501.51). Current reward after update: -1690.91, Optimal reward -1690.91
Iteration 10 took 2.27 seconds (mean sampled reward: -5331.78). Current reward after update: -1519.23, Optimal reward -1519.23
Iteration 11 took 2.19 seconds (mean sampled reward: -4354.72). Current reward after update: -1365.96, Optimal reward -1365.96
Iteration 12 took 2.27 seconds (mean sampled reward: -4141.37). Current reward after update: -1534.91, Optimal reward -1365.96
Iteration 13 took 2.21 seconds (mean sampled reward: -5032.52). Current reward after update: -644.19, Optimal reward -644.19
Iteration 14 took 2.16 seconds (mean sampled reward: -3936.77). Current reward after update: -1380.48, Optimal reward -644.19
Iteration 15 took 2.27 seconds (mean sampled reward: -4796.26). Current reward after update: -1121.76, Optimal reward -644.19
Iteration 16 took 2.12 seconds (mean sampled reward: -4366.85). Current reward after update: -995.31, Optimal reward -644.19
Iteration 17 took 2.25 seconds (mean sampled reward: -4964.64). Current reward after update: -781.77, Optimal reward -644.19
Iteration 18 took 2.45 seconds (mean sampled reward: -4999.50). Current reward after update: -1301.99, Optimal reward -644.19
Iteration 19 took 2.36 seconds (mean sampled reward: -4978.27). Current reward after update: -1013.63, Optimal reward -644.19
Iteration 20 took 2.34 seconds (mean sampled reward: -4330.80). Current reward after update: -1063.87, Optimal reward -644.19
Iteration 21 took 2.34 seconds (mean sampled reward: -4952.23). Current reward after update: -1035.27, Optimal reward -644.19
Iteration 22 took 2.16 seconds (mean sampled reward: -4021.22). Current reward after update: -1079.98, Optimal reward -644.19
Iteration 23 took 2.29 seconds (mean sampled reward: -3794.52). Current reward after update: -962.43, Optimal reward -644.19
Iteration 24 took 2.17 seconds (mean sampled reward: -3480.01). Current reward after update: -987.59, Optimal reward -644.19
Iteration 25 took 2.27 seconds (mean sampled reward: -4299.11). Current reward after update: -882.63, Optimal reward -644.19
Iteration 26 took 2.27 seconds (mean sampled reward: -4953.67). Current reward after update: -820.53, Optimal reward -644.19
Iteration 27 took 2.23 seconds (mean sampled reward: -5234.84). Current reward after update: -919.03, Optimal reward -644.19
Iteration 28 took 2.19 seconds (mean sampled reward: -4472.63). Current reward after update: -772.21, Optimal reward -644.19
Iteration 29 took 2.36 seconds (mean sampled reward: -3808.40). Current reward after update: -1083.96, Optimal reward -644.19
Iteration 30 took 2.22 seconds (mean sampled reward: -4280.17). Current reward after update: -1375.31, Optimal reward -644.19
Iteration 31 took 2.29 seconds (mean sampled reward: -3640.91). Current reward after update: -709.06, Optimal reward -644.19
Iteration 32 took 2.24 seconds (mean sampled reward: -3732.15). Current reward after update: -831.80, Optimal reward -644.19
Iteration 33 took 2.17 seconds (mean sampled reward: -3210.66). Current reward after update: -2075.28, Optimal reward -644.19
Iteration 34 took 2.26 seconds (mean sampled reward: -4508.30). Current reward after update: -750.50, Optimal reward -644.19
Iteration 35 took 2.16 seconds (mean sampled reward: -3895.08). Current reward after update: -912.18, Optimal reward -644.19
Iteration 36 took 2.25 seconds (mean sampled reward: -5440.41). Current reward after update: -926.57, Optimal reward -644.19
Iteration 37 took 2.23 seconds (mean sampled reward: -4709.63). Current reward after update: -1194.41, Optimal reward -644.19
Iteration 38 took 2.25 seconds (mean sampled reward: -4745.64). Current reward after update: -889.52, Optimal reward -644.19
Iteration 39 took 2.31 seconds (mean sampled reward: -4923.56). Current reward after update: -1179.50, Optimal reward -644.19
Iteration 40 took 2.43 seconds (mean sampled reward: -4177.60). Current reward after update: -907.23, Optimal reward -644.19
Iteration 41 took 2.24 seconds (mean sampled reward: -4117.54). Current reward after update: -1044.13, Optimal reward -644.19
Iteration 42 took 2.27 seconds (mean sampled reward: -3653.40). Current reward after update: -858.68, Optimal reward -644.19
Iteration 43 took 2.28 seconds (mean sampled reward: -4031.12). Current reward after update: -879.70, Optimal reward -644.19
Iteration 44 took 2.27 seconds (mean sampled reward: -3433.47). Current reward after update: -886.95, Optimal reward -644.19
Iteration 45 took 2.31 seconds (mean sampled reward: -3116.77). Current reward after update: -969.22, Optimal reward -644.19
Iteration 46 took 2.21 seconds (mean sampled reward: -2828.16). Current reward after update: -1143.41, Optimal reward -644.19
Iteration 47 took 2.28 seconds (mean sampled reward: -3600.97). Current reward after update: -934.23, Optimal reward -644.19
Iteration 48 took 2.28 seconds (mean sampled reward: -5536.60). Current reward after update: -1162.62, Optimal reward -644.19
Iteration 49 took 2.34 seconds (mean sampled reward: -3958.29). Current reward after update: -1268.70, Optimal reward -644.19
Iteration 50 took 2.22 seconds (mean sampled reward: -4045.28). Current reward after update: -834.89, Optimal reward -644.19
Iteration 51 took 2.34 seconds (mean sampled reward: -6203.97). Current reward after update: -1382.92, Optimal reward -644.19
Iteration 52 took 2.25 seconds (mean sampled reward: -5678.25). Current reward after update: -1249.78, Optimal reward -644.19
Iteration 53 took 2.46 seconds (mean sampled reward: -5666.17). Current reward after update: -1045.76, Optimal reward -644.19
Iteration 54 took 2.37 seconds (mean sampled reward: -6533.70). Current reward after update: -2345.99, Optimal reward -644.19
Iteration 55 took 2.18 seconds (mean sampled reward: -6412.04). Current reward after update: -933.22, Optimal reward -644.19
Iteration 56 took 2.25 seconds (mean sampled reward: -5272.67). Current reward after update: -751.53, Optimal reward -644.19
Iteration 57 took 2.26 seconds (mean sampled reward: -4630.08). Current reward after update: -920.61, Optimal reward -644.19
Iteration 58 took 2.39 seconds (mean sampled reward: -4007.74). Current reward after update: -779.63, Optimal reward -644.19
Iteration 59 took 2.36 seconds (mean sampled reward: -2210.74). Current reward after update: -856.23, Optimal reward -644.19
Iteration 60 took 2.50 seconds (mean sampled reward: -2410.35). Current reward after update: -1854.24, Optimal reward -644.19
Iteration 61 took 2.15 seconds (mean sampled reward: -3503.83). Current reward after update: -960.26, Optimal reward -644.19
Iteration 62 took 2.16 seconds (mean sampled reward: -5556.58). Current reward after update: -1045.73, Optimal reward -644.19
Iteration 63 took 2.13 seconds (mean sampled reward: -3927.55). Current reward after update: -1543.53, Optimal reward -644.19
Iteration 64 took 2.36 seconds (mean sampled reward: -4528.03). Current reward after update: -1380.33, Optimal reward -644.19
Iteration 65 took 2.25 seconds (mean sampled reward: -4347.49). Current reward after update: -1176.48, Optimal reward -644.19
Iteration 66 took 2.18 seconds (mean sampled reward: -3989.30). Current reward after update: -1380.97, Optimal reward -644.19
Iteration 67 took 2.19 seconds (mean sampled reward: -3053.28). Current reward after update: -834.40, Optimal reward -644.19
Iteration 68 took 2.27 seconds (mean sampled reward: -3563.75). Current reward after update: -1027.63, Optimal reward -644.19
Iteration 69 took 2.36 seconds (mean sampled reward: -3715.92). Current reward after update: -934.78, Optimal reward -644.19
Iteration 70 took 2.21 seconds (mean sampled reward: -4844.60). Current reward after update: -932.98, Optimal reward -644.19
Iteration 71 took 2.16 seconds (mean sampled reward: -5229.70). Current reward after update: -1165.82, Optimal reward -644.19
Iteration 72 took 2.18 seconds (mean sampled reward: -5018.10). Current reward after update: -935.47, Optimal reward -644.19
Iteration 73 took 2.17 seconds (mean sampled reward: -4573.85). Current reward after update: -1176.96, Optimal reward -644.19
Iteration 74 took 2.18 seconds (mean sampled reward: -4737.75). Current reward after update: -1260.15, Optimal reward -644.19
Iteration 75 took 2.16 seconds (mean sampled reward: -4838.22). Current reward after update: -1036.69, Optimal reward -644.19
Iteration 76 took 2.16 seconds (mean sampled reward: -5536.42). Current reward after update: -940.10, Optimal reward -644.19
Iteration 77 took 2.20 seconds (mean sampled reward: -3753.22). Current reward after update: -983.64, Optimal reward -644.19
Iteration 78 took 2.19 seconds (mean sampled reward: -4454.25). Current reward after update: -981.67, Optimal reward -644.19
Iteration 79 took 2.22 seconds (mean sampled reward: -5215.47). Current reward after update: -1043.17, Optimal reward -644.19
Iteration 80 took 2.09 seconds (mean sampled reward: -5625.90). Current reward after update: -1006.75, Optimal reward -644.19
Iteration 81 took 2.16 seconds (mean sampled reward: -4465.42). Current reward after update: -1067.90, Optimal reward -644.19
Iteration 82 took 2.24 seconds (mean sampled reward: -5282.21). Current reward after update: -1445.58, Optimal reward -644.19
Iteration 83 took 2.15 seconds (mean sampled reward: -5245.17). Current reward after update: -1221.65, Optimal reward -644.19
Iteration 84 took 2.23 seconds (mean sampled reward: -5704.61). Current reward after update: -1053.31, Optimal reward -644.19
Iteration 85 took 2.22 seconds (mean sampled reward: -5700.73). Current reward after update: -1014.83, Optimal reward -644.19
Iteration 86 took 2.20 seconds (mean sampled reward: -5428.86). Current reward after update: -1170.94, Optimal reward -644.19
Iteration 87 took 2.19 seconds (mean sampled reward: -5718.62). Current reward after update: -1124.42, Optimal reward -644.19
Iteration 88 took 2.11 seconds (mean sampled reward: -5706.60). Current reward after update: -1026.30, Optimal reward -644.19
Iteration 89 took 2.16 seconds (mean sampled reward: -5376.25). Current reward after update: -1013.74, Optimal reward -644.19
Iteration 90 took 2.24 seconds (mean sampled reward: -4415.47). Current reward after update: -952.39, Optimal reward -644.19
Iteration 91 took 2.20 seconds (mean sampled reward: -4733.19). Current reward after update: -1025.22, Optimal reward -644.19
Iteration 92 took 2.16 seconds (mean sampled reward: -4222.59). Current reward after update: -2170.16, Optimal reward -644.19
Iteration 93 took 2.19 seconds (mean sampled reward: -4718.25). Current reward after update: -1108.81, Optimal reward -644.19
Iteration 94 took 2.14 seconds (mean sampled reward: -5535.28). Current reward after update: -990.74, Optimal reward -644.19
Iteration 95 took 2.20 seconds (mean sampled reward: -4393.35). Current reward after update: -1174.64, Optimal reward -644.19
Iteration 96 took 2.17 seconds (mean sampled reward: -5266.26). Current reward after update: -1249.81, Optimal reward -644.19
Iteration 97 took 2.23 seconds (mean sampled reward: -4675.33). Current reward after update: -1098.37, Optimal reward -644.19
Iteration 98 took 2.21 seconds (mean sampled reward: -4780.29). Current reward after update: -1110.95, Optimal reward -644.19
Iteration 99 took 2.22 seconds (mean sampled reward: -4444.20). Current reward after update: -1178.18, Optimal reward -644.19
Iteration 100 took 2.16 seconds (mean sampled reward: -3791.41). Current reward after update: -1250.64, Optimal reward -644.19
Iteration 101 took 2.16 seconds (mean sampled reward: -3458.48). Current reward after update: -1367.16, Optimal reward -644.19
Iteration 102 took 2.21 seconds (mean sampled reward: -3390.19). Current reward after update: -1411.11, Optimal reward -644.19
Iteration 103 took 2.20 seconds (mean sampled reward: -3928.95). Current reward after update: -1203.27, Optimal reward -644.19
Iteration 104 took 2.23 seconds (mean sampled reward: -4872.55). Current reward after update: -1210.19, Optimal reward -644.19
Iteration 105 took 2.16 seconds (mean sampled reward: -5524.42). Current reward after update: -1158.41, Optimal reward -644.19
Iteration 106 took 2.25 seconds (mean sampled reward: -3231.88). Current reward after update: -2077.87, Optimal reward -644.19
Iteration 107 took 2.46 seconds (mean sampled reward: -3370.89). Current reward after update: -867.34, Optimal reward -644.19
Iteration 108 took 2.28 seconds (mean sampled reward: -4844.78). Current reward after update: -1161.29, Optimal reward -644.19
Iteration 109 took 2.35 seconds (mean sampled reward: -5006.49). Current reward after update: -853.50, Optimal reward -644.19
Iteration 110 took 2.28 seconds (mean sampled reward: -5032.47). Current reward after update: -766.95, Optimal reward -644.19
Iteration 111 took 2.23 seconds (mean sampled reward: -4143.35). Current reward after update: -877.16, Optimal reward -644.19
Iteration 112 took 2.18 seconds (mean sampled reward: -5002.30). Current reward after update: -872.78, Optimal reward -644.19
Iteration 113 took 2.34 seconds (mean sampled reward: -5707.68). Current reward after update: -1243.85, Optimal reward -644.19
Iteration 114 took 2.16 seconds (mean sampled reward: -3620.11). Current reward after update: -790.12, Optimal reward -644.19
Iteration 115 took 2.20 seconds (mean sampled reward: -3968.65). Current reward after update: -1136.10, Optimal reward -644.19
Iteration 116 took 2.19 seconds (mean sampled reward: -3327.24). Current reward after update: -1205.50, Optimal reward -644.19
Iteration 117 took 2.27 seconds (mean sampled reward: -2889.28). Current reward after update: -961.61, Optimal reward -644.19
Iteration 118 took 2.26 seconds (mean sampled reward: -3397.69). Current reward after update: -1191.16, Optimal reward -644.19
Iteration 119 took 2.18 seconds (mean sampled reward: -3143.27). Current reward after update: -1386.04, Optimal reward -644.19
Iteration 120 took 2.19 seconds (mean sampled reward: -3396.97). Current reward after update: -1142.75, Optimal reward -644.19
Iteration 121 took 2.17 seconds (mean sampled reward: -3308.03). Current reward after update: -1062.45, Optimal reward -644.19
Iteration 122 took 2.24 seconds (mean sampled reward: -3522.76). Current reward after update: -1059.47, Optimal reward -644.19
Iteration 123 took 2.16 seconds (mean sampled reward: -3814.14). Current reward after update: -833.90, Optimal reward -644.19
Iteration 124 took 2.16 seconds (mean sampled reward: -2864.47). Current reward after update: -1087.83, Optimal reward -644.19
Iteration 125 took 2.23 seconds (mean sampled reward: -2807.73). Current reward after update: -1096.51, Optimal reward -644.19
Iteration 126 took 2.19 seconds (mean sampled reward: -3150.87). Current reward after update: -914.82, Optimal reward -644.19
Iteration 127 took 2.17 seconds (mean sampled reward: -3192.21). Current reward after update: -1094.19, Optimal reward -644.19
Iteration 128 took 2.25 seconds (mean sampled reward: -3933.94). Current reward after update: -1231.98, Optimal reward -644.19
Iteration 129 took 2.30 seconds (mean sampled reward: -4185.59). Current reward after update: -1248.17, Optimal reward -644.19
Iteration 130 took 2.22 seconds (mean sampled reward: -5437.22). Current reward after update: -1450.89, Optimal reward -644.19
Iteration 131 took 2.18 seconds (mean sampled reward: -5313.94). Current reward after update: -1900.75, Optimal reward -644.19
Iteration 132 took 2.14 seconds (mean sampled reward: -5269.61). Current reward after update: -1087.56, Optimal reward -644.19
Iteration 133 took 2.16 seconds (mean sampled reward: -4671.23). Current reward after update: -1092.10, Optimal reward -644.19
Iteration 134 took 2.17 seconds (mean sampled reward: -5669.13). Current reward after update: -1026.48, Optimal reward -644.19
Iteration 135 took 2.15 seconds (mean sampled reward: -5573.56). Current reward after update: -2063.97, Optimal reward -644.19
Iteration 136 took 2.17 seconds (mean sampled reward: -4163.76). Current reward after update: -1137.85, Optimal reward -644.19
Iteration 137 took 2.20 seconds (mean sampled reward: -6159.39). Current reward after update: -1518.02, Optimal reward -644.19
Iteration 138 took 2.27 seconds (mean sampled reward: -6106.49). Current reward after update: -1457.90, Optimal reward -644.19
Iteration 139 took 2.15 seconds (mean sampled reward: -4908.56). Current reward after update: -1114.78, Optimal reward -644.19
Iteration 140 took 2.26 seconds (mean sampled reward: -5391.74). Current reward after update: -1305.48, Optimal reward -644.19
Iteration 141 took 2.22 seconds (mean sampled reward: -5941.09). Current reward after update: -1114.36, Optimal reward -644.19
Iteration 142 took 2.20 seconds (mean sampled reward: -5419.13). Current reward after update: -1587.24, Optimal reward -644.19
Iteration 143 took 2.18 seconds (mean sampled reward: -6462.97). Current reward after update: -1079.97, Optimal reward -644.19
Iteration 144 took 2.14 seconds (mean sampled reward: -6860.12). Current reward after update: -1297.14, Optimal reward -644.19
Iteration 145 took 2.14 seconds (mean sampled reward: -6395.06). Current reward after update: -1336.52, Optimal reward -644.19
Iteration 146 took 2.15 seconds (mean sampled reward: -6495.60). Current reward after update: -1438.98, Optimal reward -644.19
Iteration 147 took 2.11 seconds (mean sampled reward: -6621.94). Current reward after update: -1552.58, Optimal reward -644.19
Iteration 148 took 2.13 seconds (mean sampled reward: -5607.66). Current reward after update: -1359.50, Optimal reward -644.19
Iteration 149 took 2.16 seconds (mean sampled reward: -5481.10). Current reward after update: -2881.97, Optimal reward -644.19
Iteration 150 took 2.15 seconds (mean sampled reward: -5887.66). Current reward after update: -1204.51, Optimal reward -644.19
Iteration 151 took 2.12 seconds (mean sampled reward: -6219.81). Current reward after update: -1082.12, Optimal reward -644.19
Iteration 152 took 2.12 seconds (mean sampled reward: -5054.92). Current reward after update: -2304.24, Optimal reward -644.19
Iteration 153 took 2.14 seconds (mean sampled reward: -5444.64). Current reward after update: -1252.69, Optimal reward -644.19
Iteration 154 took 2.15 seconds (mean sampled reward: -5452.53). Current reward after update: -1270.65, Optimal reward -644.19
Iteration 155 took 2.18 seconds (mean sampled reward: -5981.17). Current reward after update: -1406.55, Optimal reward -644.19
Iteration 156 took 2.18 seconds (mean sampled reward: -5501.24). Current reward after update: -1307.77, Optimal reward -644.19
Iteration 157 took 2.16 seconds (mean sampled reward: -6513.58). Current reward after update: -955.01, Optimal reward -644.19
Iteration 158 took 2.19 seconds (mean sampled reward: -6093.25). Current reward after update: -1137.05, Optimal reward -644.19
Iteration 159 took 2.12 seconds (mean sampled reward: -5972.23). Current reward after update: -767.45, Optimal reward -644.19
Iteration 160 took 2.18 seconds (mean sampled reward: -5630.66). Current reward after update: -745.18, Optimal reward -644.19
Iteration 161 took 2.21 seconds (mean sampled reward: -5178.26). Current reward after update: -1510.24, Optimal reward -644.19
Iteration 162 took 2.18 seconds (mean sampled reward: -5564.89). Current reward after update: -1265.19, Optimal reward -644.19
Iteration 163 took 2.23 seconds (mean sampled reward: -5589.09). Current reward after update: -969.08, Optimal reward -644.19
Iteration 164 took 2.15 seconds (mean sampled reward: -6178.23). Current reward after update: -919.02, Optimal reward -644.19
Iteration 165 took 2.18 seconds (mean sampled reward: -5599.56). Current reward after update: -1159.42, Optimal reward -644.19
Iteration 166 took 2.15 seconds (mean sampled reward: -4801.93). Current reward after update: -1020.80, Optimal reward -644.19
Iteration 167 took 2.17 seconds (mean sampled reward: -5600.36). Current reward after update: -954.86, Optimal reward -644.19
Iteration 168 took 2.16 seconds (mean sampled reward: -5690.05). Current reward after update: -871.40, Optimal reward -644.19
Iteration 169 took 2.15 seconds (mean sampled reward: -4805.18). Current reward after update: -1220.65, Optimal reward -644.19
Iteration 170 took 2.14 seconds (mean sampled reward: -6255.14). Current reward after update: -1482.53, Optimal reward -644.19
Iteration 171 took 2.24 seconds (mean sampled reward: -4724.11). Current reward after update: -931.29, Optimal reward -644.19
Iteration 172 took 2.26 seconds (mean sampled reward: -4216.89). Current reward after update: -795.09, Optimal reward -644.19
Iteration 173 took 2.28 seconds (mean sampled reward: -5187.90). Current reward after update: -829.86, Optimal reward -644.19
Iteration 174 took 2.21 seconds (mean sampled reward: -5803.43). Current reward after update: -927.07, Optimal reward -644.19
Iteration 175 took 2.20 seconds (mean sampled reward: -4915.96). Current reward after update: -1372.11, Optimal reward -644.19
Iteration 176 took 2.22 seconds (mean sampled reward: -4925.51). Current reward after update: -932.08, Optimal reward -644.19
Iteration 177 took 2.20 seconds (mean sampled reward: -6298.06). Current reward after update: -1285.86, Optimal reward -644.19
Iteration 178 took 2.19 seconds (mean sampled reward: -6767.11). Current reward after update: -1539.64, Optimal reward -644.19
Iteration 179 took 2.17 seconds (mean sampled reward: -6565.01). Current reward after update: -1635.99, Optimal reward -644.19
Iteration 180 took 2.16 seconds (mean sampled reward: -6763.40). Current reward after update: -1189.16, Optimal reward -644.19
Iteration 181 took 2.19 seconds (mean sampled reward: -6730.91). Current reward after update: -1785.83, Optimal reward -644.19
Iteration 182 took 2.12 seconds (mean sampled reward: -6701.97). Current reward after update: -1295.75, Optimal reward -644.19
Iteration 183 took 2.18 seconds (mean sampled reward: -6294.64). Current reward after update: -1462.78, Optimal reward -644.19
Iteration 184 took 2.23 seconds (mean sampled reward: -4470.32). Current reward after update: -1055.36, Optimal reward -644.19
Iteration 185 took 2.15 seconds (mean sampled reward: -5447.61). Current reward after update: -1179.50, Optimal reward -644.19
Iteration 186 took 2.18 seconds (mean sampled reward: -5412.29). Current reward after update: -894.44, Optimal reward -644.19
Iteration 187 took 2.22 seconds (mean sampled reward: -3874.56). Current reward after update: -941.60, Optimal reward -644.19
Iteration 188 took 2.18 seconds (mean sampled reward: -4026.80). Current reward after update: -1443.71, Optimal reward -644.19
Iteration 189 took 2.21 seconds (mean sampled reward: -3921.45). Current reward after update: -1617.29, Optimal reward -644.19
Iteration 190 took 2.18 seconds (mean sampled reward: -4513.47). Current reward after update: -4383.41, Optimal reward -644.19
Iteration 191 took 2.21 seconds (mean sampled reward: -4346.67). Current reward after update: -1040.95, Optimal reward -644.19
Iteration 192 took 2.23 seconds (mean sampled reward: -4026.21). Current reward after update: -1015.92, Optimal reward -644.19
Iteration 193 took 2.17 seconds (mean sampled reward: -6301.14). Current reward after update: -978.65, Optimal reward -644.19
Iteration 194 took 2.20 seconds (mean sampled reward: -5494.04). Current reward after update: -1065.46, Optimal reward -644.19
Iteration 195 took 2.20 seconds (mean sampled reward: -4625.69). Current reward after update: -1226.31, Optimal reward -644.19
Iteration 196 took 2.20 seconds (mean sampled reward: -5084.83). Current reward after update: -1223.15, Optimal reward -644.19
Iteration 197 took 2.17 seconds (mean sampled reward: -5007.21). Current reward after update: -887.72, Optimal reward -644.19
Iteration 198 took 2.17 seconds (mean sampled reward: -4043.61). Current reward after update: -856.56, Optimal reward -644.19
Iteration 199 took 2.19 seconds (mean sampled reward: -4109.60). Current reward after update: -1092.70, Optimal reward -644.19
Iteration 200 took 2.19 seconds (mean sampled reward: -4318.58). Current reward after update: -1095.75, Optimal reward -644.19
Iteration 1 took 2.25 seconds (mean sampled reward: -7444.98). Current reward after update: -3958.32, Optimal reward -3958.32
Iteration 2 took 2.30 seconds (mean sampled reward: -6944.53). Current reward after update: -4863.97, Optimal reward -3958.32
Iteration 3 took 2.32 seconds (mean sampled reward: -7212.57). Current reward after update: -4249.69, Optimal reward -3958.32
Iteration 4 took 2.44 seconds (mean sampled reward: -7248.56). Current reward after update: -3042.70, Optimal reward -3042.70
Iteration 5 took 2.41 seconds (mean sampled reward: -7167.85). Current reward after update: -2890.58, Optimal reward -2890.58
Iteration 6 took 2.36 seconds (mean sampled reward: -6970.77). Current reward after update: -2634.09, Optimal reward -2634.09
Iteration 7 took 2.39 seconds (mean sampled reward: -7009.10). Current reward after update: -2443.45, Optimal reward -2443.45
Iteration 8 took 2.49 seconds (mean sampled reward: -5575.05). Current reward after update: -2112.57, Optimal reward -2112.57
Iteration 9 took 2.25 seconds (mean sampled reward: -4527.83). Current reward after update: -1760.18, Optimal reward -1760.18
Iteration 10 took 2.10 seconds (mean sampled reward: -4117.38). Current reward after update: -1787.43, Optimal reward -1760.18
Iteration 11 took 2.37 seconds (mean sampled reward: -5391.86). Current reward after update: -1736.37, Optimal reward -1736.37
Iteration 12 took 2.43 seconds (mean sampled reward: -6117.31). Current reward after update: -1676.50, Optimal reward -1676.50
Iteration 13 took 2.26 seconds (mean sampled reward: -6123.61). Current reward after update: -1254.93, Optimal reward -1254.93
Iteration 14 took 2.28 seconds (mean sampled reward: -6605.43). Current reward after update: -1889.07, Optimal reward -1254.93
Iteration 15 took 2.22 seconds (mean sampled reward: -5873.78). Current reward after update: -882.58, Optimal reward -882.58
Iteration 16 took 2.49 seconds (mean sampled reward: -5681.56). Current reward after update: -941.97, Optimal reward -882.58
Iteration 17 took 2.37 seconds (mean sampled reward: -6339.40). Current reward after update: -1570.59, Optimal reward -882.58
Iteration 18 took 2.28 seconds (mean sampled reward: -6148.92). Current reward after update: -475.08, Optimal reward -475.08
Iteration 19 took 2.36 seconds (mean sampled reward: -5445.95). Current reward after update: -881.27, Optimal reward -475.08
Iteration 20 took 2.35 seconds (mean sampled reward: -5067.25). Current reward after update: -887.67, Optimal reward -475.08
Iteration 21 took 2.34 seconds (mean sampled reward: -5518.93). Current reward after update: -744.54, Optimal reward -475.08
Iteration 22 took 2.21 seconds (mean sampled reward: -5221.41). Current reward after update: -580.78, Optimal reward -475.08
Iteration 23 took 2.27 seconds (mean sampled reward: -4451.96). Current reward after update: -708.21, Optimal reward -475.08
Iteration 24 took 2.20 seconds (mean sampled reward: -4344.34). Current reward after update: -903.36, Optimal reward -475.08
Iteration 25 took 2.22 seconds (mean sampled reward: -5763.77). Current reward after update: -720.74, Optimal reward -475.08
Iteration 26 took 2.24 seconds (mean sampled reward: -4876.21). Current reward after update: -740.36, Optimal reward -475.08
Iteration 27 took 2.24 seconds (mean sampled reward: -4862.99). Current reward after update: -1027.69, Optimal reward -475.08
Iteration 28 took 2.21 seconds (mean sampled reward: -5564.20). Current reward after update: -1084.87, Optimal reward -475.08
Iteration 29 took 2.32 seconds (mean sampled reward: -4218.76). Current reward after update: -1070.53, Optimal reward -475.08
Iteration 30 took 2.19 seconds (mean sampled reward: -4434.91). Current reward after update: -1041.24, Optimal reward -475.08
Iteration 31 took 2.15 seconds (mean sampled reward: -4655.25). Current reward after update: -986.18, Optimal reward -475.08
Iteration 32 took 2.29 seconds (mean sampled reward: -4480.38). Current reward after update: -1138.67, Optimal reward -475.08
Iteration 33 took 2.17 seconds (mean sampled reward: -4569.46). Current reward after update: -880.16, Optimal reward -475.08
Iteration 34 took 2.24 seconds (mean sampled reward: -5433.07). Current reward after update: -969.68, Optimal reward -475.08
Iteration 35 took 2.16 seconds (mean sampled reward: -4767.22). Current reward after update: -934.37, Optimal reward -475.08
Iteration 36 took 2.30 seconds (mean sampled reward: -4843.05). Current reward after update: -1065.96, Optimal reward -475.08
Iteration 37 took 2.05 seconds (mean sampled reward: -5545.92). Current reward after update: -988.80, Optimal reward -475.08
Iteration 38 took 2.13 seconds (mean sampled reward: -5842.11). Current reward after update: -1272.77, Optimal reward -475.08
Iteration 39 took 2.06 seconds (mean sampled reward: -5220.19). Current reward after update: -1013.32, Optimal reward -475.08
Iteration 40 took 1.95 seconds (mean sampled reward: -5557.45). Current reward after update: -1731.45, Optimal reward -475.08
Iteration 41 took 1.93 seconds (mean sampled reward: -5757.25). Current reward after update: -1034.44, Optimal reward -475.08
Iteration 42 took 1.96 seconds (mean sampled reward: -6166.58). Current reward after update: -1133.54, Optimal reward -475.08
Iteration 43 took 2.00 seconds (mean sampled reward: -4226.49). Current reward after update: -852.31, Optimal reward -475.08
Iteration 44 took 1.98 seconds (mean sampled reward: -5657.37). Current reward after update: -859.21, Optimal reward -475.08
Iteration 45 took 2.10 seconds (mean sampled reward: -3655.58). Current reward after update: -897.36, Optimal reward -475.08
Iteration 46 took 2.01 seconds (mean sampled reward: -3709.56). Current reward after update: -885.41, Optimal reward -475.08
Iteration 47 took 2.04 seconds (mean sampled reward: -4177.68). Current reward after update: -1089.57, Optimal reward -475.08
Iteration 48 took 1.99 seconds (mean sampled reward: -4756.39). Current reward after update: -946.26, Optimal reward -475.08
Iteration 49 took 1.92 seconds (mean sampled reward: -5539.87). Current reward after update: -919.12, Optimal reward -475.08
Iteration 50 took 1.97 seconds (mean sampled reward: -5238.59). Current reward after update: -633.50, Optimal reward -475.08
Iteration 51 took 1.99 seconds (mean sampled reward: -4941.29). Current reward after update: -746.61, Optimal reward -475.08
Iteration 52 took 2.05 seconds (mean sampled reward: -3692.07). Current reward after update: -680.21, Optimal reward -475.08
Iteration 53 took 2.04 seconds (mean sampled reward: -3744.62). Current reward after update: -770.30, Optimal reward -475.08
Iteration 54 took 2.20 seconds (mean sampled reward: -4985.18). Current reward after update: -732.08, Optimal reward -475.08
Iteration 55 took 2.28 seconds (mean sampled reward: -4902.36). Current reward after update: -602.71, Optimal reward -475.08
Iteration 56 took 2.10 seconds (mean sampled reward: -6165.70). Current reward after update: -801.08, Optimal reward -475.08
Iteration 57 took 1.97 seconds (mean sampled reward: -5843.52). Current reward after update: -624.13, Optimal reward -475.08
Iteration 58 took 1.95 seconds (mean sampled reward: -5248.51). Current reward after update: -608.22, Optimal reward -475.08
Iteration 59 took 2.01 seconds (mean sampled reward: -4383.18). Current reward after update: -606.63, Optimal reward -475.08
Iteration 60 took 2.02 seconds (mean sampled reward: -4534.63). Current reward after update: -563.95, Optimal reward -475.08
Iteration 61 took 2.10 seconds (mean sampled reward: -4368.47). Current reward after update: -552.33, Optimal reward -475.08
Iteration 62 took 1.99 seconds (mean sampled reward: -4215.83). Current reward after update: -676.92, Optimal reward -475.08
Iteration 63 took 2.00 seconds (mean sampled reward: -4808.75). Current reward after update: -542.72, Optimal reward -475.08
Iteration 64 took 2.00 seconds (mean sampled reward: -4632.78). Current reward after update: -692.44, Optimal reward -475.08
Iteration 65 took 2.06 seconds (mean sampled reward: -2789.88). Current reward after update: -465.17, Optimal reward -465.17
Iteration 66 took 2.22 seconds (mean sampled reward: -3216.75). Current reward after update: -544.91, Optimal reward -465.17
Iteration 67 took 1.99 seconds (mean sampled reward: -4875.17). Current reward after update: -680.73, Optimal reward -465.17
Iteration 68 took 1.97 seconds (mean sampled reward: -4326.89). Current reward after update: -484.55, Optimal reward -465.17
Iteration 69 took 1.99 seconds (mean sampled reward: -4262.75). Current reward after update: -646.76, Optimal reward -465.17
Iteration 70 took 2.05 seconds (mean sampled reward: -3627.15). Current reward after update: -648.97, Optimal reward -465.17
Iteration 71 took 2.02 seconds (mean sampled reward: -4971.56). Current reward after update: -603.15, Optimal reward -465.17
Iteration 72 took 2.02 seconds (mean sampled reward: -4383.33). Current reward after update: -782.06, Optimal reward -465.17
Iteration 73 took 2.03 seconds (mean sampled reward: -2354.28). Current reward after update: -1144.95, Optimal reward -465.17
Iteration 74 took 1.98 seconds (mean sampled reward: -4381.96). Current reward after update: -511.08, Optimal reward -465.17
Iteration 75 took 2.04 seconds (mean sampled reward: -3232.75). Current reward after update: -738.97, Optimal reward -465.17
Iteration 76 took 1.99 seconds (mean sampled reward: -4464.84). Current reward after update: -695.00, Optimal reward -465.17
Iteration 77 took 2.03 seconds (mean sampled reward: -2175.13). Current reward after update: -648.17, Optimal reward -465.17
Iteration 78 took 2.06 seconds (mean sampled reward: -1394.06). Current reward after update: -587.36, Optimal reward -465.17
Iteration 79 took 2.06 seconds (mean sampled reward: -1858.25). Current reward after update: -616.41, Optimal reward -465.17
Iteration 80 took 2.07 seconds (mean sampled reward: -3154.99). Current reward after update: -369.61, Optimal reward -369.61
Iteration 81 took 2.10 seconds (mean sampled reward: -2538.66). Current reward after update: -854.65, Optimal reward -369.61
Iteration 82 took 2.07 seconds (mean sampled reward: -2303.44). Current reward after update: -530.52, Optimal reward -369.61
Iteration 83 took 1.97 seconds (mean sampled reward: -4859.12). Current reward after update: -524.90, Optimal reward -369.61
Iteration 84 took 1.96 seconds (mean sampled reward: -3674.11). Current reward after update: -503.87, Optimal reward -369.61
Iteration 85 took 2.03 seconds (mean sampled reward: -3979.47). Current reward after update: -1171.68, Optimal reward -369.61
Iteration 86 took 2.01 seconds (mean sampled reward: -4027.67). Current reward after update: -419.08, Optimal reward -369.61
Iteration 87 took 1.99 seconds (mean sampled reward: -4410.27). Current reward after update: -460.21, Optimal reward -369.61
Iteration 88 took 2.07 seconds (mean sampled reward: -5830.87). Current reward after update: -618.88, Optimal reward -369.61
Iteration 89 took 2.01 seconds (mean sampled reward: -5249.36). Current reward after update: -421.36, Optimal reward -369.61
Iteration 90 took 2.06 seconds (mean sampled reward: -3138.35). Current reward after update: -705.15, Optimal reward -369.61
Iteration 91 took 2.03 seconds (mean sampled reward: -3026.85). Current reward after update: -480.93, Optimal reward -369.61
Iteration 92 took 2.10 seconds (mean sampled reward: -3573.57). Current reward after update: -618.58, Optimal reward -369.61
Iteration 93 took 2.00 seconds (mean sampled reward: -4014.19). Current reward after update: -1034.58, Optimal reward -369.61
Iteration 94 took 2.02 seconds (mean sampled reward: -1963.09). Current reward after update: -652.39, Optimal reward -369.61
Iteration 95 took 2.12 seconds (mean sampled reward: -1719.01). Current reward after update: -1383.47, Optimal reward -369.61
Iteration 96 took 2.07 seconds (mean sampled reward: -2040.40). Current reward after update: -423.21, Optimal reward -369.61
Iteration 97 took 2.08 seconds (mean sampled reward: -3842.26). Current reward after update: -444.51, Optimal reward -369.61
Iteration 98 took 2.03 seconds (mean sampled reward: -3973.19). Current reward after update: -846.42, Optimal reward -369.61
Iteration 99 took 1.99 seconds (mean sampled reward: -4248.11). Current reward after update: -539.48, Optimal reward -369.61
Iteration 100 took 2.10 seconds (mean sampled reward: -4469.17). Current reward after update: -569.05, Optimal reward -369.61
Iteration 101 took 2.00 seconds (mean sampled reward: -4273.48). Current reward after update: -666.20, Optimal reward -369.61
Iteration 102 took 2.05 seconds (mean sampled reward: -4701.01). Current reward after update: -467.16, Optimal reward -369.61
Iteration 103 took 2.04 seconds (mean sampled reward: -5308.77). Current reward after update: -920.32, Optimal reward -369.61
Iteration 104 took 1.99 seconds (mean sampled reward: -4846.27). Current reward after update: -454.52, Optimal reward -369.61
Iteration 105 took 2.02 seconds (mean sampled reward: -4797.01). Current reward after update: -578.94, Optimal reward -369.61
Iteration 106 took 2.05 seconds (mean sampled reward: -4741.53). Current reward after update: -734.69, Optimal reward -369.61
Iteration 107 took 1.99 seconds (mean sampled reward: -5106.23). Current reward after update: -799.69, Optimal reward -369.61
Iteration 108 took 2.13 seconds (mean sampled reward: -4486.52). Current reward after update: -632.54, Optimal reward -369.61
Iteration 109 took 2.12 seconds (mean sampled reward: -5041.44). Current reward after update: -1009.70, Optimal reward -369.61
Iteration 110 took 2.11 seconds (mean sampled reward: -4801.29). Current reward after update: -540.63, Optimal reward -369.61
Iteration 111 took 1.95 seconds (mean sampled reward: -5388.86). Current reward after update: -712.71, Optimal reward -369.61
Iteration 112 took 2.11 seconds (mean sampled reward: -4028.45). Current reward after update: -801.06, Optimal reward -369.61
Iteration 113 took 2.18 seconds (mean sampled reward: -3133.40). Current reward after update: -576.62, Optimal reward -369.61
Iteration 114 took 2.07 seconds (mean sampled reward: -4617.10). Current reward after update: -690.20, Optimal reward -369.61
Iteration 115 took 2.12 seconds (mean sampled reward: -3394.35). Current reward after update: -642.31, Optimal reward -369.61
Iteration 116 took 2.03 seconds (mean sampled reward: -3257.68). Current reward after update: -541.51, Optimal reward -369.61
Iteration 117 took 2.00 seconds (mean sampled reward: -2559.82). Current reward after update: -521.30, Optimal reward -369.61
Iteration 118 took 2.03 seconds (mean sampled reward: -2771.87). Current reward after update: -1084.09, Optimal reward -369.61
Iteration 119 took 1.99 seconds (mean sampled reward: -4047.20). Current reward after update: -575.33, Optimal reward -369.61
Iteration 120 took 1.97 seconds (mean sampled reward: -4650.02). Current reward after update: -650.97, Optimal reward -369.61
Iteration 121 took 2.21 seconds (mean sampled reward: -5627.06). Current reward after update: -645.90, Optimal reward -369.61
Iteration 122 took 2.08 seconds (mean sampled reward: -4450.93). Current reward after update: -517.26, Optimal reward -369.61
Iteration 123 took 2.22 seconds (mean sampled reward: -5556.78). Current reward after update: -2073.10, Optimal reward -369.61
Iteration 124 took 2.09 seconds (mean sampled reward: -4679.33). Current reward after update: -688.07, Optimal reward -369.61
Iteration 125 took 2.05 seconds (mean sampled reward: -3986.78). Current reward after update: -712.99, Optimal reward -369.61
Iteration 126 took 2.13 seconds (mean sampled reward: -4245.30). Current reward after update: -800.90, Optimal reward -369.61
Iteration 127 took 2.14 seconds (mean sampled reward: -4973.60). Current reward after update: -658.75, Optimal reward -369.61
Iteration 128 took 2.11 seconds (mean sampled reward: -1976.39). Current reward after update: -540.00, Optimal reward -369.61
Iteration 129 took 2.20 seconds (mean sampled reward: -3522.31). Current reward after update: -643.40, Optimal reward -369.61
Iteration 130 took 2.11 seconds (mean sampled reward: -4019.26). Current reward after update: -626.61, Optimal reward -369.61
Iteration 131 took 2.15 seconds (mean sampled reward: -2163.00). Current reward after update: -667.14, Optimal reward -369.61
Iteration 132 took 2.14 seconds (mean sampled reward: -2272.09). Current reward after update: -554.73, Optimal reward -369.61
Iteration 133 took 2.15 seconds (mean sampled reward: -2519.67). Current reward after update: -630.25, Optimal reward -369.61
Iteration 134 took 2.12 seconds (mean sampled reward: -4474.58). Current reward after update: -608.91, Optimal reward -369.61
Iteration 135 took 2.09 seconds (mean sampled reward: -3647.53). Current reward after update: -617.42, Optimal reward -369.61
Iteration 136 took 2.15 seconds (mean sampled reward: -2052.56). Current reward after update: -691.67, Optimal reward -369.61
Iteration 137 took 2.15 seconds (mean sampled reward: -2369.51). Current reward after update: -2517.22, Optimal reward -369.61
Iteration 138 took 2.09 seconds (mean sampled reward: -2505.05). Current reward after update: -908.40, Optimal reward -369.61
Iteration 139 took 2.19 seconds (mean sampled reward: -1953.81). Current reward after update: -498.31, Optimal reward -369.61
Iteration 140 took 2.15 seconds (mean sampled reward: -2388.23). Current reward after update: -680.82, Optimal reward -369.61
Iteration 141 took 2.16 seconds (mean sampled reward: -2534.72). Current reward after update: -739.30, Optimal reward -369.61
Iteration 142 took 2.16 seconds (mean sampled reward: -2220.05). Current reward after update: -630.45, Optimal reward -369.61
Iteration 143 took 2.14 seconds (mean sampled reward: -2028.36). Current reward after update: -878.24, Optimal reward -369.61
Iteration 144 took 2.22 seconds (mean sampled reward: -2216.26). Current reward after update: -1161.26, Optimal reward -369.61
Iteration 145 took 2.13 seconds (mean sampled reward: -2032.42). Current reward after update: -574.52, Optimal reward -369.61
Iteration 146 took 2.22 seconds (mean sampled reward: -2043.26). Current reward after update: -576.53, Optimal reward -369.61
Iteration 147 took 2.24 seconds (mean sampled reward: -2698.16). Current reward after update: -741.39, Optimal reward -369.61
Iteration 148 took 2.16 seconds (mean sampled reward: -3428.62). Current reward after update: -616.68, Optimal reward -369.61
Iteration 149 took 2.05 seconds (mean sampled reward: -4658.99). Current reward after update: -751.15, Optimal reward -369.61
Iteration 150 took 2.14 seconds (mean sampled reward: -4799.78). Current reward after update: -697.92, Optimal reward -369.61
Iteration 151 took 2.18 seconds (mean sampled reward: -3964.36). Current reward after update: -472.80, Optimal reward -369.61
Iteration 152 took 2.07 seconds (mean sampled reward: -4226.09). Current reward after update: -441.91, Optimal reward -369.61
Iteration 153 took 2.05 seconds (mean sampled reward: -3884.02). Current reward after update: -1009.99, Optimal reward -369.61
Iteration 154 took 2.11 seconds (mean sampled reward: -2707.96). Current reward after update: -517.45, Optimal reward -369.61
Iteration 155 took 2.12 seconds (mean sampled reward: -1782.13). Current reward after update: -465.66, Optimal reward -369.61
Iteration 156 took 2.18 seconds (mean sampled reward: -2547.32). Current reward after update: -553.25, Optimal reward -369.61
Iteration 157 took 2.08 seconds (mean sampled reward: -2505.82). Current reward after update: -607.44, Optimal reward -369.61
Iteration 158 took 2.11 seconds (mean sampled reward: -2434.81). Current reward after update: -502.51, Optimal reward -369.61
Iteration 159 took 2.14 seconds (mean sampled reward: -2269.12). Current reward after update: -617.10, Optimal reward -369.61
Iteration 160 took 2.14 seconds (mean sampled reward: -2375.91). Current reward after update: -610.00, Optimal reward -369.61
Iteration 161 took 2.12 seconds (mean sampled reward: -4257.66). Current reward after update: -2488.68, Optimal reward -369.61
Iteration 162 took 2.05 seconds (mean sampled reward: -3031.51). Current reward after update: -588.44, Optimal reward -369.61
Iteration 163 took 2.00 seconds (mean sampled reward: -4463.79). Current reward after update: -580.18, Optimal reward -369.61
Iteration 164 took 2.11 seconds (mean sampled reward: -3080.50). Current reward after update: -552.07, Optimal reward -369.61
Iteration 165 took 2.07 seconds (mean sampled reward: -4733.96). Current reward after update: -1028.56, Optimal reward -369.61
Iteration 166 took 2.13 seconds (mean sampled reward: -3769.52). Current reward after update: -422.09, Optimal reward -369.61
Iteration 167 took 2.15 seconds (mean sampled reward: -3284.72). Current reward after update: -456.35, Optimal reward -369.61
Iteration 168 took 2.08 seconds (mean sampled reward: -4180.96). Current reward after update: -446.57, Optimal reward -369.61
Iteration 169 took 2.07 seconds (mean sampled reward: -3058.09). Current reward after update: -1021.81, Optimal reward -369.61
Iteration 170 took 2.01 seconds (mean sampled reward: -3789.67). Current reward after update: -687.80, Optimal reward -369.61
Iteration 171 took 1.97 seconds (mean sampled reward: -4516.48). Current reward after update: -370.96, Optimal reward -369.61
Iteration 172 took 2.07 seconds (mean sampled reward: -4092.38). Current reward after update: -314.02, Optimal reward -314.02
Iteration 173 took 2.11 seconds (mean sampled reward: -3298.26). Current reward after update: -423.32, Optimal reward -314.02
Iteration 174 took 2.08 seconds (mean sampled reward: -2091.18). Current reward after update: -356.97, Optimal reward -314.02
Iteration 175 took 2.05 seconds (mean sampled reward: -2082.32). Current reward after update: -667.50, Optimal reward -314.02
Iteration 176 took 2.10 seconds (mean sampled reward: -2539.66). Current reward after update: -418.46, Optimal reward -314.02
Iteration 177 took 2.10 seconds (mean sampled reward: -2220.28). Current reward after update: -704.49, Optimal reward -314.02
Iteration 178 took 2.11 seconds (mean sampled reward: -3408.77). Current reward after update: -428.66, Optimal reward -314.02
Iteration 179 took 2.06 seconds (mean sampled reward: -2462.64). Current reward after update: -919.95, Optimal reward -314.02
Iteration 180 took 2.04 seconds (mean sampled reward: -3440.99). Current reward after update: -345.33, Optimal reward -314.02
Iteration 181 took 2.05 seconds (mean sampled reward: -2978.09). Current reward after update: -398.97, Optimal reward -314.02
Iteration 182 took 2.03 seconds (mean sampled reward: -3405.57). Current reward after update: -700.47, Optimal reward -314.02
Iteration 183 took 2.09 seconds (mean sampled reward: -2965.58). Current reward after update: -386.23, Optimal reward -314.02
Iteration 184 took 2.10 seconds (mean sampled reward: -2402.66). Current reward after update: -321.60, Optimal reward -314.02
Iteration 185 took 2.07 seconds (mean sampled reward: -3419.46). Current reward after update: -378.15, Optimal reward -314.02
Iteration 186 took 2.05 seconds (mean sampled reward: -2753.39). Current reward after update: -884.60, Optimal reward -314.02
Iteration 187 took 2.02 seconds (mean sampled reward: -3863.43). Current reward after update: -396.69, Optimal reward -314.02
Iteration 188 took 2.16 seconds (mean sampled reward: -1673.59). Current reward after update: -304.09, Optimal reward -304.09
Iteration 189 took 2.05 seconds (mean sampled reward: -2570.47). Current reward after update: -466.97, Optimal reward -304.09
Iteration 190 took 2.08 seconds (mean sampled reward: -2610.97). Current reward after update: -336.66, Optimal reward -304.09
Iteration 191 took 2.07 seconds (mean sampled reward: -2820.40). Current reward after update: -1315.27, Optimal reward -304.09
Iteration 192 took 2.02 seconds (mean sampled reward: -4132.97). Current reward after update: -459.87, Optimal reward -304.09
Iteration 193 took 2.12 seconds (mean sampled reward: -2805.16). Current reward after update: -1862.18, Optimal reward -304.09
Iteration 194 took 2.06 seconds (mean sampled reward: -3812.36). Current reward after update: -304.03, Optimal reward -304.03
Iteration 195 took 2.07 seconds (mean sampled reward: -2445.68). Current reward after update: -497.20, Optimal reward -304.03
Iteration 196 took 2.11 seconds (mean sampled reward: -2812.95). Current reward after update: -317.76, Optimal reward -304.03
Iteration 197 took 2.09 seconds (mean sampled reward: -2336.97). Current reward after update: -454.57, Optimal reward -304.03
Iteration 198 took 2.10 seconds (mean sampled reward: -3877.60). Current reward after update: -381.68, Optimal reward -304.03
Iteration 199 took 2.10 seconds (mean sampled reward: -2967.69). Current reward after update: -380.67, Optimal reward -304.03
Iteration 200 took 2.12 seconds (mean sampled reward: -2278.07). Current reward after update: -351.24, Optimal reward -304.03
Max force: 20 Sigma: 0.8 mean rewards: -428.9230664449219, best rewards:-304.0283118567765

Iteration 1 took 2.40 seconds (mean sampled reward: -7627.42). Current reward after update: -7492.51, Optimal reward -7492.51
Iteration 2 took 2.29 seconds (mean sampled reward: -7616.39). Current reward after update: -7326.05, Optimal reward -7326.05
Iteration 3 took 2.24 seconds (mean sampled reward: -7589.81). Current reward after update: -7285.14, Optimal reward -7285.14
Iteration 4 took 2.25 seconds (mean sampled reward: -7529.79). Current reward after update: -7250.91, Optimal reward -7250.91
Iteration 5 took 2.40 seconds (mean sampled reward: -7536.18). Current reward after update: -7251.22, Optimal reward -7250.91
Iteration 6 took 2.30 seconds (mean sampled reward: -7526.80). Current reward after update: -7124.19, Optimal reward -7124.19
Iteration 7 took 2.35 seconds (mean sampled reward: -7562.78). Current reward after update: -6975.96, Optimal reward -6975.96
Iteration 8 took 2.67 seconds (mean sampled reward: -7485.37). Current reward after update: -6349.71, Optimal reward -6349.71
Iteration 9 took 2.37 seconds (mean sampled reward: -6886.08). Current reward after update: -6150.92, Optimal reward -6150.92
Iteration 10 took 2.30 seconds (mean sampled reward: -7092.10). Current reward after update: -6135.48, Optimal reward -6135.48
Iteration 11 took 2.35 seconds (mean sampled reward: -7175.41). Current reward after update: -5812.76, Optimal reward -5812.76
Iteration 12 took 2.47 seconds (mean sampled reward: -6584.59). Current reward after update: -5663.16, Optimal reward -5663.16
Iteration 13 took 2.47 seconds (mean sampled reward: -6382.25). Current reward after update: -5570.41, Optimal reward -5570.41
Iteration 14 took 2.62 seconds (mean sampled reward: -6529.78). Current reward after update: -5438.48, Optimal reward -5438.48
Iteration 15 took 2.66 seconds (mean sampled reward: -6758.36). Current reward after update: -5417.27, Optimal reward -5417.27
Iteration 16 took 2.47 seconds (mean sampled reward: -6865.56). Current reward after update: -5376.96, Optimal reward -5376.96
Iteration 17 took 2.48 seconds (mean sampled reward: -6203.40). Current reward after update: -5239.98, Optimal reward -5239.98
Iteration 18 took 2.46 seconds (mean sampled reward: -6567.05). Current reward after update: -5714.61, Optimal reward -5239.98
Iteration 19 took 2.44 seconds (mean sampled reward: -6776.38). Current reward after update: -5533.50, Optimal reward -5239.98
Iteration 20 took 2.45 seconds (mean sampled reward: -6422.93). Current reward after update: -5237.44, Optimal reward -5237.44
Iteration 21 took 2.42 seconds (mean sampled reward: -5886.05). Current reward after update: -5180.34, Optimal reward -5180.34
Iteration 22 took 2.63 seconds (mean sampled reward: -5913.79). Current reward after update: -5666.96, Optimal reward -5180.34
Iteration 23 took 2.46 seconds (mean sampled reward: -5696.73). Current reward after update: -5208.47, Optimal reward -5180.34
Iteration 24 took 2.50 seconds (mean sampled reward: -5648.55). Current reward after update: -5580.09, Optimal reward -5180.34
Iteration 25 took 2.43 seconds (mean sampled reward: -5546.22). Current reward after update: -5034.36, Optimal reward -5034.36
Iteration 26 took 2.47 seconds (mean sampled reward: -5552.63). Current reward after update: -4499.52, Optimal reward -4499.52
Iteration 27 took 2.50 seconds (mean sampled reward: -5086.51). Current reward after update: -4628.01, Optimal reward -4499.52
Iteration 28 took 2.51 seconds (mean sampled reward: -5204.74). Current reward after update: -4133.77, Optimal reward -4133.77
Iteration 29 took 2.59 seconds (mean sampled reward: -5101.67). Current reward after update: -3754.01, Optimal reward -3754.01
Iteration 30 took 2.61 seconds (mean sampled reward: -5369.82). Current reward after update: -3202.17, Optimal reward -3202.17
Iteration 31 took 2.44 seconds (mean sampled reward: -5912.37). Current reward after update: -3247.92, Optimal reward -3202.17
Iteration 32 took 2.51 seconds (mean sampled reward: -5654.07). Current reward after update: -3184.21, Optimal reward -3184.21
Iteration 33 took 2.35 seconds (mean sampled reward: -5579.67). Current reward after update: -3125.01, Optimal reward -3125.01
Iteration 34 took 2.38 seconds (mean sampled reward: -5622.68). Current reward after update: -2631.33, Optimal reward -2631.33
Iteration 35 took 2.30 seconds (mean sampled reward: -5220.81). Current reward after update: -2093.99, Optimal reward -2093.99
Iteration 36 took 2.38 seconds (mean sampled reward: -4524.87). Current reward after update: -1802.34, Optimal reward -1802.34
Iteration 37 took 2.44 seconds (mean sampled reward: -5265.60). Current reward after update: -2452.63, Optimal reward -1802.34
Iteration 38 took 2.45 seconds (mean sampled reward: -4877.57). Current reward after update: -1786.70, Optimal reward -1786.70
Iteration 39 took 2.50 seconds (mean sampled reward: -5236.95). Current reward after update: -1783.27, Optimal reward -1783.27
Iteration 40 took 2.46 seconds (mean sampled reward: -5251.27). Current reward after update: -1638.56, Optimal reward -1638.56
Iteration 41 took 2.51 seconds (mean sampled reward: -5709.71). Current reward after update: -1967.10, Optimal reward -1638.56
Iteration 42 took 2.57 seconds (mean sampled reward: -5268.94). Current reward after update: -1764.78, Optimal reward -1638.56
Iteration 43 took 2.36 seconds (mean sampled reward: -4752.24). Current reward after update: -1759.28, Optimal reward -1638.56
Iteration 44 took 2.36 seconds (mean sampled reward: -4233.47). Current reward after update: -1852.39, Optimal reward -1638.56
Iteration 45 took 2.43 seconds (mean sampled reward: -4077.35). Current reward after update: -1735.76, Optimal reward -1638.56
Iteration 46 took 2.44 seconds (mean sampled reward: -4207.16). Current reward after update: -1901.76, Optimal reward -1638.56
Iteration 47 took 2.37 seconds (mean sampled reward: -3861.82). Current reward after update: -2589.35, Optimal reward -1638.56
Iteration 48 took 2.37 seconds (mean sampled reward: -3817.90). Current reward after update: -1773.71, Optimal reward -1638.56
Iteration 49 took 2.29 seconds (mean sampled reward: -4168.12). Current reward after update: -1538.61, Optimal reward -1538.61
Iteration 50 took 2.34 seconds (mean sampled reward: -4358.05). Current reward after update: -1679.93, Optimal reward -1538.61
Iteration 51 took 2.39 seconds (mean sampled reward: -4766.50). Current reward after update: -1368.67, Optimal reward -1368.67
Iteration 52 took 2.48 seconds (mean sampled reward: -5271.02). Current reward after update: -1440.92, Optimal reward -1368.67
Iteration 53 took 2.46 seconds (mean sampled reward: -4955.29). Current reward after update: -1611.87, Optimal reward -1368.67
Iteration 54 took 2.71 seconds (mean sampled reward: -4523.04). Current reward after update: -1289.30, Optimal reward -1289.30
Iteration 55 took 2.76 seconds (mean sampled reward: -4215.12). Current reward after update: -1606.94, Optimal reward -1289.30
Iteration 56 took 2.49 seconds (mean sampled reward: -3803.34). Current reward after update: -1151.40, Optimal reward -1151.40
Iteration 57 took 2.53 seconds (mean sampled reward: -4384.42). Current reward after update: -1596.19, Optimal reward -1151.40
Iteration 58 took 2.44 seconds (mean sampled reward: -4863.89). Current reward after update: -1418.52, Optimal reward -1151.40
Iteration 59 took 2.63 seconds (mean sampled reward: -4640.50). Current reward after update: -1553.67, Optimal reward -1151.40
Iteration 60 took 2.54 seconds (mean sampled reward: -5063.40). Current reward after update: -1298.78, Optimal reward -1151.40
Iteration 61 took 2.49 seconds (mean sampled reward: -4808.63). Current reward after update: -1346.00, Optimal reward -1151.40
Iteration 62 took 2.38 seconds (mean sampled reward: -5070.64). Current reward after update: -5215.29, Optimal reward -1151.40
Iteration 63 took 2.68 seconds (mean sampled reward: -5234.78). Current reward after update: -1242.56, Optimal reward -1151.40
Iteration 64 took 2.45 seconds (mean sampled reward: -5542.46). Current reward after update: -1170.33, Optimal reward -1151.40
Iteration 65 took 2.59 seconds (mean sampled reward: -5411.41). Current reward after update: -1207.49, Optimal reward -1151.40
Iteration 66 took 2.46 seconds (mean sampled reward: -5222.01). Current reward after update: -1239.28, Optimal reward -1151.40
Iteration 67 took 2.59 seconds (mean sampled reward: -5057.74). Current reward after update: -1297.18, Optimal reward -1151.40
Iteration 68 took 2.46 seconds (mean sampled reward: -4602.39). Current reward after update: -4412.65, Optimal reward -1151.40
Iteration 69 took 2.53 seconds (mean sampled reward: -4520.89). Current reward after update: -1123.84, Optimal reward -1123.84
Iteration 70 took 2.40 seconds (mean sampled reward: -4381.46). Current reward after update: -1153.45, Optimal reward -1123.84
Iteration 71 took 2.42 seconds (mean sampled reward: -4117.87). Current reward after update: -1154.77, Optimal reward -1123.84
Iteration 72 took 2.61 seconds (mean sampled reward: -4502.86). Current reward after update: -1216.24, Optimal reward -1123.84
Iteration 73 took 2.46 seconds (mean sampled reward: -3892.34). Current reward after update: -1213.19, Optimal reward -1123.84
Iteration 74 took 2.43 seconds (mean sampled reward: -3887.65). Current reward after update: -1916.14, Optimal reward -1123.84
Iteration 75 took 2.38 seconds (mean sampled reward: -3729.12). Current reward after update: -1292.94, Optimal reward -1123.84
Iteration 76 took 2.39 seconds (mean sampled reward: -3487.27). Current reward after update: -1327.99, Optimal reward -1123.84
Iteration 77 took 2.43 seconds (mean sampled reward: -3160.74). Current reward after update: -1211.14, Optimal reward -1123.84
Iteration 78 took 2.39 seconds (mean sampled reward: -2902.67). Current reward after update: -1137.75, Optimal reward -1123.84
Iteration 79 took 2.50 seconds (mean sampled reward: -2966.74). Current reward after update: -1125.17, Optimal reward -1123.84
Iteration 80 took 2.36 seconds (mean sampled reward: -3020.82). Current reward after update: -1590.18, Optimal reward -1123.84
Iteration 81 took 2.46 seconds (mean sampled reward: -2881.74). Current reward after update: -1135.95, Optimal reward -1123.84
Iteration 82 took 2.41 seconds (mean sampled reward: -3286.74). Current reward after update: -1141.89, Optimal reward -1123.84
Iteration 83 took 2.43 seconds (mean sampled reward: -3581.65). Current reward after update: -1079.15, Optimal reward -1079.15
Iteration 84 took 2.35 seconds (mean sampled reward: -3620.62). Current reward after update: -1220.51, Optimal reward -1079.15
Iteration 85 took 2.49 seconds (mean sampled reward: -3451.70). Current reward after update: -1012.41, Optimal reward -1012.41
Iteration 86 took 2.49 seconds (mean sampled reward: -4065.11). Current reward after update: -1074.87, Optimal reward -1012.41
Iteration 87 took 2.53 seconds (mean sampled reward: -3831.26). Current reward after update: -1133.69, Optimal reward -1012.41
Iteration 88 took 2.53 seconds (mean sampled reward: -3663.30). Current reward after update: -975.15, Optimal reward -975.15
Iteration 89 took 2.41 seconds (mean sampled reward: -3926.85). Current reward after update: -1380.53, Optimal reward -975.15
Iteration 90 took 2.44 seconds (mean sampled reward: -3282.13). Current reward after update: -1209.01, Optimal reward -975.15
Iteration 91 took 2.41 seconds (mean sampled reward: -3444.51). Current reward after update: -1492.48, Optimal reward -975.15
Iteration 92 took 2.43 seconds (mean sampled reward: -3568.92). Current reward after update: -1431.03, Optimal reward -975.15
Iteration 93 took 2.41 seconds (mean sampled reward: -3202.52). Current reward after update: -1298.05, Optimal reward -975.15
Iteration 94 took 2.37 seconds (mean sampled reward: -3297.41). Current reward after update: -1512.57, Optimal reward -975.15
Iteration 95 took 2.43 seconds (mean sampled reward: -3919.44). Current reward after update: -1682.62, Optimal reward -975.15
Iteration 96 took 2.42 seconds (mean sampled reward: -3576.67). Current reward after update: -1443.33, Optimal reward -975.15
Iteration 97 took 2.45 seconds (mean sampled reward: -3848.12). Current reward after update: -1467.97, Optimal reward -975.15
Iteration 98 took 2.40 seconds (mean sampled reward: -3791.65). Current reward after update: -5606.29, Optimal reward -975.15
Iteration 99 took 2.32 seconds (mean sampled reward: -3882.21). Current reward after update: -1600.42, Optimal reward -975.15
Iteration 100 took 2.45 seconds (mean sampled reward: -3913.79). Current reward after update: -2671.99, Optimal reward -975.15
Iteration 101 took 2.37 seconds (mean sampled reward: -4210.17). Current reward after update: -1375.19, Optimal reward -975.15
Iteration 102 took 2.42 seconds (mean sampled reward: -4381.48). Current reward after update: -1651.27, Optimal reward -975.15
Iteration 103 took 2.37 seconds (mean sampled reward: -4257.33). Current reward after update: -1507.52, Optimal reward -975.15
Iteration 104 took 2.39 seconds (mean sampled reward: -4539.82). Current reward after update: -1615.87, Optimal reward -975.15
Iteration 105 took 2.39 seconds (mean sampled reward: -4770.62). Current reward after update: -1491.25, Optimal reward -975.15
Iteration 106 took 2.47 seconds (mean sampled reward: -4510.73). Current reward after update: -1527.12, Optimal reward -975.15
Iteration 107 took 2.33 seconds (mean sampled reward: -4424.09). Current reward after update: -1458.46, Optimal reward -975.15
Iteration 108 took 2.32 seconds (mean sampled reward: -4206.91). Current reward after update: -2420.16, Optimal reward -975.15
Iteration 109 took 2.68 seconds (mean sampled reward: -4399.20). Current reward after update: -1457.47, Optimal reward -975.15
Iteration 110 took 2.40 seconds (mean sampled reward: -4387.97). Current reward after update: -1371.91, Optimal reward -975.15
Iteration 111 took 2.38 seconds (mean sampled reward: -4524.47). Current reward after update: -1358.51, Optimal reward -975.15
Iteration 112 took 2.56 seconds (mean sampled reward: -4612.99). Current reward after update: -1355.54, Optimal reward -975.15
Iteration 113 took 2.52 seconds (mean sampled reward: -4627.58). Current reward after update: -1391.50, Optimal reward -975.15
Iteration 114 took 2.39 seconds (mean sampled reward: -4596.24). Current reward after update: -1177.65, Optimal reward -975.15
Iteration 115 took 2.35 seconds (mean sampled reward: -4274.72). Current reward after update: -1454.83, Optimal reward -975.15
Iteration 116 took 2.41 seconds (mean sampled reward: -4525.43). Current reward after update: -1227.89, Optimal reward -975.15
Iteration 117 took 2.39 seconds (mean sampled reward: -4571.54). Current reward after update: -1525.47, Optimal reward -975.15
Iteration 118 took 2.45 seconds (mean sampled reward: -4213.64). Current reward after update: -1324.38, Optimal reward -975.15
Iteration 119 took 2.44 seconds (mean sampled reward: -4709.34). Current reward after update: -1199.24, Optimal reward -975.15
Iteration 120 took 2.39 seconds (mean sampled reward: -5031.97). Current reward after update: -1305.71, Optimal reward -975.15
Iteration 121 took 2.51 seconds (mean sampled reward: -4480.02). Current reward after update: -1449.63, Optimal reward -975.15
Iteration 122 took 2.40 seconds (mean sampled reward: -4612.93). Current reward after update: -1344.39, Optimal reward -975.15
Iteration 123 took 2.47 seconds (mean sampled reward: -4741.99). Current reward after update: -1248.84, Optimal reward -975.15
Iteration 124 took 2.37 seconds (mean sampled reward: -5335.20). Current reward after update: -1466.09, Optimal reward -975.15
Iteration 125 took 2.33 seconds (mean sampled reward: -5359.48). Current reward after update: -5814.14, Optimal reward -975.15
Iteration 126 took 2.47 seconds (mean sampled reward: -5258.80). Current reward after update: -1752.87, Optimal reward -975.15
Iteration 127 took 2.37 seconds (mean sampled reward: -6089.30). Current reward after update: -1719.38, Optimal reward -975.15
Iteration 128 took 2.33 seconds (mean sampled reward: -4876.02). Current reward after update: -1436.48, Optimal reward -975.15
Iteration 129 took 2.33 seconds (mean sampled reward: -4413.82). Current reward after update: -1549.69, Optimal reward -975.15
Iteration 130 took 2.39 seconds (mean sampled reward: -4745.54). Current reward after update: -1308.11, Optimal reward -975.15
Iteration 131 took 2.38 seconds (mean sampled reward: -4111.22). Current reward after update: -1376.39, Optimal reward -975.15
Iteration 132 took 2.37 seconds (mean sampled reward: -4070.68). Current reward after update: -1409.48, Optimal reward -975.15
Iteration 133 took 2.32 seconds (mean sampled reward: -4017.89). Current reward after update: -2642.35, Optimal reward -975.15
Iteration 134 took 2.34 seconds (mean sampled reward: -3640.30). Current reward after update: -1278.66, Optimal reward -975.15
Iteration 135 took 2.35 seconds (mean sampled reward: -3870.90). Current reward after update: -1454.63, Optimal reward -975.15
Iteration 136 took 2.29 seconds (mean sampled reward: -3778.75). Current reward after update: -1445.85, Optimal reward -975.15
Iteration 137 took 2.36 seconds (mean sampled reward: -4124.07). Current reward after update: -1435.08, Optimal reward -975.15
Iteration 138 took 2.31 seconds (mean sampled reward: -4203.88). Current reward after update: -1493.60, Optimal reward -975.15
Iteration 139 took 2.30 seconds (mean sampled reward: -4041.36). Current reward after update: -1683.70, Optimal reward -975.15
Iteration 140 took 2.29 seconds (mean sampled reward: -3696.97). Current reward after update: -1418.83, Optimal reward -975.15
Iteration 141 took 2.28 seconds (mean sampled reward: -3904.66). Current reward after update: -1676.23, Optimal reward -975.15
Iteration 142 took 2.40 seconds (mean sampled reward: -4054.02). Current reward after update: -1411.46, Optimal reward -975.15
Iteration 143 took 2.36 seconds (mean sampled reward: -4135.70). Current reward after update: -1563.32, Optimal reward -975.15
Iteration 144 took 2.34 seconds (mean sampled reward: -4385.81). Current reward after update: -1394.61, Optimal reward -975.15
Iteration 145 took 2.32 seconds (mean sampled reward: -5357.89). Current reward after update: -1571.20, Optimal reward -975.15
Iteration 146 took 2.36 seconds (mean sampled reward: -4312.95). Current reward after update: -2416.47, Optimal reward -975.15
Iteration 147 took 2.33 seconds (mean sampled reward: -4451.04). Current reward after update: -1725.27, Optimal reward -975.15
Iteration 148 took 2.40 seconds (mean sampled reward: -4381.82). Current reward after update: -2131.08, Optimal reward -975.15
Iteration 149 took 2.33 seconds (mean sampled reward: -4644.66). Current reward after update: -1528.88, Optimal reward -975.15
Iteration 150 took 2.36 seconds (mean sampled reward: -4388.65). Current reward after update: -1475.24, Optimal reward -975.15
Iteration 151 took 2.33 seconds (mean sampled reward: -3837.91). Current reward after update: -1505.28, Optimal reward -975.15
Iteration 152 took 2.43 seconds (mean sampled reward: -4859.80). Current reward after update: -1515.76, Optimal reward -975.15
Iteration 153 took 2.33 seconds (mean sampled reward: -4223.23). Current reward after update: -1596.18, Optimal reward -975.15
Iteration 154 took 2.34 seconds (mean sampled reward: -4642.48). Current reward after update: -1352.08, Optimal reward -975.15
Iteration 155 took 2.40 seconds (mean sampled reward: -5198.07). Current reward after update: -1466.93, Optimal reward -975.15
Iteration 156 took 2.43 seconds (mean sampled reward: -4678.80). Current reward after update: -1549.78, Optimal reward -975.15
Iteration 157 took 2.42 seconds (mean sampled reward: -4538.14). Current reward after update: -1542.66, Optimal reward -975.15
Iteration 158 took 2.38 seconds (mean sampled reward: -5017.83). Current reward after update: -1540.24, Optimal reward -975.15
Iteration 159 took 2.53 seconds (mean sampled reward: -4946.88). Current reward after update: -1565.24, Optimal reward -975.15
Iteration 160 took 2.37 seconds (mean sampled reward: -4254.99). Current reward after update: -1648.54, Optimal reward -975.15
Iteration 161 took 2.48 seconds (mean sampled reward: -4095.53). Current reward after update: -1515.94, Optimal reward -975.15
Iteration 162 took 2.47 seconds (mean sampled reward: -4366.91). Current reward after update: -1486.63, Optimal reward -975.15
Iteration 163 took 2.49 seconds (mean sampled reward: -4350.31). Current reward after update: -1533.91, Optimal reward -975.15
Iteration 164 took 2.47 seconds (mean sampled reward: -4624.51). Current reward after update: -3032.15, Optimal reward -975.15
Iteration 165 took 2.39 seconds (mean sampled reward: -4753.61). Current reward after update: -1470.20, Optimal reward -975.15
Iteration 166 took 2.46 seconds (mean sampled reward: -4883.20). Current reward after update: -2099.64, Optimal reward -975.15
Iteration 167 took 2.40 seconds (mean sampled reward: -4116.79). Current reward after update: -1469.45, Optimal reward -975.15
Iteration 168 took 2.34 seconds (mean sampled reward: -4921.51). Current reward after update: -1464.56, Optimal reward -975.15
Iteration 169 took 2.31 seconds (mean sampled reward: -4613.02). Current reward after update: -1671.94, Optimal reward -975.15
Iteration 170 took 2.31 seconds (mean sampled reward: -4382.36). Current reward after update: -1583.87, Optimal reward -975.15
Iteration 171 took 2.38 seconds (mean sampled reward: -4025.95). Current reward after update: -1455.50, Optimal reward -975.15
Iteration 172 took 2.36 seconds (mean sampled reward: -3728.65). Current reward after update: -1482.47, Optimal reward -975.15
Iteration 173 took 2.28 seconds (mean sampled reward: -3752.37). Current reward after update: -1513.09, Optimal reward -975.15
Iteration 174 took 2.31 seconds (mean sampled reward: -4229.14). Current reward after update: -1403.26, Optimal reward -975.15
Iteration 175 took 2.31 seconds (mean sampled reward: -3887.94). Current reward after update: -1531.10, Optimal reward -975.15
Iteration 176 took 2.32 seconds (mean sampled reward: -3865.59). Current reward after update: -1467.91, Optimal reward -975.15
Iteration 177 took 2.34 seconds (mean sampled reward: -4131.51). Current reward after update: -1371.92, Optimal reward -975.15
Iteration 178 took 2.30 seconds (mean sampled reward: -3865.00). Current reward after update: -1363.01, Optimal reward -975.15
Iteration 179 took 2.33 seconds (mean sampled reward: -4167.45). Current reward after update: -1299.90, Optimal reward -975.15
Iteration 180 took 2.35 seconds (mean sampled reward: -3989.40). Current reward after update: -2464.80, Optimal reward -975.15
Iteration 181 took 2.33 seconds (mean sampled reward: -4240.00). Current reward after update: -1418.24, Optimal reward -975.15
Iteration 182 took 2.35 seconds (mean sampled reward: -4148.26). Current reward after update: -1307.77, Optimal reward -975.15
Iteration 183 took 2.32 seconds (mean sampled reward: -3719.86). Current reward after update: -1453.88, Optimal reward -975.15
Iteration 184 took 2.32 seconds (mean sampled reward: -3320.31). Current reward after update: -1280.56, Optimal reward -975.15
Iteration 185 took 2.34 seconds (mean sampled reward: -3718.73). Current reward after update: -1351.04, Optimal reward -975.15
Iteration 186 took 2.32 seconds (mean sampled reward: -3932.84). Current reward after update: -1975.84, Optimal reward -975.15
Iteration 187 took 2.30 seconds (mean sampled reward: -3627.80). Current reward after update: -1270.31, Optimal reward -975.15
Iteration 188 took 2.39 seconds (mean sampled reward: -3855.82). Current reward after update: -1471.06, Optimal reward -975.15
Iteration 189 took 2.32 seconds (mean sampled reward: -3658.31). Current reward after update: -2546.22, Optimal reward -975.15
Iteration 190 took 2.34 seconds (mean sampled reward: -3316.96). Current reward after update: -1444.20, Optimal reward -975.15
Iteration 191 took 2.34 seconds (mean sampled reward: -3681.36). Current reward after update: -1539.60, Optimal reward -975.15
Iteration 192 took 2.29 seconds (mean sampled reward: -3287.10). Current reward after update: -1446.66, Optimal reward -975.15
Iteration 193 took 2.30 seconds (mean sampled reward: -3244.84). Current reward after update: -1487.27, Optimal reward -975.15
Iteration 194 took 2.32 seconds (mean sampled reward: -3237.57). Current reward after update: -1375.71, Optimal reward -975.15
Iteration 195 took 2.28 seconds (mean sampled reward: -3433.20). Current reward after update: -4234.49, Optimal reward -975.15
Iteration 196 took 2.37 seconds (mean sampled reward: -3538.15). Current reward after update: -1527.04, Optimal reward -975.15
Iteration 197 took 2.34 seconds (mean sampled reward: -3635.87). Current reward after update: -1513.51, Optimal reward -975.15
Iteration 198 took 2.38 seconds (mean sampled reward: -3702.65). Current reward after update: -1496.27, Optimal reward -975.15
Iteration 199 took 2.37 seconds (mean sampled reward: -3615.65). Current reward after update: -1440.75, Optimal reward -975.15
Iteration 200 took 2.39 seconds (mean sampled reward: -3423.36). Current reward after update: -1371.26, Optimal reward -975.15
Iteration 1 took 2.37 seconds (mean sampled reward: -7629.28). Current reward after update: -7311.30, Optimal reward -7311.30
Iteration 2 took 2.27 seconds (mean sampled reward: -7629.61). Current reward after update: -7356.74, Optimal reward -7311.30
Iteration 3 took 2.61 seconds (mean sampled reward: -7631.56). Current reward after update: -7386.32, Optimal reward -7311.30
Iteration 4 took 2.31 seconds (mean sampled reward: -7588.02). Current reward after update: -7258.90, Optimal reward -7258.90
Iteration 5 took 2.51 seconds (mean sampled reward: -7566.17). Current reward after update: -7264.34, Optimal reward -7258.90
Iteration 6 took 2.25 seconds (mean sampled reward: -7535.89). Current reward after update: -7352.86, Optimal reward -7258.90
Iteration 7 took 2.32 seconds (mean sampled reward: -7530.79). Current reward after update: -7263.44, Optimal reward -7258.90
Iteration 8 took 2.24 seconds (mean sampled reward: -7519.82). Current reward after update: -7236.64, Optimal reward -7236.64
Iteration 9 took 2.29 seconds (mean sampled reward: -7546.32). Current reward after update: -7227.05, Optimal reward -7227.05
Iteration 10 took 2.32 seconds (mean sampled reward: -7538.32). Current reward after update: -7224.38, Optimal reward -7224.38
Iteration 11 took 2.46 seconds (mean sampled reward: -7516.79). Current reward after update: -7204.68, Optimal reward -7204.68
Iteration 12 took 2.43 seconds (mean sampled reward: -7444.47). Current reward after update: -6933.94, Optimal reward -6933.94
Iteration 13 took 2.54 seconds (mean sampled reward: -7340.01). Current reward after update: -6722.65, Optimal reward -6722.65
Iteration 14 took 2.40 seconds (mean sampled reward: -7276.89). Current reward after update: -6443.30, Optimal reward -6443.30
Iteration 15 took 2.58 seconds (mean sampled reward: -7024.86). Current reward after update: -5773.49, Optimal reward -5773.49
Iteration 16 took 2.53 seconds (mean sampled reward: -7346.68). Current reward after update: -6015.35, Optimal reward -5773.49
Iteration 17 took 2.31 seconds (mean sampled reward: -7096.51). Current reward after update: -5464.63, Optimal reward -5464.63
Iteration 18 took 2.44 seconds (mean sampled reward: -7088.99). Current reward after update: -5430.51, Optimal reward -5430.51
Iteration 19 took 2.45 seconds (mean sampled reward: -7061.19). Current reward after update: -5250.62, Optimal reward -5250.62
Iteration 20 took 2.43 seconds (mean sampled reward: -6328.30). Current reward after update: -4724.14, Optimal reward -4724.14
Iteration 21 took 2.46 seconds (mean sampled reward: -6204.68). Current reward after update: -4463.98, Optimal reward -4463.98
Iteration 22 took 2.36 seconds (mean sampled reward: -6018.88). Current reward after update: -4434.85, Optimal reward -4434.85
Iteration 23 took 2.32 seconds (mean sampled reward: -6254.21). Current reward after update: -4490.52, Optimal reward -4434.85
Iteration 24 took 2.38 seconds (mean sampled reward: -6125.14). Current reward after update: -4423.17, Optimal reward -4423.17
Iteration 25 took 2.43 seconds (mean sampled reward: -5745.71). Current reward after update: -4350.27, Optimal reward -4350.27
Iteration 26 took 2.34 seconds (mean sampled reward: -5943.58). Current reward after update: -4210.22, Optimal reward -4210.22
Iteration 27 took 2.33 seconds (mean sampled reward: -6284.76). Current reward after update: -4181.97, Optimal reward -4181.97
Iteration 28 took 2.28 seconds (mean sampled reward: -6533.15). Current reward after update: -4248.11, Optimal reward -4181.97
Iteration 29 took 2.26 seconds (mean sampled reward: -6357.72). Current reward after update: -4167.04, Optimal reward -4167.04
Iteration 30 took 2.24 seconds (mean sampled reward: -6505.15). Current reward after update: -4195.53, Optimal reward -4167.04
Iteration 31 took 2.27 seconds (mean sampled reward: -6578.41). Current reward after update: -4776.67, Optimal reward -4167.04
Iteration 32 took 2.37 seconds (mean sampled reward: -6358.10). Current reward after update: -7067.17, Optimal reward -4167.04
Iteration 33 took 2.31 seconds (mean sampled reward: -6430.74). Current reward after update: -4154.41, Optimal reward -4154.41
Iteration 34 took 2.21 seconds (mean sampled reward: -6365.91). Current reward after update: -4073.15, Optimal reward -4073.15
Iteration 35 took 2.26 seconds (mean sampled reward: -6364.43). Current reward after update: -4022.95, Optimal reward -4022.95
Iteration 36 took 2.25 seconds (mean sampled reward: -6341.79). Current reward after update: -4075.42, Optimal reward -4022.95
Iteration 37 took 2.31 seconds (mean sampled reward: -5993.10). Current reward after update: -4063.24, Optimal reward -4022.95
Iteration 38 took 2.24 seconds (mean sampled reward: -5639.50). Current reward after update: -5144.19, Optimal reward -4022.95
Iteration 39 took 2.44 seconds (mean sampled reward: -5652.77). Current reward after update: -3964.11, Optimal reward -3964.11
Iteration 40 took 2.43 seconds (mean sampled reward: -6092.17). Current reward after update: -3813.71, Optimal reward -3813.71
Iteration 41 took 2.36 seconds (mean sampled reward: -6198.81). Current reward after update: -4053.36, Optimal reward -3813.71
Iteration 42 took 2.26 seconds (mean sampled reward: -5595.73). Current reward after update: -4012.67, Optimal reward -3813.71
Iteration 43 took 2.34 seconds (mean sampled reward: -5780.37). Current reward after update: -4006.47, Optimal reward -3813.71
Iteration 44 took 2.28 seconds (mean sampled reward: -5760.25). Current reward after update: -3943.06, Optimal reward -3813.71
Iteration 45 took 2.31 seconds (mean sampled reward: -5172.98). Current reward after update: -4096.94, Optimal reward -3813.71
Iteration 46 took 2.28 seconds (mean sampled reward: -5685.54). Current reward after update: -3671.62, Optimal reward -3671.62
Iteration 47 took 2.22 seconds (mean sampled reward: -6057.91). Current reward after update: -3836.62, Optimal reward -3671.62
Iteration 48 took 2.23 seconds (mean sampled reward: -6283.74). Current reward after update: -3868.80, Optimal reward -3671.62
Iteration 49 took 2.16 seconds (mean sampled reward: -6133.18). Current reward after update: -3941.07, Optimal reward -3671.62
Iteration 50 took 2.21 seconds (mean sampled reward: -6289.19). Current reward after update: -3951.03, Optimal reward -3671.62
Iteration 51 took 2.20 seconds (mean sampled reward: -5606.97). Current reward after update: -3614.18, Optimal reward -3614.18
Iteration 52 took 2.20 seconds (mean sampled reward: -5994.03). Current reward after update: -3646.83, Optimal reward -3614.18
Iteration 53 took 2.30 seconds (mean sampled reward: -5414.37). Current reward after update: -3489.20, Optimal reward -3489.20
Iteration 54 took 2.41 seconds (mean sampled reward: -5906.77). Current reward after update: -3779.04, Optimal reward -3489.20
Iteration 55 took 2.24 seconds (mean sampled reward: -5511.12). Current reward after update: -3424.43, Optimal reward -3424.43
Iteration 56 took 2.29 seconds (mean sampled reward: -5639.10). Current reward after update: -3064.76, Optimal reward -3064.76
Iteration 57 took 2.19 seconds (mean sampled reward: -5553.20). Current reward after update: -2822.99, Optimal reward -2822.99
Iteration 58 took 2.26 seconds (mean sampled reward: -4989.96). Current reward after update: -2685.04, Optimal reward -2685.04
Iteration 59 took 2.34 seconds (mean sampled reward: -5095.00). Current reward after update: -2702.42, Optimal reward -2685.04
Iteration 60 took 2.35 seconds (mean sampled reward: -5782.04). Current reward after update: -2680.17, Optimal reward -2680.17
Iteration 61 took 2.29 seconds (mean sampled reward: -6231.51). Current reward after update: -2706.52, Optimal reward -2680.17
Iteration 62 took 2.21 seconds (mean sampled reward: -5914.32). Current reward after update: -2714.19, Optimal reward -2680.17
Iteration 63 took 2.28 seconds (mean sampled reward: -6009.08). Current reward after update: -3432.07, Optimal reward -2680.17
Iteration 64 took 2.16 seconds (mean sampled reward: -6024.16). Current reward after update: -2812.98, Optimal reward -2680.17
Iteration 65 took 2.19 seconds (mean sampled reward: -6514.70). Current reward after update: -2691.30, Optimal reward -2680.17
Iteration 66 took 2.30 seconds (mean sampled reward: -5986.96). Current reward after update: -2669.11, Optimal reward -2669.11
Iteration 67 took 2.17 seconds (mean sampled reward: -6033.59). Current reward after update: -2652.58, Optimal reward -2652.58
Iteration 68 took 2.18 seconds (mean sampled reward: -6383.21). Current reward after update: -2800.54, Optimal reward -2652.58
Iteration 69 took 2.16 seconds (mean sampled reward: -5879.39). Current reward after update: -4445.31, Optimal reward -2652.58
Iteration 70 took 2.23 seconds (mean sampled reward: -4986.55). Current reward after update: -2686.66, Optimal reward -2652.58
Iteration 71 took 2.19 seconds (mean sampled reward: -4330.82). Current reward after update: -3163.32, Optimal reward -2652.58
Iteration 72 took 2.19 seconds (mean sampled reward: -4534.53). Current reward after update: -2665.32, Optimal reward -2652.58
Iteration 73 took 2.22 seconds (mean sampled reward: -4414.35). Current reward after update: -3792.46, Optimal reward -2652.58
Iteration 74 took 2.25 seconds (mean sampled reward: -4061.67). Current reward after update: -2623.70, Optimal reward -2623.70
Iteration 75 took 2.24 seconds (mean sampled reward: -3946.19). Current reward after update: -2683.77, Optimal reward -2623.70
Iteration 76 took 2.18 seconds (mean sampled reward: -3915.74). Current reward after update: -2599.63, Optimal reward -2599.63
Iteration 77 took 2.23 seconds (mean sampled reward: -3813.39). Current reward after update: -2954.27, Optimal reward -2599.63
Iteration 78 took 2.39 seconds (mean sampled reward: -3773.03). Current reward after update: -2610.54, Optimal reward -2599.63
Iteration 79 took 2.29 seconds (mean sampled reward: -4800.00). Current reward after update: -2549.19, Optimal reward -2549.19
Iteration 80 took 2.32 seconds (mean sampled reward: -6071.15). Current reward after update: -3116.34, Optimal reward -2549.19
Iteration 81 took 2.27 seconds (mean sampled reward: -5573.94). Current reward after update: -2580.72, Optimal reward -2549.19
Iteration 82 took 2.34 seconds (mean sampled reward: -5412.62). Current reward after update: -3279.67, Optimal reward -2549.19
Iteration 83 took 2.26 seconds (mean sampled reward: -5025.70). Current reward after update: -6896.65, Optimal reward -2549.19
Iteration 84 took 2.28 seconds (mean sampled reward: -5037.87). Current reward after update: -4243.31, Optimal reward -2549.19
Iteration 85 took 2.25 seconds (mean sampled reward: -4758.44). Current reward after update: -2639.98, Optimal reward -2549.19
Iteration 86 took 2.29 seconds (mean sampled reward: -4942.06). Current reward after update: -2749.98, Optimal reward -2549.19
Iteration 87 took 2.25 seconds (mean sampled reward: -5345.75). Current reward after update: -2621.72, Optimal reward -2549.19
Iteration 88 took 2.27 seconds (mean sampled reward: -4076.29). Current reward after update: -2542.18, Optimal reward -2542.18
Iteration 89 took 2.32 seconds (mean sampled reward: -4406.94). Current reward after update: -2621.00, Optimal reward -2542.18
Iteration 90 took 2.24 seconds (mean sampled reward: -4559.26). Current reward after update: -3015.68, Optimal reward -2542.18
Iteration 91 took 2.26 seconds (mean sampled reward: -3948.83). Current reward after update: -2536.04, Optimal reward -2536.04
Iteration 92 took 2.29 seconds (mean sampled reward: -4234.18). Current reward after update: -4210.55, Optimal reward -2536.04
Iteration 93 took 2.26 seconds (mean sampled reward: -3764.70). Current reward after update: -3324.74, Optimal reward -2536.04
Iteration 94 took 2.24 seconds (mean sampled reward: -3826.23). Current reward after update: -3007.75, Optimal reward -2536.04
Iteration 95 took 2.28 seconds (mean sampled reward: -3768.88). Current reward after update: -2580.53, Optimal reward -2536.04
Iteration 96 took 2.41 seconds (mean sampled reward: -3848.79). Current reward after update: -4030.39, Optimal reward -2536.04
Iteration 97 took 2.22 seconds (mean sampled reward: -3499.27). Current reward after update: -2821.65, Optimal reward -2536.04
Iteration 98 took 2.27 seconds (mean sampled reward: -3428.07). Current reward after update: -3509.88, Optimal reward -2536.04
Iteration 99 took 2.20 seconds (mean sampled reward: -4237.23). Current reward after update: -2677.14, Optimal reward -2536.04
Iteration 100 took 2.26 seconds (mean sampled reward: -3893.56). Current reward after update: -2623.24, Optimal reward -2536.04
Iteration 101 took 2.24 seconds (mean sampled reward: -3780.91). Current reward after update: -3068.71, Optimal reward -2536.04
Iteration 102 took 2.23 seconds (mean sampled reward: -3631.93). Current reward after update: -2654.55, Optimal reward -2536.04
Iteration 103 took 2.21 seconds (mean sampled reward: -3513.53). Current reward after update: -3385.27, Optimal reward -2536.04
Iteration 104 took 2.17 seconds (mean sampled reward: -3476.37). Current reward after update: -2617.53, Optimal reward -2536.04
Iteration 105 took 2.19 seconds (mean sampled reward: -3432.01). Current reward after update: -2585.06, Optimal reward -2536.04
Iteration 106 took 2.37 seconds (mean sampled reward: -4039.83). Current reward after update: -2647.04, Optimal reward -2536.04
Iteration 107 took 2.47 seconds (mean sampled reward: -3592.13). Current reward after update: -4135.94, Optimal reward -2536.04
Iteration 108 took 2.29 seconds (mean sampled reward: -3545.58). Current reward after update: -3070.98, Optimal reward -2536.04
Iteration 109 took 2.23 seconds (mean sampled reward: -4261.78). Current reward after update: -2597.27, Optimal reward -2536.04
Iteration 110 took 2.38 seconds (mean sampled reward: -4927.90). Current reward after update: -3079.78, Optimal reward -2536.04
Iteration 111 took 2.26 seconds (mean sampled reward: -5587.57). Current reward after update: -3266.78, Optimal reward -2536.04
Iteration 112 took 2.37 seconds (mean sampled reward: -5292.52). Current reward after update: -2857.64, Optimal reward -2536.04
Iteration 113 took 2.48 seconds (mean sampled reward: -5416.39). Current reward after update: -2625.70, Optimal reward -2536.04
Iteration 114 took 2.39 seconds (mean sampled reward: -5321.32). Current reward after update: -3667.62, Optimal reward -2536.04
Iteration 115 took 2.40 seconds (mean sampled reward: -5505.32). Current reward after update: -2705.08, Optimal reward -2536.04
Iteration 116 took 2.33 seconds (mean sampled reward: -4443.78). Current reward after update: -2607.78, Optimal reward -2536.04
Iteration 117 took 2.24 seconds (mean sampled reward: -5102.51). Current reward after update: -2617.53, Optimal reward -2536.04
Iteration 118 took 2.25 seconds (mean sampled reward: -4131.27). Current reward after update: -3270.29, Optimal reward -2536.04
Iteration 119 took 2.44 seconds (mean sampled reward: -4780.18). Current reward after update: -2620.49, Optimal reward -2536.04
Iteration 120 took 2.16 seconds (mean sampled reward: -5631.09). Current reward after update: -2581.94, Optimal reward -2536.04
Iteration 121 took 2.27 seconds (mean sampled reward: -3478.83). Current reward after update: -3405.26, Optimal reward -2536.04
Iteration 122 took 2.23 seconds (mean sampled reward: -4543.45). Current reward after update: -2720.67, Optimal reward -2536.04
Iteration 123 took 2.22 seconds (mean sampled reward: -4229.26). Current reward after update: -2558.44, Optimal reward -2536.04
Iteration 124 took 2.26 seconds (mean sampled reward: -5851.81). Current reward after update: -2616.95, Optimal reward -2536.04
Iteration 125 took 2.28 seconds (mean sampled reward: -6094.23). Current reward after update: -2594.10, Optimal reward -2536.04
Iteration 126 took 2.20 seconds (mean sampled reward: -4150.26). Current reward after update: -2607.98, Optimal reward -2536.04
Iteration 127 took 2.23 seconds (mean sampled reward: -5344.04). Current reward after update: -2660.55, Optimal reward -2536.04
Iteration 128 took 2.25 seconds (mean sampled reward: -4826.23). Current reward after update: -3829.68, Optimal reward -2536.04
Iteration 129 took 2.20 seconds (mean sampled reward: -4143.96). Current reward after update: -3247.82, Optimal reward -2536.04
Iteration 130 took 2.31 seconds (mean sampled reward: -4033.19). Current reward after update: -2561.79, Optimal reward -2536.04
Iteration 131 took 2.29 seconds (mean sampled reward: -4237.24). Current reward after update: -3236.86, Optimal reward -2536.04
Iteration 132 took 2.25 seconds (mean sampled reward: -4885.43). Current reward after update: -4057.86, Optimal reward -2536.04
Iteration 133 took 2.30 seconds (mean sampled reward: -4690.28). Current reward after update: -2570.62, Optimal reward -2536.04
Iteration 134 took 2.24 seconds (mean sampled reward: -4596.65). Current reward after update: -2540.96, Optimal reward -2536.04
Iteration 135 took 2.26 seconds (mean sampled reward: -3653.03). Current reward after update: -3221.92, Optimal reward -2536.04
Iteration 136 took 2.19 seconds (mean sampled reward: -3618.16). Current reward after update: -2543.16, Optimal reward -2536.04
Iteration 137 took 2.10 seconds (mean sampled reward: -3571.30). Current reward after update: -2574.19, Optimal reward -2536.04
Iteration 138 took 2.27 seconds (mean sampled reward: -5072.75). Current reward after update: -4246.03, Optimal reward -2536.04
Iteration 139 took 2.22 seconds (mean sampled reward: -3728.00). Current reward after update: -2734.88, Optimal reward -2536.04
Iteration 140 took 2.23 seconds (mean sampled reward: -4819.09). Current reward after update: -2597.50, Optimal reward -2536.04
Iteration 141 took 2.20 seconds (mean sampled reward: -3756.57). Current reward after update: -2779.28, Optimal reward -2536.04
Iteration 142 took 2.23 seconds (mean sampled reward: -3948.76). Current reward after update: -2583.37, Optimal reward -2536.04
Iteration 143 took 2.22 seconds (mean sampled reward: -5217.66). Current reward after update: -2557.90, Optimal reward -2536.04
Iteration 144 took 2.27 seconds (mean sampled reward: -4760.53). Current reward after update: -2723.52, Optimal reward -2536.04
Iteration 145 took 2.27 seconds (mean sampled reward: -4947.68). Current reward after update: -2642.42, Optimal reward -2536.04
Iteration 146 took 2.25 seconds (mean sampled reward: -3778.09). Current reward after update: -2759.39, Optimal reward -2536.04
Iteration 147 took 2.22 seconds (mean sampled reward: -4547.94). Current reward after update: -2995.85, Optimal reward -2536.04
Iteration 148 took 2.22 seconds (mean sampled reward: -5701.56). Current reward after update: -3218.97, Optimal reward -2536.04
Iteration 149 took 2.24 seconds (mean sampled reward: -5145.28). Current reward after update: -2600.17, Optimal reward -2536.04
Iteration 150 took 2.27 seconds (mean sampled reward: -5799.05). Current reward after update: -3590.47, Optimal reward -2536.04
Iteration 151 took 2.23 seconds (mean sampled reward: -4746.47). Current reward after update: -3616.16, Optimal reward -2536.04
Iteration 152 took 2.28 seconds (mean sampled reward: -4957.01). Current reward after update: -3239.55, Optimal reward -2536.04
Iteration 153 took 2.25 seconds (mean sampled reward: -4104.07). Current reward after update: -2776.42, Optimal reward -2536.04
Iteration 154 took 2.26 seconds (mean sampled reward: -3936.81). Current reward after update: -2662.13, Optimal reward -2536.04
Iteration 155 took 2.29 seconds (mean sampled reward: -3824.01). Current reward after update: -2700.46, Optimal reward -2536.04
Iteration 156 took 2.22 seconds (mean sampled reward: -3651.28). Current reward after update: -2741.84, Optimal reward -2536.04
Iteration 157 took 2.33 seconds (mean sampled reward: -3691.97). Current reward after update: -2572.87, Optimal reward -2536.04
Iteration 158 took 2.28 seconds (mean sampled reward: -4040.75). Current reward after update: -2531.77, Optimal reward -2531.77
Iteration 159 took 2.21 seconds (mean sampled reward: -3927.17). Current reward after update: -2621.29, Optimal reward -2531.77
Iteration 160 took 2.27 seconds (mean sampled reward: -3695.13). Current reward after update: -2589.74, Optimal reward -2531.77
Iteration 161 took 2.32 seconds (mean sampled reward: -3481.41). Current reward after update: -2606.83, Optimal reward -2531.77
Iteration 162 took 2.27 seconds (mean sampled reward: -3279.54). Current reward after update: -3039.65, Optimal reward -2531.77
Iteration 163 took 2.27 seconds (mean sampled reward: -3391.04). Current reward after update: -2568.45, Optimal reward -2531.77
Iteration 164 took 2.26 seconds (mean sampled reward: -3475.64). Current reward after update: -2910.55, Optimal reward -2531.77
Iteration 165 took 2.31 seconds (mean sampled reward: -3742.24). Current reward after update: -3749.42, Optimal reward -2531.77
Iteration 166 took 2.33 seconds (mean sampled reward: -3492.47). Current reward after update: -2638.54, Optimal reward -2531.77
Iteration 167 took 2.37 seconds (mean sampled reward: -3313.31). Current reward after update: -2946.66, Optimal reward -2531.77
Iteration 168 took 2.33 seconds (mean sampled reward: -5156.06). Current reward after update: -2602.89, Optimal reward -2531.77
Iteration 169 took 2.34 seconds (mean sampled reward: -3798.20). Current reward after update: -2737.86, Optimal reward -2531.77
Iteration 170 took 2.31 seconds (mean sampled reward: -3253.99). Current reward after update: -3118.28, Optimal reward -2531.77
Iteration 171 took 2.32 seconds (mean sampled reward: -3601.68). Current reward after update: -3162.07, Optimal reward -2531.77
Iteration 172 took 2.32 seconds (mean sampled reward: -3823.18). Current reward after update: -3261.43, Optimal reward -2531.77
Iteration 173 took 2.26 seconds (mean sampled reward: -3330.12). Current reward after update: -2788.05, Optimal reward -2531.77
Iteration 174 took 2.35 seconds (mean sampled reward: -3437.60). Current reward after update: -2545.95, Optimal reward -2531.77
Iteration 175 took 2.37 seconds (mean sampled reward: -3599.74). Current reward after update: -3263.46, Optimal reward -2531.77
Iteration 176 took 2.34 seconds (mean sampled reward: -3482.29). Current reward after update: -2575.04, Optimal reward -2531.77
Iteration 177 took 2.32 seconds (mean sampled reward: -3282.01). Current reward after update: -3857.60, Optimal reward -2531.77
Iteration 178 took 2.29 seconds (mean sampled reward: -3407.85). Current reward after update: -2521.03, Optimal reward -2521.03
Iteration 179 took 2.39 seconds (mean sampled reward: -3517.44). Current reward after update: -2571.11, Optimal reward -2521.03
Iteration 180 took 2.35 seconds (mean sampled reward: -3524.35). Current reward after update: -3254.50, Optimal reward -2521.03
Iteration 181 took 2.24 seconds (mean sampled reward: -3342.10). Current reward after update: -2590.86, Optimal reward -2521.03
Iteration 182 took 2.29 seconds (mean sampled reward: -3502.18). Current reward after update: -2567.28, Optimal reward -2521.03
Iteration 183 took 2.29 seconds (mean sampled reward: -3819.78). Current reward after update: -2750.31, Optimal reward -2521.03
Iteration 184 took 2.35 seconds (mean sampled reward: -4064.98). Current reward after update: -2663.13, Optimal reward -2521.03
Iteration 185 took 2.25 seconds (mean sampled reward: -3868.63). Current reward after update: -2631.19, Optimal reward -2521.03
Iteration 186 took 2.25 seconds (mean sampled reward: -3527.05). Current reward after update: -2969.86, Optimal reward -2521.03
Iteration 187 took 2.27 seconds (mean sampled reward: -3369.75). Current reward after update: -2566.91, Optimal reward -2521.03
Iteration 188 took 2.30 seconds (mean sampled reward: -4501.75). Current reward after update: -4194.29, Optimal reward -2521.03
Iteration 189 took 2.21 seconds (mean sampled reward: -3557.30). Current reward after update: -2623.81, Optimal reward -2521.03
Iteration 190 took 2.24 seconds (mean sampled reward: -3772.66). Current reward after update: -2557.29, Optimal reward -2521.03
Iteration 191 took 2.27 seconds (mean sampled reward: -4992.71). Current reward after update: -2679.46, Optimal reward -2521.03
Iteration 192 took 2.18 seconds (mean sampled reward: -3578.31). Current reward after update: -2649.61, Optimal reward -2521.03
Iteration 193 took 2.17 seconds (mean sampled reward: -3394.63). Current reward after update: -3089.15, Optimal reward -2521.03
Iteration 194 took 2.31 seconds (mean sampled reward: -3267.73). Current reward after update: -2880.79, Optimal reward -2521.03
Iteration 195 took 2.31 seconds (mean sampled reward: -3478.87). Current reward after update: -2852.98, Optimal reward -2521.03
Iteration 196 took 2.21 seconds (mean sampled reward: -3330.10). Current reward after update: -2703.48, Optimal reward -2521.03
Iteration 197 took 2.24 seconds (mean sampled reward: -3159.12). Current reward after update: -2955.58, Optimal reward -2521.03
Iteration 198 took 2.19 seconds (mean sampled reward: -3243.11). Current reward after update: -2645.32, Optimal reward -2521.03
Iteration 199 took 2.22 seconds (mean sampled reward: -3321.49). Current reward after update: -2621.75, Optimal reward -2521.03
Iteration 200 took 2.22 seconds (mean sampled reward: -3555.99). Current reward after update: -2687.93, Optimal reward -2521.03
Iteration 1 took 2.37 seconds (mean sampled reward: -7628.84). Current reward after update: -7310.20, Optimal reward -7310.20
Iteration 2 took 2.29 seconds (mean sampled reward: -7631.12). Current reward after update: -7398.27, Optimal reward -7310.20
Iteration 3 took 2.36 seconds (mean sampled reward: -7621.22). Current reward after update: -7430.04, Optimal reward -7310.20
Iteration 4 took 2.34 seconds (mean sampled reward: -7601.50). Current reward after update: -7400.80, Optimal reward -7310.20
Iteration 5 took 2.53 seconds (mean sampled reward: -7611.25). Current reward after update: -7336.89, Optimal reward -7310.20
Iteration 6 took 2.30 seconds (mean sampled reward: -7612.30). Current reward after update: -7321.64, Optimal reward -7310.20
Iteration 7 took 2.40 seconds (mean sampled reward: -7601.73). Current reward after update: -7575.80, Optimal reward -7310.20
Iteration 8 took 2.35 seconds (mean sampled reward: -7591.29). Current reward after update: -7383.42, Optimal reward -7310.20
Iteration 9 took 2.43 seconds (mean sampled reward: -7580.28). Current reward after update: -7231.14, Optimal reward -7231.14
Iteration 10 took 2.63 seconds (mean sampled reward: -7583.38). Current reward after update: -7341.26, Optimal reward -7231.14
Iteration 11 took 2.22 seconds (mean sampled reward: -7605.14). Current reward after update: -7415.53, Optimal reward -7231.14
Iteration 12 took 2.29 seconds (mean sampled reward: -7614.49). Current reward after update: -7504.89, Optimal reward -7231.14
Iteration 13 took 2.41 seconds (mean sampled reward: -7592.53). Current reward after update: -7367.17, Optimal reward -7231.14
Iteration 14 took 2.43 seconds (mean sampled reward: -7561.11). Current reward after update: -7226.77, Optimal reward -7226.77
Iteration 15 took 2.37 seconds (mean sampled reward: -7534.80). Current reward after update: -6961.45, Optimal reward -6961.45
Iteration 16 took 2.41 seconds (mean sampled reward: -7526.73). Current reward after update: -6959.31, Optimal reward -6959.31
Iteration 17 took 2.41 seconds (mean sampled reward: -7472.04). Current reward after update: -6834.64, Optimal reward -6834.64
Iteration 18 took 2.62 seconds (mean sampled reward: -7462.53). Current reward after update: -6765.31, Optimal reward -6765.31
Iteration 19 took 2.42 seconds (mean sampled reward: -7465.31). Current reward after update: -6769.35, Optimal reward -6765.31
Iteration 20 took 2.39 seconds (mean sampled reward: -7448.56). Current reward after update: -6713.36, Optimal reward -6713.36
Iteration 21 took 2.39 seconds (mean sampled reward: -7444.14). Current reward after update: -6717.01, Optimal reward -6713.36
Iteration 22 took 2.38 seconds (mean sampled reward: -7441.51). Current reward after update: -6700.12, Optimal reward -6700.12
Iteration 23 took 2.46 seconds (mean sampled reward: -7466.44). Current reward after update: -6752.19, Optimal reward -6700.12
Iteration 24 took 2.53 seconds (mean sampled reward: -7484.52). Current reward after update: -6762.90, Optimal reward -6700.12
Iteration 25 took 2.47 seconds (mean sampled reward: -7468.84). Current reward after update: -6925.15, Optimal reward -6700.12
Iteration 26 took 2.55 seconds (mean sampled reward: -7457.85). Current reward after update: -6532.55, Optimal reward -6532.55
Iteration 27 took 2.45 seconds (mean sampled reward: -7385.35). Current reward after update: -6643.36, Optimal reward -6532.55
Iteration 28 took 2.50 seconds (mean sampled reward: -7428.04). Current reward after update: -6493.38, Optimal reward -6493.38
Iteration 29 took 2.51 seconds (mean sampled reward: -7346.90). Current reward after update: -6552.30, Optimal reward -6493.38
Iteration 30 took 2.45 seconds (mean sampled reward: -7387.28). Current reward after update: -6552.23, Optimal reward -6493.38
Iteration 31 took 2.41 seconds (mean sampled reward: -7312.18). Current reward after update: -6157.28, Optimal reward -6157.28
Iteration 32 took 2.54 seconds (mean sampled reward: -7031.12). Current reward after update: -5707.21, Optimal reward -5707.21
Iteration 33 took 2.42 seconds (mean sampled reward: -7054.43). Current reward after update: -5746.70, Optimal reward -5707.21
Iteration 34 took 2.58 seconds (mean sampled reward: -7190.32). Current reward after update: -5916.61, Optimal reward -5707.21
Iteration 35 took 2.50 seconds (mean sampled reward: -7185.33). Current reward after update: -6270.32, Optimal reward -5707.21
Iteration 36 took 2.50 seconds (mean sampled reward: -6784.92). Current reward after update: -6037.06, Optimal reward -5707.21
Iteration 37 took 2.53 seconds (mean sampled reward: -6992.36). Current reward after update: -5048.35, Optimal reward -5048.35
Iteration 38 took 2.45 seconds (mean sampled reward: -7204.40). Current reward after update: -4910.05, Optimal reward -4910.05
Iteration 39 took 2.40 seconds (mean sampled reward: -7122.73). Current reward after update: -4937.64, Optimal reward -4910.05
Iteration 40 took 2.47 seconds (mean sampled reward: -7320.00). Current reward after update: -4368.92, Optimal reward -4368.92
Iteration 41 took 2.44 seconds (mean sampled reward: -7033.62). Current reward after update: -4558.27, Optimal reward -4368.92
Iteration 42 took 2.44 seconds (mean sampled reward: -7127.87). Current reward after update: -3713.04, Optimal reward -3713.04
Iteration 43 took 2.57 seconds (mean sampled reward: -7074.56). Current reward after update: -4484.62, Optimal reward -3713.04
Iteration 44 took 2.58 seconds (mean sampled reward: -6599.94). Current reward after update: -3837.40, Optimal reward -3713.04
Iteration 45 took 2.58 seconds (mean sampled reward: -6854.38). Current reward after update: -3451.48, Optimal reward -3451.48
Iteration 46 took 2.49 seconds (mean sampled reward: -6972.67). Current reward after update: -3144.33, Optimal reward -3144.33
Iteration 47 took 2.46 seconds (mean sampled reward: -7004.30). Current reward after update: -2963.06, Optimal reward -2963.06
Iteration 48 took 2.52 seconds (mean sampled reward: -6621.97). Current reward after update: -3073.23, Optimal reward -2963.06
Iteration 49 took 2.66 seconds (mean sampled reward: -6361.80). Current reward after update: -2981.74, Optimal reward -2963.06
Iteration 50 took 2.62 seconds (mean sampled reward: -7078.56). Current reward after update: -2949.40, Optimal reward -2949.40
Iteration 51 took 2.61 seconds (mean sampled reward: -7056.67). Current reward after update: -3072.39, Optimal reward -2949.40
Iteration 52 took 2.73 seconds (mean sampled reward: -6826.07). Current reward after update: -2563.85, Optimal reward -2563.85
Iteration 53 took 2.54 seconds (mean sampled reward: -6731.70). Current reward after update: -2651.73, Optimal reward -2563.85
Iteration 54 took 2.67 seconds (mean sampled reward: -6447.26). Current reward after update: -2842.63, Optimal reward -2563.85
Iteration 55 took 2.67 seconds (mean sampled reward: -6336.23). Current reward after update: -3264.56, Optimal reward -2563.85
Iteration 56 took 2.34 seconds (mean sampled reward: -7082.30). Current reward after update: -2952.49, Optimal reward -2563.85
Iteration 57 took 2.35 seconds (mean sampled reward: -6488.47). Current reward after update: -2510.29, Optimal reward -2510.29
Iteration 58 took 2.31 seconds (mean sampled reward: -6444.26). Current reward after update: -2586.05, Optimal reward -2510.29
Iteration 59 took 2.51 seconds (mean sampled reward: -7379.29). Current reward after update: -3290.53, Optimal reward -2510.29
Iteration 60 took 2.32 seconds (mean sampled reward: -6660.08). Current reward after update: -4194.25, Optimal reward -2510.29
Iteration 61 took 2.33 seconds (mean sampled reward: -6574.50). Current reward after update: -6622.68, Optimal reward -2510.29
Iteration 62 took 2.34 seconds (mean sampled reward: -6228.81). Current reward after update: -3023.24, Optimal reward -2510.29
Iteration 63 took 2.36 seconds (mean sampled reward: -6908.48). Current reward after update: -2890.56, Optimal reward -2510.29
Iteration 64 took 2.39 seconds (mean sampled reward: -6714.01). Current reward after update: -4605.06, Optimal reward -2510.29
Iteration 65 took 2.38 seconds (mean sampled reward: -6715.32). Current reward after update: -2675.88, Optimal reward -2510.29
Iteration 66 took 2.45 seconds (mean sampled reward: -6092.47). Current reward after update: -4011.51, Optimal reward -2510.29
Iteration 67 took 2.35 seconds (mean sampled reward: -6238.10). Current reward after update: -2654.85, Optimal reward -2510.29
Iteration 68 took 2.36 seconds (mean sampled reward: -6681.43). Current reward after update: -2932.37, Optimal reward -2510.29
Iteration 69 took 2.31 seconds (mean sampled reward: -6732.03). Current reward after update: -2957.59, Optimal reward -2510.29
Iteration 70 took 2.28 seconds (mean sampled reward: -6340.62). Current reward after update: -6621.17, Optimal reward -2510.29
Iteration 71 took 2.29 seconds (mean sampled reward: -6219.66). Current reward after update: -4555.50, Optimal reward -2510.29
Iteration 72 took 2.38 seconds (mean sampled reward: -6358.79). Current reward after update: -3143.56, Optimal reward -2510.29
Iteration 73 took 2.43 seconds (mean sampled reward: -6356.90). Current reward after update: -2930.54, Optimal reward -2510.29
Iteration 74 took 2.37 seconds (mean sampled reward: -5878.95). Current reward after update: -2893.87, Optimal reward -2510.29
Iteration 75 took 2.38 seconds (mean sampled reward: -6329.54). Current reward after update: -2713.01, Optimal reward -2510.29
Iteration 76 took 2.35 seconds (mean sampled reward: -5635.57). Current reward after update: -4127.07, Optimal reward -2510.29
Iteration 77 took 2.45 seconds (mean sampled reward: -5985.87). Current reward after update: -2647.40, Optimal reward -2510.29
Iteration 78 took 2.46 seconds (mean sampled reward: -6196.52). Current reward after update: -2654.58, Optimal reward -2510.29
Iteration 79 took 2.39 seconds (mean sampled reward: -5623.08). Current reward after update: -2699.19, Optimal reward -2510.29
Iteration 80 took 2.40 seconds (mean sampled reward: -6045.13). Current reward after update: -2706.74, Optimal reward -2510.29
Iteration 81 took 2.41 seconds (mean sampled reward: -5999.64). Current reward after update: -2707.72, Optimal reward -2510.29
Iteration 82 took 2.41 seconds (mean sampled reward: -5666.24). Current reward after update: -2887.89, Optimal reward -2510.29
Iteration 83 took 2.46 seconds (mean sampled reward: -6076.17). Current reward after update: -3421.84, Optimal reward -2510.29
Iteration 84 took 2.38 seconds (mean sampled reward: -5729.24). Current reward after update: -2577.57, Optimal reward -2510.29
Iteration 85 took 2.41 seconds (mean sampled reward: -6080.91). Current reward after update: -2600.14, Optimal reward -2510.29
Iteration 86 took 2.37 seconds (mean sampled reward: -5774.71). Current reward after update: -2514.45, Optimal reward -2510.29
Iteration 87 took 2.36 seconds (mean sampled reward: -4494.43). Current reward after update: -2266.08, Optimal reward -2266.08
Iteration 88 took 2.37 seconds (mean sampled reward: -4907.82). Current reward after update: -4086.02, Optimal reward -2266.08
Iteration 89 took 2.41 seconds (mean sampled reward: -5542.50). Current reward after update: -2287.21, Optimal reward -2266.08
Iteration 90 took 2.40 seconds (mean sampled reward: -5469.48). Current reward after update: -2255.09, Optimal reward -2255.09
Iteration 91 took 2.38 seconds (mean sampled reward: -5630.27). Current reward after update: -2247.96, Optimal reward -2247.96
Iteration 92 took 2.36 seconds (mean sampled reward: -5218.36). Current reward after update: -2556.93, Optimal reward -2247.96
Iteration 93 took 2.35 seconds (mean sampled reward: -6002.73). Current reward after update: -3682.83, Optimal reward -2247.96
Iteration 94 took 2.36 seconds (mean sampled reward: -5645.52). Current reward after update: -2387.13, Optimal reward -2247.96
Iteration 95 took 2.39 seconds (mean sampled reward: -5453.02). Current reward after update: -2863.89, Optimal reward -2247.96
Iteration 96 took 2.43 seconds (mean sampled reward: -5596.53). Current reward after update: -2521.13, Optimal reward -2247.96
Iteration 97 took 2.47 seconds (mean sampled reward: -5526.98). Current reward after update: -2846.71, Optimal reward -2247.96
Iteration 98 took 2.33 seconds (mean sampled reward: -5268.00). Current reward after update: -2270.23, Optimal reward -2247.96
Iteration 99 took 2.46 seconds (mean sampled reward: -5345.15). Current reward after update: -2424.29, Optimal reward -2247.96
Iteration 100 took 2.33 seconds (mean sampled reward: -5116.23). Current reward after update: -2434.83, Optimal reward -2247.96
Iteration 101 took 2.35 seconds (mean sampled reward: -5103.87). Current reward after update: -2438.69, Optimal reward -2247.96
Iteration 102 took 2.37 seconds (mean sampled reward: -4629.42). Current reward after update: -2447.08, Optimal reward -2247.96
Iteration 103 took 2.35 seconds (mean sampled reward: -4966.57). Current reward after update: -2245.78, Optimal reward -2245.78
Iteration 104 took 2.35 seconds (mean sampled reward: -5031.70). Current reward after update: -2446.26, Optimal reward -2245.78
Iteration 105 took 2.39 seconds (mean sampled reward: -6274.85). Current reward after update: -2360.93, Optimal reward -2245.78
Iteration 106 took 2.37 seconds (mean sampled reward: -5317.00). Current reward after update: -2722.38, Optimal reward -2245.78
Iteration 107 took 2.40 seconds (mean sampled reward: -5332.02). Current reward after update: -3197.43, Optimal reward -2245.78
Iteration 108 took 2.60 seconds (mean sampled reward: -6249.75). Current reward after update: -2408.48, Optimal reward -2245.78
Iteration 109 took 2.39 seconds (mean sampled reward: -6384.22). Current reward after update: -2350.06, Optimal reward -2245.78
Iteration 110 took 2.58 seconds (mean sampled reward: -5354.55). Current reward after update: -2780.17, Optimal reward -2245.78
Iteration 111 took 2.53 seconds (mean sampled reward: -6632.17). Current reward after update: -2511.34, Optimal reward -2245.78
Iteration 112 took 2.33 seconds (mean sampled reward: -5019.95). Current reward after update: -2387.30, Optimal reward -2245.78
Iteration 113 took 2.37 seconds (mean sampled reward: -5608.42). Current reward after update: -2380.55, Optimal reward -2245.78
Iteration 114 took 2.40 seconds (mean sampled reward: -6161.49). Current reward after update: -2374.91, Optimal reward -2245.78
Iteration 115 took 2.41 seconds (mean sampled reward: -6524.15). Current reward after update: -2594.87, Optimal reward -2245.78
Iteration 116 took 2.38 seconds (mean sampled reward: -7018.92). Current reward after update: -2511.71, Optimal reward -2245.78
Iteration 117 took 2.41 seconds (mean sampled reward: -5680.73). Current reward after update: -2436.93, Optimal reward -2245.78
Iteration 118 took 2.34 seconds (mean sampled reward: -5234.97). Current reward after update: -2334.27, Optimal reward -2245.78
Iteration 119 took 2.41 seconds (mean sampled reward: -5918.49). Current reward after update: -2371.86, Optimal reward -2245.78
Iteration 120 took 2.38 seconds (mean sampled reward: -5433.79). Current reward after update: -2381.45, Optimal reward -2245.78
Iteration 121 took 2.44 seconds (mean sampled reward: -5979.79). Current reward after update: -6056.90, Optimal reward -2245.78
Iteration 122 took 2.39 seconds (mean sampled reward: -5320.12). Current reward after update: -2607.84, Optimal reward -2245.78
Iteration 123 took 2.55 seconds (mean sampled reward: -4890.20). Current reward after update: -2298.79, Optimal reward -2245.78
Iteration 124 took 2.42 seconds (mean sampled reward: -5352.78). Current reward after update: -2270.99, Optimal reward -2245.78
Iteration 125 took 2.44 seconds (mean sampled reward: -5550.72). Current reward after update: -2300.70, Optimal reward -2245.78
Iteration 126 took 2.57 seconds (mean sampled reward: -5964.22). Current reward after update: -2489.39, Optimal reward -2245.78
Iteration 127 took 2.44 seconds (mean sampled reward: -5542.24). Current reward after update: -2365.58, Optimal reward -2245.78
Iteration 128 took 2.48 seconds (mean sampled reward: -6118.16). Current reward after update: -2385.61, Optimal reward -2245.78
Iteration 129 took 2.43 seconds (mean sampled reward: -5407.00). Current reward after update: -2304.43, Optimal reward -2245.78
Iteration 130 took 2.42 seconds (mean sampled reward: -5309.61). Current reward after update: -2366.59, Optimal reward -2245.78
Iteration 131 took 2.56 seconds (mean sampled reward: -5604.21). Current reward after update: -2327.12, Optimal reward -2245.78
Iteration 132 took 2.42 seconds (mean sampled reward: -5719.83). Current reward after update: -2328.98, Optimal reward -2245.78
Iteration 133 took 2.43 seconds (mean sampled reward: -5472.18). Current reward after update: -2242.31, Optimal reward -2242.31
Iteration 134 took 2.39 seconds (mean sampled reward: -5332.97). Current reward after update: -2294.55, Optimal reward -2242.31
Iteration 135 took 2.47 seconds (mean sampled reward: -4825.42). Current reward after update: -6600.00, Optimal reward -2242.31
Iteration 136 took 2.40 seconds (mean sampled reward: -5206.05). Current reward after update: -5911.59, Optimal reward -2242.31
Iteration 137 took 2.43 seconds (mean sampled reward: -5304.11). Current reward after update: -2237.05, Optimal reward -2237.05
Iteration 138 took 2.43 seconds (mean sampled reward: -5358.35). Current reward after update: -2405.00, Optimal reward -2237.05
Iteration 139 took 2.40 seconds (mean sampled reward: -5188.10). Current reward after update: -4132.08, Optimal reward -2237.05
Iteration 140 took 2.42 seconds (mean sampled reward: -5557.37). Current reward after update: -2303.97, Optimal reward -2237.05
Iteration 141 took 2.41 seconds (mean sampled reward: -5350.14). Current reward after update: -2273.69, Optimal reward -2237.05
Iteration 142 took 2.35 seconds (mean sampled reward: -5246.70). Current reward after update: -3064.98, Optimal reward -2237.05
Iteration 143 took 2.31 seconds (mean sampled reward: -5429.77). Current reward after update: -2270.16, Optimal reward -2237.05
Iteration 144 took 2.36 seconds (mean sampled reward: -6432.21). Current reward after update: -6659.89, Optimal reward -2237.05
Iteration 145 took 2.26 seconds (mean sampled reward: -6439.94). Current reward after update: -2321.14, Optimal reward -2237.05
Iteration 146 took 2.29 seconds (mean sampled reward: -5695.01). Current reward after update: -2522.25, Optimal reward -2237.05
Iteration 147 took 2.36 seconds (mean sampled reward: -5667.60). Current reward after update: -6647.60, Optimal reward -2237.05
Iteration 148 took 2.28 seconds (mean sampled reward: -5190.57). Current reward after update: -2157.93, Optimal reward -2157.93
Iteration 149 took 2.44 seconds (mean sampled reward: -6294.70). Current reward after update: -2331.57, Optimal reward -2157.93
Iteration 150 took 2.37 seconds (mean sampled reward: -6180.26). Current reward after update: -6372.54, Optimal reward -2157.93
Iteration 151 took 2.33 seconds (mean sampled reward: -6199.19). Current reward after update: -2375.98, Optimal reward -2157.93
Iteration 152 took 2.41 seconds (mean sampled reward: -5661.16). Current reward after update: -2332.76, Optimal reward -2157.93
Iteration 153 took 2.36 seconds (mean sampled reward: -5623.63). Current reward after update: -2159.62, Optimal reward -2157.93
Iteration 154 took 2.34 seconds (mean sampled reward: -5244.31). Current reward after update: -2991.28, Optimal reward -2157.93
Iteration 155 took 2.35 seconds (mean sampled reward: -4690.55). Current reward after update: -2141.39, Optimal reward -2141.39
Iteration 156 took 2.36 seconds (mean sampled reward: -4548.04). Current reward after update: -2072.58, Optimal reward -2072.58
Iteration 157 took 2.40 seconds (mean sampled reward: -5490.45). Current reward after update: -2133.47, Optimal reward -2072.58
Iteration 158 took 2.43 seconds (mean sampled reward: -5586.09). Current reward after update: -2111.57, Optimal reward -2072.58
Iteration 159 took 2.42 seconds (mean sampled reward: -5418.85). Current reward after update: -6502.18, Optimal reward -2072.58
Iteration 160 took 2.44 seconds (mean sampled reward: -5392.06). Current reward after update: -6506.81, Optimal reward -2072.58
Iteration 161 took 2.45 seconds (mean sampled reward: -5703.15). Current reward after update: -2272.91, Optimal reward -2072.58
Iteration 162 took 2.38 seconds (mean sampled reward: -5236.82). Current reward after update: -2770.94, Optimal reward -2072.58
Iteration 163 took 2.41 seconds (mean sampled reward: -5162.56). Current reward after update: -2122.21, Optimal reward -2072.58
Iteration 164 took 2.40 seconds (mean sampled reward: -5562.88). Current reward after update: -3005.35, Optimal reward -2072.58
Iteration 165 took 2.41 seconds (mean sampled reward: -5551.24). Current reward after update: -2183.12, Optimal reward -2072.58
Iteration 166 took 2.41 seconds (mean sampled reward: -5785.01). Current reward after update: -2212.21, Optimal reward -2072.58
Iteration 167 took 2.32 seconds (mean sampled reward: -5173.93). Current reward after update: -2032.17, Optimal reward -2032.17
Iteration 168 took 2.37 seconds (mean sampled reward: -5224.78). Current reward after update: -1966.32, Optimal reward -1966.32
Iteration 169 took 2.39 seconds (mean sampled reward: -5359.78). Current reward after update: -1870.48, Optimal reward -1870.48
Iteration 170 took 2.39 seconds (mean sampled reward: -5383.83). Current reward after update: -2020.08, Optimal reward -1870.48
Iteration 171 took 2.48 seconds (mean sampled reward: -5510.46). Current reward after update: -1902.89, Optimal reward -1870.48
Iteration 172 took 2.42 seconds (mean sampled reward: -5478.82). Current reward after update: -2005.67, Optimal reward -1870.48
Iteration 173 took 2.42 seconds (mean sampled reward: -5381.36). Current reward after update: -2000.72, Optimal reward -1870.48
Iteration 174 took 2.45 seconds (mean sampled reward: -5354.71). Current reward after update: -2000.93, Optimal reward -1870.48
Iteration 175 took 2.43 seconds (mean sampled reward: -5332.29). Current reward after update: -2002.73, Optimal reward -1870.48
Iteration 176 took 2.43 seconds (mean sampled reward: -5548.13). Current reward after update: -2955.43, Optimal reward -1870.48
Iteration 177 took 2.39 seconds (mean sampled reward: -5085.14). Current reward after update: -1947.58, Optimal reward -1870.48
Iteration 178 took 2.41 seconds (mean sampled reward: -5507.83). Current reward after update: -2083.32, Optimal reward -1870.48
Iteration 179 took 2.41 seconds (mean sampled reward: -4891.60). Current reward after update: -1826.00, Optimal reward -1826.00
Iteration 180 took 2.42 seconds (mean sampled reward: -5167.24). Current reward after update: -2087.08, Optimal reward -1826.00
Iteration 181 took 2.39 seconds (mean sampled reward: -4656.16). Current reward after update: -1875.55, Optimal reward -1826.00
Iteration 182 took 2.36 seconds (mean sampled reward: -4627.38). Current reward after update: -1836.31, Optimal reward -1826.00
Iteration 183 took 2.35 seconds (mean sampled reward: -5195.61). Current reward after update: -1918.58, Optimal reward -1826.00
Iteration 184 took 2.39 seconds (mean sampled reward: -4840.30). Current reward after update: -1944.98, Optimal reward -1826.00
Iteration 185 took 2.38 seconds (mean sampled reward: -4462.23). Current reward after update: -1943.38, Optimal reward -1826.00
Iteration 186 took 2.36 seconds (mean sampled reward: -3982.55). Current reward after update: -1873.92, Optimal reward -1826.00
Iteration 187 took 2.47 seconds (mean sampled reward: -4052.55). Current reward after update: -1805.67, Optimal reward -1805.67
Iteration 188 took 2.34 seconds (mean sampled reward: -4274.32). Current reward after update: -1892.82, Optimal reward -1805.67
Iteration 189 took 2.35 seconds (mean sampled reward: -4549.62). Current reward after update: -1820.31, Optimal reward -1805.67
Iteration 190 took 2.36 seconds (mean sampled reward: -4058.94). Current reward after update: -1803.73, Optimal reward -1803.73
Iteration 191 took 2.29 seconds (mean sampled reward: -4228.79). Current reward after update: -1726.56, Optimal reward -1726.56
Iteration 192 took 2.32 seconds (mean sampled reward: -4376.21). Current reward after update: -1782.33, Optimal reward -1726.56
Iteration 193 took 2.31 seconds (mean sampled reward: -4015.16). Current reward after update: -5722.61, Optimal reward -1726.56
Iteration 194 took 2.34 seconds (mean sampled reward: -3559.65). Current reward after update: -3456.64, Optimal reward -1726.56
Iteration 195 took 2.29 seconds (mean sampled reward: -3494.60). Current reward after update: -1851.91, Optimal reward -1726.56
Iteration 196 took 2.28 seconds (mean sampled reward: -3575.80). Current reward after update: -1815.88, Optimal reward -1726.56
Iteration 197 took 2.44 seconds (mean sampled reward: -3339.72). Current reward after update: -1735.77, Optimal reward -1726.56
Iteration 198 took 2.31 seconds (mean sampled reward: -3218.83). Current reward after update: -2221.69, Optimal reward -1726.56
Iteration 199 took 2.34 seconds (mean sampled reward: -3164.41). Current reward after update: -1832.08, Optimal reward -1726.56
Iteration 200 took 2.31 seconds (mean sampled reward: -2784.96). Current reward after update: -1986.38, Optimal reward -1726.56
Max force: 30 Sigma: 0.1 mean rewards: -1740.9121620919095, best rewards:-975.1487626010584

Iteration 1 took 2.36 seconds (mean sampled reward: -7629.97). Current reward after update: -7354.57, Optimal reward -7354.57
Iteration 2 took 2.38 seconds (mean sampled reward: -7633.36). Current reward after update: -7365.71, Optimal reward -7354.57
Iteration 3 took 2.45 seconds (mean sampled reward: -7637.69). Current reward after update: -7338.80, Optimal reward -7338.80
Iteration 4 took 2.29 seconds (mean sampled reward: -7519.58). Current reward after update: -6848.88, Optimal reward -6848.88
Iteration 5 took 2.20 seconds (mean sampled reward: -7260.12). Current reward after update: -5108.51, Optimal reward -5108.51
Iteration 6 took 2.45 seconds (mean sampled reward: -7049.30). Current reward after update: -3986.50, Optimal reward -3986.50
Iteration 7 took 2.20 seconds (mean sampled reward: -6887.26). Current reward after update: -3566.69, Optimal reward -3566.69
Iteration 8 took 2.34 seconds (mean sampled reward: -6340.06). Current reward after update: -3062.68, Optimal reward -3062.68
Iteration 9 took 2.23 seconds (mean sampled reward: -6259.46). Current reward after update: -3152.16, Optimal reward -3062.68
Iteration 10 took 2.19 seconds (mean sampled reward: -6733.27). Current reward after update: -3633.06, Optimal reward -3062.68
Iteration 11 took 2.23 seconds (mean sampled reward: -6592.73). Current reward after update: -3493.61, Optimal reward -3062.68
Iteration 12 took 2.26 seconds (mean sampled reward: -6556.16). Current reward after update: -3507.83, Optimal reward -3062.68
Iteration 13 took 2.26 seconds (mean sampled reward: -6421.28). Current reward after update: -3267.14, Optimal reward -3062.68
Iteration 14 took 2.31 seconds (mean sampled reward: -5941.46). Current reward after update: -3259.70, Optimal reward -3062.68
Iteration 15 took 2.23 seconds (mean sampled reward: -6150.71). Current reward after update: -3348.00, Optimal reward -3062.68
Iteration 16 took 2.30 seconds (mean sampled reward: -6306.72). Current reward after update: -3267.53, Optimal reward -3062.68
Iteration 17 took 2.25 seconds (mean sampled reward: -6626.41). Current reward after update: -2968.88, Optimal reward -2968.88
Iteration 18 took 2.28 seconds (mean sampled reward: -6587.35). Current reward after update: -3188.49, Optimal reward -2968.88
Iteration 19 took 2.19 seconds (mean sampled reward: -6261.12). Current reward after update: -2942.43, Optimal reward -2942.43
Iteration 20 took 2.31 seconds (mean sampled reward: -6399.34). Current reward after update: -2884.49, Optimal reward -2884.49
Iteration 21 took 2.38 seconds (mean sampled reward: -6066.05). Current reward after update: -2479.27, Optimal reward -2479.27
Iteration 22 took 2.12 seconds (mean sampled reward: -6246.98). Current reward after update: -1993.20, Optimal reward -1993.20
Iteration 23 took 2.25 seconds (mean sampled reward: -6234.95). Current reward after update: -2178.69, Optimal reward -1993.20
Iteration 24 took 2.22 seconds (mean sampled reward: -5227.78). Current reward after update: -1866.01, Optimal reward -1866.01
Iteration 25 took 2.21 seconds (mean sampled reward: -5183.19). Current reward after update: -1653.01, Optimal reward -1653.01
Iteration 26 took 2.23 seconds (mean sampled reward: -5765.79). Current reward after update: -1660.36, Optimal reward -1653.01
Iteration 27 took 2.13 seconds (mean sampled reward: -5982.66). Current reward after update: -1804.83, Optimal reward -1653.01
Iteration 28 took 2.13 seconds (mean sampled reward: -5479.15). Current reward after update: -1609.64, Optimal reward -1609.64
Iteration 29 took 2.15 seconds (mean sampled reward: -4158.46). Current reward after update: -6115.87, Optimal reward -1609.64
Iteration 30 took 2.10 seconds (mean sampled reward: -4940.65). Current reward after update: -1555.65, Optimal reward -1555.65
Iteration 31 took 2.41 seconds (mean sampled reward: -4241.81). Current reward after update: -1605.99, Optimal reward -1555.65
Iteration 32 took 2.19 seconds (mean sampled reward: -5052.09). Current reward after update: -1743.10, Optimal reward -1555.65
Iteration 33 took 2.17 seconds (mean sampled reward: -5207.52). Current reward after update: -1540.24, Optimal reward -1540.24
Iteration 34 took 2.11 seconds (mean sampled reward: -4052.03). Current reward after update: -1432.40, Optimal reward -1432.40
Iteration 35 took 2.23 seconds (mean sampled reward: -4197.22). Current reward after update: -1444.20, Optimal reward -1432.40
Iteration 36 took 2.17 seconds (mean sampled reward: -5523.59). Current reward after update: -1429.03, Optimal reward -1429.03
Iteration 37 took 2.30 seconds (mean sampled reward: -5347.43). Current reward after update: -1426.05, Optimal reward -1426.05
Iteration 38 took 2.15 seconds (mean sampled reward: -4896.66). Current reward after update: -5507.80, Optimal reward -1426.05
Iteration 39 took 2.17 seconds (mean sampled reward: -5255.82). Current reward after update: -1425.28, Optimal reward -1425.28
Iteration 40 took 2.16 seconds (mean sampled reward: -5778.23). Current reward after update: -1658.86, Optimal reward -1425.28
Iteration 41 took 2.16 seconds (mean sampled reward: -5113.53). Current reward after update: -1532.06, Optimal reward -1425.28
Iteration 42 took 2.20 seconds (mean sampled reward: -4525.31). Current reward after update: -1377.22, Optimal reward -1377.22
Iteration 43 took 2.28 seconds (mean sampled reward: -5117.28). Current reward after update: -1550.71, Optimal reward -1377.22
Iteration 44 took 2.25 seconds (mean sampled reward: -4277.12). Current reward after update: -1418.88, Optimal reward -1377.22
Iteration 45 took 2.26 seconds (mean sampled reward: -5083.84). Current reward after update: -1336.85, Optimal reward -1336.85
Iteration 46 took 2.20 seconds (mean sampled reward: -6162.85). Current reward after update: -1527.34, Optimal reward -1336.85
Iteration 47 took 2.16 seconds (mean sampled reward: -5902.59). Current reward after update: -1695.30, Optimal reward -1336.85
Iteration 48 took 2.16 seconds (mean sampled reward: -6182.21). Current reward after update: -1463.17, Optimal reward -1336.85
Iteration 49 took 2.31 seconds (mean sampled reward: -4282.90). Current reward after update: -1412.63, Optimal reward -1336.85
Iteration 50 took 2.26 seconds (mean sampled reward: -4406.48). Current reward after update: -1336.52, Optimal reward -1336.52
Iteration 51 took 2.46 seconds (mean sampled reward: -4629.12). Current reward after update: -1409.23, Optimal reward -1336.52
Iteration 52 took 2.45 seconds (mean sampled reward: -5243.55). Current reward after update: -1539.66, Optimal reward -1336.52
Iteration 53 took 2.35 seconds (mean sampled reward: -5143.55). Current reward after update: -1516.78, Optimal reward -1336.52
Iteration 54 took 2.69 seconds (mean sampled reward: -6066.77). Current reward after update: -1545.01, Optimal reward -1336.52
Iteration 55 took 2.69 seconds (mean sampled reward: -6121.29). Current reward after update: -1507.25, Optimal reward -1336.52
Iteration 56 took 2.31 seconds (mean sampled reward: -5673.50). Current reward after update: -1615.09, Optimal reward -1336.52
Iteration 57 took 2.28 seconds (mean sampled reward: -5116.28). Current reward after update: -1451.75, Optimal reward -1336.52
Iteration 58 took 2.37 seconds (mean sampled reward: -4841.43). Current reward after update: -1446.76, Optimal reward -1336.52
Iteration 59 took 2.29 seconds (mean sampled reward: -5104.46). Current reward after update: -1549.47, Optimal reward -1336.52
Iteration 60 took 2.46 seconds (mean sampled reward: -4956.40). Current reward after update: -1481.47, Optimal reward -1336.52
Iteration 61 took 2.28 seconds (mean sampled reward: -4042.60). Current reward after update: -1460.63, Optimal reward -1336.52
Iteration 62 took 2.25 seconds (mean sampled reward: -5025.88). Current reward after update: -2290.30, Optimal reward -1336.52
Iteration 63 took 2.24 seconds (mean sampled reward: -6143.64). Current reward after update: -1686.56, Optimal reward -1336.52
Iteration 64 took 2.23 seconds (mean sampled reward: -5903.25). Current reward after update: -1349.39, Optimal reward -1336.52
Iteration 65 took 2.29 seconds (mean sampled reward: -5667.20). Current reward after update: -1536.34, Optimal reward -1336.52
Iteration 66 took 2.27 seconds (mean sampled reward: -4943.72). Current reward after update: -1511.21, Optimal reward -1336.52
Iteration 67 took 2.25 seconds (mean sampled reward: -4656.42). Current reward after update: -1366.62, Optimal reward -1336.52
Iteration 68 took 2.23 seconds (mean sampled reward: -4828.75). Current reward after update: -1137.39, Optimal reward -1137.39
Iteration 69 took 2.29 seconds (mean sampled reward: -4582.44). Current reward after update: -1353.47, Optimal reward -1137.39
Iteration 70 took 2.33 seconds (mean sampled reward: -4787.89). Current reward after update: -1339.31, Optimal reward -1137.39
Iteration 71 took 2.20 seconds (mean sampled reward: -5130.48). Current reward after update: -1231.55, Optimal reward -1137.39
Iteration 72 took 2.15 seconds (mean sampled reward: -5631.84). Current reward after update: -1331.44, Optimal reward -1137.39
Iteration 73 took 2.25 seconds (mean sampled reward: -5426.96). Current reward after update: -1441.33, Optimal reward -1137.39
Iteration 74 took 2.25 seconds (mean sampled reward: -5015.65). Current reward after update: -3153.79, Optimal reward -1137.39
Iteration 75 took 2.34 seconds (mean sampled reward: -5500.26). Current reward after update: -6084.13, Optimal reward -1137.39
Iteration 76 took 2.21 seconds (mean sampled reward: -5498.98). Current reward after update: -1380.88, Optimal reward -1137.39
Iteration 77 took 2.23 seconds (mean sampled reward: -5034.98). Current reward after update: -1417.69, Optimal reward -1137.39
Iteration 78 took 2.22 seconds (mean sampled reward: -5829.63). Current reward after update: -1314.94, Optimal reward -1137.39
Iteration 79 took 2.28 seconds (mean sampled reward: -5688.11). Current reward after update: -1411.55, Optimal reward -1137.39
Iteration 80 took 2.21 seconds (mean sampled reward: -4670.23). Current reward after update: -1339.72, Optimal reward -1137.39
Iteration 81 took 2.26 seconds (mean sampled reward: -5113.37). Current reward after update: -1316.74, Optimal reward -1137.39
Iteration 82 took 2.18 seconds (mean sampled reward: -5048.07). Current reward after update: -1196.85, Optimal reward -1137.39
Iteration 83 took 2.21 seconds (mean sampled reward: -4879.15). Current reward after update: -1303.74, Optimal reward -1137.39
Iteration 84 took 2.30 seconds (mean sampled reward: -5529.64). Current reward after update: -1565.97, Optimal reward -1137.39
Iteration 85 took 2.19 seconds (mean sampled reward: -5731.29). Current reward after update: -1188.48, Optimal reward -1137.39
Iteration 86 took 2.26 seconds (mean sampled reward: -5832.16). Current reward after update: -1196.21, Optimal reward -1137.39
Iteration 87 took 2.27 seconds (mean sampled reward: -5457.67). Current reward after update: -1259.47, Optimal reward -1137.39
Iteration 88 took 2.22 seconds (mean sampled reward: -5609.26). Current reward after update: -1317.10, Optimal reward -1137.39
Iteration 89 took 2.27 seconds (mean sampled reward: -5497.62). Current reward after update: -1186.58, Optimal reward -1137.39
Iteration 90 took 2.20 seconds (mean sampled reward: -4457.49). Current reward after update: -1156.37, Optimal reward -1137.39
Iteration 91 took 2.19 seconds (mean sampled reward: -4691.97). Current reward after update: -1329.45, Optimal reward -1137.39
Iteration 92 took 2.18 seconds (mean sampled reward: -3687.04). Current reward after update: -1126.86, Optimal reward -1126.86
Iteration 93 took 2.30 seconds (mean sampled reward: -4126.53). Current reward after update: -1166.77, Optimal reward -1126.86
Iteration 94 took 2.27 seconds (mean sampled reward: -4515.46). Current reward after update: -1090.37, Optimal reward -1090.37
Iteration 95 took 2.34 seconds (mean sampled reward: -4164.19). Current reward after update: -1189.63, Optimal reward -1090.37
Iteration 96 took 2.24 seconds (mean sampled reward: -4206.03). Current reward after update: -1371.27, Optimal reward -1090.37
Iteration 97 took 2.32 seconds (mean sampled reward: -4108.26). Current reward after update: -1474.72, Optimal reward -1090.37
Iteration 98 took 2.32 seconds (mean sampled reward: -3319.90). Current reward after update: -1387.21, Optimal reward -1090.37
Iteration 99 took 2.30 seconds (mean sampled reward: -3054.27). Current reward after update: -1369.73, Optimal reward -1090.37
Iteration 100 took 2.30 seconds (mean sampled reward: -2957.20). Current reward after update: -2321.48, Optimal reward -1090.37
Iteration 101 took 2.30 seconds (mean sampled reward: -3153.76). Current reward after update: -1689.26, Optimal reward -1090.37
Iteration 102 took 2.24 seconds (mean sampled reward: -3691.60). Current reward after update: -1331.57, Optimal reward -1090.37
Iteration 103 took 2.28 seconds (mean sampled reward: -3476.46). Current reward after update: -1342.07, Optimal reward -1090.37
Iteration 104 took 2.28 seconds (mean sampled reward: -3745.21). Current reward after update: -1464.54, Optimal reward -1090.37
Iteration 105 took 2.31 seconds (mean sampled reward: -4381.92). Current reward after update: -1424.59, Optimal reward -1090.37
Iteration 106 took 2.36 seconds (mean sampled reward: -4240.86). Current reward after update: -1350.28, Optimal reward -1090.37
Iteration 107 took 2.60 seconds (mean sampled reward: -5569.63). Current reward after update: -6179.35, Optimal reward -1090.37
Iteration 108 took 2.23 seconds (mean sampled reward: -4963.68). Current reward after update: -1259.14, Optimal reward -1090.37
Iteration 109 took 2.28 seconds (mean sampled reward: -4270.81). Current reward after update: -1241.42, Optimal reward -1090.37
Iteration 110 took 2.31 seconds (mean sampled reward: -5012.73). Current reward after update: -1273.37, Optimal reward -1090.37
Iteration 111 took 2.33 seconds (mean sampled reward: -3993.86). Current reward after update: -1166.88, Optimal reward -1090.37
Iteration 112 took 2.29 seconds (mean sampled reward: -3692.72). Current reward after update: -1600.63, Optimal reward -1090.37
Iteration 113 took 2.23 seconds (mean sampled reward: -4604.61). Current reward after update: -1878.43, Optimal reward -1090.37
Iteration 114 took 2.22 seconds (mean sampled reward: -3927.13). Current reward after update: -1619.05, Optimal reward -1090.37
Iteration 115 took 2.23 seconds (mean sampled reward: -4320.28). Current reward after update: -1261.59, Optimal reward -1090.37
Iteration 116 took 2.20 seconds (mean sampled reward: -5315.36). Current reward after update: -1319.46, Optimal reward -1090.37
Iteration 117 took 2.29 seconds (mean sampled reward: -4892.46). Current reward after update: -1348.77, Optimal reward -1090.37
Iteration 118 took 2.22 seconds (mean sampled reward: -4998.43). Current reward after update: -1217.37, Optimal reward -1090.37
Iteration 119 took 2.24 seconds (mean sampled reward: -5114.82). Current reward after update: -1119.63, Optimal reward -1090.37
Iteration 120 took 2.29 seconds (mean sampled reward: -4347.19). Current reward after update: -1106.15, Optimal reward -1090.37
Iteration 121 took 2.21 seconds (mean sampled reward: -3513.66). Current reward after update: -1053.28, Optimal reward -1053.28
Iteration 122 took 2.24 seconds (mean sampled reward: -4005.19). Current reward after update: -1065.64, Optimal reward -1053.28
Iteration 123 took 2.26 seconds (mean sampled reward: -3893.23). Current reward after update: -1134.39, Optimal reward -1053.28
Iteration 124 took 2.26 seconds (mean sampled reward: -3619.64). Current reward after update: -983.89, Optimal reward -983.89
Iteration 125 took 2.21 seconds (mean sampled reward: -4726.23). Current reward after update: -2386.92, Optimal reward -983.89
Iteration 126 took 2.24 seconds (mean sampled reward: -3828.07). Current reward after update: -944.24, Optimal reward -944.24
Iteration 127 took 2.22 seconds (mean sampled reward: -4082.68). Current reward after update: -1040.36, Optimal reward -944.24
Iteration 128 took 2.20 seconds (mean sampled reward: -4308.26). Current reward after update: -987.16, Optimal reward -944.24
Iteration 129 took 2.27 seconds (mean sampled reward: -3998.74). Current reward after update: -1082.31, Optimal reward -944.24
Iteration 130 took 2.23 seconds (mean sampled reward: -3776.59). Current reward after update: -1008.63, Optimal reward -944.24
Iteration 131 took 2.28 seconds (mean sampled reward: -3715.66). Current reward after update: -1032.71, Optimal reward -944.24
Iteration 132 took 2.22 seconds (mean sampled reward: -3800.89). Current reward after update: -1008.83, Optimal reward -944.24
Iteration 133 took 2.24 seconds (mean sampled reward: -3358.14). Current reward after update: -1130.79, Optimal reward -944.24
Iteration 134 took 2.23 seconds (mean sampled reward: -3612.24). Current reward after update: -1012.36, Optimal reward -944.24
Iteration 135 took 2.20 seconds (mean sampled reward: -3683.13). Current reward after update: -1055.90, Optimal reward -944.24
Iteration 136 took 2.28 seconds (mean sampled reward: -3635.93). Current reward after update: -1408.78, Optimal reward -944.24
Iteration 137 took 2.25 seconds (mean sampled reward: -3538.98). Current reward after update: -922.94, Optimal reward -922.94
Iteration 138 took 2.24 seconds (mean sampled reward: -4104.44). Current reward after update: -1077.15, Optimal reward -922.94
Iteration 139 took 2.27 seconds (mean sampled reward: -4642.93). Current reward after update: -5699.67, Optimal reward -922.94
Iteration 140 took 2.31 seconds (mean sampled reward: -5806.62). Current reward after update: -1464.97, Optimal reward -922.94
Iteration 141 took 2.29 seconds (mean sampled reward: -5674.84). Current reward after update: -1783.01, Optimal reward -922.94
Iteration 142 took 2.30 seconds (mean sampled reward: -5370.58). Current reward after update: -6000.30, Optimal reward -922.94
Iteration 143 took 2.25 seconds (mean sampled reward: -4577.18). Current reward after update: -1286.87, Optimal reward -922.94
Iteration 144 took 2.31 seconds (mean sampled reward: -4315.04). Current reward after update: -1354.10, Optimal reward -922.94
Iteration 145 took 2.21 seconds (mean sampled reward: -3921.31). Current reward after update: -1181.26, Optimal reward -922.94
Iteration 146 took 2.26 seconds (mean sampled reward: -3539.44). Current reward after update: -1610.83, Optimal reward -922.94
Iteration 147 took 2.27 seconds (mean sampled reward: -2950.92). Current reward after update: -1065.72, Optimal reward -922.94
Iteration 148 took 2.34 seconds (mean sampled reward: -2882.13). Current reward after update: -1018.78, Optimal reward -922.94
Iteration 149 took 2.24 seconds (mean sampled reward: -3248.33). Current reward after update: -1036.28, Optimal reward -922.94
Iteration 150 took 2.21 seconds (mean sampled reward: -4244.90). Current reward after update: -1338.24, Optimal reward -922.94
Iteration 151 took 2.23 seconds (mean sampled reward: -3479.76). Current reward after update: -1197.68, Optimal reward -922.94
Iteration 152 took 2.19 seconds (mean sampled reward: -4789.83). Current reward after update: -1150.87, Optimal reward -922.94
Iteration 153 took 2.22 seconds (mean sampled reward: -3621.07). Current reward after update: -1056.07, Optimal reward -922.94
Iteration 154 took 2.29 seconds (mean sampled reward: -4192.06). Current reward after update: -1092.33, Optimal reward -922.94
Iteration 155 took 2.30 seconds (mean sampled reward: -4153.81). Current reward after update: -1020.32, Optimal reward -922.94
Iteration 156 took 2.26 seconds (mean sampled reward: -3968.89). Current reward after update: -1028.23, Optimal reward -922.94
Iteration 157 took 2.31 seconds (mean sampled reward: -4545.39). Current reward after update: -5490.15, Optimal reward -922.94
Iteration 158 took 2.39 seconds (mean sampled reward: -4034.91). Current reward after update: -1971.64, Optimal reward -922.94
Iteration 159 took 2.25 seconds (mean sampled reward: -3581.48). Current reward after update: -889.51, Optimal reward -889.51
Iteration 160 took 2.22 seconds (mean sampled reward: -3639.82). Current reward after update: -921.92, Optimal reward -889.51
Iteration 161 took 2.36 seconds (mean sampled reward: -3939.46). Current reward after update: -875.87, Optimal reward -875.87
Iteration 162 took 2.30 seconds (mean sampled reward: -4591.45). Current reward after update: -868.23, Optimal reward -868.23
Iteration 163 took 2.27 seconds (mean sampled reward: -4608.22). Current reward after update: -5465.84, Optimal reward -868.23
Iteration 164 took 2.30 seconds (mean sampled reward: -5540.61). Current reward after update: -934.49, Optimal reward -868.23
Iteration 165 took 2.25 seconds (mean sampled reward: -5837.29). Current reward after update: -989.93, Optimal reward -868.23
Iteration 166 took 2.31 seconds (mean sampled reward: -5375.09). Current reward after update: -1001.10, Optimal reward -868.23
Iteration 167 took 2.31 seconds (mean sampled reward: -5566.85). Current reward after update: -5466.30, Optimal reward -868.23
Iteration 168 took 2.35 seconds (mean sampled reward: -3870.08). Current reward after update: -1485.93, Optimal reward -868.23
Iteration 169 took 2.23 seconds (mean sampled reward: -3854.45). Current reward after update: -1826.92, Optimal reward -868.23
Iteration 170 took 2.34 seconds (mean sampled reward: -3967.71). Current reward after update: -1066.34, Optimal reward -868.23
Iteration 171 took 2.24 seconds (mean sampled reward: -4755.45). Current reward after update: -904.68, Optimal reward -868.23
Iteration 172 took 2.21 seconds (mean sampled reward: -4487.56). Current reward after update: -1132.86, Optimal reward -868.23
Iteration 173 took 2.30 seconds (mean sampled reward: -4695.37). Current reward after update: -906.16, Optimal reward -868.23
Iteration 174 took 2.19 seconds (mean sampled reward: -4881.75). Current reward after update: -967.23, Optimal reward -868.23
Iteration 175 took 2.29 seconds (mean sampled reward: -4047.56). Current reward after update: -920.55, Optimal reward -868.23
Iteration 176 took 2.27 seconds (mean sampled reward: -3868.39). Current reward after update: -916.78, Optimal reward -868.23
Iteration 177 took 2.23 seconds (mean sampled reward: -4709.79). Current reward after update: -903.18, Optimal reward -868.23
Iteration 178 took 2.29 seconds (mean sampled reward: -4729.21). Current reward after update: -5385.51, Optimal reward -868.23
Iteration 179 took 2.33 seconds (mean sampled reward: -3932.07). Current reward after update: -1742.13, Optimal reward -868.23
Iteration 180 took 2.29 seconds (mean sampled reward: -4255.44). Current reward after update: -743.30, Optimal reward -743.30
Iteration 181 took 2.22 seconds (mean sampled reward: -5290.20). Current reward after update: -867.78, Optimal reward -743.30
Iteration 182 took 2.21 seconds (mean sampled reward: -5620.35). Current reward after update: -1156.26, Optimal reward -743.30
Iteration 183 took 2.20 seconds (mean sampled reward: -5638.53). Current reward after update: -1010.60, Optimal reward -743.30
Iteration 184 took 2.23 seconds (mean sampled reward: -4938.06). Current reward after update: -942.92, Optimal reward -743.30
Iteration 185 took 2.23 seconds (mean sampled reward: -5257.33). Current reward after update: -1047.12, Optimal reward -743.30
Iteration 186 took 2.30 seconds (mean sampled reward: -4783.32). Current reward after update: -1006.67, Optimal reward -743.30
Iteration 187 took 2.29 seconds (mean sampled reward: -5143.34). Current reward after update: -990.88, Optimal reward -743.30
Iteration 188 took 2.30 seconds (mean sampled reward: -6028.02). Current reward after update: -1171.46, Optimal reward -743.30
Iteration 189 took 2.30 seconds (mean sampled reward: -5296.24). Current reward after update: -1115.52, Optimal reward -743.30
Iteration 190 took 2.26 seconds (mean sampled reward: -5745.99). Current reward after update: -986.10, Optimal reward -743.30
Iteration 191 took 2.28 seconds (mean sampled reward: -5425.37). Current reward after update: -1095.71, Optimal reward -743.30
Iteration 192 took 2.31 seconds (mean sampled reward: -5260.94). Current reward after update: -1092.94, Optimal reward -743.30
Iteration 193 took 2.29 seconds (mean sampled reward: -5992.92). Current reward after update: -1084.95, Optimal reward -743.30
Iteration 194 took 2.27 seconds (mean sampled reward: -5978.81). Current reward after update: -1064.20, Optimal reward -743.30
Iteration 195 took 2.26 seconds (mean sampled reward: -4697.61). Current reward after update: -6260.95, Optimal reward -743.30
Iteration 196 took 2.35 seconds (mean sampled reward: -4816.40). Current reward after update: -2560.78, Optimal reward -743.30
Iteration 197 took 2.26 seconds (mean sampled reward: -5513.38). Current reward after update: -2146.60, Optimal reward -743.30
Iteration 198 took 2.29 seconds (mean sampled reward: -5157.49). Current reward after update: -1269.05, Optimal reward -743.30
Iteration 199 took 2.27 seconds (mean sampled reward: -5477.46). Current reward after update: -2162.27, Optimal reward -743.30
Iteration 200 took 2.29 seconds (mean sampled reward: -5108.56). Current reward after update: -1153.50, Optimal reward -743.30
Iteration 1 took 2.36 seconds (mean sampled reward: -7633.17). Current reward after update: -7348.53, Optimal reward -7348.53
Iteration 2 took 2.28 seconds (mean sampled reward: -7603.52). Current reward after update: -7098.74, Optimal reward -7098.74
Iteration 3 took 2.30 seconds (mean sampled reward: -7446.57). Current reward after update: -6484.37, Optimal reward -6484.37
Iteration 4 took 2.27 seconds (mean sampled reward: -7111.99). Current reward after update: -6281.52, Optimal reward -6281.52
Iteration 5 took 2.31 seconds (mean sampled reward: -6878.37). Current reward after update: -5925.85, Optimal reward -5925.85
Iteration 6 took 2.19 seconds (mean sampled reward: -6973.35). Current reward after update: -5688.17, Optimal reward -5688.17
Iteration 7 took 2.26 seconds (mean sampled reward: -6956.31). Current reward after update: -5623.89, Optimal reward -5623.89
Iteration 8 took 2.18 seconds (mean sampled reward: -6628.00). Current reward after update: -5192.52, Optimal reward -5192.52
Iteration 9 took 2.47 seconds (mean sampled reward: -6191.83). Current reward after update: -4901.68, Optimal reward -4901.68
Iteration 10 took 2.28 seconds (mean sampled reward: -5722.51). Current reward after update: -4508.87, Optimal reward -4508.87
Iteration 11 took 2.35 seconds (mean sampled reward: -5731.71). Current reward after update: -4552.42, Optimal reward -4508.87
Iteration 12 took 2.28 seconds (mean sampled reward: -5449.16). Current reward after update: -4660.24, Optimal reward -4508.87
Iteration 13 took 2.34 seconds (mean sampled reward: -5410.10). Current reward after update: -4180.38, Optimal reward -4180.38
Iteration 14 took 2.67 seconds (mean sampled reward: -5372.88). Current reward after update: -3779.19, Optimal reward -3779.19
Iteration 15 took 2.30 seconds (mean sampled reward: -5682.05). Current reward after update: -4864.04, Optimal reward -3779.19
Iteration 16 took 2.30 seconds (mean sampled reward: -5273.40). Current reward after update: -4277.98, Optimal reward -3779.19
Iteration 17 took 2.40 seconds (mean sampled reward: -5349.15). Current reward after update: -4123.68, Optimal reward -3779.19
Iteration 18 took 2.38 seconds (mean sampled reward: -5371.33). Current reward after update: -4307.55, Optimal reward -3779.19
Iteration 19 took 2.39 seconds (mean sampled reward: -4960.11). Current reward after update: -4127.39, Optimal reward -3779.19
Iteration 20 took 2.28 seconds (mean sampled reward: -5026.42). Current reward after update: -3591.28, Optimal reward -3591.28
Iteration 21 took 2.28 seconds (mean sampled reward: -5041.61). Current reward after update: -3364.49, Optimal reward -3364.49
Iteration 22 took 2.27 seconds (mean sampled reward: -5531.84). Current reward after update: -3317.54, Optimal reward -3317.54
Iteration 23 took 2.24 seconds (mean sampled reward: -5431.00). Current reward after update: -3262.34, Optimal reward -3262.34
Iteration 24 took 2.24 seconds (mean sampled reward: -4910.49). Current reward after update: -3096.33, Optimal reward -3096.33
Iteration 25 took 2.36 seconds (mean sampled reward: -4849.37). Current reward after update: -3299.76, Optimal reward -3096.33
Iteration 26 took 2.24 seconds (mean sampled reward: -4641.73). Current reward after update: -3163.80, Optimal reward -3096.33
Iteration 27 took 2.27 seconds (mean sampled reward: -4366.99). Current reward after update: -3190.99, Optimal reward -3096.33
Iteration 28 took 2.60 seconds (mean sampled reward: -4629.85). Current reward after update: -3299.00, Optimal reward -3096.33
Iteration 29 took 2.35 seconds (mean sampled reward: -4780.76). Current reward after update: -5247.72, Optimal reward -3096.33
Iteration 30 took 2.23 seconds (mean sampled reward: -4934.23). Current reward after update: -3097.27, Optimal reward -3096.33
Iteration 31 took 2.39 seconds (mean sampled reward: -4904.36). Current reward after update: -3068.07, Optimal reward -3068.07
Iteration 32 took 2.34 seconds (mean sampled reward: -5091.26). Current reward after update: -2946.91, Optimal reward -2946.91
Iteration 33 took 2.25 seconds (mean sampled reward: -4977.16). Current reward after update: -3006.61, Optimal reward -2946.91
Iteration 34 took 2.24 seconds (mean sampled reward: -5356.53). Current reward after update: -2935.63, Optimal reward -2935.63
Iteration 35 took 2.30 seconds (mean sampled reward: -5241.44). Current reward after update: -2972.04, Optimal reward -2935.63
Iteration 36 took 2.18 seconds (mean sampled reward: -5134.72). Current reward after update: -2932.90, Optimal reward -2932.90
Iteration 37 took 2.22 seconds (mean sampled reward: -4981.51). Current reward after update: -2782.16, Optimal reward -2782.16
Iteration 38 took 2.19 seconds (mean sampled reward: -5278.45). Current reward after update: -2316.31, Optimal reward -2316.31
Iteration 39 took 2.35 seconds (mean sampled reward: -4992.85). Current reward after update: -2360.43, Optimal reward -2316.31
Iteration 40 took 2.37 seconds (mean sampled reward: -5278.11). Current reward after update: -2275.68, Optimal reward -2275.68
Iteration 41 took 2.23 seconds (mean sampled reward: -4902.99). Current reward after update: -2299.95, Optimal reward -2275.68
Iteration 42 took 2.45 seconds (mean sampled reward: -3992.66). Current reward after update: -2093.90, Optimal reward -2093.90
Iteration 43 took 2.16 seconds (mean sampled reward: -3609.72). Current reward after update: -2000.91, Optimal reward -2000.91
Iteration 44 took 2.18 seconds (mean sampled reward: -5355.18). Current reward after update: -2032.46, Optimal reward -2000.91
Iteration 45 took 2.17 seconds (mean sampled reward: -4836.91). Current reward after update: -2024.83, Optimal reward -2000.91
Iteration 46 took 2.17 seconds (mean sampled reward: -4330.10). Current reward after update: -1965.07, Optimal reward -1965.07
Iteration 47 took 2.12 seconds (mean sampled reward: -4814.70). Current reward after update: -2031.78, Optimal reward -1965.07
Iteration 48 took 2.17 seconds (mean sampled reward: -4456.17). Current reward after update: -2007.82, Optimal reward -1965.07
Iteration 49 took 2.25 seconds (mean sampled reward: -4445.82). Current reward after update: -2000.66, Optimal reward -1965.07
Iteration 50 took 2.22 seconds (mean sampled reward: -4472.11). Current reward after update: -5751.51, Optimal reward -1965.07
Iteration 51 took 2.18 seconds (mean sampled reward: -4383.00). Current reward after update: -2002.44, Optimal reward -1965.07
Iteration 52 took 2.23 seconds (mean sampled reward: -3999.61). Current reward after update: -1949.50, Optimal reward -1949.50
Iteration 53 took 2.27 seconds (mean sampled reward: -3809.46). Current reward after update: -1974.67, Optimal reward -1949.50
Iteration 54 took 2.44 seconds (mean sampled reward: -3357.40). Current reward after update: -2092.62, Optimal reward -1949.50
Iteration 55 took 2.23 seconds (mean sampled reward: -3386.35). Current reward after update: -1932.69, Optimal reward -1932.69
Iteration 56 took 2.28 seconds (mean sampled reward: -3889.69). Current reward after update: -4775.45, Optimal reward -1932.69
Iteration 57 took 2.17 seconds (mean sampled reward: -4335.27). Current reward after update: -2108.63, Optimal reward -1932.69
Iteration 58 took 2.19 seconds (mean sampled reward: -4255.63). Current reward after update: -1976.96, Optimal reward -1932.69
Iteration 59 took 2.20 seconds (mean sampled reward: -4113.32). Current reward after update: -1998.81, Optimal reward -1932.69
Iteration 60 took 2.25 seconds (mean sampled reward: -3874.76). Current reward after update: -1949.22, Optimal reward -1932.69
Iteration 61 took 2.17 seconds (mean sampled reward: -3732.61). Current reward after update: -4428.74, Optimal reward -1932.69
Iteration 62 took 2.22 seconds (mean sampled reward: -4109.09). Current reward after update: -1751.77, Optimal reward -1751.77
Iteration 63 took 2.18 seconds (mean sampled reward: -4350.96). Current reward after update: -1774.19, Optimal reward -1751.77
Iteration 64 took 2.12 seconds (mean sampled reward: -4750.65). Current reward after update: -1627.08, Optimal reward -1627.08
Iteration 65 took 2.17 seconds (mean sampled reward: -4928.79). Current reward after update: -1614.33, Optimal reward -1614.33
Iteration 66 took 2.22 seconds (mean sampled reward: -4125.03). Current reward after update: -1623.68, Optimal reward -1614.33
Iteration 67 took 2.13 seconds (mean sampled reward: -3609.26). Current reward after update: -2012.32, Optimal reward -1614.33
Iteration 68 took 2.21 seconds (mean sampled reward: -4017.83). Current reward after update: -2446.35, Optimal reward -1614.33
Iteration 69 took 2.15 seconds (mean sampled reward: -3975.12). Current reward after update: -1451.72, Optimal reward -1451.72
Iteration 70 took 2.25 seconds (mean sampled reward: -4091.35). Current reward after update: -1358.12, Optimal reward -1358.12
Iteration 71 took 2.24 seconds (mean sampled reward: -3554.73). Current reward after update: -1278.85, Optimal reward -1278.85
Iteration 72 took 2.26 seconds (mean sampled reward: -3803.39). Current reward after update: -1342.19, Optimal reward -1278.85
Iteration 73 took 2.27 seconds (mean sampled reward: -3760.30). Current reward after update: -1351.38, Optimal reward -1278.85
Iteration 74 took 2.17 seconds (mean sampled reward: -3851.58). Current reward after update: -1238.20, Optimal reward -1238.20
Iteration 75 took 2.14 seconds (mean sampled reward: -3828.28). Current reward after update: -1713.44, Optimal reward -1238.20
Iteration 76 took 2.13 seconds (mean sampled reward: -4383.82). Current reward after update: -1359.70, Optimal reward -1238.20
Iteration 77 took 2.19 seconds (mean sampled reward: -3586.01). Current reward after update: -1224.17, Optimal reward -1224.17
Iteration 78 took 2.24 seconds (mean sampled reward: -4057.17). Current reward after update: -2566.72, Optimal reward -1224.17
Iteration 79 took 2.31 seconds (mean sampled reward: -4042.53). Current reward after update: -1346.39, Optimal reward -1224.17
Iteration 80 took 2.33 seconds (mean sampled reward: -4339.19). Current reward after update: -1249.42, Optimal reward -1224.17
Iteration 81 took 2.16 seconds (mean sampled reward: -4930.85). Current reward after update: -1283.81, Optimal reward -1224.17
Iteration 82 took 2.26 seconds (mean sampled reward: -4645.03). Current reward after update: -2024.72, Optimal reward -1224.17
Iteration 83 took 2.23 seconds (mean sampled reward: -4461.59). Current reward after update: -994.48, Optimal reward -994.48
Iteration 84 took 2.22 seconds (mean sampled reward: -4542.54). Current reward after update: -1093.58, Optimal reward -994.48
Iteration 85 took 2.29 seconds (mean sampled reward: -4841.02). Current reward after update: -1375.29, Optimal reward -994.48
Iteration 86 took 2.23 seconds (mean sampled reward: -4817.44). Current reward after update: -2054.26, Optimal reward -994.48
Iteration 87 took 2.17 seconds (mean sampled reward: -4924.54). Current reward after update: -1351.51, Optimal reward -994.48
Iteration 88 took 2.22 seconds (mean sampled reward: -4146.50). Current reward after update: -1235.21, Optimal reward -994.48
Iteration 89 took 2.17 seconds (mean sampled reward: -3490.24). Current reward after update: -1196.72, Optimal reward -994.48
Iteration 90 took 2.17 seconds (mean sampled reward: -3986.11). Current reward after update: -1157.14, Optimal reward -994.48
Iteration 91 took 2.18 seconds (mean sampled reward: -3982.84). Current reward after update: -966.30, Optimal reward -966.30
Iteration 92 took 2.20 seconds (mean sampled reward: -3343.34). Current reward after update: -992.18, Optimal reward -966.30
Iteration 93 took 2.19 seconds (mean sampled reward: -3896.70). Current reward after update: -1138.07, Optimal reward -966.30
Iteration 94 took 2.10 seconds (mean sampled reward: -5681.24). Current reward after update: -1543.86, Optimal reward -966.30
Iteration 95 took 2.12 seconds (mean sampled reward: -4768.14). Current reward after update: -2520.93, Optimal reward -966.30
Iteration 96 took 2.23 seconds (mean sampled reward: -4637.54). Current reward after update: -1282.11, Optimal reward -966.30
Iteration 97 took 2.21 seconds (mean sampled reward: -4494.89). Current reward after update: -2624.55, Optimal reward -966.30
Iteration 98 took 2.19 seconds (mean sampled reward: -4375.16). Current reward after update: -1504.83, Optimal reward -966.30
Iteration 99 took 2.17 seconds (mean sampled reward: -2995.75). Current reward after update: -1311.58, Optimal reward -966.30
Iteration 100 took 2.23 seconds (mean sampled reward: -2751.50). Current reward after update: -1479.23, Optimal reward -966.30
Iteration 101 took 2.34 seconds (mean sampled reward: -2822.56). Current reward after update: -1422.24, Optimal reward -966.30
Iteration 102 took 2.27 seconds (mean sampled reward: -3702.87). Current reward after update: -1416.79, Optimal reward -966.30
Iteration 103 took 2.24 seconds (mean sampled reward: -2788.62). Current reward after update: -1170.04, Optimal reward -966.30
Iteration 104 took 2.30 seconds (mean sampled reward: -2982.00). Current reward after update: -1586.55, Optimal reward -966.30
Iteration 105 took 2.19 seconds (mean sampled reward: -3030.23). Current reward after update: -1403.60, Optimal reward -966.30
Iteration 106 took 2.31 seconds (mean sampled reward: -3146.40). Current reward after update: -1362.47, Optimal reward -966.30
Iteration 107 took 2.21 seconds (mean sampled reward: -3940.57). Current reward after update: -1283.84, Optimal reward -966.30
Iteration 108 took 2.16 seconds (mean sampled reward: -3520.94). Current reward after update: -2324.78, Optimal reward -966.30
Iteration 109 took 2.30 seconds (mean sampled reward: -3167.66). Current reward after update: -1225.53, Optimal reward -966.30
Iteration 110 took 2.25 seconds (mean sampled reward: -3103.07). Current reward after update: -1115.71, Optimal reward -966.30
Iteration 111 took 2.47 seconds (mean sampled reward: -3474.69). Current reward after update: -1091.80, Optimal reward -966.30
Iteration 112 took 2.21 seconds (mean sampled reward: -3199.12). Current reward after update: -2245.45, Optimal reward -966.30
Iteration 113 took 2.25 seconds (mean sampled reward: -3417.31). Current reward after update: -1112.92, Optimal reward -966.30
Iteration 114 took 2.50 seconds (mean sampled reward: -3108.72). Current reward after update: -1098.48, Optimal reward -966.30
Iteration 115 took 2.17 seconds (mean sampled reward: -3052.28). Current reward after update: -1100.58, Optimal reward -966.30
Iteration 116 took 2.21 seconds (mean sampled reward: -4797.94). Current reward after update: -1183.04, Optimal reward -966.30
Iteration 117 took 2.13 seconds (mean sampled reward: -4618.63). Current reward after update: -1078.36, Optimal reward -966.30
Iteration 118 took 2.13 seconds (mean sampled reward: -4708.14). Current reward after update: -1059.52, Optimal reward -966.30
Iteration 119 took 2.23 seconds (mean sampled reward: -4582.46). Current reward after update: -971.32, Optimal reward -966.30
Iteration 120 took 2.21 seconds (mean sampled reward: -3644.92). Current reward after update: -1024.14, Optimal reward -966.30
Iteration 121 took 2.29 seconds (mean sampled reward: -3937.42). Current reward after update: -1034.95, Optimal reward -966.30
Iteration 122 took 2.16 seconds (mean sampled reward: -4237.37). Current reward after update: -1212.76, Optimal reward -966.30
Iteration 123 took 2.20 seconds (mean sampled reward: -4155.28). Current reward after update: -1022.42, Optimal reward -966.30
Iteration 124 took 2.18 seconds (mean sampled reward: -5391.07). Current reward after update: -1180.04, Optimal reward -966.30
Iteration 125 took 2.20 seconds (mean sampled reward: -3928.02). Current reward after update: -1087.81, Optimal reward -966.30
Iteration 126 took 2.25 seconds (mean sampled reward: -4136.44). Current reward after update: -1022.34, Optimal reward -966.30
Iteration 127 took 2.21 seconds (mean sampled reward: -3272.40). Current reward after update: -1986.31, Optimal reward -966.30
Iteration 128 took 2.17 seconds (mean sampled reward: -3661.53). Current reward after update: -1129.25, Optimal reward -966.30
Iteration 129 took 2.21 seconds (mean sampled reward: -4339.79). Current reward after update: -1312.87, Optimal reward -966.30
Iteration 130 took 2.22 seconds (mean sampled reward: -4390.63). Current reward after update: -5617.36, Optimal reward -966.30
Iteration 131 took 2.15 seconds (mean sampled reward: -5473.63). Current reward after update: -1390.42, Optimal reward -966.30
Iteration 132 took 2.22 seconds (mean sampled reward: -4929.93). Current reward after update: -1188.90, Optimal reward -966.30
Iteration 133 took 2.18 seconds (mean sampled reward: -4252.35). Current reward after update: -1063.23, Optimal reward -966.30
Iteration 134 took 2.17 seconds (mean sampled reward: -3333.48). Current reward after update: -1157.44, Optimal reward -966.30
Iteration 135 took 2.18 seconds (mean sampled reward: -3405.92). Current reward after update: -1001.63, Optimal reward -966.30
Iteration 136 took 2.15 seconds (mean sampled reward: -4930.29). Current reward after update: -1110.73, Optimal reward -966.30
Iteration 137 took 2.27 seconds (mean sampled reward: -4246.84). Current reward after update: -1076.54, Optimal reward -966.30
Iteration 138 took 2.20 seconds (mean sampled reward: -5176.59). Current reward after update: -1108.40, Optimal reward -966.30
Iteration 139 took 2.22 seconds (mean sampled reward: -5410.13). Current reward after update: -968.28, Optimal reward -966.30
Iteration 140 took 2.21 seconds (mean sampled reward: -5151.89). Current reward after update: -844.49, Optimal reward -844.49
Iteration 141 took 2.18 seconds (mean sampled reward: -4069.22). Current reward after update: -892.22, Optimal reward -844.49
Iteration 142 took 2.18 seconds (mean sampled reward: -4657.95). Current reward after update: -876.27, Optimal reward -844.49
Iteration 143 took 2.20 seconds (mean sampled reward: -4518.38). Current reward after update: -949.50, Optimal reward -844.49
Iteration 144 took 2.21 seconds (mean sampled reward: -4720.71). Current reward after update: -1025.00, Optimal reward -844.49
Iteration 145 took 2.21 seconds (mean sampled reward: -4487.84). Current reward after update: -872.65, Optimal reward -844.49
Iteration 146 took 2.22 seconds (mean sampled reward: -4508.28). Current reward after update: -1028.04, Optimal reward -844.49
Iteration 147 took 2.22 seconds (mean sampled reward: -5812.22). Current reward after update: -1088.85, Optimal reward -844.49
Iteration 148 took 2.21 seconds (mean sampled reward: -5136.45). Current reward after update: -998.11, Optimal reward -844.49
Iteration 149 took 2.23 seconds (mean sampled reward: -4624.95). Current reward after update: -966.63, Optimal reward -844.49
Iteration 150 took 2.21 seconds (mean sampled reward: -4196.78). Current reward after update: -1024.44, Optimal reward -844.49
Iteration 151 took 2.21 seconds (mean sampled reward: -3620.64). Current reward after update: -843.71, Optimal reward -843.71
Iteration 152 took 2.15 seconds (mean sampled reward: -4136.67). Current reward after update: -853.26, Optimal reward -843.71
Iteration 153 took 2.17 seconds (mean sampled reward: -5812.10). Current reward after update: -988.75, Optimal reward -843.71
Iteration 154 took 2.15 seconds (mean sampled reward: -4701.64). Current reward after update: -752.56, Optimal reward -752.56
Iteration 155 took 2.31 seconds (mean sampled reward: -5484.68). Current reward after update: -1037.94, Optimal reward -752.56
Iteration 156 took 2.18 seconds (mean sampled reward: -4786.00). Current reward after update: -947.36, Optimal reward -752.56
Iteration 157 took 2.22 seconds (mean sampled reward: -4239.20). Current reward after update: -1155.44, Optimal reward -752.56
Iteration 158 took 2.21 seconds (mean sampled reward: -3969.50). Current reward after update: -1415.83, Optimal reward -752.56
Iteration 159 took 2.21 seconds (mean sampled reward: -4436.27). Current reward after update: -1089.00, Optimal reward -752.56
Iteration 160 took 2.25 seconds (mean sampled reward: -4504.90). Current reward after update: -845.37, Optimal reward -752.56
Iteration 161 took 2.19 seconds (mean sampled reward: -4194.52). Current reward after update: -1802.12, Optimal reward -752.56
Iteration 162 took 2.25 seconds (mean sampled reward: -5119.51). Current reward after update: -697.85, Optimal reward -697.85
Iteration 163 took 2.22 seconds (mean sampled reward: -4265.97). Current reward after update: -780.37, Optimal reward -697.85
Iteration 164 took 2.21 seconds (mean sampled reward: -4687.03). Current reward after update: -811.80, Optimal reward -697.85
Iteration 165 took 2.22 seconds (mean sampled reward: -5229.49). Current reward after update: -883.09, Optimal reward -697.85
Iteration 166 took 2.17 seconds (mean sampled reward: -5144.40). Current reward after update: -410.15, Optimal reward -410.15
Iteration 167 took 2.20 seconds (mean sampled reward: -4060.65). Current reward after update: -683.97, Optimal reward -410.15
Iteration 168 took 2.16 seconds (mean sampled reward: -5159.71). Current reward after update: -794.87, Optimal reward -410.15
Iteration 169 took 2.19 seconds (mean sampled reward: -3769.69). Current reward after update: -630.94, Optimal reward -410.15
Iteration 170 took 2.23 seconds (mean sampled reward: -4376.37). Current reward after update: -735.95, Optimal reward -410.15
Iteration 171 took 2.19 seconds (mean sampled reward: -4020.40). Current reward after update: -945.11, Optimal reward -410.15
Iteration 172 took 2.17 seconds (mean sampled reward: -3832.39). Current reward after update: -700.55, Optimal reward -410.15
Iteration 173 took 2.21 seconds (mean sampled reward: -6091.76). Current reward after update: -1574.21, Optimal reward -410.15
Iteration 174 took 2.22 seconds (mean sampled reward: -4666.27). Current reward after update: -1807.91, Optimal reward -410.15
Iteration 175 took 2.25 seconds (mean sampled reward: -3615.82). Current reward after update: -959.69, Optimal reward -410.15
Iteration 176 took 2.23 seconds (mean sampled reward: -5338.47). Current reward after update: -1808.89, Optimal reward -410.15
Iteration 177 took 2.29 seconds (mean sampled reward: -4860.12). Current reward after update: -949.59, Optimal reward -410.15
Iteration 178 took 2.24 seconds (mean sampled reward: -4861.85). Current reward after update: -1079.97, Optimal reward -410.15
Iteration 179 took 2.22 seconds (mean sampled reward: -4558.61). Current reward after update: -870.91, Optimal reward -410.15
Iteration 180 took 2.27 seconds (mean sampled reward: -4391.43). Current reward after update: -1342.67, Optimal reward -410.15
Iteration 181 took 2.25 seconds (mean sampled reward: -4580.76). Current reward after update: -1062.11, Optimal reward -410.15
Iteration 182 took 2.13 seconds (mean sampled reward: -5027.51). Current reward after update: -2943.87, Optimal reward -410.15
Iteration 183 took 2.25 seconds (mean sampled reward: -5039.69). Current reward after update: -1059.38, Optimal reward -410.15
Iteration 184 took 2.18 seconds (mean sampled reward: -4961.28). Current reward after update: -2678.18, Optimal reward -410.15
Iteration 185 took 2.18 seconds (mean sampled reward: -3600.29). Current reward after update: -826.50, Optimal reward -410.15
Iteration 186 took 2.16 seconds (mean sampled reward: -3897.22). Current reward after update: -839.66, Optimal reward -410.15
Iteration 187 took 2.20 seconds (mean sampled reward: -4027.79). Current reward after update: -1126.23, Optimal reward -410.15
Iteration 188 took 2.30 seconds (mean sampled reward: -3906.74). Current reward after update: -1456.99, Optimal reward -410.15
Iteration 189 took 2.20 seconds (mean sampled reward: -3454.39). Current reward after update: -1128.32, Optimal reward -410.15
Iteration 190 took 2.28 seconds (mean sampled reward: -3390.99). Current reward after update: -968.70, Optimal reward -410.15
Iteration 191 took 2.23 seconds (mean sampled reward: -3164.00). Current reward after update: -1171.10, Optimal reward -410.15
Iteration 192 took 2.22 seconds (mean sampled reward: -3179.82). Current reward after update: -3974.96, Optimal reward -410.15
Iteration 193 took 2.24 seconds (mean sampled reward: -3607.58). Current reward after update: -895.87, Optimal reward -410.15
Iteration 194 took 2.27 seconds (mean sampled reward: -4429.80). Current reward after update: -3743.68, Optimal reward -410.15
Iteration 195 took 2.17 seconds (mean sampled reward: -4403.49). Current reward after update: -1219.15, Optimal reward -410.15
Iteration 196 took 2.15 seconds (mean sampled reward: -3365.88). Current reward after update: -1074.73, Optimal reward -410.15
Iteration 197 took 2.25 seconds (mean sampled reward: -3328.78). Current reward after update: -2505.70, Optimal reward -410.15
Iteration 198 took 2.26 seconds (mean sampled reward: -3172.06). Current reward after update: -1088.13, Optimal reward -410.15
Iteration 199 took 2.20 seconds (mean sampled reward: -2666.58). Current reward after update: -1243.85, Optimal reward -410.15
Iteration 200 took 2.20 seconds (mean sampled reward: -2466.22). Current reward after update: -1079.68, Optimal reward -410.15
Iteration 1 took 2.34 seconds (mean sampled reward: -7634.06). Current reward after update: -7525.79, Optimal reward -7525.79
Iteration 2 took 2.31 seconds (mean sampled reward: -7593.09). Current reward after update: -7300.18, Optimal reward -7300.18
Iteration 3 took 2.26 seconds (mean sampled reward: -7607.27). Current reward after update: -7204.39, Optimal reward -7204.39
Iteration 4 took 2.27 seconds (mean sampled reward: -7589.69). Current reward after update: -7232.92, Optimal reward -7204.39
Iteration 5 took 2.33 seconds (mean sampled reward: -7581.38). Current reward after update: -7266.21, Optimal reward -7204.39
Iteration 6 took 2.59 seconds (mean sampled reward: -7592.06). Current reward after update: -7166.68, Optimal reward -7166.68
Iteration 7 took 2.31 seconds (mean sampled reward: -7500.36). Current reward after update: -6771.87, Optimal reward -6771.87
Iteration 8 took 2.24 seconds (mean sampled reward: -7223.84). Current reward after update: -5959.59, Optimal reward -5959.59
Iteration 9 took 2.74 seconds (mean sampled reward: -6917.55). Current reward after update: -5299.84, Optimal reward -5299.84
Iteration 10 took 2.36 seconds (mean sampled reward: -6630.97). Current reward after update: -3996.68, Optimal reward -3996.68
Iteration 11 took 2.27 seconds (mean sampled reward: -6157.67). Current reward after update: -3410.18, Optimal reward -3410.18
Iteration 12 took 2.23 seconds (mean sampled reward: -5866.39). Current reward after update: -3082.69, Optimal reward -3082.69
Iteration 13 took 2.21 seconds (mean sampled reward: -5412.79). Current reward after update: -2975.22, Optimal reward -2975.22
Iteration 14 took 2.26 seconds (mean sampled reward: -4979.35). Current reward after update: -2428.25, Optimal reward -2428.25
Iteration 15 took 2.31 seconds (mean sampled reward: -5840.34). Current reward after update: -2628.61, Optimal reward -2428.25
Iteration 16 took 2.52 seconds (mean sampled reward: -5774.11). Current reward after update: -2370.58, Optimal reward -2370.58
Iteration 17 took 2.30 seconds (mean sampled reward: -5226.16). Current reward after update: -2194.60, Optimal reward -2194.60
Iteration 18 took 2.39 seconds (mean sampled reward: -5661.12). Current reward after update: -2331.03, Optimal reward -2194.60
Iteration 19 took 2.38 seconds (mean sampled reward: -5879.78). Current reward after update: -2170.69, Optimal reward -2170.69
Iteration 20 took 2.53 seconds (mean sampled reward: -5800.01). Current reward after update: -2311.48, Optimal reward -2170.69
Iteration 21 took 2.44 seconds (mean sampled reward: -5812.91). Current reward after update: -2306.98, Optimal reward -2170.69
Iteration 22 took 2.38 seconds (mean sampled reward: -5179.71). Current reward after update: -2259.62, Optimal reward -2170.69
Iteration 23 took 2.26 seconds (mean sampled reward: -4697.44). Current reward after update: -2170.22, Optimal reward -2170.22
Iteration 24 took 2.29 seconds (mean sampled reward: -5660.43). Current reward after update: -2100.41, Optimal reward -2100.41
Iteration 25 took 2.40 seconds (mean sampled reward: -5342.05). Current reward after update: -2177.67, Optimal reward -2100.41
Iteration 26 took 2.19 seconds (mean sampled reward: -4247.88). Current reward after update: -2113.95, Optimal reward -2100.41
Iteration 27 took 2.19 seconds (mean sampled reward: -4485.04). Current reward after update: -3046.51, Optimal reward -2100.41
Iteration 28 took 2.19 seconds (mean sampled reward: -4790.67). Current reward after update: -2128.87, Optimal reward -2100.41
Iteration 29 took 2.31 seconds (mean sampled reward: -5334.08). Current reward after update: -2360.86, Optimal reward -2100.41
Iteration 30 took 2.58 seconds (mean sampled reward: -4604.75). Current reward after update: -2260.89, Optimal reward -2100.41
Iteration 31 took 2.26 seconds (mean sampled reward: -5497.27). Current reward after update: -2211.52, Optimal reward -2100.41
Iteration 32 took 2.28 seconds (mean sampled reward: -5010.00). Current reward after update: -2166.15, Optimal reward -2100.41
Iteration 33 took 2.28 seconds (mean sampled reward: -5370.51). Current reward after update: -1891.31, Optimal reward -1891.31
Iteration 34 took 2.34 seconds (mean sampled reward: -5572.42). Current reward after update: -2001.46, Optimal reward -1891.31
Iteration 35 took 2.19 seconds (mean sampled reward: -4998.21). Current reward after update: -1792.15, Optimal reward -1792.15
Iteration 36 took 2.35 seconds (mean sampled reward: -5316.97). Current reward after update: -1939.17, Optimal reward -1792.15
Iteration 37 took 2.27 seconds (mean sampled reward: -5840.21). Current reward after update: -2182.10, Optimal reward -1792.15
Iteration 38 took 2.28 seconds (mean sampled reward: -6441.43). Current reward after update: -2081.68, Optimal reward -1792.15
Iteration 39 took 2.25 seconds (mean sampled reward: -6317.16). Current reward after update: -2269.65, Optimal reward -1792.15
Iteration 40 took 2.34 seconds (mean sampled reward: -6327.49). Current reward after update: -1969.51, Optimal reward -1792.15
Iteration 41 took 2.30 seconds (mean sampled reward: -5631.54). Current reward after update: -1984.29, Optimal reward -1792.15
Iteration 42 took 2.24 seconds (mean sampled reward: -4654.82). Current reward after update: -1828.80, Optimal reward -1792.15
Iteration 43 took 2.31 seconds (mean sampled reward: -4585.92). Current reward after update: -1720.84, Optimal reward -1720.84
Iteration 44 took 2.27 seconds (mean sampled reward: -5293.65). Current reward after update: -1750.92, Optimal reward -1720.84
Iteration 45 took 2.29 seconds (mean sampled reward: -4916.01). Current reward after update: -1664.25, Optimal reward -1664.25
Iteration 46 took 2.22 seconds (mean sampled reward: -4576.22). Current reward after update: -1719.84, Optimal reward -1664.25
Iteration 47 took 2.27 seconds (mean sampled reward: -6357.05). Current reward after update: -1791.20, Optimal reward -1664.25
Iteration 48 took 2.24 seconds (mean sampled reward: -6011.40). Current reward after update: -1728.89, Optimal reward -1664.25
Iteration 49 took 2.19 seconds (mean sampled reward: -4729.29). Current reward after update: -1802.75, Optimal reward -1664.25
Iteration 50 took 2.16 seconds (mean sampled reward: -4743.21). Current reward after update: -2054.21, Optimal reward -1664.25
Iteration 51 took 2.21 seconds (mean sampled reward: -5765.21). Current reward after update: -1693.22, Optimal reward -1664.25
Iteration 52 took 2.23 seconds (mean sampled reward: -4807.67). Current reward after update: -2259.31, Optimal reward -1664.25
Iteration 53 took 2.27 seconds (mean sampled reward: -6213.81). Current reward after update: -1603.08, Optimal reward -1603.08
Iteration 54 took 2.43 seconds (mean sampled reward: -4449.90). Current reward after update: -1545.03, Optimal reward -1545.03
Iteration 55 took 2.44 seconds (mean sampled reward: -4627.00). Current reward after update: -1677.89, Optimal reward -1545.03
Iteration 56 took 2.29 seconds (mean sampled reward: -3801.04). Current reward after update: -1640.19, Optimal reward -1545.03
Iteration 57 took 2.12 seconds (mean sampled reward: -3623.07). Current reward after update: -1643.51, Optimal reward -1545.03
Iteration 58 took 2.28 seconds (mean sampled reward: -4376.53). Current reward after update: -1646.82, Optimal reward -1545.03
Iteration 59 took 2.27 seconds (mean sampled reward: -2961.13). Current reward after update: -1654.29, Optimal reward -1545.03
Iteration 60 took 2.23 seconds (mean sampled reward: -3151.58). Current reward after update: -1644.27, Optimal reward -1545.03
Iteration 61 took 2.14 seconds (mean sampled reward: -2746.80). Current reward after update: -1622.14, Optimal reward -1545.03
Iteration 62 took 2.26 seconds (mean sampled reward: -2993.73). Current reward after update: -1574.89, Optimal reward -1545.03
Iteration 63 took 2.20 seconds (mean sampled reward: -2591.80). Current reward after update: -1694.18, Optimal reward -1545.03
Iteration 64 took 2.23 seconds (mean sampled reward: -3632.24). Current reward after update: -2127.12, Optimal reward -1545.03
Iteration 65 took 2.19 seconds (mean sampled reward: -4562.45). Current reward after update: -1514.21, Optimal reward -1514.21
Iteration 66 took 2.15 seconds (mean sampled reward: -4177.15). Current reward after update: -6613.63, Optimal reward -1514.21
Iteration 67 took 2.20 seconds (mean sampled reward: -4659.82). Current reward after update: -1578.56, Optimal reward -1514.21
Iteration 68 took 2.21 seconds (mean sampled reward: -4405.56). Current reward after update: -1615.78, Optimal reward -1514.21
Iteration 69 took 2.13 seconds (mean sampled reward: -4261.19). Current reward after update: -1571.70, Optimal reward -1514.21
Iteration 70 took 2.23 seconds (mean sampled reward: -5673.73). Current reward after update: -1638.20, Optimal reward -1514.21
Iteration 71 took 2.16 seconds (mean sampled reward: -4816.67). Current reward after update: -1504.63, Optimal reward -1504.63
Iteration 72 took 2.16 seconds (mean sampled reward: -4657.62). Current reward after update: -1554.76, Optimal reward -1504.63
Iteration 73 took 2.14 seconds (mean sampled reward: -3062.60). Current reward after update: -1640.47, Optimal reward -1504.63
Iteration 74 took 2.15 seconds (mean sampled reward: -4411.85). Current reward after update: -1582.53, Optimal reward -1504.63
Iteration 75 took 2.19 seconds (mean sampled reward: -3827.55). Current reward after update: -1659.73, Optimal reward -1504.63
Iteration 76 took 2.16 seconds (mean sampled reward: -3622.13). Current reward after update: -1614.69, Optimal reward -1504.63
Iteration 77 took 2.17 seconds (mean sampled reward: -3709.99). Current reward after update: -1640.65, Optimal reward -1504.63
Iteration 78 took 2.18 seconds (mean sampled reward: -3675.08). Current reward after update: -1883.12, Optimal reward -1504.63
Iteration 79 took 2.17 seconds (mean sampled reward: -2917.74). Current reward after update: -1541.77, Optimal reward -1504.63
Iteration 80 took 2.20 seconds (mean sampled reward: -4027.74). Current reward after update: -1564.09, Optimal reward -1504.63
Iteration 81 took 2.21 seconds (mean sampled reward: -3784.51). Current reward after update: -1740.52, Optimal reward -1504.63
Iteration 82 took 2.21 seconds (mean sampled reward: -3174.49). Current reward after update: -1476.30, Optimal reward -1476.30
Iteration 83 took 2.24 seconds (mean sampled reward: -3092.96). Current reward after update: -1452.53, Optimal reward -1452.53
Iteration 84 took 2.27 seconds (mean sampled reward: -4371.39). Current reward after update: -1397.17, Optimal reward -1397.17
Iteration 85 took 2.26 seconds (mean sampled reward: -3752.31). Current reward after update: -1388.22, Optimal reward -1388.22
Iteration 86 took 2.37 seconds (mean sampled reward: -5012.20). Current reward after update: -1472.13, Optimal reward -1388.22
Iteration 87 took 2.31 seconds (mean sampled reward: -3786.61). Current reward after update: -1641.34, Optimal reward -1388.22
Iteration 88 took 2.30 seconds (mean sampled reward: -2760.22). Current reward after update: -1420.95, Optimal reward -1388.22
Iteration 89 took 2.34 seconds (mean sampled reward: -4485.18). Current reward after update: -1457.64, Optimal reward -1388.22
Iteration 90 took 2.32 seconds (mean sampled reward: -4475.92). Current reward after update: -1388.67, Optimal reward -1388.22
Iteration 91 took 2.28 seconds (mean sampled reward: -3795.12). Current reward after update: -2253.75, Optimal reward -1388.22
Iteration 92 took 2.28 seconds (mean sampled reward: -3008.99). Current reward after update: -1338.02, Optimal reward -1338.02
Iteration 93 took 2.31 seconds (mean sampled reward: -2644.58). Current reward after update: -1355.15, Optimal reward -1338.02
Iteration 94 took 2.31 seconds (mean sampled reward: -3035.67). Current reward after update: -1597.60, Optimal reward -1338.02
Iteration 95 took 2.37 seconds (mean sampled reward: -3288.45). Current reward after update: -1357.81, Optimal reward -1338.02
Iteration 96 took 2.39 seconds (mean sampled reward: -4707.85). Current reward after update: -6394.72, Optimal reward -1338.02
Iteration 97 took 2.25 seconds (mean sampled reward: -4139.65). Current reward after update: -1241.72, Optimal reward -1241.72
Iteration 98 took 2.30 seconds (mean sampled reward: -3970.51). Current reward after update: -1322.44, Optimal reward -1241.72
Iteration 99 took 2.32 seconds (mean sampled reward: -2921.47). Current reward after update: -1309.00, Optimal reward -1241.72
Iteration 100 took 2.26 seconds (mean sampled reward: -3025.53). Current reward after update: -1432.52, Optimal reward -1241.72
Iteration 101 took 2.29 seconds (mean sampled reward: -3392.63). Current reward after update: -1374.69, Optimal reward -1241.72
Iteration 102 took 2.23 seconds (mean sampled reward: -4260.48). Current reward after update: -1296.05, Optimal reward -1241.72
Iteration 103 took 2.27 seconds (mean sampled reward: -2904.18). Current reward after update: -1287.57, Optimal reward -1241.72
Iteration 104 took 2.29 seconds (mean sampled reward: -3439.18). Current reward after update: -1330.38, Optimal reward -1241.72
Iteration 105 took 2.32 seconds (mean sampled reward: -3392.93). Current reward after update: -1347.39, Optimal reward -1241.72
Iteration 106 took 2.32 seconds (mean sampled reward: -4421.63). Current reward after update: -1303.32, Optimal reward -1241.72
Iteration 107 took 2.29 seconds (mean sampled reward: -4577.45). Current reward after update: -1331.68, Optimal reward -1241.72
Iteration 108 took 2.43 seconds (mean sampled reward: -5453.34). Current reward after update: -1339.70, Optimal reward -1241.72
Iteration 109 took 2.54 seconds (mean sampled reward: -5098.94). Current reward after update: -1880.15, Optimal reward -1241.72
Iteration 110 took 2.34 seconds (mean sampled reward: -4441.31). Current reward after update: -1352.45, Optimal reward -1241.72
Iteration 111 took 2.26 seconds (mean sampled reward: -3831.03). Current reward after update: -1815.74, Optimal reward -1241.72
Iteration 112 took 2.34 seconds (mean sampled reward: -4458.81). Current reward after update: -1491.26, Optimal reward -1241.72
Iteration 113 took 2.40 seconds (mean sampled reward: -5362.84). Current reward after update: -1480.63, Optimal reward -1241.72
Iteration 114 took 2.33 seconds (mean sampled reward: -4810.71). Current reward after update: -2030.13, Optimal reward -1241.72
Iteration 115 took 2.41 seconds (mean sampled reward: -3698.66). Current reward after update: -1478.72, Optimal reward -1241.72
Iteration 116 took 2.58 seconds (mean sampled reward: -4606.32). Current reward after update: -1459.55, Optimal reward -1241.72
Iteration 117 took 2.38 seconds (mean sampled reward: -4453.81). Current reward after update: -1410.67, Optimal reward -1241.72
Iteration 118 took 2.35 seconds (mean sampled reward: -5086.70). Current reward after update: -1418.86, Optimal reward -1241.72
Iteration 119 took 2.33 seconds (mean sampled reward: -3230.81). Current reward after update: -1352.49, Optimal reward -1241.72
Iteration 120 took 2.39 seconds (mean sampled reward: -3172.99). Current reward after update: -1490.95, Optimal reward -1241.72
Iteration 121 took 2.35 seconds (mean sampled reward: -3948.82). Current reward after update: -1316.01, Optimal reward -1241.72
Iteration 122 took 2.31 seconds (mean sampled reward: -4310.34). Current reward after update: -1366.26, Optimal reward -1241.72
Iteration 123 took 2.32 seconds (mean sampled reward: -4663.95). Current reward after update: -2462.77, Optimal reward -1241.72
Iteration 124 took 2.35 seconds (mean sampled reward: -4897.92). Current reward after update: -1350.45, Optimal reward -1241.72
Iteration 125 took 2.37 seconds (mean sampled reward: -4162.90). Current reward after update: -6531.47, Optimal reward -1241.72
Iteration 126 took 2.33 seconds (mean sampled reward: -6126.11). Current reward after update: -1323.80, Optimal reward -1241.72
Iteration 127 took 2.35 seconds (mean sampled reward: -5001.76). Current reward after update: -1289.77, Optimal reward -1241.72
Iteration 128 took 2.39 seconds (mean sampled reward: -5004.89). Current reward after update: -1353.32, Optimal reward -1241.72
Iteration 129 took 2.41 seconds (mean sampled reward: -4854.82). Current reward after update: -1522.03, Optimal reward -1241.72
Iteration 130 took 2.37 seconds (mean sampled reward: -4167.16). Current reward after update: -1323.79, Optimal reward -1241.72
Iteration 131 took 2.47 seconds (mean sampled reward: -4665.06). Current reward after update: -1386.44, Optimal reward -1241.72
Iteration 132 took 2.37 seconds (mean sampled reward: -4434.84). Current reward after update: -1624.14, Optimal reward -1241.72
Iteration 133 took 2.36 seconds (mean sampled reward: -3735.06). Current reward after update: -1279.49, Optimal reward -1241.72
Iteration 134 took 2.38 seconds (mean sampled reward: -5009.58). Current reward after update: -1396.70, Optimal reward -1241.72
Iteration 135 took 2.37 seconds (mean sampled reward: -3280.24). Current reward after update: -2516.91, Optimal reward -1241.72
Iteration 136 took 2.35 seconds (mean sampled reward: -3343.07). Current reward after update: -1437.88, Optimal reward -1241.72
Iteration 137 took 2.35 seconds (mean sampled reward: -3125.26). Current reward after update: -1335.67, Optimal reward -1241.72
Iteration 138 took 2.36 seconds (mean sampled reward: -2275.24). Current reward after update: -1341.09, Optimal reward -1241.72
Iteration 139 took 2.38 seconds (mean sampled reward: -2477.90). Current reward after update: -1267.12, Optimal reward -1241.72
Iteration 140 took 2.36 seconds (mean sampled reward: -3127.57). Current reward after update: -1357.46, Optimal reward -1241.72
Iteration 141 took 2.35 seconds (mean sampled reward: -2695.15). Current reward after update: -1564.42, Optimal reward -1241.72
Iteration 142 took 2.44 seconds (mean sampled reward: -3487.50). Current reward after update: -1446.85, Optimal reward -1241.72
Iteration 143 took 2.34 seconds (mean sampled reward: -3921.99). Current reward after update: -1970.20, Optimal reward -1241.72
Iteration 144 took 2.37 seconds (mean sampled reward: -3200.29). Current reward after update: -1294.09, Optimal reward -1241.72
Iteration 145 took 2.37 seconds (mean sampled reward: -2379.28). Current reward after update: -1339.10, Optimal reward -1241.72
Iteration 146 took 2.35 seconds (mean sampled reward: -2878.90). Current reward after update: -1303.79, Optimal reward -1241.72
Iteration 147 took 2.41 seconds (mean sampled reward: -2565.51). Current reward after update: -1313.84, Optimal reward -1241.72
Iteration 148 took 2.41 seconds (mean sampled reward: -2409.46). Current reward after update: -1210.85, Optimal reward -1210.85
Iteration 149 took 2.37 seconds (mean sampled reward: -4112.74). Current reward after update: -1311.53, Optimal reward -1210.85
Iteration 150 took 2.38 seconds (mean sampled reward: -4603.11). Current reward after update: -1269.80, Optimal reward -1210.85
Iteration 151 took 2.36 seconds (mean sampled reward: -4632.88). Current reward after update: -1403.37, Optimal reward -1210.85
Iteration 152 took 2.42 seconds (mean sampled reward: -3910.64). Current reward after update: -2032.15, Optimal reward -1210.85
Iteration 153 took 2.32 seconds (mean sampled reward: -4012.38). Current reward after update: -1311.49, Optimal reward -1210.85
Iteration 154 took 2.36 seconds (mean sampled reward: -3864.16). Current reward after update: -1986.62, Optimal reward -1210.85
Iteration 155 took 2.41 seconds (mean sampled reward: -3040.30). Current reward after update: -1307.67, Optimal reward -1210.85
Iteration 156 took 2.39 seconds (mean sampled reward: -3200.79). Current reward after update: -1253.72, Optimal reward -1210.85
Iteration 157 took 2.42 seconds (mean sampled reward: -2673.41). Current reward after update: -1930.67, Optimal reward -1210.85
Iteration 158 took 2.35 seconds (mean sampled reward: -2541.61). Current reward after update: -1283.95, Optimal reward -1210.85
Iteration 159 took 2.36 seconds (mean sampled reward: -2609.54). Current reward after update: -1287.02, Optimal reward -1210.85
Iteration 160 took 2.39 seconds (mean sampled reward: -2344.72). Current reward after update: -1406.67, Optimal reward -1210.85
Iteration 161 took 2.36 seconds (mean sampled reward: -2244.59). Current reward after update: -1234.30, Optimal reward -1210.85
Iteration 162 took 2.37 seconds (mean sampled reward: -2520.04). Current reward after update: -1249.34, Optimal reward -1210.85
Iteration 163 took 2.36 seconds (mean sampled reward: -2344.12). Current reward after update: -1259.32, Optimal reward -1210.85
Iteration 164 took 2.39 seconds (mean sampled reward: -2362.88). Current reward after update: -1277.30, Optimal reward -1210.85
Iteration 165 took 2.35 seconds (mean sampled reward: -2226.99). Current reward after update: -1365.11, Optimal reward -1210.85
Iteration 166 took 2.36 seconds (mean sampled reward: -2729.21). Current reward after update: -1319.59, Optimal reward -1210.85
Iteration 167 took 2.37 seconds (mean sampled reward: -3215.76). Current reward after update: -1322.58, Optimal reward -1210.85
Iteration 168 took 2.36 seconds (mean sampled reward: -3482.96). Current reward after update: -1307.75, Optimal reward -1210.85
Iteration 169 took 2.34 seconds (mean sampled reward: -3105.31). Current reward after update: -1241.65, Optimal reward -1210.85
Iteration 170 took 2.39 seconds (mean sampled reward: -2816.51). Current reward after update: -1293.88, Optimal reward -1210.85
Iteration 171 took 2.34 seconds (mean sampled reward: -3166.38). Current reward after update: -1264.58, Optimal reward -1210.85
Iteration 172 took 2.40 seconds (mean sampled reward: -3078.83). Current reward after update: -1294.02, Optimal reward -1210.85
Iteration 173 took 2.35 seconds (mean sampled reward: -3085.72). Current reward after update: -1249.12, Optimal reward -1210.85
Iteration 174 took 2.46 seconds (mean sampled reward: -3791.80). Current reward after update: -1251.21, Optimal reward -1210.85
Iteration 175 took 2.40 seconds (mean sampled reward: -4350.39). Current reward after update: -1259.62, Optimal reward -1210.85
Iteration 176 took 2.40 seconds (mean sampled reward: -4980.55). Current reward after update: -1276.86, Optimal reward -1210.85
Iteration 177 took 2.38 seconds (mean sampled reward: -3393.22). Current reward after update: -1224.09, Optimal reward -1210.85
Iteration 178 took 2.37 seconds (mean sampled reward: -4787.88). Current reward after update: -1298.16, Optimal reward -1210.85
Iteration 179 took 2.39 seconds (mean sampled reward: -3703.64). Current reward after update: -1300.94, Optimal reward -1210.85
Iteration 180 took 2.41 seconds (mean sampled reward: -3289.39). Current reward after update: -1568.47, Optimal reward -1210.85
Iteration 181 took 2.45 seconds (mean sampled reward: -3497.62). Current reward after update: -1311.65, Optimal reward -1210.85
Iteration 182 took 2.38 seconds (mean sampled reward: -3404.98). Current reward after update: -1487.04, Optimal reward -1210.85
Iteration 183 took 2.44 seconds (mean sampled reward: -2532.33). Current reward after update: -1278.99, Optimal reward -1210.85
Iteration 184 took 2.38 seconds (mean sampled reward: -2737.05). Current reward after update: -1921.45, Optimal reward -1210.85
Iteration 185 took 2.44 seconds (mean sampled reward: -2972.30). Current reward after update: -1272.28, Optimal reward -1210.85
Iteration 186 took 2.36 seconds (mean sampled reward: -3148.62). Current reward after update: -1782.37, Optimal reward -1210.85
Iteration 187 took 2.43 seconds (mean sampled reward: -3158.95). Current reward after update: -1278.69, Optimal reward -1210.85
Iteration 188 took 2.41 seconds (mean sampled reward: -3771.31). Current reward after update: -1286.89, Optimal reward -1210.85
Iteration 189 took 2.41 seconds (mean sampled reward: -3298.99). Current reward after update: -1822.88, Optimal reward -1210.85
Iteration 190 took 2.40 seconds (mean sampled reward: -4752.59). Current reward after update: -1272.71, Optimal reward -1210.85
Iteration 191 took 2.38 seconds (mean sampled reward: -4428.83). Current reward after update: -1236.22, Optimal reward -1210.85
Iteration 192 took 2.34 seconds (mean sampled reward: -2944.70). Current reward after update: -1272.86, Optimal reward -1210.85
Iteration 193 took 2.36 seconds (mean sampled reward: -3625.13). Current reward after update: -1290.84, Optimal reward -1210.85
Iteration 194 took 2.35 seconds (mean sampled reward: -2581.69). Current reward after update: -1256.21, Optimal reward -1210.85
Iteration 195 took 2.43 seconds (mean sampled reward: -2605.52). Current reward after update: -1254.04, Optimal reward -1210.85
Iteration 196 took 2.43 seconds (mean sampled reward: -3497.58). Current reward after update: -1426.90, Optimal reward -1210.85
Iteration 197 took 2.40 seconds (mean sampled reward: -4167.52). Current reward after update: -1278.03, Optimal reward -1210.85
Iteration 198 took 2.38 seconds (mean sampled reward: -3964.78). Current reward after update: -1282.58, Optimal reward -1210.85
Iteration 199 took 2.38 seconds (mean sampled reward: -3621.79). Current reward after update: -1438.93, Optimal reward -1210.85
Iteration 200 took 2.44 seconds (mean sampled reward: -4796.49). Current reward after update: -2288.74, Optimal reward -1210.85
Max force: 30 Sigma: 0.2 mean rewards: -788.0994468538718, best rewards:-410.15249888291964

Iteration 1 took 2.38 seconds (mean sampled reward: -7630.49). Current reward after update: -6351.48, Optimal reward -6351.48
Iteration 2 took 2.43 seconds (mean sampled reward: -7278.93). Current reward after update: -5449.08, Optimal reward -5449.08
Iteration 3 took 2.35 seconds (mean sampled reward: -7071.75). Current reward after update: -4708.84, Optimal reward -4708.84
Iteration 4 took 2.40 seconds (mean sampled reward: -6634.26). Current reward after update: -3462.98, Optimal reward -3462.98
Iteration 5 took 2.32 seconds (mean sampled reward: -5980.44). Current reward after update: -5363.02, Optimal reward -3462.98
Iteration 6 took 2.32 seconds (mean sampled reward: -5090.65). Current reward after update: -2489.69, Optimal reward -2489.69
Iteration 7 took 2.33 seconds (mean sampled reward: -5205.29). Current reward after update: -3134.98, Optimal reward -2489.69
Iteration 8 took 2.39 seconds (mean sampled reward: -5158.01). Current reward after update: -2555.91, Optimal reward -2489.69
Iteration 9 took 2.64 seconds (mean sampled reward: -5428.05). Current reward after update: -2516.03, Optimal reward -2489.69
Iteration 10 took 2.34 seconds (mean sampled reward: -5785.43). Current reward after update: -2628.48, Optimal reward -2489.69
Iteration 11 took 2.48 seconds (mean sampled reward: -6183.95). Current reward after update: -2573.53, Optimal reward -2489.69
Iteration 12 took 2.38 seconds (mean sampled reward: -5323.74). Current reward after update: -2374.61, Optimal reward -2374.61
Iteration 13 took 2.42 seconds (mean sampled reward: -6266.52). Current reward after update: -2588.14, Optimal reward -2374.61
Iteration 14 took 2.44 seconds (mean sampled reward: -5254.43). Current reward after update: -2030.08, Optimal reward -2030.08
Iteration 15 took 2.31 seconds (mean sampled reward: -5664.05). Current reward after update: -2737.35, Optimal reward -2030.08
Iteration 16 took 2.28 seconds (mean sampled reward: -5043.78). Current reward after update: -1502.98, Optimal reward -1502.98
Iteration 17 took 2.51 seconds (mean sampled reward: -6415.06). Current reward after update: -1510.28, Optimal reward -1502.98
Iteration 18 took 2.39 seconds (mean sampled reward: -4859.19). Current reward after update: -2116.11, Optimal reward -1502.98
Iteration 19 took 2.53 seconds (mean sampled reward: -5764.52). Current reward after update: -2293.81, Optimal reward -1502.98
Iteration 20 took 2.39 seconds (mean sampled reward: -5044.89). Current reward after update: -1818.54, Optimal reward -1502.98
Iteration 21 took 2.36 seconds (mean sampled reward: -4720.85). Current reward after update: -1397.67, Optimal reward -1397.67
Iteration 22 took 2.44 seconds (mean sampled reward: -6143.77). Current reward after update: -4176.08, Optimal reward -1397.67
Iteration 23 took 2.33 seconds (mean sampled reward: -5875.47). Current reward after update: -1211.38, Optimal reward -1211.38
Iteration 24 took 2.33 seconds (mean sampled reward: -5004.47). Current reward after update: -1247.55, Optimal reward -1211.38
Iteration 25 took 2.33 seconds (mean sampled reward: -5385.30). Current reward after update: -1134.77, Optimal reward -1134.77
Iteration 26 took 2.47 seconds (mean sampled reward: -6827.48). Current reward after update: -1382.01, Optimal reward -1134.77
Iteration 27 took 2.32 seconds (mean sampled reward: -5875.73). Current reward after update: -1191.74, Optimal reward -1134.77
Iteration 28 took 2.37 seconds (mean sampled reward: -5497.43). Current reward after update: -5325.09, Optimal reward -1134.77
Iteration 29 took 2.52 seconds (mean sampled reward: -4947.77). Current reward after update: -1256.72, Optimal reward -1134.77
Iteration 30 took 2.54 seconds (mean sampled reward: -4005.55). Current reward after update: -1126.98, Optimal reward -1126.98
Iteration 31 took 2.44 seconds (mean sampled reward: -4678.23). Current reward after update: -1162.89, Optimal reward -1126.98
Iteration 32 took 2.53 seconds (mean sampled reward: -6164.16). Current reward after update: -1271.10, Optimal reward -1126.98
Iteration 33 took 2.42 seconds (mean sampled reward: -5801.85). Current reward after update: -1255.70, Optimal reward -1126.98
Iteration 34 took 2.34 seconds (mean sampled reward: -5033.35). Current reward after update: -1031.89, Optimal reward -1031.89
Iteration 35 took 2.59 seconds (mean sampled reward: -3052.84). Current reward after update: -891.57, Optimal reward -891.57
Iteration 36 took 2.34 seconds (mean sampled reward: -3109.90). Current reward after update: -942.56, Optimal reward -891.57
Iteration 37 took 2.38 seconds (mean sampled reward: -3759.32). Current reward after update: -817.05, Optimal reward -817.05
Iteration 38 took 2.34 seconds (mean sampled reward: -3651.36). Current reward after update: -855.28, Optimal reward -817.05
Iteration 39 took 2.41 seconds (mean sampled reward: -4862.21). Current reward after update: -1802.06, Optimal reward -817.05
Iteration 40 took 2.34 seconds (mean sampled reward: -5747.42). Current reward after update: -999.84, Optimal reward -817.05
Iteration 41 took 2.34 seconds (mean sampled reward: -5819.47). Current reward after update: -1245.87, Optimal reward -817.05
Iteration 42 took 2.41 seconds (mean sampled reward: -6364.12). Current reward after update: -1165.41, Optimal reward -817.05
Iteration 43 took 2.34 seconds (mean sampled reward: -4676.34). Current reward after update: -1087.84, Optimal reward -817.05
Iteration 44 took 2.38 seconds (mean sampled reward: -6331.62). Current reward after update: -1142.04, Optimal reward -817.05
Iteration 45 took 2.31 seconds (mean sampled reward: -5245.22). Current reward after update: -1114.73, Optimal reward -817.05
Iteration 46 took 2.31 seconds (mean sampled reward: -4983.18). Current reward after update: -939.98, Optimal reward -817.05
Iteration 47 took 2.32 seconds (mean sampled reward: -6041.20). Current reward after update: -1091.36, Optimal reward -817.05
Iteration 48 took 2.30 seconds (mean sampled reward: -4334.71). Current reward after update: -1104.93, Optimal reward -817.05
Iteration 49 took 2.26 seconds (mean sampled reward: -3778.63). Current reward after update: -892.04, Optimal reward -817.05
Iteration 50 took 2.27 seconds (mean sampled reward: -4556.86). Current reward after update: -934.07, Optimal reward -817.05
Iteration 51 took 2.36 seconds (mean sampled reward: -4052.55). Current reward after update: -1088.91, Optimal reward -817.05
Iteration 52 took 2.33 seconds (mean sampled reward: -4724.86). Current reward after update: -987.80, Optimal reward -817.05
Iteration 53 took 2.35 seconds (mean sampled reward: -4364.47). Current reward after update: -929.91, Optimal reward -817.05
Iteration 54 took 2.66 seconds (mean sampled reward: -6130.08). Current reward after update: -1925.92, Optimal reward -817.05
Iteration 55 took 2.35 seconds (mean sampled reward: -5135.92). Current reward after update: -1019.74, Optimal reward -817.05
Iteration 56 took 2.51 seconds (mean sampled reward: -5932.16). Current reward after update: -1618.62, Optimal reward -817.05
Iteration 57 took 2.34 seconds (mean sampled reward: -6559.51). Current reward after update: -1081.11, Optimal reward -817.05
Iteration 58 took 2.59 seconds (mean sampled reward: -6121.78). Current reward after update: -964.79, Optimal reward -817.05
Iteration 59 took 2.35 seconds (mean sampled reward: -5433.75). Current reward after update: -717.78, Optimal reward -717.78
Iteration 60 took 2.43 seconds (mean sampled reward: -5654.58). Current reward after update: -995.91, Optimal reward -717.78
Iteration 61 took 2.64 seconds (mean sampled reward: -4816.37). Current reward after update: -1692.90, Optimal reward -717.78
Iteration 62 took 2.37 seconds (mean sampled reward: -3859.68). Current reward after update: -1428.95, Optimal reward -717.78
Iteration 63 took 2.36 seconds (mean sampled reward: -2798.86). Current reward after update: -902.83, Optimal reward -717.78
Iteration 64 took 2.47 seconds (mean sampled reward: -2701.32). Current reward after update: -780.70, Optimal reward -717.78
Iteration 65 took 2.36 seconds (mean sampled reward: -3705.13). Current reward after update: -835.69, Optimal reward -717.78
Iteration 66 took 2.28 seconds (mean sampled reward: -3252.33). Current reward after update: -751.02, Optimal reward -717.78
Iteration 67 took 2.30 seconds (mean sampled reward: -3192.98). Current reward after update: -854.33, Optimal reward -717.78
Iteration 68 took 2.43 seconds (mean sampled reward: -2949.13). Current reward after update: -848.87, Optimal reward -717.78
Iteration 69 took 2.38 seconds (mean sampled reward: -2751.52). Current reward after update: -859.79, Optimal reward -717.78
Iteration 70 took 2.32 seconds (mean sampled reward: -4735.19). Current reward after update: -1065.48, Optimal reward -717.78
Iteration 71 took 2.30 seconds (mean sampled reward: -5001.21). Current reward after update: -845.97, Optimal reward -717.78
Iteration 72 took 2.30 seconds (mean sampled reward: -3230.54). Current reward after update: -919.91, Optimal reward -717.78
Iteration 73 took 2.33 seconds (mean sampled reward: -2651.01). Current reward after update: -851.00, Optimal reward -717.78
Iteration 74 took 2.28 seconds (mean sampled reward: -3140.71). Current reward after update: -7089.97, Optimal reward -717.78
Iteration 75 took 2.35 seconds (mean sampled reward: -5027.07). Current reward after update: -987.63, Optimal reward -717.78
Iteration 76 took 2.26 seconds (mean sampled reward: -5023.62). Current reward after update: -785.54, Optimal reward -717.78
Iteration 77 took 2.33 seconds (mean sampled reward: -4379.17). Current reward after update: -1987.47, Optimal reward -717.78
Iteration 78 took 2.37 seconds (mean sampled reward: -5713.35). Current reward after update: -923.78, Optimal reward -717.78
Iteration 79 took 2.35 seconds (mean sampled reward: -5323.68). Current reward after update: -1080.46, Optimal reward -717.78
Iteration 80 took 2.34 seconds (mean sampled reward: -5454.19). Current reward after update: -1019.62, Optimal reward -717.78
Iteration 81 took 2.30 seconds (mean sampled reward: -3660.36). Current reward after update: -2949.29, Optimal reward -717.78
Iteration 82 took 2.38 seconds (mean sampled reward: -5882.50). Current reward after update: -874.11, Optimal reward -717.78
Iteration 83 took 2.31 seconds (mean sampled reward: -5327.99). Current reward after update: -1413.96, Optimal reward -717.78
Iteration 84 took 2.30 seconds (mean sampled reward: -5793.24). Current reward after update: -982.70, Optimal reward -717.78
Iteration 85 took 2.38 seconds (mean sampled reward: -3917.08). Current reward after update: -701.69, Optimal reward -701.69
Iteration 86 took 2.35 seconds (mean sampled reward: -4940.65). Current reward after update: -858.63, Optimal reward -701.69
Iteration 87 took 2.30 seconds (mean sampled reward: -5626.05). Current reward after update: -867.27, Optimal reward -701.69
Iteration 88 took 2.27 seconds (mean sampled reward: -5607.34). Current reward after update: -3444.24, Optimal reward -701.69
Iteration 89 took 2.32 seconds (mean sampled reward: -4852.76). Current reward after update: -4159.84, Optimal reward -701.69
Iteration 90 took 2.34 seconds (mean sampled reward: -5002.26). Current reward after update: -1135.17, Optimal reward -701.69
Iteration 91 took 2.52 seconds (mean sampled reward: -6489.84). Current reward after update: -1142.91, Optimal reward -701.69
Iteration 92 took 2.29 seconds (mean sampled reward: -5161.40). Current reward after update: -4619.41, Optimal reward -701.69
Iteration 93 took 2.38 seconds (mean sampled reward: -4432.87). Current reward after update: -889.59, Optimal reward -701.69
Iteration 94 took 2.38 seconds (mean sampled reward: -3777.67). Current reward after update: -918.57, Optimal reward -701.69
Iteration 95 took 2.31 seconds (mean sampled reward: -4824.93). Current reward after update: -866.26, Optimal reward -701.69
Iteration 96 took 2.33 seconds (mean sampled reward: -4562.90). Current reward after update: -888.53, Optimal reward -701.69
Iteration 97 took 2.31 seconds (mean sampled reward: -4072.94). Current reward after update: -1734.44, Optimal reward -701.69
Iteration 98 took 2.41 seconds (mean sampled reward: -4177.40). Current reward after update: -1011.18, Optimal reward -701.69
Iteration 99 took 2.32 seconds (mean sampled reward: -4131.15). Current reward after update: -893.96, Optimal reward -701.69
Iteration 100 took 2.36 seconds (mean sampled reward: -3329.58). Current reward after update: -2938.96, Optimal reward -701.69
Iteration 101 took 2.35 seconds (mean sampled reward: -3200.91). Current reward after update: -803.19, Optimal reward -701.69
Iteration 102 took 2.36 seconds (mean sampled reward: -4081.09). Current reward after update: -2974.35, Optimal reward -701.69
Iteration 103 took 2.33 seconds (mean sampled reward: -4038.74). Current reward after update: -2519.08, Optimal reward -701.69
Iteration 104 took 2.27 seconds (mean sampled reward: -4252.04). Current reward after update: -922.22, Optimal reward -701.69
Iteration 105 took 2.30 seconds (mean sampled reward: -3922.24). Current reward after update: -1186.68, Optimal reward -701.69
Iteration 106 took 2.40 seconds (mean sampled reward: -3414.30). Current reward after update: -835.59, Optimal reward -701.69
Iteration 107 took 2.59 seconds (mean sampled reward: -4716.73). Current reward after update: -842.95, Optimal reward -701.69
Iteration 108 took 2.41 seconds (mean sampled reward: -3939.04). Current reward after update: -2965.91, Optimal reward -701.69
Iteration 109 took 2.45 seconds (mean sampled reward: -4399.12). Current reward after update: -969.98, Optimal reward -701.69
Iteration 110 took 2.39 seconds (mean sampled reward: -5421.92). Current reward after update: -791.20, Optimal reward -701.69
Iteration 111 took 2.41 seconds (mean sampled reward: -4204.52). Current reward after update: -762.71, Optimal reward -701.69
Iteration 112 took 2.47 seconds (mean sampled reward: -6103.15). Current reward after update: -961.65, Optimal reward -701.69
Iteration 113 took 2.46 seconds (mean sampled reward: -6364.09). Current reward after update: -951.49, Optimal reward -701.69
Iteration 114 took 2.36 seconds (mean sampled reward: -5229.30). Current reward after update: -1252.27, Optimal reward -701.69
Iteration 115 took 2.33 seconds (mean sampled reward: -5188.96). Current reward after update: -988.29, Optimal reward -701.69
Iteration 116 took 2.50 seconds (mean sampled reward: -5687.55). Current reward after update: -924.56, Optimal reward -701.69
Iteration 117 took 2.38 seconds (mean sampled reward: -5630.18). Current reward after update: -2137.48, Optimal reward -701.69
Iteration 118 took 2.28 seconds (mean sampled reward: -4350.86). Current reward after update: -888.83, Optimal reward -701.69
Iteration 119 took 2.33 seconds (mean sampled reward: -4343.93). Current reward after update: -990.22, Optimal reward -701.69
Iteration 120 took 2.30 seconds (mean sampled reward: -3095.00). Current reward after update: -867.45, Optimal reward -701.69
Iteration 121 took 2.34 seconds (mean sampled reward: -3320.45). Current reward after update: -1692.96, Optimal reward -701.69
Iteration 122 took 2.24 seconds (mean sampled reward: -3774.76). Current reward after update: -862.95, Optimal reward -701.69
Iteration 123 took 2.28 seconds (mean sampled reward: -5104.10). Current reward after update: -832.18, Optimal reward -701.69
Iteration 124 took 2.44 seconds (mean sampled reward: -5849.96). Current reward after update: -846.89, Optimal reward -701.69
Iteration 125 took 2.35 seconds (mean sampled reward: -4681.01). Current reward after update: -818.66, Optimal reward -701.69
Iteration 126 took 2.33 seconds (mean sampled reward: -5350.12). Current reward after update: -797.03, Optimal reward -701.69
Iteration 127 took 2.29 seconds (mean sampled reward: -5447.43). Current reward after update: -1321.26, Optimal reward -701.69
Iteration 128 took 2.31 seconds (mean sampled reward: -5686.47). Current reward after update: -901.49, Optimal reward -701.69
Iteration 129 took 2.35 seconds (mean sampled reward: -4131.32). Current reward after update: -1451.19, Optimal reward -701.69
Iteration 130 took 2.35 seconds (mean sampled reward: -5144.22). Current reward after update: -884.61, Optimal reward -701.69
Iteration 131 took 2.37 seconds (mean sampled reward: -5403.92). Current reward after update: -836.37, Optimal reward -701.69
Iteration 132 took 2.31 seconds (mean sampled reward: -5852.96). Current reward after update: -884.95, Optimal reward -701.69
Iteration 133 took 2.29 seconds (mean sampled reward: -4973.14). Current reward after update: -889.50, Optimal reward -701.69
Iteration 134 took 2.29 seconds (mean sampled reward: -6301.45). Current reward after update: -773.17, Optimal reward -701.69
Iteration 135 took 2.27 seconds (mean sampled reward: -4905.57). Current reward after update: -2254.11, Optimal reward -701.69
Iteration 136 took 2.32 seconds (mean sampled reward: -5202.26). Current reward after update: -817.99, Optimal reward -701.69
Iteration 137 took 2.41 seconds (mean sampled reward: -5744.98). Current reward after update: -743.27, Optimal reward -701.69
Iteration 138 took 2.39 seconds (mean sampled reward: -6217.32). Current reward after update: -902.28, Optimal reward -701.69
Iteration 139 took 2.41 seconds (mean sampled reward: -6021.71). Current reward after update: -1024.29, Optimal reward -701.69
Iteration 140 took 2.31 seconds (mean sampled reward: -5086.87). Current reward after update: -838.22, Optimal reward -701.69
Iteration 141 took 2.33 seconds (mean sampled reward: -5911.48). Current reward after update: -1121.36, Optimal reward -701.69
Iteration 142 took 2.33 seconds (mean sampled reward: -6179.36). Current reward after update: -1076.46, Optimal reward -701.69
Iteration 143 took 2.46 seconds (mean sampled reward: -6764.71). Current reward after update: -987.16, Optimal reward -701.69
Iteration 144 took 2.34 seconds (mean sampled reward: -4617.99). Current reward after update: -944.40, Optimal reward -701.69
Iteration 145 took 2.26 seconds (mean sampled reward: -5219.79). Current reward after update: -1237.13, Optimal reward -701.69
Iteration 146 took 2.33 seconds (mean sampled reward: -5515.94). Current reward after update: -1697.08, Optimal reward -701.69
Iteration 147 took 2.30 seconds (mean sampled reward: -4260.66). Current reward after update: -1209.04, Optimal reward -701.69
Iteration 148 took 2.35 seconds (mean sampled reward: -3768.83). Current reward after update: -1176.45, Optimal reward -701.69
Iteration 149 took 2.34 seconds (mean sampled reward: -4938.05). Current reward after update: -1324.71, Optimal reward -701.69
Iteration 150 took 2.38 seconds (mean sampled reward: -6008.64). Current reward after update: -1479.50, Optimal reward -701.69
Iteration 151 took 2.35 seconds (mean sampled reward: -5647.68). Current reward after update: -1011.96, Optimal reward -701.69
Iteration 152 took 2.39 seconds (mean sampled reward: -5949.54). Current reward after update: -718.20, Optimal reward -701.69
Iteration 153 took 2.34 seconds (mean sampled reward: -5440.29). Current reward after update: -843.93, Optimal reward -701.69
Iteration 154 took 2.36 seconds (mean sampled reward: -3471.82). Current reward after update: -868.25, Optimal reward -701.69
Iteration 155 took 2.36 seconds (mean sampled reward: -5507.39). Current reward after update: -949.66, Optimal reward -701.69
Iteration 156 took 2.35 seconds (mean sampled reward: -4873.08). Current reward after update: -1130.43, Optimal reward -701.69
Iteration 157 took 2.31 seconds (mean sampled reward: -3099.81). Current reward after update: -949.22, Optimal reward -701.69
Iteration 158 took 2.34 seconds (mean sampled reward: -3357.65). Current reward after update: -1282.99, Optimal reward -701.69
Iteration 159 took 2.32 seconds (mean sampled reward: -3460.81). Current reward after update: -824.14, Optimal reward -701.69
Iteration 160 took 2.38 seconds (mean sampled reward: -3263.04). Current reward after update: -1539.70, Optimal reward -701.69
Iteration 161 took 2.35 seconds (mean sampled reward: -2503.34). Current reward after update: -1168.29, Optimal reward -701.69
Iteration 162 took 2.38 seconds (mean sampled reward: -3136.58). Current reward after update: -928.08, Optimal reward -701.69
Iteration 163 took 2.29 seconds (mean sampled reward: -3086.80). Current reward after update: -1030.14, Optimal reward -701.69
Iteration 164 took 2.31 seconds (mean sampled reward: -2878.05). Current reward after update: -925.65, Optimal reward -701.69
Iteration 165 took 2.36 seconds (mean sampled reward: -3066.32). Current reward after update: -1821.26, Optimal reward -701.69
Iteration 166 took 2.34 seconds (mean sampled reward: -3948.04). Current reward after update: -798.29, Optimal reward -701.69
Iteration 167 took 2.33 seconds (mean sampled reward: -3172.99). Current reward after update: -760.13, Optimal reward -701.69
Iteration 168 took 2.35 seconds (mean sampled reward: -2896.98). Current reward after update: -919.55, Optimal reward -701.69
Iteration 169 took 2.25 seconds (mean sampled reward: -2417.25). Current reward after update: -887.11, Optimal reward -701.69
Iteration 170 took 2.32 seconds (mean sampled reward: -2627.56). Current reward after update: -782.86, Optimal reward -701.69
Iteration 171 took 2.26 seconds (mean sampled reward: -2351.20). Current reward after update: -3139.02, Optimal reward -701.69
Iteration 172 took 2.25 seconds (mean sampled reward: -3066.55). Current reward after update: -1698.59, Optimal reward -701.69
Iteration 173 took 2.33 seconds (mean sampled reward: -3327.89). Current reward after update: -2236.12, Optimal reward -701.69
Iteration 174 took 2.32 seconds (mean sampled reward: -4978.87). Current reward after update: -792.96, Optimal reward -701.69
Iteration 175 took 2.28 seconds (mean sampled reward: -4537.52). Current reward after update: -716.53, Optimal reward -701.69
Iteration 176 took 2.30 seconds (mean sampled reward: -5127.82). Current reward after update: -980.37, Optimal reward -701.69
Iteration 177 took 2.22 seconds (mean sampled reward: -3520.92). Current reward after update: -1852.18, Optimal reward -701.69
Iteration 178 took 2.28 seconds (mean sampled reward: -4370.08). Current reward after update: -936.63, Optimal reward -701.69
Iteration 179 took 2.28 seconds (mean sampled reward: -2940.95). Current reward after update: -1033.26, Optimal reward -701.69
Iteration 180 took 2.25 seconds (mean sampled reward: -3731.76). Current reward after update: -865.55, Optimal reward -701.69
Iteration 181 took 2.21 seconds (mean sampled reward: -3170.79). Current reward after update: -1002.75, Optimal reward -701.69
Iteration 182 took 2.21 seconds (mean sampled reward: -2736.15). Current reward after update: -1784.52, Optimal reward -701.69
Iteration 183 took 2.25 seconds (mean sampled reward: -2906.62). Current reward after update: -922.57, Optimal reward -701.69
Iteration 184 took 2.21 seconds (mean sampled reward: -2988.06). Current reward after update: -900.93, Optimal reward -701.69
Iteration 185 took 2.18 seconds (mean sampled reward: -2589.58). Current reward after update: -892.16, Optimal reward -701.69
Iteration 186 took 2.21 seconds (mean sampled reward: -2931.71). Current reward after update: -972.25, Optimal reward -701.69
Iteration 187 took 2.22 seconds (mean sampled reward: -2619.16). Current reward after update: -1857.10, Optimal reward -701.69
Iteration 188 took 2.21 seconds (mean sampled reward: -2694.54). Current reward after update: -995.50, Optimal reward -701.69
Iteration 189 took 2.22 seconds (mean sampled reward: -3020.15). Current reward after update: -952.65, Optimal reward -701.69
Iteration 190 took 2.23 seconds (mean sampled reward: -2639.48). Current reward after update: -890.93, Optimal reward -701.69
Iteration 191 took 2.21 seconds (mean sampled reward: -2558.09). Current reward after update: -975.12, Optimal reward -701.69
Iteration 192 took 2.41 seconds (mean sampled reward: -3089.92). Current reward after update: -857.84, Optimal reward -701.69
Iteration 193 took 2.19 seconds (mean sampled reward: -2584.90). Current reward after update: -1528.65, Optimal reward -701.69
Iteration 194 took 2.25 seconds (mean sampled reward: -3035.85). Current reward after update: -892.38, Optimal reward -701.69
Iteration 195 took 2.17 seconds (mean sampled reward: -2589.82). Current reward after update: -908.19, Optimal reward -701.69
Iteration 196 took 2.23 seconds (mean sampled reward: -2934.67). Current reward after update: -876.68, Optimal reward -701.69
Iteration 197 took 2.26 seconds (mean sampled reward: -2867.25). Current reward after update: -801.87, Optimal reward -701.69
Iteration 198 took 2.25 seconds (mean sampled reward: -2967.04). Current reward after update: -876.44, Optimal reward -701.69
Iteration 199 took 2.27 seconds (mean sampled reward: -3631.72). Current reward after update: -827.88, Optimal reward -701.69
Iteration 200 took 2.27 seconds (mean sampled reward: -3867.31). Current reward after update: -839.45, Optimal reward -701.69
Iteration 1 took 2.36 seconds (mean sampled reward: -7631.08). Current reward after update: -6615.16, Optimal reward -6615.16
Iteration 2 took 2.27 seconds (mean sampled reward: -7317.73). Current reward after update: -6254.75, Optimal reward -6254.75
Iteration 3 took 2.31 seconds (mean sampled reward: -7215.42). Current reward after update: -5559.06, Optimal reward -5559.06
Iteration 4 took 2.34 seconds (mean sampled reward: -7225.63). Current reward after update: -5326.06, Optimal reward -5326.06
Iteration 5 took 2.45 seconds (mean sampled reward: -6657.02). Current reward after update: -4772.26, Optimal reward -4772.26
Iteration 6 took 2.25 seconds (mean sampled reward: -7335.41). Current reward after update: -4418.48, Optimal reward -4418.48
Iteration 7 took 2.35 seconds (mean sampled reward: -6832.52). Current reward after update: -3911.77, Optimal reward -3911.77
Iteration 8 took 2.22 seconds (mean sampled reward: -6823.83). Current reward after update: -2589.17, Optimal reward -2589.17
Iteration 9 took 2.36 seconds (mean sampled reward: -5969.10). Current reward after update: -1638.78, Optimal reward -1638.78
Iteration 10 took 2.26 seconds (mean sampled reward: -5984.79). Current reward after update: -1949.22, Optimal reward -1638.78
Iteration 11 took 2.30 seconds (mean sampled reward: -6198.18). Current reward after update: -1911.82, Optimal reward -1638.78
Iteration 12 took 2.29 seconds (mean sampled reward: -5544.24). Current reward after update: -1898.68, Optimal reward -1638.78
Iteration 13 took 2.15 seconds (mean sampled reward: -5013.82). Current reward after update: -1703.28, Optimal reward -1638.78
Iteration 14 took 2.23 seconds (mean sampled reward: -5748.19). Current reward after update: -1930.76, Optimal reward -1638.78
Iteration 15 took 2.28 seconds (mean sampled reward: -5711.96). Current reward after update: -3843.04, Optimal reward -1638.78
Iteration 16 took 2.23 seconds (mean sampled reward: -5960.94). Current reward after update: -2119.70, Optimal reward -1638.78
Iteration 17 took 2.22 seconds (mean sampled reward: -5411.81). Current reward after update: -1621.05, Optimal reward -1621.05
Iteration 18 took 2.31 seconds (mean sampled reward: -5890.80). Current reward after update: -1504.17, Optimal reward -1504.17
Iteration 19 took 2.30 seconds (mean sampled reward: -5488.55). Current reward after update: -1554.02, Optimal reward -1504.17
Iteration 20 took 2.47 seconds (mean sampled reward: -6267.44). Current reward after update: -1870.28, Optimal reward -1504.17
Iteration 21 took 2.33 seconds (mean sampled reward: -5898.34). Current reward after update: -1966.04, Optimal reward -1504.17
Iteration 22 took 2.22 seconds (mean sampled reward: -5828.35). Current reward after update: -6733.56, Optimal reward -1504.17
Iteration 23 took 2.22 seconds (mean sampled reward: -5838.93). Current reward after update: -1727.74, Optimal reward -1504.17
Iteration 24 took 2.20 seconds (mean sampled reward: -4940.60). Current reward after update: -1932.88, Optimal reward -1504.17
Iteration 25 took 2.23 seconds (mean sampled reward: -4744.49). Current reward after update: -1712.18, Optimal reward -1504.17
Iteration 26 took 2.18 seconds (mean sampled reward: -5018.81). Current reward after update: -3881.31, Optimal reward -1504.17
Iteration 27 took 2.20 seconds (mean sampled reward: -5064.37). Current reward after update: -2737.96, Optimal reward -1504.17
Iteration 28 took 2.38 seconds (mean sampled reward: -6071.18). Current reward after update: -1698.38, Optimal reward -1504.17
Iteration 29 took 2.33 seconds (mean sampled reward: -5917.75). Current reward after update: -1698.68, Optimal reward -1504.17
Iteration 30 took 2.15 seconds (mean sampled reward: -6042.10). Current reward after update: -1911.79, Optimal reward -1504.17
Iteration 31 took 2.25 seconds (mean sampled reward: -4817.22). Current reward after update: -1512.81, Optimal reward -1504.17
Iteration 32 took 2.26 seconds (mean sampled reward: -4624.74). Current reward after update: -1524.51, Optimal reward -1504.17
Iteration 33 took 2.32 seconds (mean sampled reward: -3980.02). Current reward after update: -3413.79, Optimal reward -1504.17
Iteration 34 took 2.22 seconds (mean sampled reward: -4735.77). Current reward after update: -1500.78, Optimal reward -1500.78
Iteration 35 took 2.23 seconds (mean sampled reward: -4153.88). Current reward after update: -4838.78, Optimal reward -1500.78
Iteration 36 took 2.25 seconds (mean sampled reward: -4082.12). Current reward after update: -1638.34, Optimal reward -1500.78
Iteration 37 took 2.30 seconds (mean sampled reward: -5375.98). Current reward after update: -1780.10, Optimal reward -1500.78
Iteration 38 took 2.24 seconds (mean sampled reward: -4205.72). Current reward after update: -1573.01, Optimal reward -1500.78
Iteration 39 took 2.25 seconds (mean sampled reward: -5084.95). Current reward after update: -1420.32, Optimal reward -1420.32
Iteration 40 took 2.35 seconds (mean sampled reward: -5690.99). Current reward after update: -1599.42, Optimal reward -1420.32
Iteration 41 took 2.31 seconds (mean sampled reward: -5567.09). Current reward after update: -1589.75, Optimal reward -1420.32
Iteration 42 took 2.24 seconds (mean sampled reward: -4760.42). Current reward after update: -1397.24, Optimal reward -1397.24
Iteration 43 took 2.22 seconds (mean sampled reward: -5392.47). Current reward after update: -1436.60, Optimal reward -1397.24
Iteration 44 took 2.30 seconds (mean sampled reward: -5627.51). Current reward after update: -1783.90, Optimal reward -1397.24
Iteration 45 took 2.28 seconds (mean sampled reward: -5653.19). Current reward after update: -1641.63, Optimal reward -1397.24
Iteration 46 took 2.26 seconds (mean sampled reward: -5598.45). Current reward after update: -1245.27, Optimal reward -1245.27
Iteration 47 took 2.26 seconds (mean sampled reward: -4718.19). Current reward after update: -1279.36, Optimal reward -1245.27
Iteration 48 took 2.21 seconds (mean sampled reward: -4512.97). Current reward after update: -1356.93, Optimal reward -1245.27
Iteration 49 took 2.23 seconds (mean sampled reward: -4731.68). Current reward after update: -1383.92, Optimal reward -1245.27
Iteration 50 took 2.25 seconds (mean sampled reward: -4348.20). Current reward after update: -1529.49, Optimal reward -1245.27
Iteration 51 took 2.35 seconds (mean sampled reward: -4179.30). Current reward after update: -1095.48, Optimal reward -1095.48
Iteration 52 took 2.26 seconds (mean sampled reward: -3714.83). Current reward after update: -1240.24, Optimal reward -1095.48
Iteration 53 took 2.33 seconds (mean sampled reward: -3180.15). Current reward after update: -1189.74, Optimal reward -1095.48
Iteration 54 took 2.25 seconds (mean sampled reward: -3351.11). Current reward after update: -1028.46, Optimal reward -1028.46
Iteration 55 took 2.37 seconds (mean sampled reward: -3775.36). Current reward after update: -1282.03, Optimal reward -1028.46
Iteration 56 took 2.41 seconds (mean sampled reward: -3845.97). Current reward after update: -1035.48, Optimal reward -1028.46
Iteration 57 took 2.26 seconds (mean sampled reward: -3006.79). Current reward after update: -997.50, Optimal reward -997.50
Iteration 58 took 2.25 seconds (mean sampled reward: -3417.92). Current reward after update: -1229.62, Optimal reward -997.50
Iteration 59 took 2.27 seconds (mean sampled reward: -4686.69). Current reward after update: -1065.34, Optimal reward -997.50
Iteration 60 took 2.25 seconds (mean sampled reward: -4407.14). Current reward after update: -1151.91, Optimal reward -997.50
Iteration 61 took 2.31 seconds (mean sampled reward: -4006.96). Current reward after update: -1040.14, Optimal reward -997.50
Iteration 62 took 2.33 seconds (mean sampled reward: -4166.46). Current reward after update: -1165.00, Optimal reward -997.50
Iteration 63 took 2.36 seconds (mean sampled reward: -4917.50). Current reward after update: -1265.47, Optimal reward -997.50
Iteration 64 took 2.33 seconds (mean sampled reward: -4448.98). Current reward after update: -1327.53, Optimal reward -997.50
Iteration 65 took 2.26 seconds (mean sampled reward: -5096.34). Current reward after update: -1931.53, Optimal reward -997.50
Iteration 66 took 2.33 seconds (mean sampled reward: -5252.48). Current reward after update: -1601.09, Optimal reward -997.50
Iteration 67 took 2.27 seconds (mean sampled reward: -5589.20). Current reward after update: -1408.93, Optimal reward -997.50
Iteration 68 took 2.25 seconds (mean sampled reward: -5540.64). Current reward after update: -2671.30, Optimal reward -997.50
Iteration 69 took 2.34 seconds (mean sampled reward: -3227.74). Current reward after update: -1081.26, Optimal reward -997.50
Iteration 70 took 2.29 seconds (mean sampled reward: -4980.22). Current reward after update: -1059.29, Optimal reward -997.50
Iteration 71 took 2.26 seconds (mean sampled reward: -3566.18). Current reward after update: -1117.44, Optimal reward -997.50
Iteration 72 took 2.33 seconds (mean sampled reward: -4482.92). Current reward after update: -1611.57, Optimal reward -997.50
Iteration 73 took 2.25 seconds (mean sampled reward: -3615.52). Current reward after update: -1544.55, Optimal reward -997.50
Iteration 74 took 2.27 seconds (mean sampled reward: -3647.12). Current reward after update: -1681.82, Optimal reward -997.50
Iteration 75 took 2.27 seconds (mean sampled reward: -3446.42). Current reward after update: -1142.49, Optimal reward -997.50
Iteration 76 took 2.31 seconds (mean sampled reward: -3134.10). Current reward after update: -1176.21, Optimal reward -997.50
Iteration 77 took 2.19 seconds (mean sampled reward: -2890.30). Current reward after update: -1254.26, Optimal reward -997.50
Iteration 78 took 2.19 seconds (mean sampled reward: -2825.33). Current reward after update: -1106.52, Optimal reward -997.50
Iteration 79 took 2.22 seconds (mean sampled reward: -3129.99). Current reward after update: -1077.25, Optimal reward -997.50
Iteration 80 took 2.30 seconds (mean sampled reward: -3201.67). Current reward after update: -1080.88, Optimal reward -997.50
Iteration 81 took 2.20 seconds (mean sampled reward: -3058.71). Current reward after update: -3893.54, Optimal reward -997.50
Iteration 82 took 2.20 seconds (mean sampled reward: -2770.91). Current reward after update: -1151.13, Optimal reward -997.50
Iteration 83 took 2.23 seconds (mean sampled reward: -3195.15). Current reward after update: -1168.44, Optimal reward -997.50
Iteration 84 took 2.29 seconds (mean sampled reward: -3310.52). Current reward after update: -1926.64, Optimal reward -997.50
Iteration 85 took 2.43 seconds (mean sampled reward: -3464.96). Current reward after update: -1347.39, Optimal reward -997.50
Iteration 86 took 2.27 seconds (mean sampled reward: -4578.96). Current reward after update: -3711.29, Optimal reward -997.50
Iteration 87 took 2.29 seconds (mean sampled reward: -4746.90). Current reward after update: -1770.66, Optimal reward -997.50
Iteration 88 took 2.33 seconds (mean sampled reward: -4692.06). Current reward after update: -1271.85, Optimal reward -997.50
Iteration 89 took 2.39 seconds (mean sampled reward: -6345.11). Current reward after update: -1316.32, Optimal reward -997.50
Iteration 90 took 2.24 seconds (mean sampled reward: -5068.63). Current reward after update: -1306.15, Optimal reward -997.50
Iteration 91 took 2.27 seconds (mean sampled reward: -4402.69). Current reward after update: -1362.54, Optimal reward -997.50
Iteration 92 took 2.32 seconds (mean sampled reward: -3708.61). Current reward after update: -1355.80, Optimal reward -997.50
Iteration 93 took 2.27 seconds (mean sampled reward: -3975.98). Current reward after update: -2356.86, Optimal reward -997.50
Iteration 94 took 2.31 seconds (mean sampled reward: -4789.30). Current reward after update: -1326.09, Optimal reward -997.50
Iteration 95 took 2.28 seconds (mean sampled reward: -4340.92). Current reward after update: -1065.15, Optimal reward -997.50
Iteration 96 took 2.31 seconds (mean sampled reward: -4210.87). Current reward after update: -1110.36, Optimal reward -997.50
Iteration 97 took 2.34 seconds (mean sampled reward: -4720.65). Current reward after update: -1333.43, Optimal reward -997.50
Iteration 98 took 2.34 seconds (mean sampled reward: -5667.77). Current reward after update: -5210.66, Optimal reward -997.50
Iteration 99 took 2.36 seconds (mean sampled reward: -6135.67). Current reward after update: -1200.15, Optimal reward -997.50
Iteration 100 took 2.39 seconds (mean sampled reward: -4648.34). Current reward after update: -1091.95, Optimal reward -997.50
Iteration 101 took 2.33 seconds (mean sampled reward: -4055.66). Current reward after update: -1187.90, Optimal reward -997.50
Iteration 102 took 2.32 seconds (mean sampled reward: -4906.93). Current reward after update: -1056.79, Optimal reward -997.50
Iteration 103 took 2.36 seconds (mean sampled reward: -4114.06). Current reward after update: -985.77, Optimal reward -985.77
Iteration 104 took 2.29 seconds (mean sampled reward: -4966.93). Current reward after update: -1097.90, Optimal reward -985.77
Iteration 105 took 2.36 seconds (mean sampled reward: -5530.24). Current reward after update: -1127.27, Optimal reward -985.77
Iteration 106 took 2.37 seconds (mean sampled reward: -5767.65). Current reward after update: -1214.99, Optimal reward -985.77
Iteration 107 took 2.37 seconds (mean sampled reward: -5480.42). Current reward after update: -2890.15, Optimal reward -985.77
Iteration 108 took 2.60 seconds (mean sampled reward: -5703.09). Current reward after update: -1257.04, Optimal reward -985.77
Iteration 109 took 2.59 seconds (mean sampled reward: -5452.15). Current reward after update: -1442.48, Optimal reward -985.77
Iteration 110 took 2.44 seconds (mean sampled reward: -5615.57). Current reward after update: -1212.24, Optimal reward -985.77
Iteration 111 took 2.43 seconds (mean sampled reward: -3949.87). Current reward after update: -792.57, Optimal reward -792.57
Iteration 112 took 2.36 seconds (mean sampled reward: -3749.34). Current reward after update: -2319.36, Optimal reward -792.57
Iteration 113 took 2.62 seconds (mean sampled reward: -3753.10). Current reward after update: -1087.44, Optimal reward -792.57
Iteration 114 took 2.52 seconds (mean sampled reward: -4530.09). Current reward after update: -6017.90, Optimal reward -792.57
Iteration 115 took 2.60 seconds (mean sampled reward: -4380.45). Current reward after update: -1041.14, Optimal reward -792.57
Iteration 116 took 2.30 seconds (mean sampled reward: -5294.34). Current reward after update: -1082.49, Optimal reward -792.57
Iteration 117 took 2.62 seconds (mean sampled reward: -4447.61). Current reward after update: -6932.90, Optimal reward -792.57
Iteration 118 took 2.42 seconds (mean sampled reward: -4910.78). Current reward after update: -682.68, Optimal reward -682.68
Iteration 119 took 2.33 seconds (mean sampled reward: -5733.35). Current reward after update: -1192.21, Optimal reward -682.68
Iteration 120 took 2.29 seconds (mean sampled reward: -5111.71). Current reward after update: -884.76, Optimal reward -682.68
Iteration 121 took 2.26 seconds (mean sampled reward: -4517.19). Current reward after update: -1128.65, Optimal reward -682.68
Iteration 122 took 2.34 seconds (mean sampled reward: -6269.91). Current reward after update: -1114.82, Optimal reward -682.68
Iteration 123 took 2.28 seconds (mean sampled reward: -6121.88). Current reward after update: -1187.04, Optimal reward -682.68
Iteration 124 took 2.34 seconds (mean sampled reward: -6701.36). Current reward after update: -1212.52, Optimal reward -682.68
Iteration 125 took 2.41 seconds (mean sampled reward: -6529.65). Current reward after update: -1718.98, Optimal reward -682.68
Iteration 126 took 2.30 seconds (mean sampled reward: -6239.54). Current reward after update: -1655.00, Optimal reward -682.68
Iteration 127 took 2.31 seconds (mean sampled reward: -5965.48). Current reward after update: -1721.28, Optimal reward -682.68
Iteration 128 took 2.33 seconds (mean sampled reward: -5537.67). Current reward after update: -1660.29, Optimal reward -682.68
Iteration 129 took 2.35 seconds (mean sampled reward: -5437.41). Current reward after update: -1081.57, Optimal reward -682.68
Iteration 130 took 2.28 seconds (mean sampled reward: -4950.55). Current reward after update: -1457.38, Optimal reward -682.68
Iteration 131 took 2.34 seconds (mean sampled reward: -5407.87). Current reward after update: -2642.65, Optimal reward -682.68
Iteration 132 took 2.30 seconds (mean sampled reward: -5419.82). Current reward after update: -1226.62, Optimal reward -682.68
Iteration 133 took 2.35 seconds (mean sampled reward: -5231.03). Current reward after update: -1446.55, Optimal reward -682.68
Iteration 134 took 2.36 seconds (mean sampled reward: -5243.06). Current reward after update: -1347.79, Optimal reward -682.68
Iteration 135 took 2.40 seconds (mean sampled reward: -5433.72). Current reward after update: -1648.49, Optimal reward -682.68
Iteration 136 took 2.39 seconds (mean sampled reward: -5147.54). Current reward after update: -1429.41, Optimal reward -682.68
Iteration 137 took 2.41 seconds (mean sampled reward: -4955.38). Current reward after update: -1025.51, Optimal reward -682.68
Iteration 138 took 2.40 seconds (mean sampled reward: -5167.73). Current reward after update: -1502.17, Optimal reward -682.68
Iteration 139 took 2.41 seconds (mean sampled reward: -5045.08). Current reward after update: -1158.33, Optimal reward -682.68
Iteration 140 took 2.38 seconds (mean sampled reward: -5934.62). Current reward after update: -979.96, Optimal reward -682.68
Iteration 141 took 2.36 seconds (mean sampled reward: -5586.69). Current reward after update: -1161.04, Optimal reward -682.68
Iteration 142 took 2.32 seconds (mean sampled reward: -6499.85). Current reward after update: -1459.71, Optimal reward -682.68
Iteration 143 took 2.32 seconds (mean sampled reward: -6491.87). Current reward after update: -1639.76, Optimal reward -682.68
Iteration 144 took 2.39 seconds (mean sampled reward: -6043.53). Current reward after update: -1424.72, Optimal reward -682.68
Iteration 145 took 2.36 seconds (mean sampled reward: -6147.65). Current reward after update: -1832.98, Optimal reward -682.68
Iteration 146 took 2.39 seconds (mean sampled reward: -5486.54). Current reward after update: -1150.94, Optimal reward -682.68
Iteration 147 took 2.32 seconds (mean sampled reward: -5356.43). Current reward after update: -1188.57, Optimal reward -682.68
Iteration 148 took 2.34 seconds (mean sampled reward: -5536.04). Current reward after update: -1284.79, Optimal reward -682.68
Iteration 149 took 2.33 seconds (mean sampled reward: -5890.11). Current reward after update: -6973.09, Optimal reward -682.68
Iteration 150 took 2.32 seconds (mean sampled reward: -6295.31). Current reward after update: -1704.57, Optimal reward -682.68
Iteration 151 took 2.32 seconds (mean sampled reward: -5585.63). Current reward after update: -1618.03, Optimal reward -682.68
Iteration 152 took 2.27 seconds (mean sampled reward: -6081.19). Current reward after update: -1778.02, Optimal reward -682.68
Iteration 153 took 2.26 seconds (mean sampled reward: -5000.06). Current reward after update: -4232.69, Optimal reward -682.68
Iteration 154 took 2.30 seconds (mean sampled reward: -4995.39). Current reward after update: -1250.11, Optimal reward -682.68
Iteration 155 took 2.30 seconds (mean sampled reward: -5768.56). Current reward after update: -1369.05, Optimal reward -682.68
Iteration 156 took 2.29 seconds (mean sampled reward: -6362.23). Current reward after update: -1745.85, Optimal reward -682.68
Iteration 157 took 2.31 seconds (mean sampled reward: -5741.63). Current reward after update: -1389.21, Optimal reward -682.68
Iteration 158 took 2.31 seconds (mean sampled reward: -5604.14). Current reward after update: -1481.64, Optimal reward -682.68
Iteration 159 took 2.35 seconds (mean sampled reward: -6690.49). Current reward after update: -1362.22, Optimal reward -682.68
Iteration 160 took 2.31 seconds (mean sampled reward: -6273.71). Current reward after update: -1614.57, Optimal reward -682.68
Iteration 161 took 2.41 seconds (mean sampled reward: -6294.77). Current reward after update: -1408.27, Optimal reward -682.68
Iteration 162 took 2.37 seconds (mean sampled reward: -5751.55). Current reward after update: -1453.33, Optimal reward -682.68
Iteration 163 took 2.37 seconds (mean sampled reward: -4918.95). Current reward after update: -1133.76, Optimal reward -682.68
Iteration 164 took 2.34 seconds (mean sampled reward: -5078.63). Current reward after update: -1290.20, Optimal reward -682.68
Iteration 165 took 2.45 seconds (mean sampled reward: -4958.54). Current reward after update: -7042.50, Optimal reward -682.68
Iteration 166 took 2.46 seconds (mean sampled reward: -5312.89). Current reward after update: -851.64, Optimal reward -682.68
Iteration 167 took 2.47 seconds (mean sampled reward: -4406.23). Current reward after update: -1066.83, Optimal reward -682.68
Iteration 168 took 2.47 seconds (mean sampled reward: -4873.20). Current reward after update: -1854.23, Optimal reward -682.68
Iteration 169 took 2.39 seconds (mean sampled reward: -4729.15). Current reward after update: -1221.17, Optimal reward -682.68
Iteration 170 took 2.39 seconds (mean sampled reward: -4662.74). Current reward after update: -1219.39, Optimal reward -682.68
Iteration 171 took 2.40 seconds (mean sampled reward: -4723.20). Current reward after update: -1451.39, Optimal reward -682.68
Iteration 172 took 2.39 seconds (mean sampled reward: -5214.13). Current reward after update: -1525.90, Optimal reward -682.68
Iteration 173 took 2.36 seconds (mean sampled reward: -5403.00). Current reward after update: -1942.02, Optimal reward -682.68
Iteration 174 took 2.49 seconds (mean sampled reward: -6185.68). Current reward after update: -2309.50, Optimal reward -682.68
Iteration 175 took 2.48 seconds (mean sampled reward: -6317.10). Current reward after update: -1745.23, Optimal reward -682.68
Iteration 176 took 2.36 seconds (mean sampled reward: -5474.32). Current reward after update: -1744.72, Optimal reward -682.68
Iteration 177 took 2.40 seconds (mean sampled reward: -4867.55). Current reward after update: -1342.90, Optimal reward -682.68
Iteration 178 took 2.42 seconds (mean sampled reward: -5135.44). Current reward after update: -1677.24, Optimal reward -682.68
Iteration 179 took 2.47 seconds (mean sampled reward: -4789.34). Current reward after update: -1375.65, Optimal reward -682.68
Iteration 180 took 2.43 seconds (mean sampled reward: -6098.45). Current reward after update: -1491.54, Optimal reward -682.68
Iteration 181 took 2.44 seconds (mean sampled reward: -6411.47). Current reward after update: -4414.90, Optimal reward -682.68
Iteration 182 took 2.41 seconds (mean sampled reward: -5585.45). Current reward after update: -2073.61, Optimal reward -682.68
Iteration 183 took 2.45 seconds (mean sampled reward: -5289.97). Current reward after update: -1446.39, Optimal reward -682.68
Iteration 184 took 2.38 seconds (mean sampled reward: -5006.44). Current reward after update: -1438.17, Optimal reward -682.68
Iteration 185 took 2.50 seconds (mean sampled reward: -4918.92). Current reward after update: -1672.61, Optimal reward -682.68
Iteration 186 took 2.36 seconds (mean sampled reward: -5214.11). Current reward after update: -1610.31, Optimal reward -682.68
Iteration 187 took 2.43 seconds (mean sampled reward: -4557.55). Current reward after update: -1397.49, Optimal reward -682.68
Iteration 188 took 2.46 seconds (mean sampled reward: -4519.72). Current reward after update: -1311.81, Optimal reward -682.68
Iteration 189 took 2.40 seconds (mean sampled reward: -4107.89). Current reward after update: -1219.47, Optimal reward -682.68
Iteration 190 took 2.41 seconds (mean sampled reward: -4117.12). Current reward after update: -1329.83, Optimal reward -682.68
Iteration 191 took 2.42 seconds (mean sampled reward: -4142.70). Current reward after update: -1251.68, Optimal reward -682.68
Iteration 192 took 2.39 seconds (mean sampled reward: -4175.46). Current reward after update: -1382.52, Optimal reward -682.68
Iteration 193 took 2.45 seconds (mean sampled reward: -4327.96). Current reward after update: -1269.73, Optimal reward -682.68
Iteration 194 took 2.43 seconds (mean sampled reward: -4198.77). Current reward after update: -1202.99, Optimal reward -682.68
Iteration 195 took 2.42 seconds (mean sampled reward: -4952.33). Current reward after update: -1154.80, Optimal reward -682.68
Iteration 196 took 2.41 seconds (mean sampled reward: -4860.12). Current reward after update: -1298.21, Optimal reward -682.68
Iteration 197 took 2.42 seconds (mean sampled reward: -4653.71). Current reward after update: -1324.97, Optimal reward -682.68
Iteration 198 took 2.44 seconds (mean sampled reward: -5099.25). Current reward after update: -1767.36, Optimal reward -682.68
Iteration 199 took 2.38 seconds (mean sampled reward: -4444.49). Current reward after update: -1148.95, Optimal reward -682.68
Iteration 200 took 2.31 seconds (mean sampled reward: -4393.96). Current reward after update: -1783.86, Optimal reward -682.68
Iteration 1 took 2.35 seconds (mean sampled reward: -7645.98). Current reward after update: -6761.62, Optimal reward -6761.62
Iteration 2 took 2.34 seconds (mean sampled reward: -7525.92). Current reward after update: -4415.87, Optimal reward -4415.87
Iteration 3 took 2.37 seconds (mean sampled reward: -7035.98). Current reward after update: -3067.93, Optimal reward -3067.93
Iteration 4 took 2.25 seconds (mean sampled reward: -6122.20). Current reward after update: -3323.24, Optimal reward -3067.93
Iteration 5 took 2.22 seconds (mean sampled reward: -5934.93). Current reward after update: -2922.58, Optimal reward -2922.58
Iteration 6 took 2.26 seconds (mean sampled reward: -6386.11). Current reward after update: -2584.49, Optimal reward -2584.49
Iteration 7 took 2.22 seconds (mean sampled reward: -6744.84). Current reward after update: -2490.14, Optimal reward -2490.14
Iteration 8 took 2.27 seconds (mean sampled reward: -6479.92). Current reward after update: -1970.84, Optimal reward -1970.84
Iteration 9 took 2.18 seconds (mean sampled reward: -6916.20). Current reward after update: -2127.49, Optimal reward -1970.84
Iteration 10 took 2.33 seconds (mean sampled reward: -6203.77). Current reward after update: -1853.01, Optimal reward -1853.01
Iteration 11 took 2.23 seconds (mean sampled reward: -6020.56). Current reward after update: -1964.76, Optimal reward -1853.01
Iteration 12 took 2.26 seconds (mean sampled reward: -6088.34). Current reward after update: -2189.20, Optimal reward -1853.01
Iteration 13 took 2.57 seconds (mean sampled reward: -6862.97). Current reward after update: -2275.70, Optimal reward -1853.01
Iteration 14 took 2.29 seconds (mean sampled reward: -5416.68). Current reward after update: -2065.05, Optimal reward -1853.01
Iteration 15 took 2.20 seconds (mean sampled reward: -6086.49). Current reward after update: -1827.33, Optimal reward -1827.33
Iteration 16 took 2.38 seconds (mean sampled reward: -6326.31). Current reward after update: -2213.52, Optimal reward -1827.33
Iteration 17 took 2.37 seconds (mean sampled reward: -5624.56). Current reward after update: -2100.37, Optimal reward -1827.33
Iteration 18 took 2.45 seconds (mean sampled reward: -5317.50). Current reward after update: -2173.40, Optimal reward -1827.33
Iteration 19 took 2.29 seconds (mean sampled reward: -6104.15). Current reward after update: -1922.57, Optimal reward -1827.33
Iteration 20 took 2.18 seconds (mean sampled reward: -5960.82). Current reward after update: -1933.85, Optimal reward -1827.33
Iteration 21 took 2.15 seconds (mean sampled reward: -5275.35). Current reward after update: -6764.00, Optimal reward -1827.33
Iteration 22 took 2.33 seconds (mean sampled reward: -5393.93). Current reward after update: -1598.75, Optimal reward -1598.75
Iteration 23 took 2.22 seconds (mean sampled reward: -6212.99). Current reward after update: -2149.30, Optimal reward -1598.75
Iteration 24 took 2.33 seconds (mean sampled reward: -5923.33). Current reward after update: -2522.01, Optimal reward -1598.75
Iteration 25 took 2.23 seconds (mean sampled reward: -5716.94). Current reward after update: -1998.54, Optimal reward -1598.75
Iteration 26 took 2.38 seconds (mean sampled reward: -5398.41). Current reward after update: -2045.73, Optimal reward -1598.75
Iteration 27 took 2.17 seconds (mean sampled reward: -5387.71). Current reward after update: -1918.12, Optimal reward -1598.75
Iteration 28 took 2.12 seconds (mean sampled reward: -5497.97). Current reward after update: -1891.20, Optimal reward -1598.75
Iteration 29 took 2.28 seconds (mean sampled reward: -5203.64). Current reward after update: -1681.31, Optimal reward -1598.75
Iteration 30 took 2.28 seconds (mean sampled reward: -5406.36). Current reward after update: -1598.69, Optimal reward -1598.69
Iteration 31 took 2.26 seconds (mean sampled reward: -4253.98). Current reward after update: -1555.91, Optimal reward -1555.91
Iteration 32 took 2.35 seconds (mean sampled reward: -3899.54). Current reward after update: -1335.96, Optimal reward -1335.96
Iteration 33 took 2.25 seconds (mean sampled reward: -3922.43). Current reward after update: -1496.95, Optimal reward -1335.96
Iteration 34 took 2.24 seconds (mean sampled reward: -3568.52). Current reward after update: -1364.03, Optimal reward -1335.96
Iteration 35 took 2.22 seconds (mean sampled reward: -3476.33). Current reward after update: -1422.60, Optimal reward -1335.96
Iteration 36 took 2.27 seconds (mean sampled reward: -3865.46). Current reward after update: -1332.35, Optimal reward -1332.35
Iteration 37 took 2.35 seconds (mean sampled reward: -3806.67). Current reward after update: -1277.09, Optimal reward -1277.09
Iteration 38 took 2.45 seconds (mean sampled reward: -4640.77). Current reward after update: -1429.14, Optimal reward -1277.09
Iteration 39 took 2.35 seconds (mean sampled reward: -3874.84). Current reward after update: -1364.42, Optimal reward -1277.09
Iteration 40 took 2.36 seconds (mean sampled reward: -3602.18). Current reward after update: -1390.02, Optimal reward -1277.09
Iteration 41 took 2.32 seconds (mean sampled reward: -4121.40). Current reward after update: -1281.45, Optimal reward -1277.09
Iteration 42 took 2.35 seconds (mean sampled reward: -4090.13). Current reward after update: -1446.65, Optimal reward -1277.09
Iteration 43 took 2.35 seconds (mean sampled reward: -4633.94). Current reward after update: -1574.86, Optimal reward -1277.09
Iteration 44 took 2.34 seconds (mean sampled reward: -5202.91). Current reward after update: -1408.91, Optimal reward -1277.09
Iteration 45 took 2.41 seconds (mean sampled reward: -5479.16). Current reward after update: -1146.63, Optimal reward -1146.63
Iteration 46 took 2.47 seconds (mean sampled reward: -5151.00). Current reward after update: -1138.30, Optimal reward -1138.30
Iteration 47 took 2.40 seconds (mean sampled reward: -5089.62). Current reward after update: -957.67, Optimal reward -957.67
Iteration 48 took 2.42 seconds (mean sampled reward: -5762.18). Current reward after update: -997.89, Optimal reward -957.67
Iteration 49 took 2.40 seconds (mean sampled reward: -5312.07). Current reward after update: -877.45, Optimal reward -877.45
Iteration 50 took 2.34 seconds (mean sampled reward: -5445.14). Current reward after update: -683.90, Optimal reward -683.90
Iteration 51 took 2.20 seconds (mean sampled reward: -4416.00). Current reward after update: -603.79, Optimal reward -603.79
Iteration 52 took 2.18 seconds (mean sampled reward: -5353.20). Current reward after update: -982.36, Optimal reward -603.79
Iteration 53 took 2.20 seconds (mean sampled reward: -4692.48). Current reward after update: -809.15, Optimal reward -603.79
Iteration 54 took 2.26 seconds (mean sampled reward: -5292.68). Current reward after update: -836.47, Optimal reward -603.79
Iteration 55 took 2.53 seconds (mean sampled reward: -4945.86). Current reward after update: -769.20, Optimal reward -603.79
Iteration 56 took 2.40 seconds (mean sampled reward: -3844.31). Current reward after update: -2613.60, Optimal reward -603.79
Iteration 57 took 2.30 seconds (mean sampled reward: -5821.41). Current reward after update: -896.18, Optimal reward -603.79
Iteration 58 took 2.24 seconds (mean sampled reward: -5261.81). Current reward after update: -941.36, Optimal reward -603.79
Iteration 59 took 2.32 seconds (mean sampled reward: -4955.75). Current reward after update: -863.35, Optimal reward -603.79
Iteration 60 took 2.41 seconds (mean sampled reward: -4688.98). Current reward after update: -790.47, Optimal reward -603.79
Iteration 61 took 2.29 seconds (mean sampled reward: -4861.21). Current reward after update: -650.46, Optimal reward -603.79
Iteration 62 took 2.29 seconds (mean sampled reward: -5372.95). Current reward after update: -896.97, Optimal reward -603.79
Iteration 63 took 2.22 seconds (mean sampled reward: -4641.44). Current reward after update: -751.74, Optimal reward -603.79
Iteration 64 took 2.23 seconds (mean sampled reward: -5907.16). Current reward after update: -748.29, Optimal reward -603.79
Iteration 65 took 2.16 seconds (mean sampled reward: -5240.07). Current reward after update: -597.25, Optimal reward -597.25
Iteration 66 took 2.23 seconds (mean sampled reward: -5653.13). Current reward after update: -933.83, Optimal reward -597.25
Iteration 67 took 2.22 seconds (mean sampled reward: -5841.43). Current reward after update: -911.72, Optimal reward -597.25
Iteration 68 took 2.17 seconds (mean sampled reward: -5141.08). Current reward after update: -1041.04, Optimal reward -597.25
Iteration 69 took 2.18 seconds (mean sampled reward: -4668.49). Current reward after update: -5098.14, Optimal reward -597.25
Iteration 70 took 2.24 seconds (mean sampled reward: -5416.10). Current reward after update: -841.71, Optimal reward -597.25
Iteration 71 took 2.31 seconds (mean sampled reward: -5603.67). Current reward after update: -777.43, Optimal reward -597.25
Iteration 72 took 2.28 seconds (mean sampled reward: -4947.07). Current reward after update: -1697.86, Optimal reward -597.25
Iteration 73 took 2.19 seconds (mean sampled reward: -5076.46). Current reward after update: -757.86, Optimal reward -597.25
Iteration 74 took 2.21 seconds (mean sampled reward: -5495.28). Current reward after update: -787.21, Optimal reward -597.25
Iteration 75 took 2.29 seconds (mean sampled reward: -5191.09). Current reward after update: -801.94, Optimal reward -597.25
Iteration 76 took 2.25 seconds (mean sampled reward: -4232.46). Current reward after update: -702.32, Optimal reward -597.25
Iteration 77 took 2.35 seconds (mean sampled reward: -4470.64). Current reward after update: -650.37, Optimal reward -597.25
Iteration 78 took 2.28 seconds (mean sampled reward: -4884.61). Current reward after update: -862.85, Optimal reward -597.25
Iteration 79 took 2.35 seconds (mean sampled reward: -5422.26). Current reward after update: -991.68, Optimal reward -597.25
Iteration 80 took 2.29 seconds (mean sampled reward: -5696.65). Current reward after update: -736.31, Optimal reward -597.25
Iteration 81 took 2.28 seconds (mean sampled reward: -5948.98). Current reward after update: -842.30, Optimal reward -597.25
Iteration 82 took 2.36 seconds (mean sampled reward: -5584.81). Current reward after update: -898.22, Optimal reward -597.25
Iteration 83 took 2.31 seconds (mean sampled reward: -5832.03). Current reward after update: -807.63, Optimal reward -597.25
Iteration 84 took 2.39 seconds (mean sampled reward: -4604.27). Current reward after update: -2130.11, Optimal reward -597.25
Iteration 85 took 2.40 seconds (mean sampled reward: -4365.01). Current reward after update: -1802.45, Optimal reward -597.25
Iteration 86 took 2.42 seconds (mean sampled reward: -4699.00). Current reward after update: -5102.50, Optimal reward -597.25
Iteration 87 took 2.34 seconds (mean sampled reward: -4839.90). Current reward after update: -575.88, Optimal reward -575.88
Iteration 88 took 2.23 seconds (mean sampled reward: -5144.85). Current reward after update: -1097.34, Optimal reward -575.88
Iteration 89 took 2.32 seconds (mean sampled reward: -5052.95). Current reward after update: -612.83, Optimal reward -575.88
Iteration 90 took 2.22 seconds (mean sampled reward: -6067.75). Current reward after update: -723.67, Optimal reward -575.88
Iteration 91 took 2.20 seconds (mean sampled reward: -6106.08). Current reward after update: -671.65, Optimal reward -575.88
Iteration 92 took 2.28 seconds (mean sampled reward: -6605.39). Current reward after update: -613.19, Optimal reward -575.88
Iteration 93 took 2.24 seconds (mean sampled reward: -6481.45). Current reward after update: -563.12, Optimal reward -563.12
Iteration 94 took 2.21 seconds (mean sampled reward: -5889.22). Current reward after update: -927.25, Optimal reward -563.12
Iteration 95 took 2.27 seconds (mean sampled reward: -6260.65). Current reward after update: -900.45, Optimal reward -563.12
Iteration 96 took 2.37 seconds (mean sampled reward: -5027.85). Current reward after update: -668.35, Optimal reward -563.12
Iteration 97 took 2.27 seconds (mean sampled reward: -4862.17). Current reward after update: -652.57, Optimal reward -563.12
Iteration 98 took 2.30 seconds (mean sampled reward: -5110.56). Current reward after update: -679.15, Optimal reward -563.12
Iteration 99 took 2.29 seconds (mean sampled reward: -6178.76). Current reward after update: -817.55, Optimal reward -563.12
Iteration 100 took 2.45 seconds (mean sampled reward: -6500.85). Current reward after update: -535.51, Optimal reward -535.51
Iteration 101 took 2.32 seconds (mean sampled reward: -5272.89). Current reward after update: -2242.98, Optimal reward -535.51
Iteration 102 took 2.34 seconds (mean sampled reward: -4651.31). Current reward after update: -1112.82, Optimal reward -535.51
Iteration 103 took 2.31 seconds (mean sampled reward: -4609.64). Current reward after update: -535.33, Optimal reward -535.33
Iteration 104 took 2.33 seconds (mean sampled reward: -4316.12). Current reward after update: -582.53, Optimal reward -535.33
Iteration 105 took 2.27 seconds (mean sampled reward: -4161.32). Current reward after update: -566.91, Optimal reward -535.33
Iteration 106 took 2.29 seconds (mean sampled reward: -4104.32). Current reward after update: -613.09, Optimal reward -535.33
Iteration 107 took 2.47 seconds (mean sampled reward: -4043.77). Current reward after update: -1377.84, Optimal reward -535.33
Iteration 108 took 2.32 seconds (mean sampled reward: -3931.42). Current reward after update: -581.83, Optimal reward -535.33
Iteration 109 took 2.35 seconds (mean sampled reward: -4552.07). Current reward after update: -661.53, Optimal reward -535.33
Iteration 110 took 2.32 seconds (mean sampled reward: -4773.27). Current reward after update: -585.39, Optimal reward -535.33
Iteration 111 took 2.24 seconds (mean sampled reward: -4039.69). Current reward after update: -698.38, Optimal reward -535.33
Iteration 112 took 2.19 seconds (mean sampled reward: -4403.27). Current reward after update: -645.78, Optimal reward -535.33
Iteration 113 took 2.20 seconds (mean sampled reward: -3435.30). Current reward after update: -562.57, Optimal reward -535.33
Iteration 114 took 2.33 seconds (mean sampled reward: -4839.97). Current reward after update: -503.56, Optimal reward -503.56
Iteration 115 took 2.17 seconds (mean sampled reward: -4500.49). Current reward after update: -522.65, Optimal reward -503.56
Iteration 116 took 2.17 seconds (mean sampled reward: -4650.75). Current reward after update: -538.66, Optimal reward -503.56
Iteration 117 took 2.27 seconds (mean sampled reward: -4108.79). Current reward after update: -569.42, Optimal reward -503.56
Iteration 118 took 2.21 seconds (mean sampled reward: -4152.54). Current reward after update: -452.42, Optimal reward -452.42
Iteration 119 took 2.21 seconds (mean sampled reward: -4703.78). Current reward after update: -426.21, Optimal reward -426.21
Iteration 120 took 2.35 seconds (mean sampled reward: -5618.23). Current reward after update: -496.94, Optimal reward -426.21
Iteration 121 took 2.23 seconds (mean sampled reward: -5039.36). Current reward after update: -497.80, Optimal reward -426.21
Iteration 122 took 2.27 seconds (mean sampled reward: -5282.33). Current reward after update: -591.25, Optimal reward -426.21
Iteration 123 took 2.32 seconds (mean sampled reward: -4843.00). Current reward after update: -448.87, Optimal reward -426.21
Iteration 124 took 2.21 seconds (mean sampled reward: -5384.30). Current reward after update: -562.64, Optimal reward -426.21
Iteration 125 took 2.37 seconds (mean sampled reward: -5006.44). Current reward after update: -627.89, Optimal reward -426.21
Iteration 126 took 2.21 seconds (mean sampled reward: -5560.28). Current reward after update: -5199.63, Optimal reward -426.21
Iteration 127 took 2.22 seconds (mean sampled reward: -4764.40). Current reward after update: -632.83, Optimal reward -426.21
Iteration 128 took 2.30 seconds (mean sampled reward: -5922.57). Current reward after update: -634.62, Optimal reward -426.21
Iteration 129 took 2.25 seconds (mean sampled reward: -3886.45). Current reward after update: -564.57, Optimal reward -426.21
Iteration 130 took 2.27 seconds (mean sampled reward: -4568.42). Current reward after update: -463.38, Optimal reward -426.21
Iteration 131 took 2.22 seconds (mean sampled reward: -3739.12). Current reward after update: -539.32, Optimal reward -426.21
Iteration 132 took 2.32 seconds (mean sampled reward: -5071.43). Current reward after update: -359.99, Optimal reward -359.99
Iteration 133 took 2.40 seconds (mean sampled reward: -4206.85). Current reward after update: -589.14, Optimal reward -359.99
Iteration 134 took 2.35 seconds (mean sampled reward: -4167.30). Current reward after update: -569.35, Optimal reward -359.99
Iteration 135 took 2.28 seconds (mean sampled reward: -3835.89). Current reward after update: -514.20, Optimal reward -359.99
Iteration 136 took 2.27 seconds (mean sampled reward: -3814.54). Current reward after update: -358.37, Optimal reward -358.37
Iteration 137 took 2.34 seconds (mean sampled reward: -4462.28). Current reward after update: -445.49, Optimal reward -358.37
Iteration 138 took 2.44 seconds (mean sampled reward: -4816.23). Current reward after update: -582.13, Optimal reward -358.37
Iteration 139 took 2.28 seconds (mean sampled reward: -4752.11). Current reward after update: -636.83, Optimal reward -358.37
Iteration 140 took 2.21 seconds (mean sampled reward: -3894.04). Current reward after update: -570.25, Optimal reward -358.37
Iteration 141 took 2.19 seconds (mean sampled reward: -3692.68). Current reward after update: -678.66, Optimal reward -358.37
Iteration 142 took 2.25 seconds (mean sampled reward: -5148.60). Current reward after update: -726.83, Optimal reward -358.37
Iteration 143 took 2.24 seconds (mean sampled reward: -4772.62). Current reward after update: -517.10, Optimal reward -358.37
Iteration 144 took 2.24 seconds (mean sampled reward: -4587.15). Current reward after update: -684.74, Optimal reward -358.37
Iteration 145 took 2.22 seconds (mean sampled reward: -3388.93). Current reward after update: -1906.38, Optimal reward -358.37
Iteration 146 took 2.20 seconds (mean sampled reward: -4088.70). Current reward after update: -404.76, Optimal reward -358.37
Iteration 147 took 2.24 seconds (mean sampled reward: -4455.51). Current reward after update: -669.63, Optimal reward -358.37
Iteration 148 took 2.28 seconds (mean sampled reward: -5689.60). Current reward after update: -642.83, Optimal reward -358.37
Iteration 149 took 2.22 seconds (mean sampled reward: -4613.12). Current reward after update: -483.69, Optimal reward -358.37
Iteration 150 took 2.18 seconds (mean sampled reward: -4132.08). Current reward after update: -700.24, Optimal reward -358.37
Iteration 151 took 2.24 seconds (mean sampled reward: -3577.09). Current reward after update: -422.02, Optimal reward -358.37
Iteration 152 took 2.26 seconds (mean sampled reward: -5453.29). Current reward after update: -1054.32, Optimal reward -358.37
Iteration 153 took 2.21 seconds (mean sampled reward: -4118.38). Current reward after update: -992.55, Optimal reward -358.37
Iteration 154 took 2.26 seconds (mean sampled reward: -4245.86). Current reward after update: -511.11, Optimal reward -358.37
Iteration 155 took 2.16 seconds (mean sampled reward: -3584.28). Current reward after update: -506.48, Optimal reward -358.37
Iteration 156 took 2.21 seconds (mean sampled reward: -5040.03). Current reward after update: -532.91, Optimal reward -358.37
Iteration 157 took 2.21 seconds (mean sampled reward: -3777.14). Current reward after update: -424.19, Optimal reward -358.37
Iteration 158 took 2.24 seconds (mean sampled reward: -4717.81). Current reward after update: -355.73, Optimal reward -355.73
Iteration 159 took 2.25 seconds (mean sampled reward: -4591.19). Current reward after update: -341.72, Optimal reward -341.72
Iteration 160 took 2.24 seconds (mean sampled reward: -4495.38). Current reward after update: -258.02, Optimal reward -258.02
Iteration 161 took 2.25 seconds (mean sampled reward: -4185.65). Current reward after update: -280.17, Optimal reward -258.02
Iteration 162 took 2.23 seconds (mean sampled reward: -3692.28). Current reward after update: -208.25, Optimal reward -208.25
Iteration 163 took 2.19 seconds (mean sampled reward: -3972.05). Current reward after update: -729.43, Optimal reward -208.25
Iteration 164 took 2.28 seconds (mean sampled reward: -5246.86). Current reward after update: -301.57, Optimal reward -208.25
Iteration 165 took 2.28 seconds (mean sampled reward: -4161.23). Current reward after update: -227.95, Optimal reward -208.25
Iteration 166 took 2.20 seconds (mean sampled reward: -3482.26). Current reward after update: -211.61, Optimal reward -208.25
Iteration 167 took 2.25 seconds (mean sampled reward: -5284.10). Current reward after update: -313.74, Optimal reward -208.25
Iteration 168 took 2.34 seconds (mean sampled reward: -5710.30). Current reward after update: -194.68, Optimal reward -194.68
Iteration 169 took 2.33 seconds (mean sampled reward: -5157.20). Current reward after update: -303.55, Optimal reward -194.68
Iteration 170 took 2.23 seconds (mean sampled reward: -5900.83). Current reward after update: -297.26, Optimal reward -194.68
Iteration 171 took 2.23 seconds (mean sampled reward: -5882.81). Current reward after update: -379.91, Optimal reward -194.68
Iteration 172 took 2.25 seconds (mean sampled reward: -5845.36). Current reward after update: -280.20, Optimal reward -194.68
Iteration 173 took 2.32 seconds (mean sampled reward: -5706.11). Current reward after update: -339.89, Optimal reward -194.68
Iteration 174 took 2.23 seconds (mean sampled reward: -4747.72). Current reward after update: -292.92, Optimal reward -194.68
Iteration 175 took 2.19 seconds (mean sampled reward: -5066.17). Current reward after update: -253.35, Optimal reward -194.68
Iteration 176 took 2.20 seconds (mean sampled reward: -4838.53). Current reward after update: -410.00, Optimal reward -194.68
Iteration 177 took 2.21 seconds (mean sampled reward: -4752.92). Current reward after update: -292.88, Optimal reward -194.68
Iteration 178 took 2.23 seconds (mean sampled reward: -4427.40). Current reward after update: -315.35, Optimal reward -194.68
Iteration 179 took 2.19 seconds (mean sampled reward: -4183.09). Current reward after update: -403.60, Optimal reward -194.68
Iteration 180 took 2.25 seconds (mean sampled reward: -4766.25). Current reward after update: -287.94, Optimal reward -194.68
Iteration 181 took 2.25 seconds (mean sampled reward: -5106.97). Current reward after update: -409.77, Optimal reward -194.68
Iteration 182 took 2.18 seconds (mean sampled reward: -3812.75). Current reward after update: -355.61, Optimal reward -194.68
Iteration 183 took 2.25 seconds (mean sampled reward: -4928.07). Current reward after update: -313.23, Optimal reward -194.68
Iteration 184 took 2.27 seconds (mean sampled reward: -4466.97). Current reward after update: -225.65, Optimal reward -194.68
Iteration 185 took 2.25 seconds (mean sampled reward: -5527.93). Current reward after update: -295.07, Optimal reward -194.68
Iteration 186 took 2.27 seconds (mean sampled reward: -5045.83). Current reward after update: -246.30, Optimal reward -194.68
Iteration 187 took 2.21 seconds (mean sampled reward: -4182.55). Current reward after update: -249.34, Optimal reward -194.68
Iteration 188 took 2.19 seconds (mean sampled reward: -4230.60). Current reward after update: -297.50, Optimal reward -194.68
Iteration 189 took 2.19 seconds (mean sampled reward: -4824.99). Current reward after update: -290.69, Optimal reward -194.68
Iteration 190 took 2.14 seconds (mean sampled reward: -4182.76). Current reward after update: -916.87, Optimal reward -194.68
Iteration 191 took 2.42 seconds (mean sampled reward: -4502.84). Current reward after update: -623.73, Optimal reward -194.68
Iteration 192 took 2.22 seconds (mean sampled reward: -4314.95). Current reward after update: -221.26, Optimal reward -194.68
Iteration 193 took 2.12 seconds (mean sampled reward: -4197.59). Current reward after update: -138.03, Optimal reward -138.03
Iteration 194 took 2.14 seconds (mean sampled reward: -4659.09). Current reward after update: -165.28, Optimal reward -138.03
Iteration 195 took 2.17 seconds (mean sampled reward: -4165.30). Current reward after update: -287.89, Optimal reward -138.03
Iteration 196 took 2.17 seconds (mean sampled reward: -3198.78). Current reward after update: -147.69, Optimal reward -138.03
Iteration 197 took 2.22 seconds (mean sampled reward: -3879.27). Current reward after update: -158.66, Optimal reward -138.03
Iteration 198 took 2.17 seconds (mean sampled reward: -4183.45). Current reward after update: -179.41, Optimal reward -138.03
Iteration 199 took 2.23 seconds (mean sampled reward: -3595.51). Current reward after update: -229.97, Optimal reward -138.03
Iteration 200 took 2.22 seconds (mean sampled reward: -3092.19). Current reward after update: -1416.24, Optimal reward -138.03
Max force: 30 Sigma: 0.4 mean rewards: -507.46591715185497, best rewards:-138.02651172789524

Iteration 1 took 2.39 seconds (mean sampled reward: -7492.09). Current reward after update: -4259.46, Optimal reward -4259.46
Iteration 2 took 2.25 seconds (mean sampled reward: -6960.80). Current reward after update: -3191.93, Optimal reward -3191.93
Iteration 3 took 2.41 seconds (mean sampled reward: -6551.58). Current reward after update: -2303.88, Optimal reward -2303.88
Iteration 4 took 2.24 seconds (mean sampled reward: -5841.71). Current reward after update: -2325.86, Optimal reward -2303.88
Iteration 5 took 2.30 seconds (mean sampled reward: -5118.18). Current reward after update: -2052.04, Optimal reward -2052.04
Iteration 6 took 2.31 seconds (mean sampled reward: -5708.77). Current reward after update: -2396.85, Optimal reward -2052.04
Iteration 7 took 2.24 seconds (mean sampled reward: -6012.93). Current reward after update: -2230.14, Optimal reward -2052.04
Iteration 8 took 2.48 seconds (mean sampled reward: -4914.91). Current reward after update: -2342.81, Optimal reward -2052.04
Iteration 9 took 2.21 seconds (mean sampled reward: -4176.89). Current reward after update: -1741.24, Optimal reward -1741.24
Iteration 10 took 2.45 seconds (mean sampled reward: -5661.91). Current reward after update: -1755.51, Optimal reward -1741.24
Iteration 11 took 2.38 seconds (mean sampled reward: -4761.88). Current reward after update: -1356.47, Optimal reward -1356.47
Iteration 12 took 2.21 seconds (mean sampled reward: -4705.16). Current reward after update: -1070.34, Optimal reward -1070.34
Iteration 13 took 2.19 seconds (mean sampled reward: -5733.86). Current reward after update: -737.73, Optimal reward -737.73
Iteration 14 took 2.19 seconds (mean sampled reward: -5101.96). Current reward after update: -944.25, Optimal reward -737.73
Iteration 15 took 2.45 seconds (mean sampled reward: -5382.81). Current reward after update: -1153.69, Optimal reward -737.73
Iteration 16 took 2.23 seconds (mean sampled reward: -5912.96). Current reward after update: -973.98, Optimal reward -737.73
Iteration 17 took 2.44 seconds (mean sampled reward: -6128.77). Current reward after update: -1131.98, Optimal reward -737.73
Iteration 18 took 2.33 seconds (mean sampled reward: -5118.33). Current reward after update: -973.70, Optimal reward -737.73
Iteration 19 took 2.22 seconds (mean sampled reward: -5335.63). Current reward after update: -933.77, Optimal reward -737.73
Iteration 20 took 2.28 seconds (mean sampled reward: -4589.47). Current reward after update: -768.40, Optimal reward -737.73
Iteration 21 took 2.27 seconds (mean sampled reward: -4911.87). Current reward after update: -790.08, Optimal reward -737.73
Iteration 22 took 2.20 seconds (mean sampled reward: -6228.02). Current reward after update: -2345.64, Optimal reward -737.73
Iteration 23 took 2.23 seconds (mean sampled reward: -5334.45). Current reward after update: -898.18, Optimal reward -737.73
Iteration 24 took 2.26 seconds (mean sampled reward: -4956.97). Current reward after update: -776.93, Optimal reward -737.73
Iteration 25 took 2.32 seconds (mean sampled reward: -4142.38). Current reward after update: -964.21, Optimal reward -737.73
Iteration 26 took 2.22 seconds (mean sampled reward: -5005.53). Current reward after update: -1040.39, Optimal reward -737.73
Iteration 27 took 2.35 seconds (mean sampled reward: -4525.57). Current reward after update: -770.87, Optimal reward -737.73
Iteration 28 took 2.45 seconds (mean sampled reward: -4238.34). Current reward after update: -837.15, Optimal reward -737.73
Iteration 29 took 2.21 seconds (mean sampled reward: -3695.51). Current reward after update: -852.15, Optimal reward -737.73
Iteration 30 took 2.26 seconds (mean sampled reward: -4410.80). Current reward after update: -585.00, Optimal reward -585.00
Iteration 31 took 2.42 seconds (mean sampled reward: -4229.29). Current reward after update: -441.08, Optimal reward -441.08
Iteration 32 took 2.40 seconds (mean sampled reward: -4278.20). Current reward after update: -784.93, Optimal reward -441.08
Iteration 33 took 2.37 seconds (mean sampled reward: -5130.44). Current reward after update: -620.40, Optimal reward -441.08
Iteration 34 took 2.26 seconds (mean sampled reward: -4992.78). Current reward after update: -691.88, Optimal reward -441.08
Iteration 35 took 2.23 seconds (mean sampled reward: -3497.04). Current reward after update: -571.53, Optimal reward -441.08
Iteration 36 took 2.31 seconds (mean sampled reward: -3333.05). Current reward after update: -520.00, Optimal reward -441.08
Iteration 37 took 2.22 seconds (mean sampled reward: -2879.73). Current reward after update: -1354.47, Optimal reward -441.08
Iteration 38 took 2.21 seconds (mean sampled reward: -3189.88). Current reward after update: -879.52, Optimal reward -441.08
Iteration 39 took 2.17 seconds (mean sampled reward: -2445.44). Current reward after update: -199.84, Optimal reward -199.84
Iteration 40 took 2.26 seconds (mean sampled reward: -4456.68). Current reward after update: -341.28, Optimal reward -199.84
Iteration 41 took 2.27 seconds (mean sampled reward: -2834.55). Current reward after update: -196.27, Optimal reward -196.27
Iteration 42 took 2.19 seconds (mean sampled reward: -2171.19). Current reward after update: -397.15, Optimal reward -196.27
Iteration 43 took 2.18 seconds (mean sampled reward: -3162.11). Current reward after update: -241.59, Optimal reward -196.27
Iteration 44 took 2.19 seconds (mean sampled reward: -4770.84). Current reward after update: -792.98, Optimal reward -196.27
Iteration 45 took 2.20 seconds (mean sampled reward: -5562.81). Current reward after update: -485.76, Optimal reward -196.27
Iteration 46 took 2.24 seconds (mean sampled reward: -3127.16). Current reward after update: -460.81, Optimal reward -196.27
Iteration 47 took 2.20 seconds (mean sampled reward: -4130.64). Current reward after update: -500.06, Optimal reward -196.27
Iteration 48 took 2.42 seconds (mean sampled reward: -5631.51). Current reward after update: -412.88, Optimal reward -196.27
Iteration 49 took 2.13 seconds (mean sampled reward: -5776.11). Current reward after update: -483.23, Optimal reward -196.27
Iteration 50 took 2.22 seconds (mean sampled reward: -5104.15). Current reward after update: -687.31, Optimal reward -196.27
Iteration 51 took 2.14 seconds (mean sampled reward: -4841.83). Current reward after update: -586.56, Optimal reward -196.27
Iteration 52 took 2.10 seconds (mean sampled reward: -3812.59). Current reward after update: -388.76, Optimal reward -196.27
Iteration 53 took 2.14 seconds (mean sampled reward: -2132.90). Current reward after update: -1037.02, Optimal reward -196.27
Iteration 54 took 2.12 seconds (mean sampled reward: -3190.61). Current reward after update: -581.63, Optimal reward -196.27
Iteration 55 took 2.21 seconds (mean sampled reward: -4159.90). Current reward after update: -512.31, Optimal reward -196.27
Iteration 56 took 2.13 seconds (mean sampled reward: -2788.60). Current reward after update: -430.30, Optimal reward -196.27
Iteration 57 took 2.10 seconds (mean sampled reward: -2874.69). Current reward after update: -364.72, Optimal reward -196.27
Iteration 58 took 2.17 seconds (mean sampled reward: -2691.90). Current reward after update: -428.21, Optimal reward -196.27
Iteration 59 took 2.10 seconds (mean sampled reward: -2481.47). Current reward after update: -422.49, Optimal reward -196.27
Iteration 60 took 2.28 seconds (mean sampled reward: -3341.27). Current reward after update: -466.77, Optimal reward -196.27
Iteration 61 took 2.27 seconds (mean sampled reward: -3642.35). Current reward after update: -391.08, Optimal reward -196.27
Iteration 62 took 2.14 seconds (mean sampled reward: -4595.72). Current reward after update: -1324.13, Optimal reward -196.27
Iteration 63 took 2.10 seconds (mean sampled reward: -2801.29). Current reward after update: -430.81, Optimal reward -196.27
Iteration 64 took 2.10 seconds (mean sampled reward: -1619.68). Current reward after update: -829.12, Optimal reward -196.27
Iteration 65 took 2.40 seconds (mean sampled reward: -2011.84). Current reward after update: -317.02, Optimal reward -196.27
Iteration 66 took 2.26 seconds (mean sampled reward: -2024.63). Current reward after update: -699.52, Optimal reward -196.27
Iteration 67 took 2.16 seconds (mean sampled reward: -2452.50). Current reward after update: -319.67, Optimal reward -196.27
Iteration 68 took 2.11 seconds (mean sampled reward: -2139.90). Current reward after update: -600.39, Optimal reward -196.27
Iteration 69 took 2.15 seconds (mean sampled reward: -2512.78). Current reward after update: -346.46, Optimal reward -196.27
Iteration 70 took 2.09 seconds (mean sampled reward: -2060.27). Current reward after update: -924.21, Optimal reward -196.27
Iteration 71 took 2.08 seconds (mean sampled reward: -1928.53). Current reward after update: -376.41, Optimal reward -196.27
Iteration 72 took 2.09 seconds (mean sampled reward: -3394.19). Current reward after update: -344.94, Optimal reward -196.27
Iteration 73 took 2.16 seconds (mean sampled reward: -3902.33). Current reward after update: -512.68, Optimal reward -196.27
Iteration 74 took 2.12 seconds (mean sampled reward: -3220.38). Current reward after update: -407.58, Optimal reward -196.27
Iteration 75 took 2.20 seconds (mean sampled reward: -3415.52). Current reward after update: -1899.35, Optimal reward -196.27
Iteration 76 took 2.18 seconds (mean sampled reward: -2752.67). Current reward after update: -399.66, Optimal reward -196.27
Iteration 77 took 2.18 seconds (mean sampled reward: -2323.33). Current reward after update: -459.20, Optimal reward -196.27
Iteration 78 took 2.18 seconds (mean sampled reward: -1854.90). Current reward after update: -504.12, Optimal reward -196.27
Iteration 79 took 2.16 seconds (mean sampled reward: -2160.43). Current reward after update: -434.07, Optimal reward -196.27
Iteration 80 took 2.18 seconds (mean sampled reward: -3291.28). Current reward after update: -533.93, Optimal reward -196.27
Iteration 81 took 2.18 seconds (mean sampled reward: -3148.72). Current reward after update: -1671.55, Optimal reward -196.27
Iteration 82 took 2.15 seconds (mean sampled reward: -3019.53). Current reward after update: -432.31, Optimal reward -196.27
Iteration 83 took 2.12 seconds (mean sampled reward: -3683.66). Current reward after update: -485.97, Optimal reward -196.27
Iteration 84 took 2.25 seconds (mean sampled reward: -4274.79). Current reward after update: -1785.97, Optimal reward -196.27
Iteration 85 took 2.34 seconds (mean sampled reward: -3422.38). Current reward after update: -319.77, Optimal reward -196.27
Iteration 86 took 2.23 seconds (mean sampled reward: -3387.99). Current reward after update: -564.54, Optimal reward -196.27
Iteration 87 took 2.19 seconds (mean sampled reward: -3142.24). Current reward after update: -539.83, Optimal reward -196.27
Iteration 88 took 2.16 seconds (mean sampled reward: -3512.34). Current reward after update: -476.75, Optimal reward -196.27
Iteration 89 took 2.21 seconds (mean sampled reward: -3614.72). Current reward after update: -438.47, Optimal reward -196.27
Iteration 90 took 2.19 seconds (mean sampled reward: -3319.15). Current reward after update: -1324.55, Optimal reward -196.27
Iteration 91 took 2.16 seconds (mean sampled reward: -3100.04). Current reward after update: -360.80, Optimal reward -196.27
Iteration 92 took 2.25 seconds (mean sampled reward: -3418.48). Current reward after update: -384.56, Optimal reward -196.27
Iteration 93 took 2.16 seconds (mean sampled reward: -3120.17). Current reward after update: -424.14, Optimal reward -196.27
Iteration 94 took 2.15 seconds (mean sampled reward: -3122.71). Current reward after update: -308.38, Optimal reward -196.27
Iteration 95 took 2.21 seconds (mean sampled reward: -2534.31). Current reward after update: -275.60, Optimal reward -196.27
Iteration 96 took 2.19 seconds (mean sampled reward: -1999.19). Current reward after update: -345.41, Optimal reward -196.27
Iteration 97 took 2.15 seconds (mean sampled reward: -1910.08). Current reward after update: -340.90, Optimal reward -196.27
Iteration 98 took 2.14 seconds (mean sampled reward: -2693.43). Current reward after update: -348.20, Optimal reward -196.27
Iteration 99 took 2.18 seconds (mean sampled reward: -2119.96). Current reward after update: -332.85, Optimal reward -196.27
Iteration 100 took 2.19 seconds (mean sampled reward: -1874.24). Current reward after update: -419.20, Optimal reward -196.27
Iteration 101 took 2.22 seconds (mean sampled reward: -1893.87). Current reward after update: -2150.57, Optimal reward -196.27
Iteration 102 took 2.15 seconds (mean sampled reward: -2047.86). Current reward after update: -654.13, Optimal reward -196.27
Iteration 103 took 2.15 seconds (mean sampled reward: -2118.72). Current reward after update: -406.02, Optimal reward -196.27
Iteration 104 took 2.21 seconds (mean sampled reward: -2598.59). Current reward after update: -1193.01, Optimal reward -196.27
Iteration 105 took 2.16 seconds (mean sampled reward: -2691.00). Current reward after update: -405.68, Optimal reward -196.27
Iteration 106 took 2.25 seconds (mean sampled reward: -2135.72). Current reward after update: -2158.14, Optimal reward -196.27
Iteration 107 took 2.26 seconds (mean sampled reward: -2110.00). Current reward after update: -356.92, Optimal reward -196.27
Iteration 108 took 2.13 seconds (mean sampled reward: -1837.25). Current reward after update: -327.33, Optimal reward -196.27
Iteration 109 took 2.20 seconds (mean sampled reward: -2390.87). Current reward after update: -323.15, Optimal reward -196.27
Iteration 110 took 2.25 seconds (mean sampled reward: -2146.71). Current reward after update: -1491.89, Optimal reward -196.27
Iteration 111 took 2.20 seconds (mean sampled reward: -2352.12). Current reward after update: -380.37, Optimal reward -196.27
Iteration 112 took 2.18 seconds (mean sampled reward: -2948.17). Current reward after update: -435.70, Optimal reward -196.27
Iteration 113 took 2.14 seconds (mean sampled reward: -2710.62). Current reward after update: -370.61, Optimal reward -196.27
Iteration 114 took 2.22 seconds (mean sampled reward: -2635.09). Current reward after update: -525.53, Optimal reward -196.27
Iteration 115 took 2.27 seconds (mean sampled reward: -3308.57). Current reward after update: -473.24, Optimal reward -196.27
Iteration 116 took 2.17 seconds (mean sampled reward: -3414.97). Current reward after update: -423.63, Optimal reward -196.27
Iteration 117 took 2.23 seconds (mean sampled reward: -3480.05). Current reward after update: -451.29, Optimal reward -196.27
Iteration 118 took 2.20 seconds (mean sampled reward: -3225.60). Current reward after update: -322.99, Optimal reward -196.27
Iteration 119 took 2.27 seconds (mean sampled reward: -3376.77). Current reward after update: -563.57, Optimal reward -196.27
Iteration 120 took 2.23 seconds (mean sampled reward: -3365.71). Current reward after update: -454.32, Optimal reward -196.27
Iteration 121 took 2.18 seconds (mean sampled reward: -2345.31). Current reward after update: -309.89, Optimal reward -196.27
Iteration 122 took 2.30 seconds (mean sampled reward: -2274.40). Current reward after update: -384.35, Optimal reward -196.27
Iteration 123 took 2.19 seconds (mean sampled reward: -1749.70). Current reward after update: -1086.05, Optimal reward -196.27
Iteration 124 took 2.27 seconds (mean sampled reward: -2670.74). Current reward after update: -2749.76, Optimal reward -196.27
Iteration 125 took 2.22 seconds (mean sampled reward: -3104.69). Current reward after update: -334.44, Optimal reward -196.27
Iteration 126 took 2.23 seconds (mean sampled reward: -2884.16). Current reward after update: -2099.73, Optimal reward -196.27
Iteration 127 took 2.12 seconds (mean sampled reward: -2859.70). Current reward after update: -315.90, Optimal reward -196.27
Iteration 128 took 2.13 seconds (mean sampled reward: -2470.51). Current reward after update: -327.92, Optimal reward -196.27
Iteration 129 took 2.12 seconds (mean sampled reward: -2397.58). Current reward after update: -275.26, Optimal reward -196.27
Iteration 130 took 2.28 seconds (mean sampled reward: -2572.37). Current reward after update: -363.68, Optimal reward -196.27
Iteration 131 took 2.18 seconds (mean sampled reward: -3327.49). Current reward after update: -2461.62, Optimal reward -196.27
Iteration 132 took 2.22 seconds (mean sampled reward: -2080.10). Current reward after update: -318.42, Optimal reward -196.27
Iteration 133 took 2.26 seconds (mean sampled reward: -2258.95). Current reward after update: -339.06, Optimal reward -196.27
Iteration 134 took 2.22 seconds (mean sampled reward: -2488.84). Current reward after update: -424.38, Optimal reward -196.27
Iteration 135 took 2.17 seconds (mean sampled reward: -2184.21). Current reward after update: -344.06, Optimal reward -196.27
Iteration 136 took 2.22 seconds (mean sampled reward: -2635.24). Current reward after update: -410.29, Optimal reward -196.27
Iteration 137 took 2.17 seconds (mean sampled reward: -2862.85). Current reward after update: -316.87, Optimal reward -196.27
Iteration 138 took 2.17 seconds (mean sampled reward: -2629.93). Current reward after update: -356.51, Optimal reward -196.27
Iteration 139 took 2.22 seconds (mean sampled reward: -2300.77). Current reward after update: -360.54, Optimal reward -196.27
Iteration 140 took 2.17 seconds (mean sampled reward: -2093.01). Current reward after update: -957.93, Optimal reward -196.27
Iteration 141 took 2.19 seconds (mean sampled reward: -2326.81). Current reward after update: -334.90, Optimal reward -196.27
Iteration 142 took 2.17 seconds (mean sampled reward: -3597.58). Current reward after update: -1099.91, Optimal reward -196.27
Iteration 143 took 2.15 seconds (mean sampled reward: -3078.68). Current reward after update: -372.50, Optimal reward -196.27
Iteration 144 took 2.25 seconds (mean sampled reward: -3803.03). Current reward after update: -375.69, Optimal reward -196.27
Iteration 145 took 2.27 seconds (mean sampled reward: -3395.99). Current reward after update: -349.24, Optimal reward -196.27
Iteration 146 took 2.20 seconds (mean sampled reward: -3349.53). Current reward after update: -364.16, Optimal reward -196.27
Iteration 147 took 2.21 seconds (mean sampled reward: -3445.97). Current reward after update: -252.21, Optimal reward -196.27
Iteration 148 took 2.26 seconds (mean sampled reward: -2893.88). Current reward after update: -270.95, Optimal reward -196.27
Iteration 149 took 2.23 seconds (mean sampled reward: -3517.97). Current reward after update: -311.81, Optimal reward -196.27
Iteration 150 took 2.23 seconds (mean sampled reward: -4410.89). Current reward after update: -292.76, Optimal reward -196.27
Iteration 151 took 2.26 seconds (mean sampled reward: -3669.77). Current reward after update: -698.73, Optimal reward -196.27
Iteration 152 took 2.24 seconds (mean sampled reward: -3178.02). Current reward after update: -475.44, Optimal reward -196.27
Iteration 153 took 2.31 seconds (mean sampled reward: -3940.00). Current reward after update: -296.86, Optimal reward -196.27
Iteration 154 took 2.25 seconds (mean sampled reward: -3068.10). Current reward after update: -900.01, Optimal reward -196.27
Iteration 155 took 2.22 seconds (mean sampled reward: -2960.75). Current reward after update: -525.53, Optimal reward -196.27
Iteration 156 took 2.26 seconds (mean sampled reward: -3399.03). Current reward after update: -306.51, Optimal reward -196.27
Iteration 157 took 2.29 seconds (mean sampled reward: -3642.50). Current reward after update: -281.49, Optimal reward -196.27
Iteration 158 took 2.28 seconds (mean sampled reward: -3724.32). Current reward after update: -1466.70, Optimal reward -196.27
Iteration 159 took 2.28 seconds (mean sampled reward: -3087.98). Current reward after update: -515.75, Optimal reward -196.27
Iteration 160 took 2.29 seconds (mean sampled reward: -2475.23). Current reward after update: -438.87, Optimal reward -196.27
Iteration 161 took 2.23 seconds (mean sampled reward: -3176.32). Current reward after update: -444.67, Optimal reward -196.27
Iteration 162 took 2.21 seconds (mean sampled reward: -2771.89). Current reward after update: -594.94, Optimal reward -196.27
Iteration 163 took 2.29 seconds (mean sampled reward: -3128.73). Current reward after update: -674.67, Optimal reward -196.27
Iteration 164 took 2.22 seconds (mean sampled reward: -3028.52). Current reward after update: -439.10, Optimal reward -196.27
Iteration 165 took 2.19 seconds (mean sampled reward: -4182.51). Current reward after update: -537.28, Optimal reward -196.27
Iteration 166 took 2.20 seconds (mean sampled reward: -2662.27). Current reward after update: -598.95, Optimal reward -196.27
Iteration 167 took 2.23 seconds (mean sampled reward: -2691.60). Current reward after update: -606.57, Optimal reward -196.27
Iteration 168 took 2.18 seconds (mean sampled reward: -2358.26). Current reward after update: -641.78, Optimal reward -196.27
Iteration 169 took 2.22 seconds (mean sampled reward: -2903.27). Current reward after update: -1536.02, Optimal reward -196.27
Iteration 170 took 2.17 seconds (mean sampled reward: -2331.42). Current reward after update: -1539.11, Optimal reward -196.27
Iteration 171 took 2.15 seconds (mean sampled reward: -2373.36). Current reward after update: -1137.87, Optimal reward -196.27
Iteration 172 took 2.21 seconds (mean sampled reward: -3310.27). Current reward after update: -600.01, Optimal reward -196.27
Iteration 173 took 2.25 seconds (mean sampled reward: -3116.96). Current reward after update: -491.90, Optimal reward -196.27
Iteration 174 took 2.29 seconds (mean sampled reward: -2661.55). Current reward after update: -548.37, Optimal reward -196.27
Iteration 175 took 2.23 seconds (mean sampled reward: -2251.73). Current reward after update: -589.45, Optimal reward -196.27
Iteration 176 took 2.26 seconds (mean sampled reward: -1800.68). Current reward after update: -1474.19, Optimal reward -196.27
Iteration 177 took 2.26 seconds (mean sampled reward: -1674.09). Current reward after update: -649.43, Optimal reward -196.27
Iteration 178 took 2.25 seconds (mean sampled reward: -1822.85). Current reward after update: -1049.25, Optimal reward -196.27
Iteration 179 took 2.21 seconds (mean sampled reward: -1882.13). Current reward after update: -592.68, Optimal reward -196.27
Iteration 180 took 2.30 seconds (mean sampled reward: -1591.93). Current reward after update: -666.91, Optimal reward -196.27
Iteration 181 took 2.21 seconds (mean sampled reward: -2238.09). Current reward after update: -666.68, Optimal reward -196.27
Iteration 182 took 2.37 seconds (mean sampled reward: -3166.86). Current reward after update: -546.99, Optimal reward -196.27
Iteration 183 took 2.26 seconds (mean sampled reward: -3642.41). Current reward after update: -618.48, Optimal reward -196.27
Iteration 184 took 2.26 seconds (mean sampled reward: -3142.62). Current reward after update: -955.54, Optimal reward -196.27
Iteration 185 took 2.33 seconds (mean sampled reward: -4931.08). Current reward after update: -645.19, Optimal reward -196.27
Iteration 186 took 2.25 seconds (mean sampled reward: -4973.49). Current reward after update: -610.90, Optimal reward -196.27
Iteration 187 took 2.24 seconds (mean sampled reward: -2873.38). Current reward after update: -666.52, Optimal reward -196.27
Iteration 188 took 2.28 seconds (mean sampled reward: -4904.76). Current reward after update: -559.95, Optimal reward -196.27
Iteration 189 took 2.27 seconds (mean sampled reward: -4827.71). Current reward after update: -777.43, Optimal reward -196.27
Iteration 190 took 2.28 seconds (mean sampled reward: -3757.98). Current reward after update: -729.20, Optimal reward -196.27
Iteration 191 took 2.29 seconds (mean sampled reward: -3219.26). Current reward after update: -721.47, Optimal reward -196.27
Iteration 192 took 2.22 seconds (mean sampled reward: -2449.40). Current reward after update: -643.22, Optimal reward -196.27
Iteration 193 took 2.26 seconds (mean sampled reward: -2010.59). Current reward after update: -805.76, Optimal reward -196.27
Iteration 194 took 2.23 seconds (mean sampled reward: -2347.49). Current reward after update: -573.90, Optimal reward -196.27
Iteration 195 took 2.26 seconds (mean sampled reward: -2598.38). Current reward after update: -524.44, Optimal reward -196.27
Iteration 196 took 2.22 seconds (mean sampled reward: -2246.29). Current reward after update: -1427.21, Optimal reward -196.27
Iteration 197 took 2.31 seconds (mean sampled reward: -2103.35). Current reward after update: -638.44, Optimal reward -196.27
Iteration 198 took 2.35 seconds (mean sampled reward: -3308.98). Current reward after update: -590.86, Optimal reward -196.27
Iteration 199 took 2.27 seconds (mean sampled reward: -2655.71). Current reward after update: -575.60, Optimal reward -196.27
Iteration 200 took 2.26 seconds (mean sampled reward: -2003.01). Current reward after update: -589.51, Optimal reward -196.27
Iteration 1 took 2.38 seconds (mean sampled reward: -7521.40). Current reward after update: -5089.57, Optimal reward -5089.57
Iteration 2 took 2.38 seconds (mean sampled reward: -7050.04). Current reward after update: -4052.46, Optimal reward -4052.46
Iteration 3 took 2.30 seconds (mean sampled reward: -7047.71). Current reward after update: -3732.64, Optimal reward -3732.64
Iteration 4 took 2.44 seconds (mean sampled reward: -6913.39). Current reward after update: -3278.97, Optimal reward -3278.97
Iteration 5 took 2.42 seconds (mean sampled reward: -6633.56). Current reward after update: -2330.55, Optimal reward -2330.55
Iteration 6 took 2.46 seconds (mean sampled reward: -6193.25). Current reward after update: -2249.03, Optimal reward -2249.03
Iteration 7 took 2.40 seconds (mean sampled reward: -5681.96). Current reward after update: -1830.29, Optimal reward -1830.29
Iteration 8 took 2.42 seconds (mean sampled reward: -5840.37). Current reward after update: -1917.32, Optimal reward -1830.29
Iteration 9 took 2.37 seconds (mean sampled reward: -6553.92). Current reward after update: -1840.48, Optimal reward -1830.29
Iteration 10 took 2.27 seconds (mean sampled reward: -6262.81). Current reward after update: -1676.67, Optimal reward -1676.67
Iteration 11 took 2.46 seconds (mean sampled reward: -6486.46). Current reward after update: -2028.91, Optimal reward -1676.67
Iteration 12 took 2.32 seconds (mean sampled reward: -5695.78). Current reward after update: -1689.46, Optimal reward -1676.67
Iteration 13 took 2.36 seconds (mean sampled reward: -5633.49). Current reward after update: -1974.71, Optimal reward -1676.67
Iteration 14 took 2.24 seconds (mean sampled reward: -5970.15). Current reward after update: -1543.16, Optimal reward -1543.16
Iteration 15 took 2.54 seconds (mean sampled reward: -6549.27). Current reward after update: -1284.97, Optimal reward -1284.97
Iteration 16 took 2.43 seconds (mean sampled reward: -5415.81). Current reward after update: -6706.58, Optimal reward -1284.97
Iteration 17 took 2.40 seconds (mean sampled reward: -6143.31). Current reward after update: -1382.45, Optimal reward -1284.97
Iteration 18 took 2.34 seconds (mean sampled reward: -5135.94). Current reward after update: -833.40, Optimal reward -833.40
Iteration 19 took 2.21 seconds (mean sampled reward: -4918.30). Current reward after update: -906.77, Optimal reward -833.40
Iteration 20 took 2.34 seconds (mean sampled reward: -5628.69). Current reward after update: -1135.77, Optimal reward -833.40
Iteration 21 took 2.20 seconds (mean sampled reward: -4305.41). Current reward after update: -994.47, Optimal reward -833.40
Iteration 22 took 2.21 seconds (mean sampled reward: -4365.68). Current reward after update: -701.82, Optimal reward -701.82
Iteration 23 took 2.18 seconds (mean sampled reward: -5271.14). Current reward after update: -621.67, Optimal reward -621.67
Iteration 24 took 2.19 seconds (mean sampled reward: -4702.10). Current reward after update: -706.06, Optimal reward -621.67
Iteration 25 took 2.12 seconds (mean sampled reward: -4594.68). Current reward after update: -1132.96, Optimal reward -621.67
Iteration 26 took 2.18 seconds (mean sampled reward: -3676.73). Current reward after update: -468.08, Optimal reward -468.08
Iteration 27 took 2.19 seconds (mean sampled reward: -4216.61). Current reward after update: -600.90, Optimal reward -468.08
Iteration 28 took 2.14 seconds (mean sampled reward: -3910.39). Current reward after update: -537.27, Optimal reward -468.08
Iteration 29 took 2.27 seconds (mean sampled reward: -4007.97). Current reward after update: -897.00, Optimal reward -468.08
Iteration 30 took 2.32 seconds (mean sampled reward: -4123.97). Current reward after update: -650.05, Optimal reward -468.08
Iteration 31 took 2.23 seconds (mean sampled reward: -3717.61). Current reward after update: -1469.68, Optimal reward -468.08
Iteration 32 took 2.26 seconds (mean sampled reward: -4408.42). Current reward after update: -693.52, Optimal reward -468.08
Iteration 33 took 2.27 seconds (mean sampled reward: -4600.22). Current reward after update: -481.94, Optimal reward -468.08
Iteration 34 took 2.24 seconds (mean sampled reward: -5024.10). Current reward after update: -781.32, Optimal reward -468.08
Iteration 35 took 2.17 seconds (mean sampled reward: -4360.03). Current reward after update: -717.86, Optimal reward -468.08
Iteration 36 took 2.36 seconds (mean sampled reward: -4412.20). Current reward after update: -682.99, Optimal reward -468.08
Iteration 37 took 2.53 seconds (mean sampled reward: -4644.73). Current reward after update: -855.71, Optimal reward -468.08
Iteration 38 took 2.35 seconds (mean sampled reward: -5118.00). Current reward after update: -607.42, Optimal reward -468.08
Iteration 39 took 2.25 seconds (mean sampled reward: -4759.99). Current reward after update: -764.91, Optimal reward -468.08
Iteration 40 took 2.16 seconds (mean sampled reward: -4629.45). Current reward after update: -545.33, Optimal reward -468.08
Iteration 41 took 2.25 seconds (mean sampled reward: -4951.39). Current reward after update: -716.23, Optimal reward -468.08
Iteration 42 took 2.21 seconds (mean sampled reward: -6326.47). Current reward after update: -980.84, Optimal reward -468.08
Iteration 43 took 2.18 seconds (mean sampled reward: -5896.00). Current reward after update: -906.78, Optimal reward -468.08
Iteration 44 took 2.20 seconds (mean sampled reward: -5950.77). Current reward after update: -654.00, Optimal reward -468.08
Iteration 45 took 2.21 seconds (mean sampled reward: -5801.97). Current reward after update: -914.58, Optimal reward -468.08
Iteration 46 took 2.22 seconds (mean sampled reward: -6028.11). Current reward after update: -807.43, Optimal reward -468.08
Iteration 47 took 2.18 seconds (mean sampled reward: -5735.03). Current reward after update: -782.35, Optimal reward -468.08
Iteration 48 took 2.14 seconds (mean sampled reward: -4942.22). Current reward after update: -2475.53, Optimal reward -468.08
Iteration 49 took 2.13 seconds (mean sampled reward: -4382.22). Current reward after update: -671.48, Optimal reward -468.08
Iteration 50 took 2.18 seconds (mean sampled reward: -4139.89). Current reward after update: -710.11, Optimal reward -468.08
Iteration 51 took 2.14 seconds (mean sampled reward: -4581.20). Current reward after update: -620.49, Optimal reward -468.08
Iteration 52 took 2.14 seconds (mean sampled reward: -4222.11). Current reward after update: -658.91, Optimal reward -468.08
Iteration 53 took 2.20 seconds (mean sampled reward: -3937.20). Current reward after update: -468.68, Optimal reward -468.08
Iteration 54 took 2.33 seconds (mean sampled reward: -4185.91). Current reward after update: -566.24, Optimal reward -468.08
Iteration 55 took 2.21 seconds (mean sampled reward: -4036.80). Current reward after update: -619.04, Optimal reward -468.08
Iteration 56 took 2.56 seconds (mean sampled reward: -4128.67). Current reward after update: -477.31, Optimal reward -468.08
Iteration 57 took 2.30 seconds (mean sampled reward: -4011.19). Current reward after update: -512.65, Optimal reward -468.08
Iteration 58 took 2.30 seconds (mean sampled reward: -4740.60). Current reward after update: -661.04, Optimal reward -468.08
Iteration 59 took 2.12 seconds (mean sampled reward: -4550.05). Current reward after update: -567.05, Optimal reward -468.08
Iteration 60 took 2.19 seconds (mean sampled reward: -4464.30). Current reward after update: -913.46, Optimal reward -468.08
Iteration 61 took 2.18 seconds (mean sampled reward: -4029.14). Current reward after update: -521.66, Optimal reward -468.08
Iteration 62 took 2.17 seconds (mean sampled reward: -3853.08). Current reward after update: -857.76, Optimal reward -468.08
Iteration 63 took 2.21 seconds (mean sampled reward: -4321.73). Current reward after update: -484.68, Optimal reward -468.08
Iteration 64 took 2.20 seconds (mean sampled reward: -4204.99). Current reward after update: -510.67, Optimal reward -468.08
Iteration 65 took 2.30 seconds (mean sampled reward: -4227.95). Current reward after update: -537.97, Optimal reward -468.08
Iteration 66 took 2.26 seconds (mean sampled reward: -4194.12). Current reward after update: -597.15, Optimal reward -468.08
Iteration 67 took 2.26 seconds (mean sampled reward: -4239.53). Current reward after update: -648.55, Optimal reward -468.08
Iteration 68 took 2.27 seconds (mean sampled reward: -3939.82). Current reward after update: -569.49, Optimal reward -468.08
Iteration 69 took 2.36 seconds (mean sampled reward: -4061.53). Current reward after update: -710.78, Optimal reward -468.08
Iteration 70 took 2.15 seconds (mean sampled reward: -3807.58). Current reward after update: -732.91, Optimal reward -468.08
Iteration 71 took 2.14 seconds (mean sampled reward: -3904.23). Current reward after update: -654.64, Optimal reward -468.08
Iteration 72 took 2.26 seconds (mean sampled reward: -4174.71). Current reward after update: -565.19, Optimal reward -468.08
Iteration 73 took 2.10 seconds (mean sampled reward: -4295.67). Current reward after update: -514.43, Optimal reward -468.08
Iteration 74 took 2.10 seconds (mean sampled reward: -4419.68). Current reward after update: -549.90, Optimal reward -468.08
Iteration 75 took 2.10 seconds (mean sampled reward: -4407.65). Current reward after update: -546.95, Optimal reward -468.08
Iteration 76 took 2.09 seconds (mean sampled reward: -4076.37). Current reward after update: -528.65, Optimal reward -468.08
Iteration 77 took 2.07 seconds (mean sampled reward: -4883.16). Current reward after update: -708.41, Optimal reward -468.08
Iteration 78 took 2.13 seconds (mean sampled reward: -3919.84). Current reward after update: -512.73, Optimal reward -468.08
Iteration 79 took 2.12 seconds (mean sampled reward: -4088.68). Current reward after update: -459.26, Optimal reward -459.26
Iteration 80 took 2.14 seconds (mean sampled reward: -4247.17). Current reward after update: -1707.77, Optimal reward -459.26
Iteration 81 took 2.11 seconds (mean sampled reward: -4036.73). Current reward after update: -464.99, Optimal reward -459.26
Iteration 82 took 2.21 seconds (mean sampled reward: -4759.62). Current reward after update: -477.95, Optimal reward -459.26
Iteration 83 took 2.30 seconds (mean sampled reward: -4425.57). Current reward after update: -520.09, Optimal reward -459.26
Iteration 84 took 2.18 seconds (mean sampled reward: -4786.56). Current reward after update: -413.03, Optimal reward -413.03
Iteration 85 took 2.19 seconds (mean sampled reward: -4350.77). Current reward after update: -553.04, Optimal reward -413.03
Iteration 86 took 2.12 seconds (mean sampled reward: -5200.64). Current reward after update: -435.55, Optimal reward -413.03
Iteration 87 took 2.18 seconds (mean sampled reward: -5136.07). Current reward after update: -444.57, Optimal reward -413.03
Iteration 88 took 2.15 seconds (mean sampled reward: -5525.33). Current reward after update: -434.85, Optimal reward -413.03
Iteration 89 took 2.14 seconds (mean sampled reward: -4587.11). Current reward after update: -328.81, Optimal reward -328.81
Iteration 90 took 2.21 seconds (mean sampled reward: -4147.91). Current reward after update: -437.95, Optimal reward -328.81
Iteration 91 took 2.33 seconds (mean sampled reward: -4605.34). Current reward after update: -427.80, Optimal reward -328.81
Iteration 92 took 2.28 seconds (mean sampled reward: -3616.01). Current reward after update: -449.44, Optimal reward -328.81
Iteration 93 took 2.28 seconds (mean sampled reward: -3722.05). Current reward after update: -524.46, Optimal reward -328.81
Iteration 94 took 2.17 seconds (mean sampled reward: -3496.28). Current reward after update: -455.00, Optimal reward -328.81
Iteration 95 took 2.21 seconds (mean sampled reward: -3699.48). Current reward after update: -585.76, Optimal reward -328.81
Iteration 96 took 2.21 seconds (mean sampled reward: -3632.63). Current reward after update: -657.18, Optimal reward -328.81
Iteration 97 took 2.24 seconds (mean sampled reward: -3355.42). Current reward after update: -475.55, Optimal reward -328.81
Iteration 98 took 2.22 seconds (mean sampled reward: -4185.59). Current reward after update: -453.28, Optimal reward -328.81
Iteration 99 took 2.25 seconds (mean sampled reward: -3910.75). Current reward after update: -872.90, Optimal reward -328.81
Iteration 100 took 2.18 seconds (mean sampled reward: -4081.04). Current reward after update: -500.94, Optimal reward -328.81
Iteration 101 took 2.12 seconds (mean sampled reward: -4167.80). Current reward after update: -6339.11, Optimal reward -328.81
Iteration 102 took 2.14 seconds (mean sampled reward: -3733.60). Current reward after update: -502.29, Optimal reward -328.81
Iteration 103 took 2.12 seconds (mean sampled reward: -4699.84). Current reward after update: -531.77, Optimal reward -328.81
Iteration 104 took 2.17 seconds (mean sampled reward: -5097.19). Current reward after update: -608.15, Optimal reward -328.81
Iteration 105 took 2.13 seconds (mean sampled reward: -4656.92). Current reward after update: -482.50, Optimal reward -328.81
Iteration 106 took 2.14 seconds (mean sampled reward: -4001.85). Current reward after update: -426.18, Optimal reward -328.81
Iteration 107 took 2.16 seconds (mean sampled reward: -3865.64). Current reward after update: -657.53, Optimal reward -328.81
Iteration 108 took 2.27 seconds (mean sampled reward: -4174.21). Current reward after update: -606.48, Optimal reward -328.81
Iteration 109 took 2.11 seconds (mean sampled reward: -4022.66). Current reward after update: -446.75, Optimal reward -328.81
Iteration 110 took 2.26 seconds (mean sampled reward: -3724.29). Current reward after update: -504.78, Optimal reward -328.81
Iteration 111 took 2.24 seconds (mean sampled reward: -3715.45). Current reward after update: -531.02, Optimal reward -328.81
Iteration 112 took 2.18 seconds (mean sampled reward: -3302.93). Current reward after update: -564.22, Optimal reward -328.81
Iteration 113 took 2.14 seconds (mean sampled reward: -3069.03). Current reward after update: -542.31, Optimal reward -328.81
Iteration 114 took 2.12 seconds (mean sampled reward: -3574.17). Current reward after update: -493.96, Optimal reward -328.81
Iteration 115 took 2.22 seconds (mean sampled reward: -3441.43). Current reward after update: -3055.55, Optimal reward -328.81
Iteration 116 took 2.18 seconds (mean sampled reward: -4075.37). Current reward after update: -584.03, Optimal reward -328.81
Iteration 117 took 2.11 seconds (mean sampled reward: -3949.52). Current reward after update: -552.52, Optimal reward -328.81
Iteration 118 took 2.10 seconds (mean sampled reward: -4280.45). Current reward after update: -2689.67, Optimal reward -328.81
Iteration 119 took 2.17 seconds (mean sampled reward: -4147.70). Current reward after update: -467.25, Optimal reward -328.81
Iteration 120 took 2.26 seconds (mean sampled reward: -4267.97). Current reward after update: -553.33, Optimal reward -328.81
Iteration 121 took 2.21 seconds (mean sampled reward: -3906.72). Current reward after update: -464.52, Optimal reward -328.81
Iteration 122 took 2.11 seconds (mean sampled reward: -4455.32). Current reward after update: -456.91, Optimal reward -328.81
Iteration 123 took 2.24 seconds (mean sampled reward: -4360.45). Current reward after update: -1600.87, Optimal reward -328.81
Iteration 124 took 2.24 seconds (mean sampled reward: -3902.83). Current reward after update: -1613.33, Optimal reward -328.81
Iteration 125 took 2.17 seconds (mean sampled reward: -4087.95). Current reward after update: -493.12, Optimal reward -328.81
Iteration 126 took 2.29 seconds (mean sampled reward: -4800.22). Current reward after update: -388.97, Optimal reward -328.81
Iteration 127 took 2.35 seconds (mean sampled reward: -3806.70). Current reward after update: -2396.85, Optimal reward -328.81
Iteration 128 took 2.30 seconds (mean sampled reward: -4081.34). Current reward after update: -418.54, Optimal reward -328.81
Iteration 129 took 2.29 seconds (mean sampled reward: -4137.85). Current reward after update: -584.33, Optimal reward -328.81
Iteration 130 took 2.27 seconds (mean sampled reward: -4139.78). Current reward after update: -373.80, Optimal reward -328.81
Iteration 131 took 2.28 seconds (mean sampled reward: -3732.55). Current reward after update: -425.64, Optimal reward -328.81
Iteration 132 took 2.25 seconds (mean sampled reward: -3656.63). Current reward after update: -320.07, Optimal reward -320.07
Iteration 133 took 2.25 seconds (mean sampled reward: -2970.50). Current reward after update: -321.07, Optimal reward -320.07
Iteration 134 took 2.26 seconds (mean sampled reward: -3782.14). Current reward after update: -5152.15, Optimal reward -320.07
Iteration 135 took 2.25 seconds (mean sampled reward: -4030.90). Current reward after update: -367.08, Optimal reward -320.07
Iteration 136 took 2.36 seconds (mean sampled reward: -4140.21). Current reward after update: -463.66, Optimal reward -320.07
Iteration 137 took 2.20 seconds (mean sampled reward: -3008.79). Current reward after update: -4795.76, Optimal reward -320.07
Iteration 138 took 2.15 seconds (mean sampled reward: -2162.36). Current reward after update: -348.57, Optimal reward -320.07
Iteration 139 took 2.15 seconds (mean sampled reward: -2214.15). Current reward after update: -358.79, Optimal reward -320.07
Iteration 140 took 2.18 seconds (mean sampled reward: -2246.52). Current reward after update: -372.36, Optimal reward -320.07
Iteration 141 took 2.18 seconds (mean sampled reward: -2111.79). Current reward after update: -2177.85, Optimal reward -320.07
Iteration 142 took 2.21 seconds (mean sampled reward: -1930.08). Current reward after update: -440.87, Optimal reward -320.07
Iteration 143 took 2.14 seconds (mean sampled reward: -2274.82). Current reward after update: -486.06, Optimal reward -320.07
Iteration 144 took 2.21 seconds (mean sampled reward: -2880.52). Current reward after update: -416.57, Optimal reward -320.07
Iteration 145 took 2.15 seconds (mean sampled reward: -1988.81). Current reward after update: -1650.62, Optimal reward -320.07
Iteration 146 took 2.18 seconds (mean sampled reward: -4389.64). Current reward after update: -630.11, Optimal reward -320.07
Iteration 147 took 2.16 seconds (mean sampled reward: -3012.90). Current reward after update: -282.46, Optimal reward -282.46
Iteration 148 took 2.19 seconds (mean sampled reward: -1998.32). Current reward after update: -294.59, Optimal reward -282.46
Iteration 149 took 2.20 seconds (mean sampled reward: -2766.31). Current reward after update: -405.20, Optimal reward -282.46
Iteration 150 took 2.18 seconds (mean sampled reward: -1591.88). Current reward after update: -311.93, Optimal reward -282.46
Iteration 151 took 2.20 seconds (mean sampled reward: -1642.06). Current reward after update: -369.71, Optimal reward -282.46
Iteration 152 took 2.21 seconds (mean sampled reward: -1507.60). Current reward after update: -332.72, Optimal reward -282.46
Iteration 153 took 2.18 seconds (mean sampled reward: -1853.92). Current reward after update: -365.41, Optimal reward -282.46
Iteration 154 took 2.20 seconds (mean sampled reward: -1689.01). Current reward after update: -271.12, Optimal reward -271.12
Iteration 155 took 2.21 seconds (mean sampled reward: -1718.59). Current reward after update: -695.48, Optimal reward -271.12
Iteration 156 took 2.26 seconds (mean sampled reward: -1982.96). Current reward after update: -396.39, Optimal reward -271.12
Iteration 157 took 2.25 seconds (mean sampled reward: -2874.75). Current reward after update: -440.72, Optimal reward -271.12
Iteration 158 took 2.24 seconds (mean sampled reward: -2020.22). Current reward after update: -1767.35, Optimal reward -271.12
Iteration 159 took 2.24 seconds (mean sampled reward: -1486.57). Current reward after update: -305.00, Optimal reward -271.12
Iteration 160 took 2.42 seconds (mean sampled reward: -1790.11). Current reward after update: -345.61, Optimal reward -271.12
Iteration 161 took 2.25 seconds (mean sampled reward: -2804.58). Current reward after update: -289.97, Optimal reward -271.12
Iteration 162 took 2.27 seconds (mean sampled reward: -2679.93). Current reward after update: -258.66, Optimal reward -258.66
Iteration 163 took 2.33 seconds (mean sampled reward: -2375.21). Current reward after update: -339.52, Optimal reward -258.66
Iteration 164 took 2.18 seconds (mean sampled reward: -1972.06). Current reward after update: -559.49, Optimal reward -258.66
Iteration 165 took 2.24 seconds (mean sampled reward: -1912.31). Current reward after update: -276.58, Optimal reward -258.66
Iteration 166 took 2.17 seconds (mean sampled reward: -1975.43). Current reward after update: -470.19, Optimal reward -258.66
Iteration 167 took 2.24 seconds (mean sampled reward: -3707.93). Current reward after update: -331.19, Optimal reward -258.66
Iteration 168 took 2.22 seconds (mean sampled reward: -1970.18). Current reward after update: -1751.20, Optimal reward -258.66
Iteration 169 took 2.22 seconds (mean sampled reward: -2318.94). Current reward after update: -1244.50, Optimal reward -258.66
Iteration 170 took 2.28 seconds (mean sampled reward: -1990.55). Current reward after update: -244.53, Optimal reward -244.53
Iteration 171 took 2.30 seconds (mean sampled reward: -2872.79). Current reward after update: -338.61, Optimal reward -244.53
Iteration 172 took 2.26 seconds (mean sampled reward: -2016.63). Current reward after update: -305.03, Optimal reward -244.53
Iteration 173 took 2.24 seconds (mean sampled reward: -2161.63). Current reward after update: -381.13, Optimal reward -244.53
Iteration 174 took 2.25 seconds (mean sampled reward: -2878.83). Current reward after update: -342.34, Optimal reward -244.53
Iteration 175 took 2.27 seconds (mean sampled reward: -2531.28). Current reward after update: -245.86, Optimal reward -244.53
Iteration 176 took 2.23 seconds (mean sampled reward: -2486.89). Current reward after update: -296.91, Optimal reward -244.53
Iteration 177 took 2.21 seconds (mean sampled reward: -2414.25). Current reward after update: -418.36, Optimal reward -244.53
Iteration 178 took 2.18 seconds (mean sampled reward: -2278.19). Current reward after update: -249.40, Optimal reward -244.53
Iteration 179 took 2.27 seconds (mean sampled reward: -2839.27). Current reward after update: -264.77, Optimal reward -244.53
Iteration 180 took 2.26 seconds (mean sampled reward: -2909.56). Current reward after update: -279.86, Optimal reward -244.53
Iteration 181 took 2.25 seconds (mean sampled reward: -4362.95). Current reward after update: -261.29, Optimal reward -244.53
Iteration 182 took 2.28 seconds (mean sampled reward: -5333.09). Current reward after update: -324.19, Optimal reward -244.53
Iteration 183 took 2.23 seconds (mean sampled reward: -4229.06). Current reward after update: -6891.68, Optimal reward -244.53
Iteration 184 took 2.22 seconds (mean sampled reward: -5389.47). Current reward after update: -436.62, Optimal reward -244.53
Iteration 185 took 2.25 seconds (mean sampled reward: -4176.61). Current reward after update: -280.62, Optimal reward -244.53
Iteration 186 took 2.27 seconds (mean sampled reward: -3980.08). Current reward after update: -289.76, Optimal reward -244.53
Iteration 187 took 2.26 seconds (mean sampled reward: -4834.18). Current reward after update: -297.27, Optimal reward -244.53
Iteration 188 took 2.25 seconds (mean sampled reward: -4265.90). Current reward after update: -328.57, Optimal reward -244.53
Iteration 189 took 2.21 seconds (mean sampled reward: -3420.66). Current reward after update: -249.27, Optimal reward -244.53
Iteration 190 took 2.21 seconds (mean sampled reward: -2890.85). Current reward after update: -156.25, Optimal reward -156.25
Iteration 191 took 2.19 seconds (mean sampled reward: -2372.78). Current reward after update: -179.10, Optimal reward -156.25
Iteration 192 took 2.25 seconds (mean sampled reward: -3164.61). Current reward after update: -1442.44, Optimal reward -156.25
Iteration 193 took 2.23 seconds (mean sampled reward: -2420.11). Current reward after update: -153.05, Optimal reward -153.05
Iteration 194 took 2.22 seconds (mean sampled reward: -2462.51). Current reward after update: -265.00, Optimal reward -153.05
Iteration 195 took 2.22 seconds (mean sampled reward: -3019.43). Current reward after update: -1210.10, Optimal reward -153.05
Iteration 196 took 2.18 seconds (mean sampled reward: -2764.32). Current reward after update: -222.39, Optimal reward -153.05
Iteration 197 took 2.18 seconds (mean sampled reward: -3038.75). Current reward after update: -160.94, Optimal reward -153.05
Iteration 198 took 2.19 seconds (mean sampled reward: -3792.22). Current reward after update: -278.72, Optimal reward -153.05
Iteration 199 took 2.30 seconds (mean sampled reward: -3874.21). Current reward after update: -232.43, Optimal reward -153.05
Iteration 200 took 2.18 seconds (mean sampled reward: -3702.03). Current reward after update: -200.50, Optimal reward -153.05
Iteration 1 took 2.36 seconds (mean sampled reward: -7515.44). Current reward after update: -5503.51, Optimal reward -5503.51
Iteration 2 took 2.27 seconds (mean sampled reward: -7197.03). Current reward after update: -4476.56, Optimal reward -4476.56
Iteration 3 took 2.25 seconds (mean sampled reward: -6801.20). Current reward after update: -3579.21, Optimal reward -3579.21
Iteration 4 took 2.40 seconds (mean sampled reward: -5843.84). Current reward after update: -3130.09, Optimal reward -3130.09
Iteration 5 took 2.42 seconds (mean sampled reward: -6346.99). Current reward after update: -2852.76, Optimal reward -2852.76
Iteration 6 took 2.41 seconds (mean sampled reward: -7032.32). Current reward after update: -2873.83, Optimal reward -2852.76
Iteration 7 took 2.44 seconds (mean sampled reward: -6541.52). Current reward after update: -2841.80, Optimal reward -2841.80
Iteration 8 took 2.26 seconds (mean sampled reward: -6165.25). Current reward after update: -2731.44, Optimal reward -2731.44
Iteration 9 took 2.29 seconds (mean sampled reward: -5575.74). Current reward after update: -2267.69, Optimal reward -2267.69
Iteration 10 took 2.64 seconds (mean sampled reward: -6637.74). Current reward after update: -1667.53, Optimal reward -1667.53
Iteration 11 took 2.38 seconds (mean sampled reward: -5457.99). Current reward after update: -1589.58, Optimal reward -1589.58
Iteration 12 took 2.25 seconds (mean sampled reward: -3753.84). Current reward after update: -1375.64, Optimal reward -1375.64
Iteration 13 took 2.41 seconds (mean sampled reward: -3958.56). Current reward after update: -974.22, Optimal reward -974.22
Iteration 14 took 2.24 seconds (mean sampled reward: -5706.39). Current reward after update: -880.99, Optimal reward -880.99
Iteration 15 took 2.27 seconds (mean sampled reward: -5830.65). Current reward after update: -914.00, Optimal reward -880.99
Iteration 16 took 2.26 seconds (mean sampled reward: -3925.53). Current reward after update: -647.36, Optimal reward -647.36
Iteration 17 took 2.25 seconds (mean sampled reward: -4215.09). Current reward after update: -1590.47, Optimal reward -647.36
Iteration 18 took 2.24 seconds (mean sampled reward: -4846.91). Current reward after update: -558.29, Optimal reward -558.29
Iteration 19 took 2.37 seconds (mean sampled reward: -3504.57). Current reward after update: -2509.25, Optimal reward -558.29
Iteration 20 took 2.30 seconds (mean sampled reward: -3019.59). Current reward after update: -672.70, Optimal reward -558.29
Iteration 21 took 2.32 seconds (mean sampled reward: -5001.03). Current reward after update: -1431.28, Optimal reward -558.29
Iteration 22 took 2.25 seconds (mean sampled reward: -5719.67). Current reward after update: -827.34, Optimal reward -558.29
Iteration 23 took 2.28 seconds (mean sampled reward: -5230.91). Current reward after update: -987.94, Optimal reward -558.29
Iteration 24 took 2.32 seconds (mean sampled reward: -5123.96). Current reward after update: -644.66, Optimal reward -558.29
Iteration 25 took 2.17 seconds (mean sampled reward: -3267.30). Current reward after update: -671.53, Optimal reward -558.29
Iteration 26 took 2.19 seconds (mean sampled reward: -3048.87). Current reward after update: -594.57, Optimal reward -558.29
Iteration 27 took 2.24 seconds (mean sampled reward: -2269.34). Current reward after update: -1070.24, Optimal reward -558.29
Iteration 28 took 2.30 seconds (mean sampled reward: -2175.80). Current reward after update: -765.37, Optimal reward -558.29
Iteration 29 took 2.23 seconds (mean sampled reward: -2247.42). Current reward after update: -642.78, Optimal reward -558.29
Iteration 30 took 2.25 seconds (mean sampled reward: -3069.60). Current reward after update: -534.69, Optimal reward -534.69
Iteration 31 took 2.29 seconds (mean sampled reward: -2441.83). Current reward after update: -603.78, Optimal reward -534.69
Iteration 32 took 2.22 seconds (mean sampled reward: -2547.68). Current reward after update: -632.33, Optimal reward -534.69
Iteration 33 took 2.58 seconds (mean sampled reward: -3142.22). Current reward after update: -720.23, Optimal reward -534.69
Iteration 34 took 2.39 seconds (mean sampled reward: -3960.06). Current reward after update: -752.71, Optimal reward -534.69
Iteration 35 took 2.29 seconds (mean sampled reward: -2855.41). Current reward after update: -1542.15, Optimal reward -534.69
Iteration 36 took 2.27 seconds (mean sampled reward: -4375.83). Current reward after update: -1048.70, Optimal reward -534.69
Iteration 37 took 2.22 seconds (mean sampled reward: -3722.56). Current reward after update: -881.14, Optimal reward -534.69
Iteration 38 took 2.23 seconds (mean sampled reward: -3482.19). Current reward after update: -982.05, Optimal reward -534.69
Iteration 39 took 2.22 seconds (mean sampled reward: -3399.83). Current reward after update: -1020.72, Optimal reward -534.69
Iteration 40 took 2.32 seconds (mean sampled reward: -3479.13). Current reward after update: -711.76, Optimal reward -534.69
Iteration 41 took 2.39 seconds (mean sampled reward: -3219.38). Current reward after update: -6950.53, Optimal reward -534.69
Iteration 42 took 2.30 seconds (mean sampled reward: -3583.91). Current reward after update: -944.27, Optimal reward -534.69
Iteration 43 took 2.34 seconds (mean sampled reward: -3925.92). Current reward after update: -730.71, Optimal reward -534.69
Iteration 44 took 2.23 seconds (mean sampled reward: -3880.49). Current reward after update: -794.71, Optimal reward -534.69
Iteration 45 took 2.42 seconds (mean sampled reward: -3798.54). Current reward after update: -708.44, Optimal reward -534.69
Iteration 46 took 2.51 seconds (mean sampled reward: -3319.08). Current reward after update: -683.56, Optimal reward -534.69
Iteration 47 took 2.25 seconds (mean sampled reward: -3360.37). Current reward after update: -798.24, Optimal reward -534.69
Iteration 48 took 2.22 seconds (mean sampled reward: -3094.01). Current reward after update: -739.87, Optimal reward -534.69
Iteration 49 took 2.20 seconds (mean sampled reward: -3233.92). Current reward after update: -655.86, Optimal reward -534.69
Iteration 50 took 2.34 seconds (mean sampled reward: -3396.70). Current reward after update: -834.48, Optimal reward -534.69
Iteration 51 took 2.35 seconds (mean sampled reward: -3418.82). Current reward after update: -774.43, Optimal reward -534.69
Iteration 52 took 2.33 seconds (mean sampled reward: -3891.79). Current reward after update: -2792.70, Optimal reward -534.69
Iteration 53 took 2.44 seconds (mean sampled reward: -3843.29). Current reward after update: -674.65, Optimal reward -534.69
Iteration 54 took 2.43 seconds (mean sampled reward: -3509.45). Current reward after update: -740.85, Optimal reward -534.69
Iteration 55 took 2.38 seconds (mean sampled reward: -3266.97). Current reward after update: -759.76, Optimal reward -534.69
Iteration 56 took 2.25 seconds (mean sampled reward: -3589.55). Current reward after update: -658.70, Optimal reward -534.69
Iteration 57 took 2.36 seconds (mean sampled reward: -3682.16). Current reward after update: -669.71, Optimal reward -534.69
Iteration 58 took 2.34 seconds (mean sampled reward: -3638.07). Current reward after update: -860.05, Optimal reward -534.69
Iteration 59 took 2.28 seconds (mean sampled reward: -3071.33). Current reward after update: -550.08, Optimal reward -534.69
Iteration 60 took 2.40 seconds (mean sampled reward: -3655.30). Current reward after update: -457.99, Optimal reward -457.99
Iteration 61 took 2.50 seconds (mean sampled reward: -3563.81). Current reward after update: -505.78, Optimal reward -457.99
Iteration 62 took 2.49 seconds (mean sampled reward: -3365.29). Current reward after update: -562.29, Optimal reward -457.99
Iteration 63 took 2.47 seconds (mean sampled reward: -4219.51). Current reward after update: -553.08, Optimal reward -457.99
Iteration 64 took 2.40 seconds (mean sampled reward: -5266.86). Current reward after update: -770.28, Optimal reward -457.99
Iteration 65 took 2.37 seconds (mean sampled reward: -5205.79). Current reward after update: -491.37, Optimal reward -457.99
Iteration 66 took 2.43 seconds (mean sampled reward: -4785.89). Current reward after update: -664.27, Optimal reward -457.99
Iteration 67 took 2.44 seconds (mean sampled reward: -4653.56). Current reward after update: -378.06, Optimal reward -378.06
Iteration 68 took 2.40 seconds (mean sampled reward: -4325.16). Current reward after update: -538.48, Optimal reward -378.06
Iteration 69 took 2.45 seconds (mean sampled reward: -4341.63). Current reward after update: -773.66, Optimal reward -378.06
Iteration 70 took 2.38 seconds (mean sampled reward: -4642.53). Current reward after update: -483.86, Optimal reward -378.06
Iteration 71 took 2.46 seconds (mean sampled reward: -5210.19). Current reward after update: -429.47, Optimal reward -378.06
Iteration 72 took 2.36 seconds (mean sampled reward: -4614.81). Current reward after update: -488.00, Optimal reward -378.06
Iteration 73 took 2.36 seconds (mean sampled reward: -4786.56). Current reward after update: -527.07, Optimal reward -378.06
Iteration 74 took 2.34 seconds (mean sampled reward: -4492.91). Current reward after update: -472.73, Optimal reward -378.06
Iteration 75 took 2.27 seconds (mean sampled reward: -4421.64). Current reward after update: -2352.84, Optimal reward -378.06
Iteration 76 took 2.25 seconds (mean sampled reward: -4714.97). Current reward after update: -559.38, Optimal reward -378.06
Iteration 77 took 2.23 seconds (mean sampled reward: -5184.84). Current reward after update: -407.67, Optimal reward -378.06
Iteration 78 took 2.30 seconds (mean sampled reward: -4685.60). Current reward after update: -526.07, Optimal reward -378.06
Iteration 79 took 2.21 seconds (mean sampled reward: -5171.71). Current reward after update: -515.29, Optimal reward -378.06
Iteration 80 took 2.23 seconds (mean sampled reward: -3567.20). Current reward after update: -626.10, Optimal reward -378.06
Iteration 81 took 2.25 seconds (mean sampled reward: -5147.42). Current reward after update: -485.26, Optimal reward -378.06
Iteration 82 took 2.31 seconds (mean sampled reward: -4750.71). Current reward after update: -288.81, Optimal reward -288.81
Iteration 83 took 2.26 seconds (mean sampled reward: -4791.43). Current reward after update: -478.50, Optimal reward -288.81
Iteration 84 took 2.21 seconds (mean sampled reward: -4691.91). Current reward after update: -561.72, Optimal reward -288.81
Iteration 85 took 2.27 seconds (mean sampled reward: -4545.27). Current reward after update: -563.47, Optimal reward -288.81
Iteration 86 took 2.22 seconds (mean sampled reward: -4436.44). Current reward after update: -508.06, Optimal reward -288.81
Iteration 87 took 2.31 seconds (mean sampled reward: -3595.92). Current reward after update: -593.56, Optimal reward -288.81
Iteration 88 took 2.32 seconds (mean sampled reward: -3655.23). Current reward after update: -913.35, Optimal reward -288.81
Iteration 89 took 2.27 seconds (mean sampled reward: -4568.19). Current reward after update: -544.40, Optimal reward -288.81
Iteration 90 took 2.31 seconds (mean sampled reward: -5123.90). Current reward after update: -563.47, Optimal reward -288.81
Iteration 91 took 2.20 seconds (mean sampled reward: -4731.45). Current reward after update: -430.39, Optimal reward -288.81
Iteration 92 took 2.26 seconds (mean sampled reward: -4902.71). Current reward after update: -500.95, Optimal reward -288.81
Iteration 93 took 2.23 seconds (mean sampled reward: -3819.51). Current reward after update: -641.95, Optimal reward -288.81
Iteration 94 took 2.22 seconds (mean sampled reward: -4341.20). Current reward after update: -3374.40, Optimal reward -288.81
Iteration 95 took 2.28 seconds (mean sampled reward: -3157.35). Current reward after update: -748.20, Optimal reward -288.81
Iteration 96 took 2.34 seconds (mean sampled reward: -3591.25). Current reward after update: -445.76, Optimal reward -288.81
Iteration 97 took 2.29 seconds (mean sampled reward: -3835.24). Current reward after update: -493.20, Optimal reward -288.81
Iteration 98 took 2.35 seconds (mean sampled reward: -3907.15). Current reward after update: -1385.66, Optimal reward -288.81
Iteration 99 took 2.31 seconds (mean sampled reward: -3929.49). Current reward after update: -395.47, Optimal reward -288.81
Iteration 100 took 2.37 seconds (mean sampled reward: -4073.46). Current reward after update: -6747.72, Optimal reward -288.81
Iteration 101 took 2.35 seconds (mean sampled reward: -4392.97). Current reward after update: -1185.89, Optimal reward -288.81
Iteration 102 took 2.24 seconds (mean sampled reward: -4428.74). Current reward after update: -734.80, Optimal reward -288.81
Iteration 103 took 2.45 seconds (mean sampled reward: -4217.97). Current reward after update: -3064.82, Optimal reward -288.81
Iteration 104 took 2.29 seconds (mean sampled reward: -4261.09). Current reward after update: -667.98, Optimal reward -288.81
Iteration 105 took 2.36 seconds (mean sampled reward: -4241.03). Current reward after update: -2954.60, Optimal reward -288.81
Iteration 106 took 2.33 seconds (mean sampled reward: -4147.75). Current reward after update: -699.37, Optimal reward -288.81
Iteration 107 took 2.31 seconds (mean sampled reward: -4505.90). Current reward after update: -846.65, Optimal reward -288.81
Iteration 108 took 2.39 seconds (mean sampled reward: -4075.35). Current reward after update: -2707.96, Optimal reward -288.81
Iteration 109 took 2.28 seconds (mean sampled reward: -4266.77). Current reward after update: -836.27, Optimal reward -288.81
Iteration 110 took 2.31 seconds (mean sampled reward: -3851.67). Current reward after update: -2211.90, Optimal reward -288.81
Iteration 111 took 2.39 seconds (mean sampled reward: -4197.73). Current reward after update: -931.18, Optimal reward -288.81
Iteration 112 took 2.33 seconds (mean sampled reward: -4054.50). Current reward after update: -828.57, Optimal reward -288.81
Iteration 113 took 2.32 seconds (mean sampled reward: -4649.06). Current reward after update: -903.33, Optimal reward -288.81
Iteration 114 took 2.35 seconds (mean sampled reward: -3708.49). Current reward after update: -828.94, Optimal reward -288.81
Iteration 115 took 2.51 seconds (mean sampled reward: -4953.14). Current reward after update: -829.02, Optimal reward -288.81
Iteration 116 took 2.32 seconds (mean sampled reward: -3417.05). Current reward after update: -770.36, Optimal reward -288.81
Iteration 117 took 2.33 seconds (mean sampled reward: -5098.85). Current reward after update: -1019.28, Optimal reward -288.81
Iteration 118 took 2.33 seconds (mean sampled reward: -3529.11). Current reward after update: -754.18, Optimal reward -288.81
Iteration 119 took 2.32 seconds (mean sampled reward: -3700.27). Current reward after update: -1416.02, Optimal reward -288.81
Iteration 120 took 2.25 seconds (mean sampled reward: -2764.24). Current reward after update: -1845.56, Optimal reward -288.81
Iteration 121 took 2.25 seconds (mean sampled reward: -3124.82). Current reward after update: -653.21, Optimal reward -288.81
Iteration 122 took 2.40 seconds (mean sampled reward: -3755.91). Current reward after update: -785.48, Optimal reward -288.81
Iteration 123 took 2.32 seconds (mean sampled reward: -3354.44). Current reward after update: -734.70, Optimal reward -288.81
Iteration 124 took 2.27 seconds (mean sampled reward: -3567.78). Current reward after update: -663.03, Optimal reward -288.81
Iteration 125 took 2.30 seconds (mean sampled reward: -3608.18). Current reward after update: -585.61, Optimal reward -288.81
Iteration 126 took 2.28 seconds (mean sampled reward: -4998.60). Current reward after update: -1284.00, Optimal reward -288.81
Iteration 127 took 2.37 seconds (mean sampled reward: -5214.47). Current reward after update: -875.18, Optimal reward -288.81
Iteration 128 took 2.26 seconds (mean sampled reward: -5979.57). Current reward after update: -912.37, Optimal reward -288.81
Iteration 129 took 2.31 seconds (mean sampled reward: -5533.09). Current reward after update: -762.76, Optimal reward -288.81
Iteration 130 took 2.34 seconds (mean sampled reward: -4537.48). Current reward after update: -731.35, Optimal reward -288.81
Iteration 131 took 2.35 seconds (mean sampled reward: -3871.09). Current reward after update: -834.71, Optimal reward -288.81
Iteration 132 took 2.41 seconds (mean sampled reward: -3694.04). Current reward after update: -1995.51, Optimal reward -288.81
Iteration 133 took 2.48 seconds (mean sampled reward: -4271.27). Current reward after update: -731.63, Optimal reward -288.81
Iteration 134 took 2.34 seconds (mean sampled reward: -4128.66). Current reward after update: -623.76, Optimal reward -288.81
Iteration 135 took 2.39 seconds (mean sampled reward: -4830.21). Current reward after update: -663.97, Optimal reward -288.81
Iteration 136 took 2.38 seconds (mean sampled reward: -4634.11). Current reward after update: -1792.53, Optimal reward -288.81
Iteration 137 took 2.47 seconds (mean sampled reward: -5771.84). Current reward after update: -849.82, Optimal reward -288.81
Iteration 138 took 2.44 seconds (mean sampled reward: -5153.42). Current reward after update: -941.69, Optimal reward -288.81
Iteration 139 took 2.33 seconds (mean sampled reward: -5469.43). Current reward after update: -1554.54, Optimal reward -288.81
Iteration 140 took 2.45 seconds (mean sampled reward: -4814.25). Current reward after update: -859.34, Optimal reward -288.81
Iteration 141 took 2.39 seconds (mean sampled reward: -3877.78). Current reward after update: -729.11, Optimal reward -288.81
Iteration 142 took 2.41 seconds (mean sampled reward: -3957.11). Current reward after update: -6877.10, Optimal reward -288.81
Iteration 143 took 2.45 seconds (mean sampled reward: -4827.04). Current reward after update: -825.44, Optimal reward -288.81
Iteration 144 took 2.38 seconds (mean sampled reward: -4248.89). Current reward after update: -810.34, Optimal reward -288.81
Iteration 145 took 2.44 seconds (mean sampled reward: -4542.10). Current reward after update: -995.56, Optimal reward -288.81
Iteration 146 took 2.43 seconds (mean sampled reward: -4076.81). Current reward after update: -1297.49, Optimal reward -288.81
Iteration 147 took 2.35 seconds (mean sampled reward: -4282.78). Current reward after update: -623.08, Optimal reward -288.81
Iteration 148 took 2.40 seconds (mean sampled reward: -4519.20). Current reward after update: -6466.79, Optimal reward -288.81
Iteration 149 took 2.42 seconds (mean sampled reward: -4189.02). Current reward after update: -771.92, Optimal reward -288.81
Iteration 150 took 2.37 seconds (mean sampled reward: -4739.62). Current reward after update: -841.50, Optimal reward -288.81
Iteration 151 took 2.35 seconds (mean sampled reward: -4937.50). Current reward after update: -813.29, Optimal reward -288.81
Iteration 152 took 2.33 seconds (mean sampled reward: -4972.65). Current reward after update: -678.04, Optimal reward -288.81
Iteration 153 took 2.32 seconds (mean sampled reward: -3522.26). Current reward after update: -680.73, Optimal reward -288.81
Iteration 154 took 2.33 seconds (mean sampled reward: -3708.05). Current reward after update: -1014.84, Optimal reward -288.81
Iteration 155 took 2.36 seconds (mean sampled reward: -3957.58). Current reward after update: -646.85, Optimal reward -288.81
Iteration 156 took 2.36 seconds (mean sampled reward: -5120.57). Current reward after update: -746.86, Optimal reward -288.81
Iteration 157 took 2.41 seconds (mean sampled reward: -5262.15). Current reward after update: -629.34, Optimal reward -288.81
Iteration 158 took 2.59 seconds (mean sampled reward: -5091.18). Current reward after update: -6876.10, Optimal reward -288.81
Iteration 159 took 2.43 seconds (mean sampled reward: -5192.89). Current reward after update: -681.10, Optimal reward -288.81
Iteration 160 took 2.45 seconds (mean sampled reward: -4416.41). Current reward after update: -586.39, Optimal reward -288.81
Iteration 161 took 2.39 seconds (mean sampled reward: -5379.83). Current reward after update: -691.16, Optimal reward -288.81
Iteration 162 took 2.35 seconds (mean sampled reward: -3764.33). Current reward after update: -650.70, Optimal reward -288.81
Iteration 163 took 2.29 seconds (mean sampled reward: -2803.40). Current reward after update: -609.76, Optimal reward -288.81
Iteration 164 took 2.28 seconds (mean sampled reward: -2278.54). Current reward after update: -573.31, Optimal reward -288.81
Iteration 165 took 2.26 seconds (mean sampled reward: -2934.25). Current reward after update: -598.96, Optimal reward -288.81
Iteration 166 took 2.38 seconds (mean sampled reward: -3332.34). Current reward after update: -609.65, Optimal reward -288.81
Iteration 167 took 2.30 seconds (mean sampled reward: -4147.64). Current reward after update: -558.04, Optimal reward -288.81
Iteration 168 took 2.29 seconds (mean sampled reward: -2154.84). Current reward after update: -600.46, Optimal reward -288.81
Iteration 169 took 2.20 seconds (mean sampled reward: -2275.89). Current reward after update: -543.71, Optimal reward -288.81
Iteration 170 took 2.28 seconds (mean sampled reward: -3530.65). Current reward after update: -638.27, Optimal reward -288.81
Iteration 171 took 2.31 seconds (mean sampled reward: -3057.52). Current reward after update: -659.60, Optimal reward -288.81
Iteration 172 took 2.36 seconds (mean sampled reward: -3481.34). Current reward after update: -2864.12, Optimal reward -288.81
Iteration 173 took 2.36 seconds (mean sampled reward: -3449.32). Current reward after update: -612.25, Optimal reward -288.81
Iteration 174 took 2.35 seconds (mean sampled reward: -4243.20). Current reward after update: -651.85, Optimal reward -288.81
Iteration 175 took 2.33 seconds (mean sampled reward: -3804.23). Current reward after update: -584.09, Optimal reward -288.81
Iteration 176 took 2.34 seconds (mean sampled reward: -4631.76). Current reward after update: -1480.77, Optimal reward -288.81
Iteration 177 took 2.34 seconds (mean sampled reward: -4724.52). Current reward after update: -714.96, Optimal reward -288.81
Iteration 178 took 2.27 seconds (mean sampled reward: -5120.70). Current reward after update: -7061.96, Optimal reward -288.81
Iteration 179 took 2.26 seconds (mean sampled reward: -5477.50). Current reward after update: -782.39, Optimal reward -288.81
Iteration 180 took 2.29 seconds (mean sampled reward: -4499.20). Current reward after update: -1148.64, Optimal reward -288.81
Iteration 181 took 2.31 seconds (mean sampled reward: -4340.55). Current reward after update: -719.31, Optimal reward -288.81
Iteration 182 took 2.35 seconds (mean sampled reward: -3127.12). Current reward after update: -915.67, Optimal reward -288.81
Iteration 183 took 2.27 seconds (mean sampled reward: -3491.56). Current reward after update: -681.37, Optimal reward -288.81
Iteration 184 took 2.25 seconds (mean sampled reward: -4500.04). Current reward after update: -591.57, Optimal reward -288.81
Iteration 185 took 2.34 seconds (mean sampled reward: -3464.82). Current reward after update: -657.94, Optimal reward -288.81
Iteration 186 took 2.25 seconds (mean sampled reward: -2640.24). Current reward after update: -772.59, Optimal reward -288.81
Iteration 187 took 2.29 seconds (mean sampled reward: -4811.76). Current reward after update: -3402.37, Optimal reward -288.81
Iteration 188 took 2.26 seconds (mean sampled reward: -3836.77). Current reward after update: -632.99, Optimal reward -288.81
Iteration 189 took 2.26 seconds (mean sampled reward: -3084.78). Current reward after update: -590.16, Optimal reward -288.81
Iteration 190 took 2.28 seconds (mean sampled reward: -3119.56). Current reward after update: -822.56, Optimal reward -288.81
Iteration 191 took 2.25 seconds (mean sampled reward: -4040.49). Current reward after update: -689.28, Optimal reward -288.81
Iteration 192 took 2.39 seconds (mean sampled reward: -4265.28). Current reward after update: -617.58, Optimal reward -288.81
Iteration 193 took 2.33 seconds (mean sampled reward: -4107.70). Current reward after update: -674.78, Optimal reward -288.81
Iteration 194 took 2.30 seconds (mean sampled reward: -4065.63). Current reward after update: -681.91, Optimal reward -288.81
Iteration 195 took 2.26 seconds (mean sampled reward: -4452.24). Current reward after update: -843.60, Optimal reward -288.81
Iteration 196 took 2.31 seconds (mean sampled reward: -4659.90). Current reward after update: -601.39, Optimal reward -288.81
Iteration 197 took 2.36 seconds (mean sampled reward: -3881.76). Current reward after update: -633.94, Optimal reward -288.81
Iteration 198 took 2.27 seconds (mean sampled reward: -3817.81). Current reward after update: -524.89, Optimal reward -288.81
Iteration 199 took 2.37 seconds (mean sampled reward: -4339.76). Current reward after update: -668.40, Optimal reward -288.81
Iteration 200 took 2.37 seconds (mean sampled reward: -4400.18). Current reward after update: -625.51, Optimal reward -288.81
Max force: 30 Sigma: 0.8 mean rewards: -212.70972389819394, best rewards:-153.05203081788972

Iteration 1 took 2.40 seconds (mean sampled reward: -7628.42). Current reward after update: -7422.79, Optimal reward -7422.79
Iteration 2 took 2.33 seconds (mean sampled reward: -7615.44). Current reward after update: -7491.31, Optimal reward -7422.79
Iteration 3 took 2.22 seconds (mean sampled reward: -7619.70). Current reward after update: -7368.97, Optimal reward -7368.97
Iteration 4 took 2.41 seconds (mean sampled reward: -7620.30). Current reward after update: -7414.18, Optimal reward -7368.97
Iteration 5 took 2.27 seconds (mean sampled reward: -7627.29). Current reward after update: -7578.67, Optimal reward -7368.97
Iteration 6 took 2.29 seconds (mean sampled reward: -7596.26). Current reward after update: -7576.84, Optimal reward -7368.97
Iteration 7 took 2.22 seconds (mean sampled reward: -7560.80). Current reward after update: -7246.51, Optimal reward -7246.51
Iteration 8 took 2.18 seconds (mean sampled reward: -7577.28). Current reward after update: -7239.96, Optimal reward -7239.96
Iteration 9 took 2.28 seconds (mean sampled reward: -7565.07). Current reward after update: -7200.49, Optimal reward -7200.49
Iteration 10 took 2.35 seconds (mean sampled reward: -7596.37). Current reward after update: -7134.20, Optimal reward -7134.20
Iteration 11 took 2.21 seconds (mean sampled reward: -7589.04). Current reward after update: -7247.37, Optimal reward -7134.20
Iteration 12 took 2.40 seconds (mean sampled reward: -7615.93). Current reward after update: -7209.76, Optimal reward -7134.20
Iteration 13 took 2.21 seconds (mean sampled reward: -7558.59). Current reward after update: -7141.69, Optimal reward -7134.20
Iteration 14 took 2.36 seconds (mean sampled reward: -7563.03). Current reward after update: -7266.08, Optimal reward -7134.20
Iteration 15 took 2.14 seconds (mean sampled reward: -7612.10). Current reward after update: -7196.19, Optimal reward -7134.20
Iteration 16 took 2.39 seconds (mean sampled reward: -7583.07). Current reward after update: -7189.57, Optimal reward -7134.20
Iteration 17 took 2.18 seconds (mean sampled reward: -7586.89). Current reward after update: -7250.29, Optimal reward -7134.20
Iteration 18 took 2.33 seconds (mean sampled reward: -7568.61). Current reward after update: -7189.62, Optimal reward -7134.20
Iteration 19 took 2.17 seconds (mean sampled reward: -7569.29). Current reward after update: -7215.09, Optimal reward -7134.20
Iteration 20 took 2.18 seconds (mean sampled reward: -7518.56). Current reward after update: -7123.03, Optimal reward -7123.03
Iteration 21 took 2.31 seconds (mean sampled reward: -7537.26). Current reward after update: -7181.04, Optimal reward -7123.03
Iteration 22 took 2.13 seconds (mean sampled reward: -7567.47). Current reward after update: -7614.49, Optimal reward -7123.03
Iteration 23 took 2.23 seconds (mean sampled reward: -7584.45). Current reward after update: -7136.01, Optimal reward -7123.03
Iteration 24 took 2.15 seconds (mean sampled reward: -7583.24). Current reward after update: -7184.02, Optimal reward -7123.03
Iteration 25 took 2.11 seconds (mean sampled reward: -7587.94). Current reward after update: -7278.89, Optimal reward -7123.03
Iteration 26 took 2.15 seconds (mean sampled reward: -7591.88). Current reward after update: -7085.92, Optimal reward -7085.92
Iteration 27 took 2.17 seconds (mean sampled reward: -7596.89). Current reward after update: -7138.37, Optimal reward -7085.92
Iteration 28 took 2.28 seconds (mean sampled reward: -7589.78). Current reward after update: -7258.52, Optimal reward -7085.92
Iteration 29 took 2.24 seconds (mean sampled reward: -7592.41). Current reward after update: -7206.38, Optimal reward -7085.92
Iteration 30 took 2.24 seconds (mean sampled reward: -7597.34). Current reward after update: -7457.70, Optimal reward -7085.92
Iteration 31 took 2.25 seconds (mean sampled reward: -7591.58). Current reward after update: -7143.88, Optimal reward -7085.92
Iteration 32 took 2.27 seconds (mean sampled reward: -7571.86). Current reward after update: -7106.71, Optimal reward -7085.92
Iteration 33 took 2.20 seconds (mean sampled reward: -7601.86). Current reward after update: -7128.30, Optimal reward -7085.92
Iteration 34 took 2.37 seconds (mean sampled reward: -7573.83). Current reward after update: -7093.19, Optimal reward -7085.92
Iteration 35 took 2.26 seconds (mean sampled reward: -7575.53). Current reward after update: -7151.19, Optimal reward -7085.92
Iteration 36 took 2.20 seconds (mean sampled reward: -7562.32). Current reward after update: -7087.16, Optimal reward -7085.92
Iteration 37 took 2.16 seconds (mean sampled reward: -7550.91). Current reward after update: -7150.25, Optimal reward -7085.92
Iteration 38 took 2.19 seconds (mean sampled reward: -7522.01). Current reward after update: -7138.22, Optimal reward -7085.92
Iteration 39 took 2.31 seconds (mean sampled reward: -7566.13). Current reward after update: -7158.44, Optimal reward -7085.92
Iteration 40 took 2.22 seconds (mean sampled reward: -7583.31). Current reward after update: -7159.78, Optimal reward -7085.92
Iteration 41 took 2.12 seconds (mean sampled reward: -7549.57). Current reward after update: -7056.94, Optimal reward -7056.94
Iteration 42 took 2.11 seconds (mean sampled reward: -7565.34). Current reward after update: -7250.28, Optimal reward -7056.94
Iteration 43 took 2.12 seconds (mean sampled reward: -7562.11). Current reward after update: -7264.14, Optimal reward -7056.94
Iteration 44 took 2.18 seconds (mean sampled reward: -7565.05). Current reward after update: -7251.73, Optimal reward -7056.94
Iteration 45 took 2.18 seconds (mean sampled reward: -7570.53). Current reward after update: -7116.97, Optimal reward -7056.94
Iteration 46 took 2.13 seconds (mean sampled reward: -7538.24). Current reward after update: -7583.25, Optimal reward -7056.94
Iteration 47 took 2.12 seconds (mean sampled reward: -7536.73). Current reward after update: -7075.70, Optimal reward -7056.94
Iteration 48 took 2.16 seconds (mean sampled reward: -7544.37). Current reward after update: -7136.89, Optimal reward -7056.94
Iteration 49 took 2.15 seconds (mean sampled reward: -7548.07). Current reward after update: -7196.83, Optimal reward -7056.94
Iteration 50 took 2.17 seconds (mean sampled reward: -7571.62). Current reward after update: -7576.16, Optimal reward -7056.94
Iteration 51 took 2.17 seconds (mean sampled reward: -7570.15). Current reward after update: -7151.14, Optimal reward -7056.94
Iteration 52 took 2.27 seconds (mean sampled reward: -7548.80). Current reward after update: -7133.57, Optimal reward -7056.94
Iteration 53 took 2.26 seconds (mean sampled reward: -7546.14). Current reward after update: -7104.00, Optimal reward -7056.94
Iteration 54 took 2.25 seconds (mean sampled reward: -7529.42). Current reward after update: -7049.95, Optimal reward -7049.95
Iteration 55 took 2.20 seconds (mean sampled reward: -7536.37). Current reward after update: -7081.51, Optimal reward -7049.95
Iteration 56 took 2.29 seconds (mean sampled reward: -7549.79). Current reward after update: -7106.23, Optimal reward -7049.95
Iteration 57 took 2.47 seconds (mean sampled reward: -7550.55). Current reward after update: -7556.21, Optimal reward -7049.95
Iteration 58 took 2.38 seconds (mean sampled reward: -7534.68). Current reward after update: -7137.17, Optimal reward -7049.95
Iteration 59 took 2.40 seconds (mean sampled reward: -7537.69). Current reward after update: -7078.46, Optimal reward -7049.95
Iteration 60 took 2.24 seconds (mean sampled reward: -7528.02). Current reward after update: -7199.89, Optimal reward -7049.95
Iteration 61 took 2.29 seconds (mean sampled reward: -7531.65). Current reward after update: -7129.55, Optimal reward -7049.95
Iteration 62 took 2.33 seconds (mean sampled reward: -7527.74). Current reward after update: -7064.46, Optimal reward -7049.95
Iteration 63 took 2.23 seconds (mean sampled reward: -7565.42). Current reward after update: -7176.25, Optimal reward -7049.95
Iteration 64 took 2.23 seconds (mean sampled reward: -7580.21). Current reward after update: -7074.29, Optimal reward -7049.95
Iteration 65 took 2.30 seconds (mean sampled reward: -7559.14). Current reward after update: -7132.93, Optimal reward -7049.95
Iteration 66 took 2.24 seconds (mean sampled reward: -7535.96). Current reward after update: -7090.84, Optimal reward -7049.95
Iteration 67 took 2.23 seconds (mean sampled reward: -7537.88). Current reward after update: -7140.06, Optimal reward -7049.95
Iteration 68 took 2.40 seconds (mean sampled reward: -7540.56). Current reward after update: -7138.96, Optimal reward -7049.95
Iteration 69 took 2.28 seconds (mean sampled reward: -7541.40). Current reward after update: -7160.64, Optimal reward -7049.95
Iteration 70 took 2.33 seconds (mean sampled reward: -7564.07). Current reward after update: -7207.04, Optimal reward -7049.95
Iteration 71 took 2.25 seconds (mean sampled reward: -7550.06). Current reward after update: -7147.86, Optimal reward -7049.95
Iteration 72 took 2.29 seconds (mean sampled reward: -7532.82). Current reward after update: -7147.36, Optimal reward -7049.95
Iteration 73 took 2.21 seconds (mean sampled reward: -7528.60). Current reward after update: -7137.20, Optimal reward -7049.95
Iteration 74 took 2.31 seconds (mean sampled reward: -7554.00). Current reward after update: -7156.61, Optimal reward -7049.95
Iteration 75 took 2.31 seconds (mean sampled reward: -7506.10). Current reward after update: -7064.75, Optimal reward -7049.95
Iteration 76 took 2.36 seconds (mean sampled reward: -7543.40). Current reward after update: -7097.76, Optimal reward -7049.95
Iteration 77 took 2.29 seconds (mean sampled reward: -7526.54). Current reward after update: -7404.49, Optimal reward -7049.95
Iteration 78 took 2.41 seconds (mean sampled reward: -7530.64). Current reward after update: -7575.61, Optimal reward -7049.95
Iteration 79 took 2.28 seconds (mean sampled reward: -7561.24). Current reward after update: -7560.83, Optimal reward -7049.95
Iteration 80 took 2.43 seconds (mean sampled reward: -7554.85). Current reward after update: -7039.42, Optimal reward -7039.42
Iteration 81 took 2.36 seconds (mean sampled reward: -7543.96). Current reward after update: -7156.59, Optimal reward -7039.42
Iteration 82 took 2.28 seconds (mean sampled reward: -7531.61). Current reward after update: -7573.93, Optimal reward -7039.42
Iteration 83 took 2.33 seconds (mean sampled reward: -7555.57). Current reward after update: -7111.50, Optimal reward -7039.42
Iteration 84 took 2.20 seconds (mean sampled reward: -7561.54). Current reward after update: -7096.49, Optimal reward -7039.42
Iteration 85 took 2.26 seconds (mean sampled reward: -7518.54). Current reward after update: -6949.56, Optimal reward -6949.56
Iteration 86 took 2.23 seconds (mean sampled reward: -7330.15). Current reward after update: -6724.65, Optimal reward -6724.65
Iteration 87 took 2.21 seconds (mean sampled reward: -7283.13). Current reward after update: -6986.37, Optimal reward -6724.65
Iteration 88 took 2.25 seconds (mean sampled reward: -7174.64). Current reward after update: -6750.44, Optimal reward -6724.65
Iteration 89 took 2.19 seconds (mean sampled reward: -7021.78). Current reward after update: -6513.01, Optimal reward -6513.01
Iteration 90 took 2.30 seconds (mean sampled reward: -7102.12). Current reward after update: -6146.08, Optimal reward -6146.08
Iteration 91 took 2.19 seconds (mean sampled reward: -6913.43). Current reward after update: -6113.81, Optimal reward -6113.81
Iteration 92 took 2.23 seconds (mean sampled reward: -6810.71). Current reward after update: -6051.80, Optimal reward -6051.80
Iteration 93 took 2.40 seconds (mean sampled reward: -6645.93). Current reward after update: -5637.29, Optimal reward -5637.29
Iteration 94 took 2.36 seconds (mean sampled reward: -6526.15). Current reward after update: -5466.73, Optimal reward -5466.73
Iteration 95 took 2.54 seconds (mean sampled reward: -6282.36). Current reward after update: -4675.47, Optimal reward -4675.47
Iteration 96 took 2.52 seconds (mean sampled reward: -6194.86). Current reward after update: -3951.21, Optimal reward -3951.21
Iteration 97 took 2.54 seconds (mean sampled reward: -6367.05). Current reward after update: -3728.48, Optimal reward -3728.48
Iteration 98 took 2.54 seconds (mean sampled reward: -6033.18). Current reward after update: -3877.54, Optimal reward -3728.48
Iteration 99 took 2.38 seconds (mean sampled reward: -6163.80). Current reward after update: -3740.11, Optimal reward -3728.48
Iteration 100 took 2.44 seconds (mean sampled reward: -6371.44). Current reward after update: -4007.13, Optimal reward -3728.48
Iteration 101 took 2.33 seconds (mean sampled reward: -6136.50). Current reward after update: -3717.83, Optimal reward -3717.83
Iteration 102 took 2.40 seconds (mean sampled reward: -5820.74). Current reward after update: -3665.04, Optimal reward -3665.04
Iteration 103 took 2.38 seconds (mean sampled reward: -5966.34). Current reward after update: -3663.03, Optimal reward -3663.03
Iteration 104 took 2.34 seconds (mean sampled reward: -5949.91). Current reward after update: -4435.12, Optimal reward -3663.03
Iteration 105 took 2.36 seconds (mean sampled reward: -5391.66). Current reward after update: -3637.04, Optimal reward -3637.04
Iteration 106 took 2.36 seconds (mean sampled reward: -5138.11). Current reward after update: -4274.50, Optimal reward -3637.04
Iteration 107 took 2.29 seconds (mean sampled reward: -5975.46). Current reward after update: -3816.47, Optimal reward -3637.04
Iteration 108 took 2.52 seconds (mean sampled reward: -5845.38). Current reward after update: -6197.55, Optimal reward -3637.04
Iteration 109 took 2.33 seconds (mean sampled reward: -5935.60). Current reward after update: -3628.46, Optimal reward -3628.46
Iteration 110 took 2.36 seconds (mean sampled reward: -6095.90). Current reward after update: -3434.04, Optimal reward -3434.04
Iteration 111 took 2.51 seconds (mean sampled reward: -6240.65). Current reward after update: -3600.70, Optimal reward -3434.04
Iteration 112 took 2.47 seconds (mean sampled reward: -5609.91). Current reward after update: -4165.91, Optimal reward -3434.04
Iteration 113 took 2.43 seconds (mean sampled reward: -5799.60). Current reward after update: -3523.40, Optimal reward -3434.04
Iteration 114 took 2.50 seconds (mean sampled reward: -5829.46). Current reward after update: -3466.27, Optimal reward -3434.04
Iteration 115 took 2.60 seconds (mean sampled reward: -5787.06). Current reward after update: -3539.01, Optimal reward -3434.04
Iteration 116 took 2.32 seconds (mean sampled reward: -5735.29). Current reward after update: -3425.85, Optimal reward -3425.85
Iteration 117 took 2.33 seconds (mean sampled reward: -5668.81). Current reward after update: -2934.08, Optimal reward -2934.08
Iteration 118 took 2.28 seconds (mean sampled reward: -5956.82). Current reward after update: -4371.09, Optimal reward -2934.08
Iteration 119 took 2.39 seconds (mean sampled reward: -5717.68). Current reward after update: -2960.25, Optimal reward -2934.08
Iteration 120 took 2.34 seconds (mean sampled reward: -5962.42). Current reward after update: -3193.83, Optimal reward -2934.08
Iteration 121 took 2.35 seconds (mean sampled reward: -5681.31). Current reward after update: -3155.52, Optimal reward -2934.08
Iteration 122 took 2.35 seconds (mean sampled reward: -4870.47). Current reward after update: -3239.26, Optimal reward -2934.08
Iteration 123 took 2.27 seconds (mean sampled reward: -5433.38). Current reward after update: -3171.04, Optimal reward -2934.08
Iteration 124 took 2.30 seconds (mean sampled reward: -5886.96). Current reward after update: -3308.96, Optimal reward -2934.08
Iteration 125 took 2.36 seconds (mean sampled reward: -5899.82). Current reward after update: -3115.13, Optimal reward -2934.08
Iteration 126 took 2.34 seconds (mean sampled reward: -5957.68). Current reward after update: -3157.04, Optimal reward -2934.08
Iteration 127 took 2.47 seconds (mean sampled reward: -5992.20). Current reward after update: -2981.79, Optimal reward -2934.08
Iteration 128 took 2.42 seconds (mean sampled reward: -6509.01). Current reward after update: -3757.82, Optimal reward -2934.08
Iteration 129 took 2.25 seconds (mean sampled reward: -6361.41). Current reward after update: -3218.78, Optimal reward -2934.08
Iteration 130 took 2.26 seconds (mean sampled reward: -6278.36). Current reward after update: -3116.59, Optimal reward -2934.08
Iteration 131 took 2.32 seconds (mean sampled reward: -6753.42). Current reward after update: -3189.71, Optimal reward -2934.08
Iteration 132 took 2.30 seconds (mean sampled reward: -6434.37). Current reward after update: -2978.10, Optimal reward -2934.08
Iteration 133 took 2.33 seconds (mean sampled reward: -5343.33). Current reward after update: -3243.17, Optimal reward -2934.08
Iteration 134 took 2.31 seconds (mean sampled reward: -4810.03). Current reward after update: -2807.72, Optimal reward -2807.72
Iteration 135 took 2.33 seconds (mean sampled reward: -5118.74). Current reward after update: -2541.95, Optimal reward -2541.95
Iteration 136 took 2.31 seconds (mean sampled reward: -5322.59). Current reward after update: -2586.07, Optimal reward -2541.95
Iteration 137 took 2.39 seconds (mean sampled reward: -6069.67). Current reward after update: -2705.36, Optimal reward -2541.95
Iteration 138 took 2.25 seconds (mean sampled reward: -5927.42). Current reward after update: -2651.19, Optimal reward -2541.95
Iteration 139 took 2.32 seconds (mean sampled reward: -5869.67). Current reward after update: -6420.39, Optimal reward -2541.95
Iteration 140 took 2.27 seconds (mean sampled reward: -5950.65). Current reward after update: -2688.28, Optimal reward -2541.95
Iteration 141 took 2.29 seconds (mean sampled reward: -6191.49). Current reward after update: -2786.07, Optimal reward -2541.95
Iteration 142 took 2.28 seconds (mean sampled reward: -5709.11). Current reward after update: -2745.25, Optimal reward -2541.95
Iteration 143 took 2.33 seconds (mean sampled reward: -5690.64). Current reward after update: -2994.74, Optimal reward -2541.95
Iteration 144 took 2.31 seconds (mean sampled reward: -5902.79). Current reward after update: -2525.08, Optimal reward -2525.08
Iteration 145 took 2.32 seconds (mean sampled reward: -5553.03). Current reward after update: -2448.36, Optimal reward -2448.36
Iteration 146 took 2.23 seconds (mean sampled reward: -5518.88). Current reward after update: -2253.12, Optimal reward -2253.12
Iteration 147 took 2.30 seconds (mean sampled reward: -5215.49). Current reward after update: -4068.13, Optimal reward -2253.12
Iteration 148 took 2.31 seconds (mean sampled reward: -4820.50). Current reward after update: -2429.06, Optimal reward -2253.12
Iteration 149 took 2.24 seconds (mean sampled reward: -4653.20). Current reward after update: -2474.35, Optimal reward -2253.12
Iteration 150 took 2.34 seconds (mean sampled reward: -5603.92). Current reward after update: -2533.83, Optimal reward -2253.12
Iteration 151 took 2.24 seconds (mean sampled reward: -5191.95). Current reward after update: -2645.29, Optimal reward -2253.12
Iteration 152 took 2.32 seconds (mean sampled reward: -5646.06). Current reward after update: -2469.48, Optimal reward -2253.12
Iteration 153 took 2.24 seconds (mean sampled reward: -5656.16). Current reward after update: -2197.27, Optimal reward -2197.27
Iteration 154 took 2.24 seconds (mean sampled reward: -4861.89). Current reward after update: -2561.02, Optimal reward -2197.27
Iteration 155 took 2.24 seconds (mean sampled reward: -4774.29). Current reward after update: -2583.94, Optimal reward -2197.27
Iteration 156 took 2.25 seconds (mean sampled reward: -4952.77). Current reward after update: -2532.33, Optimal reward -2197.27
Iteration 157 took 2.33 seconds (mean sampled reward: -5903.30). Current reward after update: -2271.52, Optimal reward -2197.27
Iteration 158 took 2.39 seconds (mean sampled reward: -5517.85). Current reward after update: -2184.87, Optimal reward -2184.87
Iteration 159 took 2.29 seconds (mean sampled reward: -5228.45). Current reward after update: -2561.91, Optimal reward -2184.87
Iteration 160 took 2.32 seconds (mean sampled reward: -5095.41). Current reward after update: -2022.29, Optimal reward -2022.29
Iteration 161 took 2.31 seconds (mean sampled reward: -4994.72). Current reward after update: -2907.70, Optimal reward -2022.29
Iteration 162 took 2.33 seconds (mean sampled reward: -4914.50). Current reward after update: -2243.14, Optimal reward -2022.29
Iteration 163 took 2.32 seconds (mean sampled reward: -4511.96). Current reward after update: -2038.09, Optimal reward -2022.29
Iteration 164 took 2.32 seconds (mean sampled reward: -4549.66). Current reward after update: -2050.26, Optimal reward -2022.29
Iteration 165 took 2.40 seconds (mean sampled reward: -4624.66). Current reward after update: -2026.19, Optimal reward -2022.29
Iteration 166 took 2.38 seconds (mean sampled reward: -4622.11). Current reward after update: -3086.77, Optimal reward -2022.29
Iteration 167 took 2.39 seconds (mean sampled reward: -4528.68). Current reward after update: -1906.87, Optimal reward -1906.87
Iteration 168 took 2.31 seconds (mean sampled reward: -4500.04). Current reward after update: -2060.40, Optimal reward -1906.87
Iteration 169 took 2.37 seconds (mean sampled reward: -4874.72). Current reward after update: -2028.71, Optimal reward -1906.87
Iteration 170 took 2.36 seconds (mean sampled reward: -4734.46). Current reward after update: -1965.41, Optimal reward -1906.87
Iteration 171 took 2.36 seconds (mean sampled reward: -4613.44). Current reward after update: -1926.11, Optimal reward -1906.87
Iteration 172 took 2.40 seconds (mean sampled reward: -4956.39). Current reward after update: -1909.52, Optimal reward -1906.87
Iteration 173 took 2.35 seconds (mean sampled reward: -5578.95). Current reward after update: -1942.85, Optimal reward -1906.87
Iteration 174 took 2.30 seconds (mean sampled reward: -5641.43). Current reward after update: -2169.94, Optimal reward -1906.87
Iteration 175 took 2.32 seconds (mean sampled reward: -5727.39). Current reward after update: -2062.47, Optimal reward -1906.87
Iteration 176 took 2.29 seconds (mean sampled reward: -5491.27). Current reward after update: -1959.53, Optimal reward -1906.87
Iteration 177 took 2.31 seconds (mean sampled reward: -5789.73). Current reward after update: -2531.16, Optimal reward -1906.87
Iteration 178 took 2.29 seconds (mean sampled reward: -5768.11). Current reward after update: -2893.64, Optimal reward -1906.87
Iteration 179 took 2.29 seconds (mean sampled reward: -5620.60). Current reward after update: -2310.93, Optimal reward -1906.87
Iteration 180 took 2.27 seconds (mean sampled reward: -5535.01). Current reward after update: -2107.42, Optimal reward -1906.87
Iteration 181 took 2.32 seconds (mean sampled reward: -4697.26). Current reward after update: -1874.76, Optimal reward -1874.76
Iteration 182 took 2.33 seconds (mean sampled reward: -5389.13). Current reward after update: -1969.06, Optimal reward -1874.76
Iteration 183 took 2.24 seconds (mean sampled reward: -4544.88). Current reward after update: -1879.01, Optimal reward -1874.76
Iteration 184 took 2.26 seconds (mean sampled reward: -4674.21). Current reward after update: -6251.20, Optimal reward -1874.76
Iteration 185 took 2.33 seconds (mean sampled reward: -5219.79). Current reward after update: -2149.58, Optimal reward -1874.76
Iteration 186 took 2.28 seconds (mean sampled reward: -4515.81). Current reward after update: -1843.31, Optimal reward -1843.31
Iteration 187 took 2.32 seconds (mean sampled reward: -4805.69). Current reward after update: -1932.40, Optimal reward -1843.31
Iteration 188 took 2.29 seconds (mean sampled reward: -5017.48). Current reward after update: -6424.59, Optimal reward -1843.31
Iteration 189 took 2.30 seconds (mean sampled reward: -5821.41). Current reward after update: -2018.78, Optimal reward -1843.31
Iteration 190 took 2.24 seconds (mean sampled reward: -5399.36). Current reward after update: -2025.92, Optimal reward -1843.31
Iteration 191 took 2.26 seconds (mean sampled reward: -5796.52). Current reward after update: -2117.36, Optimal reward -1843.31
Iteration 192 took 2.29 seconds (mean sampled reward: -5932.42). Current reward after update: -6493.01, Optimal reward -1843.31
Iteration 193 took 2.32 seconds (mean sampled reward: -5965.36). Current reward after update: -2315.32, Optimal reward -1843.31
Iteration 194 took 2.24 seconds (mean sampled reward: -5302.86). Current reward after update: -2239.37, Optimal reward -1843.31
Iteration 195 took 2.26 seconds (mean sampled reward: -4883.35). Current reward after update: -2319.11, Optimal reward -1843.31
Iteration 196 took 2.27 seconds (mean sampled reward: -4797.93). Current reward after update: -2130.58, Optimal reward -1843.31
Iteration 197 took 2.27 seconds (mean sampled reward: -5189.96). Current reward after update: -2254.45, Optimal reward -1843.31
Iteration 198 took 2.28 seconds (mean sampled reward: -4772.06). Current reward after update: -2159.98, Optimal reward -1843.31
Iteration 199 took 2.24 seconds (mean sampled reward: -4574.48). Current reward after update: -2376.72, Optimal reward -1843.31
Iteration 200 took 2.32 seconds (mean sampled reward: -5327.78). Current reward after update: -2793.36, Optimal reward -1843.31
Iteration 1 took 2.38 seconds (mean sampled reward: -7625.86). Current reward after update: -7338.84, Optimal reward -7338.84
Iteration 2 took 2.32 seconds (mean sampled reward: -7636.00). Current reward after update: -7332.58, Optimal reward -7332.58
Iteration 3 took 2.40 seconds (mean sampled reward: -7592.36). Current reward after update: -7599.96, Optimal reward -7332.58
Iteration 4 took 2.37 seconds (mean sampled reward: -7619.12). Current reward after update: -7630.33, Optimal reward -7332.58
Iteration 5 took 2.26 seconds (mean sampled reward: -7616.47). Current reward after update: -7635.27, Optimal reward -7332.58
Iteration 6 took 2.40 seconds (mean sampled reward: -7594.77). Current reward after update: -6548.28, Optimal reward -6548.28
Iteration 7 took 2.43 seconds (mean sampled reward: -7247.26). Current reward after update: -6212.36, Optimal reward -6212.36
Iteration 8 took 2.55 seconds (mean sampled reward: -6990.37). Current reward after update: -6114.87, Optimal reward -6114.87
Iteration 9 took 2.33 seconds (mean sampled reward: -7024.72). Current reward after update: -6090.03, Optimal reward -6090.03
Iteration 10 took 2.43 seconds (mean sampled reward: -6953.92). Current reward after update: -6055.41, Optimal reward -6055.41
Iteration 11 took 2.38 seconds (mean sampled reward: -6762.56). Current reward after update: -5877.42, Optimal reward -5877.42
Iteration 12 took 2.47 seconds (mean sampled reward: -6979.61). Current reward after update: -5839.79, Optimal reward -5839.79
Iteration 13 took 2.39 seconds (mean sampled reward: -6739.79). Current reward after update: -5705.69, Optimal reward -5705.69
Iteration 14 took 2.41 seconds (mean sampled reward: -6761.55). Current reward after update: -5592.39, Optimal reward -5592.39
Iteration 15 took 2.86 seconds (mean sampled reward: -6615.01). Current reward after update: -5434.18, Optimal reward -5434.18
Iteration 16 took 2.42 seconds (mean sampled reward: -6156.88). Current reward after update: -5363.84, Optimal reward -5363.84
Iteration 17 took 2.35 seconds (mean sampled reward: -6160.80). Current reward after update: -5611.22, Optimal reward -5363.84
Iteration 18 took 2.46 seconds (mean sampled reward: -5916.31). Current reward after update: -5300.41, Optimal reward -5300.41
Iteration 19 took 2.51 seconds (mean sampled reward: -5689.60). Current reward after update: -5194.93, Optimal reward -5194.93
Iteration 20 took 2.41 seconds (mean sampled reward: -5687.24). Current reward after update: -5658.40, Optimal reward -5194.93
Iteration 21 took 2.38 seconds (mean sampled reward: -5894.58). Current reward after update: -5121.36, Optimal reward -5121.36
Iteration 22 took 2.43 seconds (mean sampled reward: -5926.94). Current reward after update: -5210.87, Optimal reward -5121.36
Iteration 23 took 2.48 seconds (mean sampled reward: -5854.06). Current reward after update: -5153.34, Optimal reward -5121.36
Iteration 24 took 2.31 seconds (mean sampled reward: -5877.18). Current reward after update: -4793.56, Optimal reward -4793.56
Iteration 25 took 2.42 seconds (mean sampled reward: -5913.62). Current reward after update: -4313.95, Optimal reward -4313.95
Iteration 26 took 2.33 seconds (mean sampled reward: -5608.75). Current reward after update: -4200.92, Optimal reward -4200.92
Iteration 27 took 2.40 seconds (mean sampled reward: -5778.02). Current reward after update: -4364.56, Optimal reward -4200.92
Iteration 28 took 2.38 seconds (mean sampled reward: -5813.87). Current reward after update: -4165.24, Optimal reward -4165.24
Iteration 29 took 2.33 seconds (mean sampled reward: -5939.55). Current reward after update: -4118.08, Optimal reward -4118.08
Iteration 30 took 2.47 seconds (mean sampled reward: -5325.09). Current reward after update: -4055.88, Optimal reward -4055.88
Iteration 31 took 2.44 seconds (mean sampled reward: -5491.71). Current reward after update: -4068.10, Optimal reward -4055.88
Iteration 32 took 2.61 seconds (mean sampled reward: -5651.74). Current reward after update: -4087.47, Optimal reward -4055.88
Iteration 33 took 2.49 seconds (mean sampled reward: -5174.55). Current reward after update: -4017.08, Optimal reward -4017.08
Iteration 34 took 2.36 seconds (mean sampled reward: -4949.86). Current reward after update: -4368.53, Optimal reward -4017.08
Iteration 35 took 2.41 seconds (mean sampled reward: -5079.39). Current reward after update: -4160.24, Optimal reward -4017.08
Iteration 36 took 2.36 seconds (mean sampled reward: -4908.35). Current reward after update: -3940.93, Optimal reward -3940.93
Iteration 37 took 2.33 seconds (mean sampled reward: -5006.43). Current reward after update: -3938.25, Optimal reward -3938.25
Iteration 38 took 2.39 seconds (mean sampled reward: -4937.99). Current reward after update: -3927.24, Optimal reward -3927.24
Iteration 39 took 2.49 seconds (mean sampled reward: -4928.18). Current reward after update: -3930.45, Optimal reward -3927.24
Iteration 40 took 2.36 seconds (mean sampled reward: -5003.41). Current reward after update: -4001.13, Optimal reward -3927.24
Iteration 41 took 2.39 seconds (mean sampled reward: -4797.96). Current reward after update: -3850.16, Optimal reward -3850.16
Iteration 42 took 2.43 seconds (mean sampled reward: -4908.84). Current reward after update: -3896.91, Optimal reward -3850.16
Iteration 43 took 2.43 seconds (mean sampled reward: -4831.89). Current reward after update: -3928.72, Optimal reward -3850.16
Iteration 44 took 2.41 seconds (mean sampled reward: -4860.93). Current reward after update: -4101.32, Optimal reward -3850.16
Iteration 45 took 2.36 seconds (mean sampled reward: -4962.55). Current reward after update: -3990.22, Optimal reward -3850.16
Iteration 46 took 2.38 seconds (mean sampled reward: -4848.12). Current reward after update: -3767.63, Optimal reward -3767.63
Iteration 47 took 2.39 seconds (mean sampled reward: -5448.11). Current reward after update: -3858.01, Optimal reward -3767.63
Iteration 48 took 2.33 seconds (mean sampled reward: -5051.87). Current reward after update: -3797.08, Optimal reward -3767.63
Iteration 49 took 2.36 seconds (mean sampled reward: -5271.44). Current reward after update: -4993.79, Optimal reward -3767.63
Iteration 50 took 2.38 seconds (mean sampled reward: -5020.25). Current reward after update: -3805.33, Optimal reward -3767.63
Iteration 51 took 2.30 seconds (mean sampled reward: -5273.59). Current reward after update: -4347.77, Optimal reward -3767.63
Iteration 52 took 2.31 seconds (mean sampled reward: -5094.17). Current reward after update: -3851.13, Optimal reward -3767.63
Iteration 53 took 2.47 seconds (mean sampled reward: -4621.80). Current reward after update: -3752.78, Optimal reward -3752.78
Iteration 54 took 2.30 seconds (mean sampled reward: -4739.92). Current reward after update: -3847.26, Optimal reward -3752.78
Iteration 55 took 2.35 seconds (mean sampled reward: -4614.30). Current reward after update: -3835.63, Optimal reward -3752.78
Iteration 56 took 2.34 seconds (mean sampled reward: -4535.78). Current reward after update: -3758.69, Optimal reward -3752.78
Iteration 57 took 2.36 seconds (mean sampled reward: -4510.57). Current reward after update: -3768.84, Optimal reward -3752.78
Iteration 58 took 2.34 seconds (mean sampled reward: -4677.66). Current reward after update: -3766.96, Optimal reward -3752.78
Iteration 59 took 2.39 seconds (mean sampled reward: -4768.29). Current reward after update: -3718.28, Optimal reward -3718.28
Iteration 60 took 2.50 seconds (mean sampled reward: -4663.60). Current reward after update: -3661.06, Optimal reward -3661.06
Iteration 61 took 2.62 seconds (mean sampled reward: -4674.63). Current reward after update: -5139.18, Optimal reward -3661.06
Iteration 62 took 2.34 seconds (mean sampled reward: -4770.55). Current reward after update: -3710.73, Optimal reward -3661.06
Iteration 63 took 2.35 seconds (mean sampled reward: -4808.45). Current reward after update: -3661.33, Optimal reward -3661.06
Iteration 64 took 2.31 seconds (mean sampled reward: -4854.27). Current reward after update: -3681.53, Optimal reward -3661.06
Iteration 65 took 2.33 seconds (mean sampled reward: -4664.71). Current reward after update: -3686.57, Optimal reward -3661.06
Iteration 66 took 2.30 seconds (mean sampled reward: -4764.95). Current reward after update: -3634.82, Optimal reward -3634.82
Iteration 67 took 2.37 seconds (mean sampled reward: -4712.06). Current reward after update: -3624.20, Optimal reward -3624.20
Iteration 68 took 2.35 seconds (mean sampled reward: -4867.15). Current reward after update: -3626.78, Optimal reward -3624.20
Iteration 69 took 2.39 seconds (mean sampled reward: -4653.68). Current reward after update: -3676.42, Optimal reward -3624.20
Iteration 70 took 2.31 seconds (mean sampled reward: -4584.33). Current reward after update: -3562.61, Optimal reward -3562.61
Iteration 71 took 2.35 seconds (mean sampled reward: -4655.55). Current reward after update: -3651.32, Optimal reward -3562.61
Iteration 72 took 2.36 seconds (mean sampled reward: -4804.39). Current reward after update: -3513.31, Optimal reward -3513.31
Iteration 73 took 2.29 seconds (mean sampled reward: -4943.29). Current reward after update: -3545.81, Optimal reward -3513.31
Iteration 74 took 2.35 seconds (mean sampled reward: -4864.19). Current reward after update: -3581.90, Optimal reward -3513.31
Iteration 75 took 2.36 seconds (mean sampled reward: -4609.52). Current reward after update: -3489.92, Optimal reward -3489.92
Iteration 76 took 2.29 seconds (mean sampled reward: -4503.65). Current reward after update: -3462.69, Optimal reward -3462.69
Iteration 77 took 2.33 seconds (mean sampled reward: -4433.28). Current reward after update: -3434.02, Optimal reward -3434.02
Iteration 78 took 2.44 seconds (mean sampled reward: -4565.50). Current reward after update: -3417.09, Optimal reward -3417.09
Iteration 79 took 2.40 seconds (mean sampled reward: -4495.62). Current reward after update: -3431.61, Optimal reward -3417.09
Iteration 80 took 2.43 seconds (mean sampled reward: -4459.94). Current reward after update: -3321.01, Optimal reward -3321.01
Iteration 81 took 2.39 seconds (mean sampled reward: -4366.04). Current reward after update: -3401.00, Optimal reward -3321.01
Iteration 82 took 2.36 seconds (mean sampled reward: -4370.13). Current reward after update: -4247.67, Optimal reward -3321.01
Iteration 83 took 2.43 seconds (mean sampled reward: -4662.41). Current reward after update: -3379.66, Optimal reward -3321.01
Iteration 84 took 2.43 seconds (mean sampled reward: -4689.07). Current reward after update: -4178.51, Optimal reward -3321.01
Iteration 85 took 2.40 seconds (mean sampled reward: -4678.65). Current reward after update: -4168.83, Optimal reward -3321.01
Iteration 86 took 2.38 seconds (mean sampled reward: -4782.82). Current reward after update: -4460.44, Optimal reward -3321.01
Iteration 87 took 2.43 seconds (mean sampled reward: -4842.04). Current reward after update: -3328.99, Optimal reward -3321.01
Iteration 88 took 2.34 seconds (mean sampled reward: -4618.45). Current reward after update: -3280.57, Optimal reward -3280.57
Iteration 89 took 2.42 seconds (mean sampled reward: -4807.91). Current reward after update: -3367.58, Optimal reward -3280.57
Iteration 90 took 2.43 seconds (mean sampled reward: -5118.63). Current reward after update: -3353.72, Optimal reward -3280.57
Iteration 91 took 2.36 seconds (mean sampled reward: -4872.45). Current reward after update: -3301.46, Optimal reward -3280.57
Iteration 92 took 2.36 seconds (mean sampled reward: -5221.10). Current reward after update: -3403.53, Optimal reward -3280.57
Iteration 93 took 2.35 seconds (mean sampled reward: -5133.68). Current reward after update: -3893.25, Optimal reward -3280.57
Iteration 94 took 2.41 seconds (mean sampled reward: -5100.08). Current reward after update: -3173.43, Optimal reward -3173.43
Iteration 95 took 2.38 seconds (mean sampled reward: -4601.29). Current reward after update: -3160.67, Optimal reward -3160.67
Iteration 96 took 2.44 seconds (mean sampled reward: -4726.28). Current reward after update: -3184.71, Optimal reward -3160.67
Iteration 97 took 2.48 seconds (mean sampled reward: -5463.07). Current reward after update: -3242.19, Optimal reward -3160.67
Iteration 98 took 2.52 seconds (mean sampled reward: -5078.65). Current reward after update: -3273.34, Optimal reward -3160.67
Iteration 99 took 2.35 seconds (mean sampled reward: -4653.24). Current reward after update: -3238.13, Optimal reward -3160.67
Iteration 100 took 2.29 seconds (mean sampled reward: -4763.45). Current reward after update: -3232.63, Optimal reward -3160.67
Iteration 101 took 2.29 seconds (mean sampled reward: -4686.96). Current reward after update: -3348.35, Optimal reward -3160.67
Iteration 102 took 2.26 seconds (mean sampled reward: -4630.39). Current reward after update: -2907.06, Optimal reward -2907.06
Iteration 103 took 2.29 seconds (mean sampled reward: -4915.74). Current reward after update: -3247.87, Optimal reward -2907.06
Iteration 104 took 2.27 seconds (mean sampled reward: -4975.27). Current reward after update: -3310.80, Optimal reward -2907.06
Iteration 105 took 2.27 seconds (mean sampled reward: -4944.46). Current reward after update: -3212.22, Optimal reward -2907.06
Iteration 106 took 2.30 seconds (mean sampled reward: -4852.25). Current reward after update: -3381.14, Optimal reward -2907.06
Iteration 107 took 2.39 seconds (mean sampled reward: -5087.77). Current reward after update: -3241.03, Optimal reward -2907.06
Iteration 108 took 2.39 seconds (mean sampled reward: -5111.29). Current reward after update: -3306.27, Optimal reward -2907.06
Iteration 109 took 2.33 seconds (mean sampled reward: -4675.25). Current reward after update: -3303.07, Optimal reward -2907.06
Iteration 110 took 2.37 seconds (mean sampled reward: -4599.10). Current reward after update: -3280.69, Optimal reward -2907.06
Iteration 111 took 2.27 seconds (mean sampled reward: -4297.22). Current reward after update: -3314.72, Optimal reward -2907.06
Iteration 112 took 2.24 seconds (mean sampled reward: -4557.70). Current reward after update: -3282.88, Optimal reward -2907.06
Iteration 113 took 2.24 seconds (mean sampled reward: -5047.62). Current reward after update: -3304.24, Optimal reward -2907.06
Iteration 114 took 2.26 seconds (mean sampled reward: -4737.83). Current reward after update: -3235.74, Optimal reward -2907.06
Iteration 115 took 2.25 seconds (mean sampled reward: -4512.09). Current reward after update: -3276.65, Optimal reward -2907.06
Iteration 116 took 2.22 seconds (mean sampled reward: -5091.58). Current reward after update: -3299.47, Optimal reward -2907.06
Iteration 117 took 2.24 seconds (mean sampled reward: -5295.68). Current reward after update: -3259.02, Optimal reward -2907.06
Iteration 118 took 2.30 seconds (mean sampled reward: -5033.70). Current reward after update: -3067.08, Optimal reward -2907.06
Iteration 119 took 2.36 seconds (mean sampled reward: -4693.76). Current reward after update: -3194.37, Optimal reward -2907.06
Iteration 120 took 2.35 seconds (mean sampled reward: -5110.03). Current reward after update: -3119.28, Optimal reward -2907.06
Iteration 121 took 2.34 seconds (mean sampled reward: -5176.61). Current reward after update: -3161.19, Optimal reward -2907.06
Iteration 122 took 2.34 seconds (mean sampled reward: -5058.96). Current reward after update: -2856.66, Optimal reward -2856.66
Iteration 123 took 2.37 seconds (mean sampled reward: -4306.12). Current reward after update: -2617.34, Optimal reward -2617.34
Iteration 124 took 2.43 seconds (mean sampled reward: -4201.47). Current reward after update: -2574.70, Optimal reward -2574.70
Iteration 125 took 2.50 seconds (mean sampled reward: -4059.98). Current reward after update: -2792.91, Optimal reward -2574.70
Iteration 126 took 2.41 seconds (mean sampled reward: -3735.04). Current reward after update: -2231.10, Optimal reward -2231.10
Iteration 127 took 2.39 seconds (mean sampled reward: -3340.75). Current reward after update: -2195.10, Optimal reward -2195.10
Iteration 128 took 2.35 seconds (mean sampled reward: -4090.95). Current reward after update: -2236.66, Optimal reward -2195.10
Iteration 129 took 2.35 seconds (mean sampled reward: -3738.18). Current reward after update: -2614.54, Optimal reward -2195.10
Iteration 130 took 2.39 seconds (mean sampled reward: -3997.61). Current reward after update: -2117.28, Optimal reward -2117.28
Iteration 131 took 2.34 seconds (mean sampled reward: -4004.16). Current reward after update: -2574.81, Optimal reward -2117.28
Iteration 132 took 2.37 seconds (mean sampled reward: -4207.43). Current reward after update: -2215.43, Optimal reward -2117.28
Iteration 133 took 2.38 seconds (mean sampled reward: -3379.99). Current reward after update: -2139.97, Optimal reward -2117.28
Iteration 134 took 2.40 seconds (mean sampled reward: -3468.95). Current reward after update: -5544.91, Optimal reward -2117.28
Iteration 135 took 2.42 seconds (mean sampled reward: -3196.33). Current reward after update: -1934.29, Optimal reward -1934.29
Iteration 136 took 2.35 seconds (mean sampled reward: -2867.40). Current reward after update: -2661.86, Optimal reward -1934.29
Iteration 137 took 2.39 seconds (mean sampled reward: -3205.68). Current reward after update: -1713.56, Optimal reward -1713.56
Iteration 138 took 2.36 seconds (mean sampled reward: -3175.11). Current reward after update: -1823.99, Optimal reward -1713.56
Iteration 139 took 2.36 seconds (mean sampled reward: -2911.51). Current reward after update: -2013.44, Optimal reward -1713.56
Iteration 140 took 2.34 seconds (mean sampled reward: -3432.01). Current reward after update: -1816.37, Optimal reward -1713.56
Iteration 141 took 2.36 seconds (mean sampled reward: -3341.52). Current reward after update: -1799.15, Optimal reward -1713.56
Iteration 142 took 2.39 seconds (mean sampled reward: -3201.76). Current reward after update: -1618.80, Optimal reward -1618.80
Iteration 143 took 2.37 seconds (mean sampled reward: -3003.15). Current reward after update: -1580.73, Optimal reward -1580.73
Iteration 144 took 2.41 seconds (mean sampled reward: -2855.31). Current reward after update: -1485.03, Optimal reward -1485.03
Iteration 145 took 2.39 seconds (mean sampled reward: -3272.53). Current reward after update: -1510.84, Optimal reward -1485.03
Iteration 146 took 2.37 seconds (mean sampled reward: -2760.42). Current reward after update: -1463.38, Optimal reward -1463.38
Iteration 147 took 2.37 seconds (mean sampled reward: -2735.97). Current reward after update: -1465.12, Optimal reward -1463.38
Iteration 148 took 2.36 seconds (mean sampled reward: -3181.41). Current reward after update: -1491.21, Optimal reward -1463.38
Iteration 149 took 2.36 seconds (mean sampled reward: -3492.33). Current reward after update: -1524.68, Optimal reward -1463.38
Iteration 150 took 2.35 seconds (mean sampled reward: -4082.00). Current reward after update: -2025.84, Optimal reward -1463.38
Iteration 151 took 2.37 seconds (mean sampled reward: -3460.28). Current reward after update: -1563.61, Optimal reward -1463.38
Iteration 152 took 2.47 seconds (mean sampled reward: -3266.62). Current reward after update: -1505.25, Optimal reward -1463.38
Iteration 153 took 2.39 seconds (mean sampled reward: -2734.29). Current reward after update: -1567.96, Optimal reward -1463.38
Iteration 154 took 2.35 seconds (mean sampled reward: -3429.76). Current reward after update: -2171.52, Optimal reward -1463.38
Iteration 155 took 2.35 seconds (mean sampled reward: -2568.03). Current reward after update: -1487.40, Optimal reward -1463.38
Iteration 156 took 2.39 seconds (mean sampled reward: -2260.45). Current reward after update: -1561.30, Optimal reward -1463.38
Iteration 157 took 2.42 seconds (mean sampled reward: -2129.98). Current reward after update: -1456.80, Optimal reward -1456.80
Iteration 158 took 2.48 seconds (mean sampled reward: -2133.77). Current reward after update: -1668.57, Optimal reward -1456.80
Iteration 159 took 2.43 seconds (mean sampled reward: -2270.37). Current reward after update: -1470.03, Optimal reward -1456.80
Iteration 160 took 2.45 seconds (mean sampled reward: -2543.65). Current reward after update: -1494.83, Optimal reward -1456.80
Iteration 161 took 2.37 seconds (mean sampled reward: -2887.94). Current reward after update: -1457.44, Optimal reward -1456.80
Iteration 162 took 2.41 seconds (mean sampled reward: -3647.42). Current reward after update: -1566.66, Optimal reward -1456.80
Iteration 163 took 2.45 seconds (mean sampled reward: -3116.63). Current reward after update: -1278.20, Optimal reward -1278.20
Iteration 164 took 2.37 seconds (mean sampled reward: -3937.27). Current reward after update: -1246.46, Optimal reward -1246.46
Iteration 165 took 2.35 seconds (mean sampled reward: -3929.45). Current reward after update: -1328.97, Optimal reward -1246.46
Iteration 166 took 2.40 seconds (mean sampled reward: -3320.29). Current reward after update: -1372.65, Optimal reward -1246.46
Iteration 167 took 2.37 seconds (mean sampled reward: -2881.37). Current reward after update: -1336.41, Optimal reward -1246.46
Iteration 168 took 2.44 seconds (mean sampled reward: -2279.83). Current reward after update: -1404.49, Optimal reward -1246.46
Iteration 169 took 2.34 seconds (mean sampled reward: -2733.89). Current reward after update: -1256.94, Optimal reward -1246.46
Iteration 170 took 2.40 seconds (mean sampled reward: -2945.94). Current reward after update: -1520.24, Optimal reward -1246.46
Iteration 171 took 2.39 seconds (mean sampled reward: -2840.83). Current reward after update: -1466.67, Optimal reward -1246.46
Iteration 172 took 2.36 seconds (mean sampled reward: -2822.57). Current reward after update: -1285.83, Optimal reward -1246.46
Iteration 173 took 2.37 seconds (mean sampled reward: -2676.20). Current reward after update: -1455.75, Optimal reward -1246.46
Iteration 174 took 2.38 seconds (mean sampled reward: -2519.59). Current reward after update: -2057.36, Optimal reward -1246.46
Iteration 175 took 2.42 seconds (mean sampled reward: -2719.28). Current reward after update: -1272.00, Optimal reward -1246.46
Iteration 176 took 2.39 seconds (mean sampled reward: -2558.39). Current reward after update: -1110.88, Optimal reward -1110.88
Iteration 177 took 2.36 seconds (mean sampled reward: -2126.50). Current reward after update: -1180.05, Optimal reward -1110.88
Iteration 178 took 2.39 seconds (mean sampled reward: -2467.48). Current reward after update: -1145.25, Optimal reward -1110.88
Iteration 179 took 2.36 seconds (mean sampled reward: -2375.33). Current reward after update: -1081.04, Optimal reward -1081.04
Iteration 180 took 2.33 seconds (mean sampled reward: -2523.79). Current reward after update: -1232.05, Optimal reward -1081.04
Iteration 181 took 2.38 seconds (mean sampled reward: -2579.11). Current reward after update: -1282.65, Optimal reward -1081.04
Iteration 182 took 2.32 seconds (mean sampled reward: -3268.21). Current reward after update: -1217.20, Optimal reward -1081.04
Iteration 183 took 2.38 seconds (mean sampled reward: -2430.45). Current reward after update: -2107.59, Optimal reward -1081.04
Iteration 184 took 2.39 seconds (mean sampled reward: -2435.16). Current reward after update: -1393.70, Optimal reward -1081.04
Iteration 185 took 2.45 seconds (mean sampled reward: -2714.63). Current reward after update: -1769.89, Optimal reward -1081.04
Iteration 186 took 2.41 seconds (mean sampled reward: -2546.31). Current reward after update: -1322.94, Optimal reward -1081.04
Iteration 187 took 2.37 seconds (mean sampled reward: -2410.94). Current reward after update: -1298.78, Optimal reward -1081.04
Iteration 188 took 2.35 seconds (mean sampled reward: -2883.45). Current reward after update: -1477.19, Optimal reward -1081.04
Iteration 189 took 2.37 seconds (mean sampled reward: -3072.68). Current reward after update: -1235.38, Optimal reward -1081.04
Iteration 190 took 2.39 seconds (mean sampled reward: -3182.10). Current reward after update: -1324.77, Optimal reward -1081.04
Iteration 191 took 2.33 seconds (mean sampled reward: -3003.98). Current reward after update: -1385.83, Optimal reward -1081.04
Iteration 192 took 2.40 seconds (mean sampled reward: -3628.98). Current reward after update: -1562.84, Optimal reward -1081.04
Iteration 193 took 2.33 seconds (mean sampled reward: -3561.83). Current reward after update: -1573.29, Optimal reward -1081.04
Iteration 194 took 2.41 seconds (mean sampled reward: -3423.18). Current reward after update: -1806.63, Optimal reward -1081.04
Iteration 195 took 2.32 seconds (mean sampled reward: -4409.03). Current reward after update: -2258.83, Optimal reward -1081.04
Iteration 196 took 2.40 seconds (mean sampled reward: -4750.39). Current reward after update: -1587.22, Optimal reward -1081.04
Iteration 197 took 2.38 seconds (mean sampled reward: -4012.74). Current reward after update: -2027.67, Optimal reward -1081.04
Iteration 198 took 2.34 seconds (mean sampled reward: -4262.58). Current reward after update: -1567.26, Optimal reward -1081.04
Iteration 199 took 2.41 seconds (mean sampled reward: -3246.37). Current reward after update: -1439.94, Optimal reward -1081.04
Iteration 200 took 2.38 seconds (mean sampled reward: -3684.46). Current reward after update: -1471.04, Optimal reward -1081.04
Iteration 1 took 2.44 seconds (mean sampled reward: -7630.36). Current reward after update: -7397.86, Optimal reward -7397.86
Iteration 2 took 2.35 seconds (mean sampled reward: -7607.37). Current reward after update: -7351.16, Optimal reward -7351.16
Iteration 3 took 2.31 seconds (mean sampled reward: -7616.51). Current reward after update: -7437.35, Optimal reward -7351.16
Iteration 4 took 2.51 seconds (mean sampled reward: -7617.30). Current reward after update: -7279.47, Optimal reward -7279.47
Iteration 5 took 2.53 seconds (mean sampled reward: -7588.48). Current reward after update: -7540.82, Optimal reward -7279.47
Iteration 6 took 2.32 seconds (mean sampled reward: -7586.64). Current reward after update: -7263.08, Optimal reward -7263.08
Iteration 7 took 2.32 seconds (mean sampled reward: -7568.85). Current reward after update: -7244.00, Optimal reward -7244.00
Iteration 8 took 2.69 seconds (mean sampled reward: -7486.84). Current reward after update: -6458.01, Optimal reward -6458.01
Iteration 9 took 2.48 seconds (mean sampled reward: -7097.05). Current reward after update: -6330.78, Optimal reward -6330.78
Iteration 10 took 2.34 seconds (mean sampled reward: -7055.14). Current reward after update: -6303.81, Optimal reward -6303.81
Iteration 11 took 2.38 seconds (mean sampled reward: -6970.10). Current reward after update: -6171.98, Optimal reward -6171.98
Iteration 12 took 2.37 seconds (mean sampled reward: -6902.62). Current reward after update: -6236.95, Optimal reward -6171.98
Iteration 13 took 2.30 seconds (mean sampled reward: -7082.75). Current reward after update: -6301.01, Optimal reward -6171.98
Iteration 14 took 2.32 seconds (mean sampled reward: -6770.98). Current reward after update: -6166.68, Optimal reward -6166.68
Iteration 15 took 2.47 seconds (mean sampled reward: -6556.81). Current reward after update: -6160.60, Optimal reward -6160.60
Iteration 16 took 2.44 seconds (mean sampled reward: -6774.05). Current reward after update: -6202.15, Optimal reward -6160.60
Iteration 17 took 2.32 seconds (mean sampled reward: -7020.73). Current reward after update: -6316.76, Optimal reward -6160.60
Iteration 18 took 2.44 seconds (mean sampled reward: -6980.70). Current reward after update: -6150.29, Optimal reward -6150.29
Iteration 19 took 2.42 seconds (mean sampled reward: -6755.93). Current reward after update: -6108.56, Optimal reward -6108.56
Iteration 20 took 2.27 seconds (mean sampled reward: -6656.86). Current reward after update: -6500.37, Optimal reward -6108.56
Iteration 21 took 2.28 seconds (mean sampled reward: -6525.49). Current reward after update: -6065.45, Optimal reward -6065.45
Iteration 22 took 2.26 seconds (mean sampled reward: -6768.66). Current reward after update: -5975.48, Optimal reward -5975.48
Iteration 23 took 2.26 seconds (mean sampled reward: -6602.07). Current reward after update: -5671.57, Optimal reward -5671.57
Iteration 24 took 2.32 seconds (mean sampled reward: -6548.89). Current reward after update: -5929.08, Optimal reward -5671.57
Iteration 25 took 2.36 seconds (mean sampled reward: -6639.72). Current reward after update: -6258.13, Optimal reward -5671.57
Iteration 26 took 2.26 seconds (mean sampled reward: -6640.62). Current reward after update: -5759.33, Optimal reward -5671.57
Iteration 27 took 2.36 seconds (mean sampled reward: -6914.73). Current reward after update: -5495.63, Optimal reward -5495.63
Iteration 28 took 2.24 seconds (mean sampled reward: -6642.01). Current reward after update: -5599.54, Optimal reward -5495.63
Iteration 29 took 2.32 seconds (mean sampled reward: -6717.07). Current reward after update: -5344.32, Optimal reward -5344.32
Iteration 30 took 2.39 seconds (mean sampled reward: -6590.27). Current reward after update: -5282.54, Optimal reward -5282.54
Iteration 31 took 2.30 seconds (mean sampled reward: -6277.71). Current reward after update: -5202.74, Optimal reward -5202.74
Iteration 32 took 2.40 seconds (mean sampled reward: -6325.32). Current reward after update: -5056.03, Optimal reward -5056.03
Iteration 33 took 2.34 seconds (mean sampled reward: -6436.75). Current reward after update: -5215.64, Optimal reward -5056.03
Iteration 34 took 2.25 seconds (mean sampled reward: -6135.73). Current reward after update: -4916.69, Optimal reward -4916.69
Iteration 35 took 2.28 seconds (mean sampled reward: -5998.83). Current reward after update: -4721.93, Optimal reward -4721.93
Iteration 36 took 2.26 seconds (mean sampled reward: -6260.69). Current reward after update: -4405.14, Optimal reward -4405.14
Iteration 37 took 2.24 seconds (mean sampled reward: -6000.87). Current reward after update: -4266.31, Optimal reward -4266.31
Iteration 38 took 2.24 seconds (mean sampled reward: -5873.15). Current reward after update: -4376.89, Optimal reward -4266.31
Iteration 39 took 2.21 seconds (mean sampled reward: -5742.56). Current reward after update: -4278.25, Optimal reward -4266.31
Iteration 40 took 2.21 seconds (mean sampled reward: -5769.98). Current reward after update: -4223.29, Optimal reward -4223.29
Iteration 41 took 2.16 seconds (mean sampled reward: -5790.55). Current reward after update: -3951.20, Optimal reward -3951.20
Iteration 42 took 2.26 seconds (mean sampled reward: -6121.84). Current reward after update: -3577.98, Optimal reward -3577.98
Iteration 43 took 2.31 seconds (mean sampled reward: -5934.61). Current reward after update: -3536.32, Optimal reward -3536.32
Iteration 44 took 2.22 seconds (mean sampled reward: -5685.98). Current reward after update: -3976.39, Optimal reward -3536.32
Iteration 45 took 2.25 seconds (mean sampled reward: -5948.29). Current reward after update: -3525.26, Optimal reward -3525.26
Iteration 46 took 2.33 seconds (mean sampled reward: -5817.19). Current reward after update: -3526.61, Optimal reward -3525.26
Iteration 47 took 2.28 seconds (mean sampled reward: -6007.76). Current reward after update: -3907.74, Optimal reward -3525.26
Iteration 48 took 2.27 seconds (mean sampled reward: -5685.20). Current reward after update: -3402.65, Optimal reward -3402.65
Iteration 49 took 2.46 seconds (mean sampled reward: -5449.34). Current reward after update: -3337.48, Optimal reward -3337.48
Iteration 50 took 2.44 seconds (mean sampled reward: -5352.74). Current reward after update: -3164.70, Optimal reward -3164.70
Iteration 51 took 2.39 seconds (mean sampled reward: -5141.52). Current reward after update: -3923.19, Optimal reward -3164.70
Iteration 52 took 2.37 seconds (mean sampled reward: -5478.00). Current reward after update: -3507.22, Optimal reward -3164.70
Iteration 53 took 2.50 seconds (mean sampled reward: -5261.02). Current reward after update: -4339.58, Optimal reward -3164.70
Iteration 54 took 2.41 seconds (mean sampled reward: -5114.68). Current reward after update: -3382.59, Optimal reward -3164.70
Iteration 55 took 2.44 seconds (mean sampled reward: -5640.79). Current reward after update: -3254.16, Optimal reward -3164.70
Iteration 56 took 2.35 seconds (mean sampled reward: -5685.27). Current reward after update: -2909.23, Optimal reward -2909.23
Iteration 57 took 2.32 seconds (mean sampled reward: -5528.80). Current reward after update: -3409.26, Optimal reward -2909.23
Iteration 58 took 2.28 seconds (mean sampled reward: -5380.10). Current reward after update: -2892.04, Optimal reward -2892.04
Iteration 59 took 2.37 seconds (mean sampled reward: -5594.02). Current reward after update: -3599.95, Optimal reward -2892.04
Iteration 60 took 2.32 seconds (mean sampled reward: -5690.50). Current reward after update: -3400.13, Optimal reward -2892.04
Iteration 61 took 2.53 seconds (mean sampled reward: -5638.53). Current reward after update: -2662.26, Optimal reward -2662.26
Iteration 62 took 2.38 seconds (mean sampled reward: -5898.74). Current reward after update: -6218.68, Optimal reward -2662.26
Iteration 63 took 2.43 seconds (mean sampled reward: -5710.08). Current reward after update: -3070.37, Optimal reward -2662.26
Iteration 64 took 2.32 seconds (mean sampled reward: -6122.35). Current reward after update: -3234.09, Optimal reward -2662.26
Iteration 65 took 2.29 seconds (mean sampled reward: -6280.13). Current reward after update: -3280.13, Optimal reward -2662.26
Iteration 66 took 2.28 seconds (mean sampled reward: -6429.35). Current reward after update: -3290.76, Optimal reward -2662.26
Iteration 67 took 2.28 seconds (mean sampled reward: -5997.21). Current reward after update: -3226.15, Optimal reward -2662.26
Iteration 68 took 2.24 seconds (mean sampled reward: -5458.93). Current reward after update: -3012.27, Optimal reward -2662.26
Iteration 69 took 2.20 seconds (mean sampled reward: -5253.49). Current reward after update: -3039.21, Optimal reward -2662.26
Iteration 70 took 2.35 seconds (mean sampled reward: -5393.86). Current reward after update: -3008.73, Optimal reward -2662.26
Iteration 71 took 2.21 seconds (mean sampled reward: -5499.79). Current reward after update: -4027.17, Optimal reward -2662.26
Iteration 72 took 2.18 seconds (mean sampled reward: -5187.64). Current reward after update: -3119.60, Optimal reward -2662.26
Iteration 73 took 2.20 seconds (mean sampled reward: -4936.92). Current reward after update: -2797.71, Optimal reward -2662.26
Iteration 74 took 2.23 seconds (mean sampled reward: -4794.18). Current reward after update: -2760.70, Optimal reward -2662.26
Iteration 75 took 2.28 seconds (mean sampled reward: -4864.15). Current reward after update: -4001.59, Optimal reward -2662.26
Iteration 76 took 2.25 seconds (mean sampled reward: -5820.88). Current reward after update: -3260.53, Optimal reward -2662.26
Iteration 77 took 2.29 seconds (mean sampled reward: -5906.77). Current reward after update: -3236.95, Optimal reward -2662.26
Iteration 78 took 2.42 seconds (mean sampled reward: -5903.97). Current reward after update: -3215.80, Optimal reward -2662.26
Iteration 79 took 2.36 seconds (mean sampled reward: -5842.96). Current reward after update: -3469.72, Optimal reward -2662.26
Iteration 80 took 2.43 seconds (mean sampled reward: -5289.02). Current reward after update: -3713.11, Optimal reward -2662.26
Iteration 81 took 2.43 seconds (mean sampled reward: -5291.38). Current reward after update: -3408.07, Optimal reward -2662.26
Iteration 82 took 2.41 seconds (mean sampled reward: -4930.37). Current reward after update: -2898.78, Optimal reward -2662.26
Iteration 83 took 2.30 seconds (mean sampled reward: -4809.69). Current reward after update: -2757.20, Optimal reward -2662.26
Iteration 84 took 2.35 seconds (mean sampled reward: -4501.36). Current reward after update: -2792.17, Optimal reward -2662.26
Iteration 85 took 2.28 seconds (mean sampled reward: -4584.60). Current reward after update: -2800.55, Optimal reward -2662.26
Iteration 86 took 2.43 seconds (mean sampled reward: -5055.72). Current reward after update: -2916.39, Optimal reward -2662.26
Iteration 87 took 2.42 seconds (mean sampled reward: -4995.94). Current reward after update: -2735.35, Optimal reward -2662.26
Iteration 88 took 2.40 seconds (mean sampled reward: -4882.29). Current reward after update: -2741.05, Optimal reward -2662.26
Iteration 89 took 2.41 seconds (mean sampled reward: -4759.67). Current reward after update: -2825.29, Optimal reward -2662.26
Iteration 90 took 2.33 seconds (mean sampled reward: -4848.97). Current reward after update: -2943.54, Optimal reward -2662.26
Iteration 91 took 2.29 seconds (mean sampled reward: -4776.55). Current reward after update: -2892.37, Optimal reward -2662.26
Iteration 92 took 2.31 seconds (mean sampled reward: -5185.29). Current reward after update: -6528.19, Optimal reward -2662.26
Iteration 93 took 2.38 seconds (mean sampled reward: -5688.29). Current reward after update: -3844.61, Optimal reward -2662.26
Iteration 94 took 2.37 seconds (mean sampled reward: -5666.56). Current reward after update: -2816.04, Optimal reward -2662.26
Iteration 95 took 2.32 seconds (mean sampled reward: -4914.74). Current reward after update: -2830.98, Optimal reward -2662.26
Iteration 96 took 2.34 seconds (mean sampled reward: -5696.30). Current reward after update: -2927.17, Optimal reward -2662.26
Iteration 97 took 2.35 seconds (mean sampled reward: -5802.80). Current reward after update: -3081.58, Optimal reward -2662.26
Iteration 98 took 2.44 seconds (mean sampled reward: -5877.85). Current reward after update: -2903.95, Optimal reward -2662.26
Iteration 99 took 2.42 seconds (mean sampled reward: -6039.98). Current reward after update: -2624.78, Optimal reward -2624.78
Iteration 100 took 2.39 seconds (mean sampled reward: -6002.54). Current reward after update: -2844.26, Optimal reward -2624.78
Iteration 101 took 2.41 seconds (mean sampled reward: -6594.43). Current reward after update: -2866.63, Optimal reward -2624.78
Iteration 102 took 2.34 seconds (mean sampled reward: -5582.09). Current reward after update: -2789.27, Optimal reward -2624.78
Iteration 103 took 2.34 seconds (mean sampled reward: -5623.01). Current reward after update: -2855.49, Optimal reward -2624.78
Iteration 104 took 2.34 seconds (mean sampled reward: -5765.63). Current reward after update: -2931.46, Optimal reward -2624.78
Iteration 105 took 2.28 seconds (mean sampled reward: -5468.09). Current reward after update: -2788.00, Optimal reward -2624.78
Iteration 106 took 2.37 seconds (mean sampled reward: -6179.18). Current reward after update: -2916.27, Optimal reward -2624.78
Iteration 107 took 2.44 seconds (mean sampled reward: -5756.54). Current reward after update: -2833.27, Optimal reward -2624.78
Iteration 108 took 2.54 seconds (mean sampled reward: -5896.00). Current reward after update: -3018.03, Optimal reward -2624.78
Iteration 109 took 2.41 seconds (mean sampled reward: -5346.73). Current reward after update: -3477.90, Optimal reward -2624.78
Iteration 110 took 2.33 seconds (mean sampled reward: -4951.30). Current reward after update: -2740.29, Optimal reward -2624.78
Iteration 111 took 2.50 seconds (mean sampled reward: -5286.69). Current reward after update: -2663.00, Optimal reward -2624.78
Iteration 112 took 2.24 seconds (mean sampled reward: -4828.05). Current reward after update: -2685.55, Optimal reward -2624.78
Iteration 113 took 2.24 seconds (mean sampled reward: -4653.34). Current reward after update: -2416.45, Optimal reward -2416.45
Iteration 114 took 2.23 seconds (mean sampled reward: -5028.67). Current reward after update: -4577.07, Optimal reward -2416.45
Iteration 115 took 2.29 seconds (mean sampled reward: -4968.13). Current reward after update: -2873.82, Optimal reward -2416.45
Iteration 116 took 2.20 seconds (mean sampled reward: -5032.75). Current reward after update: -2715.50, Optimal reward -2416.45
Iteration 117 took 2.19 seconds (mean sampled reward: -4910.44). Current reward after update: -2975.40, Optimal reward -2416.45
Iteration 118 took 2.20 seconds (mean sampled reward: -4905.45). Current reward after update: -2548.17, Optimal reward -2416.45
Iteration 119 took 2.20 seconds (mean sampled reward: -4767.58). Current reward after update: -2491.86, Optimal reward -2416.45
Iteration 120 took 2.22 seconds (mean sampled reward: -5527.82). Current reward after update: -2676.32, Optimal reward -2416.45
Iteration 121 took 2.21 seconds (mean sampled reward: -4682.57). Current reward after update: -2745.64, Optimal reward -2416.45
Iteration 122 took 2.34 seconds (mean sampled reward: -4279.83). Current reward after update: -2758.27, Optimal reward -2416.45
Iteration 123 took 2.20 seconds (mean sampled reward: -4802.16). Current reward after update: -2677.97, Optimal reward -2416.45
Iteration 124 took 2.25 seconds (mean sampled reward: -4121.75). Current reward after update: -2646.57, Optimal reward -2416.45
Iteration 125 took 2.25 seconds (mean sampled reward: -4117.43). Current reward after update: -2477.43, Optimal reward -2416.45
Iteration 126 took 2.16 seconds (mean sampled reward: -4745.35). Current reward after update: -2669.74, Optimal reward -2416.45
Iteration 127 took 2.14 seconds (mean sampled reward: -4518.54). Current reward after update: -2576.85, Optimal reward -2416.45
Iteration 128 took 2.36 seconds (mean sampled reward: -5560.60). Current reward after update: -3182.17, Optimal reward -2416.45
Iteration 129 took 2.23 seconds (mean sampled reward: -5386.96). Current reward after update: -2511.37, Optimal reward -2416.45
Iteration 130 took 2.25 seconds (mean sampled reward: -5000.69). Current reward after update: -2618.48, Optimal reward -2416.45
Iteration 131 took 2.29 seconds (mean sampled reward: -5164.09). Current reward after update: -2630.01, Optimal reward -2416.45
Iteration 132 took 2.24 seconds (mean sampled reward: -5327.49). Current reward after update: -2618.87, Optimal reward -2416.45
Iteration 133 took 2.26 seconds (mean sampled reward: -5385.88). Current reward after update: -2593.23, Optimal reward -2416.45
Iteration 134 took 2.24 seconds (mean sampled reward: -5043.88). Current reward after update: -2631.24, Optimal reward -2416.45
Iteration 135 took 2.26 seconds (mean sampled reward: -4449.12). Current reward after update: -2615.57, Optimal reward -2416.45
Iteration 136 took 2.21 seconds (mean sampled reward: -4659.08). Current reward after update: -3512.16, Optimal reward -2416.45
Iteration 137 took 2.22 seconds (mean sampled reward: -4519.66). Current reward after update: -2565.28, Optimal reward -2416.45
Iteration 138 took 2.31 seconds (mean sampled reward: -4605.50). Current reward after update: -2852.98, Optimal reward -2416.45
Iteration 139 took 2.28 seconds (mean sampled reward: -4582.81). Current reward after update: -2528.01, Optimal reward -2416.45
Iteration 140 took 2.29 seconds (mean sampled reward: -4427.78). Current reward after update: -2527.22, Optimal reward -2416.45
Iteration 141 took 2.22 seconds (mean sampled reward: -4439.07). Current reward after update: -2454.42, Optimal reward -2416.45
Iteration 142 took 2.24 seconds (mean sampled reward: -4083.34). Current reward after update: -2466.82, Optimal reward -2416.45
Iteration 143 took 2.23 seconds (mean sampled reward: -4187.27). Current reward after update: -3516.70, Optimal reward -2416.45
Iteration 144 took 2.26 seconds (mean sampled reward: -4456.82). Current reward after update: -2388.59, Optimal reward -2388.59
Iteration 145 took 2.25 seconds (mean sampled reward: -4072.37). Current reward after update: -3550.64, Optimal reward -2388.59
Iteration 146 took 2.26 seconds (mean sampled reward: -4189.84). Current reward after update: -2449.53, Optimal reward -2388.59
Iteration 147 took 2.23 seconds (mean sampled reward: -4412.37). Current reward after update: -3074.71, Optimal reward -2388.59
Iteration 148 took 2.18 seconds (mean sampled reward: -4239.01). Current reward after update: -2432.51, Optimal reward -2388.59
Iteration 149 took 2.20 seconds (mean sampled reward: -4211.55). Current reward after update: -2399.60, Optimal reward -2388.59
Iteration 150 took 2.23 seconds (mean sampled reward: -4007.57). Current reward after update: -2501.90, Optimal reward -2388.59
Iteration 151 took 2.27 seconds (mean sampled reward: -4069.67). Current reward after update: -2391.88, Optimal reward -2388.59
Iteration 152 took 2.21 seconds (mean sampled reward: -4076.62). Current reward after update: -2501.85, Optimal reward -2388.59
Iteration 153 took 2.18 seconds (mean sampled reward: -4086.85). Current reward after update: -2679.05, Optimal reward -2388.59
Iteration 154 took 2.17 seconds (mean sampled reward: -4331.42). Current reward after update: -2426.70, Optimal reward -2388.59
Iteration 155 took 2.14 seconds (mean sampled reward: -4202.32). Current reward after update: -3375.05, Optimal reward -2388.59
Iteration 156 took 2.15 seconds (mean sampled reward: -4419.24). Current reward after update: -2536.81, Optimal reward -2388.59
Iteration 157 took 2.18 seconds (mean sampled reward: -3888.86). Current reward after update: -2517.18, Optimal reward -2388.59
Iteration 158 took 2.20 seconds (mean sampled reward: -4376.64). Current reward after update: -6805.84, Optimal reward -2388.59
Iteration 159 took 2.20 seconds (mean sampled reward: -4848.17). Current reward after update: -2510.34, Optimal reward -2388.59
Iteration 160 took 2.21 seconds (mean sampled reward: -4737.64). Current reward after update: -2548.22, Optimal reward -2388.59
Iteration 161 took 2.28 seconds (mean sampled reward: -4850.76). Current reward after update: -2439.48, Optimal reward -2388.59
Iteration 162 took 2.20 seconds (mean sampled reward: -5518.93). Current reward after update: -2672.42, Optimal reward -2388.59
Iteration 163 took 2.14 seconds (mean sampled reward: -4483.22). Current reward after update: -2464.85, Optimal reward -2388.59
Iteration 164 took 2.31 seconds (mean sampled reward: -4842.55). Current reward after update: -2598.91, Optimal reward -2388.59
Iteration 165 took 2.29 seconds (mean sampled reward: -5327.82). Current reward after update: -3723.66, Optimal reward -2388.59
Iteration 166 took 2.20 seconds (mean sampled reward: -4342.31). Current reward after update: -2457.53, Optimal reward -2388.59
Iteration 167 took 2.20 seconds (mean sampled reward: -3907.99). Current reward after update: -2566.71, Optimal reward -2388.59
Iteration 168 took 2.13 seconds (mean sampled reward: -4287.74). Current reward after update: -2482.68, Optimal reward -2388.59
Iteration 169 took 2.16 seconds (mean sampled reward: -3717.86). Current reward after update: -3684.36, Optimal reward -2388.59
Iteration 170 took 2.19 seconds (mean sampled reward: -4269.50). Current reward after update: -2476.32, Optimal reward -2388.59
Iteration 171 took 2.27 seconds (mean sampled reward: -4413.13). Current reward after update: -2351.37, Optimal reward -2351.37
Iteration 172 took 2.16 seconds (mean sampled reward: -3879.59). Current reward after update: -3297.42, Optimal reward -2351.37
Iteration 173 took 2.12 seconds (mean sampled reward: -3926.95). Current reward after update: -2205.73, Optimal reward -2205.73
Iteration 174 took 2.16 seconds (mean sampled reward: -4307.67). Current reward after update: -2195.04, Optimal reward -2195.04
Iteration 175 took 2.17 seconds (mean sampled reward: -4365.22). Current reward after update: -3140.01, Optimal reward -2195.04
Iteration 176 took 2.14 seconds (mean sampled reward: -3739.63). Current reward after update: -3329.03, Optimal reward -2195.04
Iteration 177 took 2.19 seconds (mean sampled reward: -4178.43). Current reward after update: -3677.08, Optimal reward -2195.04
Iteration 178 took 2.17 seconds (mean sampled reward: -4126.61). Current reward after update: -3724.09, Optimal reward -2195.04
Iteration 179 took 2.17 seconds (mean sampled reward: -5146.73). Current reward after update: -4109.13, Optimal reward -2195.04
Iteration 180 took 2.15 seconds (mean sampled reward: -4895.63). Current reward after update: -6822.64, Optimal reward -2195.04
Iteration 181 took 2.21 seconds (mean sampled reward: -5502.01). Current reward after update: -2094.37, Optimal reward -2094.37
Iteration 182 took 2.24 seconds (mean sampled reward: -5490.42). Current reward after update: -2280.73, Optimal reward -2094.37
Iteration 183 took 2.15 seconds (mean sampled reward: -4443.41). Current reward after update: -2196.11, Optimal reward -2094.37
Iteration 184 took 2.17 seconds (mean sampled reward: -4447.04). Current reward after update: -6995.49, Optimal reward -2094.37
Iteration 185 took 2.15 seconds (mean sampled reward: -4638.97). Current reward after update: -2199.19, Optimal reward -2094.37
Iteration 186 took 2.21 seconds (mean sampled reward: -5354.21). Current reward after update: -2181.76, Optimal reward -2094.37
Iteration 187 took 2.18 seconds (mean sampled reward: -5626.14). Current reward after update: -2184.24, Optimal reward -2094.37
Iteration 188 took 2.19 seconds (mean sampled reward: -5122.23). Current reward after update: -2112.02, Optimal reward -2094.37
Iteration 189 took 2.18 seconds (mean sampled reward: -5616.10). Current reward after update: -2155.33, Optimal reward -2094.37
Iteration 190 took 2.17 seconds (mean sampled reward: -4806.73). Current reward after update: -2163.81, Optimal reward -2094.37
Iteration 191 took 2.19 seconds (mean sampled reward: -5098.27). Current reward after update: -2186.21, Optimal reward -2094.37
Iteration 192 took 2.12 seconds (mean sampled reward: -4299.57). Current reward after update: -2263.14, Optimal reward -2094.37
Iteration 193 took 2.11 seconds (mean sampled reward: -4283.82). Current reward after update: -2196.38, Optimal reward -2094.37
Iteration 194 took 2.12 seconds (mean sampled reward: -4558.20). Current reward after update: -2212.04, Optimal reward -2094.37
Iteration 195 took 2.17 seconds (mean sampled reward: -4894.47). Current reward after update: -2227.98, Optimal reward -2094.37
Iteration 196 took 2.24 seconds (mean sampled reward: -5373.14). Current reward after update: -2238.58, Optimal reward -2094.37
Iteration 197 took 2.13 seconds (mean sampled reward: -4721.96). Current reward after update: -2183.15, Optimal reward -2094.37
Iteration 198 took 2.12 seconds (mean sampled reward: -4391.55). Current reward after update: -2178.37, Optimal reward -2094.37
Iteration 199 took 2.17 seconds (mean sampled reward: -4097.10). Current reward after update: -2181.99, Optimal reward -2094.37
Iteration 200 took 2.12 seconds (mean sampled reward: -4177.46). Current reward after update: -4540.89, Optimal reward -2094.37
Max force: 40 Sigma: 0.1 mean rewards: -1672.9088880035854, best rewards:-1081.0425052098149

Iteration 1 took 2.40 seconds (mean sampled reward: -7638.37). Current reward after update: -7362.15, Optimal reward -7362.15
Iteration 2 took 2.29 seconds (mean sampled reward: -7608.85). Current reward after update: -6754.58, Optimal reward -6754.58
Iteration 3 took 2.50 seconds (mean sampled reward: -7501.23). Current reward after update: -6088.38, Optimal reward -6088.38
Iteration 4 took 2.52 seconds (mean sampled reward: -7246.71). Current reward after update: -5548.87, Optimal reward -5548.87
Iteration 5 took 2.41 seconds (mean sampled reward: -6801.87). Current reward after update: -5421.00, Optimal reward -5421.00
Iteration 6 took 2.30 seconds (mean sampled reward: -6763.14). Current reward after update: -4219.88, Optimal reward -4219.88
Iteration 7 took 2.35 seconds (mean sampled reward: -6543.84). Current reward after update: -4205.86, Optimal reward -4205.86
Iteration 8 took 2.57 seconds (mean sampled reward: -5735.42). Current reward after update: -4164.73, Optimal reward -4164.73
Iteration 9 took 2.61 seconds (mean sampled reward: -6111.09). Current reward after update: -3904.65, Optimal reward -3904.65
Iteration 10 took 2.50 seconds (mean sampled reward: -6233.39). Current reward after update: -4214.86, Optimal reward -3904.65
Iteration 11 took 2.51 seconds (mean sampled reward: -6232.89). Current reward after update: -3439.56, Optimal reward -3439.56
Iteration 12 took 2.52 seconds (mean sampled reward: -6187.73). Current reward after update: -2569.54, Optimal reward -2569.54
Iteration 13 took 2.49 seconds (mean sampled reward: -6077.37). Current reward after update: -2819.04, Optimal reward -2569.54
Iteration 14 took 2.47 seconds (mean sampled reward: -5909.53). Current reward after update: -1891.22, Optimal reward -1891.22
Iteration 15 took 2.49 seconds (mean sampled reward: -5554.23). Current reward after update: -2106.64, Optimal reward -1891.22
Iteration 16 took 2.28 seconds (mean sampled reward: -4550.62). Current reward after update: -1733.30, Optimal reward -1733.30
Iteration 17 took 2.40 seconds (mean sampled reward: -4907.23). Current reward after update: -1756.19, Optimal reward -1733.30
Iteration 18 took 2.31 seconds (mean sampled reward: -4662.32). Current reward after update: -1403.38, Optimal reward -1403.38
Iteration 19 took 2.43 seconds (mean sampled reward: -4632.04). Current reward after update: -1588.85, Optimal reward -1403.38
Iteration 20 took 2.25 seconds (mean sampled reward: -4122.88). Current reward after update: -1632.65, Optimal reward -1403.38
Iteration 21 took 2.32 seconds (mean sampled reward: -4546.24). Current reward after update: -1623.02, Optimal reward -1403.38
Iteration 22 took 2.26 seconds (mean sampled reward: -4504.49). Current reward after update: -2544.16, Optimal reward -1403.38
Iteration 23 took 2.25 seconds (mean sampled reward: -4077.69). Current reward after update: -1485.36, Optimal reward -1403.38
Iteration 24 took 2.40 seconds (mean sampled reward: -5389.82). Current reward after update: -1356.73, Optimal reward -1356.73
Iteration 25 took 2.27 seconds (mean sampled reward: -5150.18). Current reward after update: -1023.99, Optimal reward -1023.99
Iteration 26 took 2.41 seconds (mean sampled reward: -5526.90). Current reward after update: -1260.51, Optimal reward -1023.99
Iteration 27 took 2.23 seconds (mean sampled reward: -6102.98). Current reward after update: -1566.88, Optimal reward -1023.99
Iteration 28 took 2.31 seconds (mean sampled reward: -6038.26). Current reward after update: -1273.99, Optimal reward -1023.99
Iteration 29 took 2.41 seconds (mean sampled reward: -5040.31). Current reward after update: -1245.52, Optimal reward -1023.99
Iteration 30 took 2.27 seconds (mean sampled reward: -6130.61). Current reward after update: -999.06, Optimal reward -999.06
Iteration 31 took 2.51 seconds (mean sampled reward: -6820.49). Current reward after update: -1162.18, Optimal reward -999.06
Iteration 32 took 2.27 seconds (mean sampled reward: -5012.57). Current reward after update: -873.03, Optimal reward -873.03
Iteration 33 took 2.30 seconds (mean sampled reward: -5296.20). Current reward after update: -1070.45, Optimal reward -873.03
Iteration 34 took 2.42 seconds (mean sampled reward: -5183.26). Current reward after update: -1002.48, Optimal reward -873.03
Iteration 35 took 2.30 seconds (mean sampled reward: -6027.15). Current reward after update: -6977.05, Optimal reward -873.03
Iteration 36 took 2.38 seconds (mean sampled reward: -5493.96). Current reward after update: -912.28, Optimal reward -873.03
Iteration 37 took 2.28 seconds (mean sampled reward: -4885.48). Current reward after update: -1011.55, Optimal reward -873.03
Iteration 38 took 2.26 seconds (mean sampled reward: -3616.65). Current reward after update: -727.43, Optimal reward -727.43
Iteration 39 took 2.34 seconds (mean sampled reward: -3712.96). Current reward after update: -691.56, Optimal reward -691.56
Iteration 40 took 2.36 seconds (mean sampled reward: -5284.60). Current reward after update: -704.37, Optimal reward -691.56
Iteration 41 took 2.26 seconds (mean sampled reward: -3500.99). Current reward after update: -1184.26, Optimal reward -691.56
Iteration 42 took 2.26 seconds (mean sampled reward: -3734.99). Current reward after update: -718.04, Optimal reward -691.56
Iteration 43 took 2.25 seconds (mean sampled reward: -3020.44). Current reward after update: -800.40, Optimal reward -691.56
Iteration 44 took 2.30 seconds (mean sampled reward: -3658.87). Current reward after update: -704.81, Optimal reward -691.56
Iteration 45 took 2.27 seconds (mean sampled reward: -3278.17). Current reward after update: -6937.38, Optimal reward -691.56
Iteration 46 took 2.26 seconds (mean sampled reward: -4281.97). Current reward after update: -779.77, Optimal reward -691.56
Iteration 47 took 2.27 seconds (mean sampled reward: -5149.67). Current reward after update: -699.83, Optimal reward -691.56
Iteration 48 took 2.28 seconds (mean sampled reward: -4116.23). Current reward after update: -1024.63, Optimal reward -691.56
Iteration 49 took 2.23 seconds (mean sampled reward: -4917.02). Current reward after update: -1045.70, Optimal reward -691.56
Iteration 50 took 2.27 seconds (mean sampled reward: -4462.25). Current reward after update: -659.02, Optimal reward -659.02
Iteration 51 took 2.22 seconds (mean sampled reward: -5168.68). Current reward after update: -659.45, Optimal reward -659.02
Iteration 52 took 2.17 seconds (mean sampled reward: -5163.27). Current reward after update: -685.97, Optimal reward -659.02
Iteration 53 took 2.20 seconds (mean sampled reward: -4659.12). Current reward after update: -615.77, Optimal reward -615.77
Iteration 54 took 2.36 seconds (mean sampled reward: -2994.70). Current reward after update: -793.50, Optimal reward -615.77
Iteration 55 took 2.34 seconds (mean sampled reward: -3339.12). Current reward after update: -702.62, Optimal reward -615.77
Iteration 56 took 2.34 seconds (mean sampled reward: -3934.29). Current reward after update: -791.28, Optimal reward -615.77
Iteration 57 took 2.29 seconds (mean sampled reward: -4909.31). Current reward after update: -1054.44, Optimal reward -615.77
Iteration 58 took 2.36 seconds (mean sampled reward: -3676.91). Current reward after update: -666.02, Optimal reward -615.77
Iteration 59 took 2.18 seconds (mean sampled reward: -3954.58). Current reward after update: -814.16, Optimal reward -615.77
Iteration 60 took 2.42 seconds (mean sampled reward: -3351.51). Current reward after update: -582.88, Optimal reward -582.88
Iteration 61 took 2.21 seconds (mean sampled reward: -4198.22). Current reward after update: -583.07, Optimal reward -582.88
Iteration 62 took 2.19 seconds (mean sampled reward: -4734.80). Current reward after update: -626.59, Optimal reward -582.88
Iteration 63 took 2.24 seconds (mean sampled reward: -3913.06). Current reward after update: -1016.93, Optimal reward -582.88
Iteration 64 took 2.16 seconds (mean sampled reward: -3796.23). Current reward after update: -616.87, Optimal reward -582.88
Iteration 65 took 2.21 seconds (mean sampled reward: -5300.10). Current reward after update: -541.05, Optimal reward -541.05
Iteration 66 took 2.17 seconds (mean sampled reward: -4897.04). Current reward after update: -681.74, Optimal reward -541.05
Iteration 67 took 2.25 seconds (mean sampled reward: -4448.61). Current reward after update: -612.89, Optimal reward -541.05
Iteration 68 took 2.15 seconds (mean sampled reward: -2864.53). Current reward after update: -610.03, Optimal reward -541.05
Iteration 69 took 2.16 seconds (mean sampled reward: -3489.07). Current reward after update: -659.49, Optimal reward -541.05
Iteration 70 took 2.17 seconds (mean sampled reward: -3273.19). Current reward after update: -603.95, Optimal reward -541.05
Iteration 71 took 2.34 seconds (mean sampled reward: -3452.10). Current reward after update: -575.11, Optimal reward -541.05
Iteration 72 took 2.20 seconds (mean sampled reward: -3016.00). Current reward after update: -571.03, Optimal reward -541.05
Iteration 73 took 2.25 seconds (mean sampled reward: -3542.66). Current reward after update: -604.63, Optimal reward -541.05
Iteration 74 took 2.21 seconds (mean sampled reward: -2177.59). Current reward after update: -651.80, Optimal reward -541.05
Iteration 75 took 2.20 seconds (mean sampled reward: -2850.25). Current reward after update: -686.39, Optimal reward -541.05
Iteration 76 took 2.22 seconds (mean sampled reward: -1620.74). Current reward after update: -562.39, Optimal reward -541.05
Iteration 77 took 2.24 seconds (mean sampled reward: -2246.82). Current reward after update: -648.87, Optimal reward -541.05
Iteration 78 took 2.22 seconds (mean sampled reward: -4212.29). Current reward after update: -602.38, Optimal reward -541.05
Iteration 79 took 2.26 seconds (mean sampled reward: -4389.44). Current reward after update: -605.43, Optimal reward -541.05
Iteration 80 took 2.19 seconds (mean sampled reward: -3466.73). Current reward after update: -631.32, Optimal reward -541.05
Iteration 81 took 2.25 seconds (mean sampled reward: -2198.54). Current reward after update: -599.48, Optimal reward -541.05
Iteration 82 took 2.24 seconds (mean sampled reward: -3215.19). Current reward after update: -632.42, Optimal reward -541.05
Iteration 83 took 2.25 seconds (mean sampled reward: -4350.75). Current reward after update: -511.08, Optimal reward -511.08
Iteration 84 took 2.23 seconds (mean sampled reward: -2777.06). Current reward after update: -564.41, Optimal reward -511.08
Iteration 85 took 2.23 seconds (mean sampled reward: -1584.70). Current reward after update: -517.66, Optimal reward -511.08
Iteration 86 took 2.24 seconds (mean sampled reward: -2071.24). Current reward after update: -1175.01, Optimal reward -511.08
Iteration 87 took 2.26 seconds (mean sampled reward: -2298.84). Current reward after update: -617.24, Optimal reward -511.08
Iteration 88 took 2.26 seconds (mean sampled reward: -2408.35). Current reward after update: -547.99, Optimal reward -511.08
Iteration 89 took 2.18 seconds (mean sampled reward: -4606.65). Current reward after update: -634.23, Optimal reward -511.08
Iteration 90 took 2.29 seconds (mean sampled reward: -3018.41). Current reward after update: -608.44, Optimal reward -511.08
Iteration 91 took 2.36 seconds (mean sampled reward: -4664.61). Current reward after update: -650.84, Optimal reward -511.08
Iteration 92 took 2.34 seconds (mean sampled reward: -5817.68). Current reward after update: -738.35, Optimal reward -511.08
Iteration 93 took 2.20 seconds (mean sampled reward: -4652.33). Current reward after update: -761.44, Optimal reward -511.08
Iteration 94 took 2.22 seconds (mean sampled reward: -5907.91). Current reward after update: -820.97, Optimal reward -511.08
Iteration 95 took 2.27 seconds (mean sampled reward: -5145.39). Current reward after update: -710.63, Optimal reward -511.08
Iteration 96 took 2.36 seconds (mean sampled reward: -5616.99). Current reward after update: -979.06, Optimal reward -511.08
Iteration 97 took 2.28 seconds (mean sampled reward: -5024.44). Current reward after update: -1016.94, Optimal reward -511.08
Iteration 98 took 2.24 seconds (mean sampled reward: -5977.33). Current reward after update: -637.22, Optimal reward -511.08
Iteration 99 took 2.23 seconds (mean sampled reward: -5339.22). Current reward after update: -6916.88, Optimal reward -511.08
Iteration 100 took 2.29 seconds (mean sampled reward: -5311.47). Current reward after update: -730.22, Optimal reward -511.08
Iteration 101 took 2.29 seconds (mean sampled reward: -6200.32). Current reward after update: -714.93, Optimal reward -511.08
Iteration 102 took 2.32 seconds (mean sampled reward: -5920.48). Current reward after update: -712.90, Optimal reward -511.08
Iteration 103 took 2.35 seconds (mean sampled reward: -5199.39). Current reward after update: -646.68, Optimal reward -511.08
Iteration 104 took 2.25 seconds (mean sampled reward: -4148.67). Current reward after update: -816.03, Optimal reward -511.08
Iteration 105 took 2.32 seconds (mean sampled reward: -3121.02). Current reward after update: -620.59, Optimal reward -511.08
Iteration 106 took 2.35 seconds (mean sampled reward: -3460.92). Current reward after update: -1115.91, Optimal reward -511.08
Iteration 107 took 2.58 seconds (mean sampled reward: -3122.96). Current reward after update: -706.24, Optimal reward -511.08
Iteration 108 took 2.38 seconds (mean sampled reward: -2789.46). Current reward after update: -1012.13, Optimal reward -511.08
Iteration 109 took 2.39 seconds (mean sampled reward: -2673.60). Current reward after update: -693.71, Optimal reward -511.08
Iteration 110 took 2.26 seconds (mean sampled reward: -2881.71). Current reward after update: -906.19, Optimal reward -511.08
Iteration 111 took 2.27 seconds (mean sampled reward: -2990.81). Current reward after update: -610.49, Optimal reward -511.08
Iteration 112 took 2.30 seconds (mean sampled reward: -4613.38). Current reward after update: -591.64, Optimal reward -511.08
Iteration 113 took 2.35 seconds (mean sampled reward: -3329.42). Current reward after update: -706.84, Optimal reward -511.08
Iteration 114 took 2.27 seconds (mean sampled reward: -4781.13). Current reward after update: -684.70, Optimal reward -511.08
Iteration 115 took 2.31 seconds (mean sampled reward: -3291.55). Current reward after update: -617.78, Optimal reward -511.08
Iteration 116 took 2.30 seconds (mean sampled reward: -2880.78). Current reward after update: -789.93, Optimal reward -511.08
Iteration 117 took 2.42 seconds (mean sampled reward: -2505.05). Current reward after update: -754.02, Optimal reward -511.08
Iteration 118 took 2.41 seconds (mean sampled reward: -2902.17). Current reward after update: -960.72, Optimal reward -511.08
Iteration 119 took 2.35 seconds (mean sampled reward: -3198.17). Current reward after update: -1069.74, Optimal reward -511.08
Iteration 120 took 2.32 seconds (mean sampled reward: -3664.85). Current reward after update: -635.98, Optimal reward -511.08
Iteration 121 took 2.34 seconds (mean sampled reward: -3773.83). Current reward after update: -815.17, Optimal reward -511.08
Iteration 122 took 2.30 seconds (mean sampled reward: -3477.06). Current reward after update: -486.52, Optimal reward -486.52
Iteration 123 took 2.45 seconds (mean sampled reward: -3235.35). Current reward after update: -645.95, Optimal reward -486.52
Iteration 124 took 2.37 seconds (mean sampled reward: -3026.35). Current reward after update: -664.22, Optimal reward -486.52
Iteration 125 took 2.37 seconds (mean sampled reward: -2795.26). Current reward after update: -520.11, Optimal reward -486.52
Iteration 126 took 2.35 seconds (mean sampled reward: -3108.35). Current reward after update: -648.84, Optimal reward -486.52
Iteration 127 took 2.38 seconds (mean sampled reward: -3232.55). Current reward after update: -583.55, Optimal reward -486.52
Iteration 128 took 2.41 seconds (mean sampled reward: -3437.48). Current reward after update: -1077.00, Optimal reward -486.52
Iteration 129 took 2.33 seconds (mean sampled reward: -2993.94). Current reward after update: -6929.88, Optimal reward -486.52
Iteration 130 took 2.33 seconds (mean sampled reward: -3000.55). Current reward after update: -507.56, Optimal reward -486.52
Iteration 131 took 2.49 seconds (mean sampled reward: -3169.15). Current reward after update: -5594.10, Optimal reward -486.52
Iteration 132 took 2.32 seconds (mean sampled reward: -3794.47). Current reward after update: -5215.86, Optimal reward -486.52
Iteration 133 took 2.38 seconds (mean sampled reward: -3121.53). Current reward after update: -5438.73, Optimal reward -486.52
Iteration 134 took 2.31 seconds (mean sampled reward: -3328.12). Current reward after update: -706.94, Optimal reward -486.52
Iteration 135 took 2.34 seconds (mean sampled reward: -3660.70). Current reward after update: -710.31, Optimal reward -486.52
Iteration 136 took 2.32 seconds (mean sampled reward: -4096.26). Current reward after update: -585.61, Optimal reward -486.52
Iteration 137 took 2.32 seconds (mean sampled reward: -3466.67). Current reward after update: -878.56, Optimal reward -486.52
Iteration 138 took 2.33 seconds (mean sampled reward: -4578.11). Current reward after update: -664.22, Optimal reward -486.52
Iteration 139 took 2.35 seconds (mean sampled reward: -5372.57). Current reward after update: -792.12, Optimal reward -486.52
Iteration 140 took 2.34 seconds (mean sampled reward: -5337.37). Current reward after update: -681.66, Optimal reward -486.52
Iteration 141 took 2.33 seconds (mean sampled reward: -4541.57). Current reward after update: -605.21, Optimal reward -486.52
Iteration 142 took 2.31 seconds (mean sampled reward: -5793.92). Current reward after update: -612.88, Optimal reward -486.52
Iteration 143 took 2.34 seconds (mean sampled reward: -4443.99). Current reward after update: -540.97, Optimal reward -486.52
Iteration 144 took 2.32 seconds (mean sampled reward: -3594.30). Current reward after update: -800.40, Optimal reward -486.52
Iteration 145 took 2.32 seconds (mean sampled reward: -4389.33). Current reward after update: -2626.40, Optimal reward -486.52
Iteration 146 took 2.31 seconds (mean sampled reward: -3800.26). Current reward after update: -693.27, Optimal reward -486.52
Iteration 147 took 2.31 seconds (mean sampled reward: -4896.54). Current reward after update: -1017.37, Optimal reward -486.52
Iteration 148 took 2.32 seconds (mean sampled reward: -4107.23). Current reward after update: -681.99, Optimal reward -486.52
Iteration 149 took 2.31 seconds (mean sampled reward: -3597.33). Current reward after update: -1691.82, Optimal reward -486.52
Iteration 150 took 2.32 seconds (mean sampled reward: -3916.41). Current reward after update: -737.02, Optimal reward -486.52
Iteration 151 took 2.35 seconds (mean sampled reward: -4491.51). Current reward after update: -687.77, Optimal reward -486.52
Iteration 152 took 2.36 seconds (mean sampled reward: -4064.44). Current reward after update: -688.34, Optimal reward -486.52
Iteration 153 took 2.29 seconds (mean sampled reward: -3072.57). Current reward after update: -617.20, Optimal reward -486.52
Iteration 154 took 2.29 seconds (mean sampled reward: -2687.32). Current reward after update: -1373.36, Optimal reward -486.52
Iteration 155 took 2.29 seconds (mean sampled reward: -3182.71). Current reward after update: -595.81, Optimal reward -486.52
Iteration 156 took 2.36 seconds (mean sampled reward: -3191.76). Current reward after update: -563.54, Optimal reward -486.52
Iteration 157 took 2.31 seconds (mean sampled reward: -3780.10). Current reward after update: -557.17, Optimal reward -486.52
Iteration 158 took 2.37 seconds (mean sampled reward: -3531.75). Current reward after update: -6851.84, Optimal reward -486.52
Iteration 159 took 2.37 seconds (mean sampled reward: -3888.82). Current reward after update: -909.19, Optimal reward -486.52
Iteration 160 took 2.33 seconds (mean sampled reward: -3681.97). Current reward after update: -6865.68, Optimal reward -486.52
Iteration 161 took 2.31 seconds (mean sampled reward: -4549.59). Current reward after update: -846.28, Optimal reward -486.52
Iteration 162 took 2.36 seconds (mean sampled reward: -4140.32). Current reward after update: -745.70, Optimal reward -486.52
Iteration 163 took 2.36 seconds (mean sampled reward: -3916.27). Current reward after update: -689.01, Optimal reward -486.52
Iteration 164 took 2.36 seconds (mean sampled reward: -3472.18). Current reward after update: -861.51, Optimal reward -486.52
Iteration 165 took 2.39 seconds (mean sampled reward: -3671.53). Current reward after update: -765.91, Optimal reward -486.52
Iteration 166 took 2.35 seconds (mean sampled reward: -3612.18). Current reward after update: -561.29, Optimal reward -486.52
Iteration 167 took 2.43 seconds (mean sampled reward: -3455.91). Current reward after update: -704.17, Optimal reward -486.52
Iteration 168 took 2.44 seconds (mean sampled reward: -3811.46). Current reward after update: -765.61, Optimal reward -486.52
Iteration 169 took 2.35 seconds (mean sampled reward: -3883.43). Current reward after update: -671.71, Optimal reward -486.52
Iteration 170 took 2.25 seconds (mean sampled reward: -4399.56). Current reward after update: -609.36, Optimal reward -486.52
Iteration 171 took 2.31 seconds (mean sampled reward: -4611.63). Current reward after update: -683.65, Optimal reward -486.52
Iteration 172 took 2.32 seconds (mean sampled reward: -4391.47). Current reward after update: -721.81, Optimal reward -486.52
Iteration 173 took 2.32 seconds (mean sampled reward: -5545.41). Current reward after update: -762.82, Optimal reward -486.52
Iteration 174 took 2.38 seconds (mean sampled reward: -3947.11). Current reward after update: -1048.40, Optimal reward -486.52
Iteration 175 took 2.33 seconds (mean sampled reward: -4760.20). Current reward after update: -1779.89, Optimal reward -486.52
Iteration 176 took 2.25 seconds (mean sampled reward: -5836.04). Current reward after update: -1554.45, Optimal reward -486.52
Iteration 177 took 2.21 seconds (mean sampled reward: -6178.49). Current reward after update: -975.21, Optimal reward -486.52
Iteration 178 took 2.22 seconds (mean sampled reward: -5809.71). Current reward after update: -1047.84, Optimal reward -486.52
Iteration 179 took 2.28 seconds (mean sampled reward: -4957.37). Current reward after update: -817.69, Optimal reward -486.52
Iteration 180 took 2.28 seconds (mean sampled reward: -3142.98). Current reward after update: -694.94, Optimal reward -486.52
Iteration 181 took 2.35 seconds (mean sampled reward: -3372.78). Current reward after update: -1310.63, Optimal reward -486.52
Iteration 182 took 2.38 seconds (mean sampled reward: -2880.17). Current reward after update: -2748.27, Optimal reward -486.52
Iteration 183 took 2.42 seconds (mean sampled reward: -3171.22). Current reward after update: -742.36, Optimal reward -486.52
Iteration 184 took 2.37 seconds (mean sampled reward: -3049.51). Current reward after update: -754.97, Optimal reward -486.52
Iteration 185 took 2.43 seconds (mean sampled reward: -2908.45). Current reward after update: -1560.88, Optimal reward -486.52
Iteration 186 took 2.36 seconds (mean sampled reward: -3499.93). Current reward after update: -679.32, Optimal reward -486.52
Iteration 187 took 2.35 seconds (mean sampled reward: -3071.99). Current reward after update: -1437.72, Optimal reward -486.52
Iteration 188 took 2.37 seconds (mean sampled reward: -2671.77). Current reward after update: -688.41, Optimal reward -486.52
Iteration 189 took 2.38 seconds (mean sampled reward: -2600.73). Current reward after update: -2844.82, Optimal reward -486.52
Iteration 190 took 2.30 seconds (mean sampled reward: -2492.41). Current reward after update: -657.92, Optimal reward -486.52
Iteration 191 took 2.35 seconds (mean sampled reward: -3034.25). Current reward after update: -645.19, Optimal reward -486.52
Iteration 192 took 2.38 seconds (mean sampled reward: -2584.97). Current reward after update: -2887.82, Optimal reward -486.52
Iteration 193 took 2.36 seconds (mean sampled reward: -2892.30). Current reward after update: -2102.28, Optimal reward -486.52
Iteration 194 took 2.35 seconds (mean sampled reward: -2591.82). Current reward after update: -841.20, Optimal reward -486.52
Iteration 195 took 2.37 seconds (mean sampled reward: -2814.79). Current reward after update: -584.27, Optimal reward -486.52
Iteration 196 took 2.35 seconds (mean sampled reward: -2917.70). Current reward after update: -575.01, Optimal reward -486.52
Iteration 197 took 2.33 seconds (mean sampled reward: -2959.53). Current reward after update: -1092.67, Optimal reward -486.52
Iteration 198 took 2.38 seconds (mean sampled reward: -3031.63). Current reward after update: -508.97, Optimal reward -486.52
Iteration 199 took 2.40 seconds (mean sampled reward: -2866.04). Current reward after update: -2288.85, Optimal reward -486.52
Iteration 200 took 2.42 seconds (mean sampled reward: -2881.71). Current reward after update: -599.33, Optimal reward -486.52
Iteration 1 took 2.45 seconds (mean sampled reward: -7636.60). Current reward after update: -7483.49, Optimal reward -7483.49
Iteration 2 took 2.33 seconds (mean sampled reward: -7629.65). Current reward after update: -6493.99, Optimal reward -6493.99
Iteration 3 took 2.28 seconds (mean sampled reward: -7460.90). Current reward after update: -6122.73, Optimal reward -6122.73
Iteration 4 took 2.27 seconds (mean sampled reward: -7241.87). Current reward after update: -5909.86, Optimal reward -5909.86
Iteration 5 took 2.46 seconds (mean sampled reward: -7001.64). Current reward after update: -5763.49, Optimal reward -5763.49
Iteration 6 took 2.40 seconds (mean sampled reward: -6789.62). Current reward after update: -5520.39, Optimal reward -5520.39
Iteration 7 took 2.44 seconds (mean sampled reward: -6422.54). Current reward after update: -5406.94, Optimal reward -5406.94
Iteration 8 took 2.44 seconds (mean sampled reward: -6839.28). Current reward after update: -5014.81, Optimal reward -5014.81
Iteration 9 took 2.49 seconds (mean sampled reward: -6302.50). Current reward after update: -4825.10, Optimal reward -4825.10
Iteration 10 took 2.45 seconds (mean sampled reward: -6197.17). Current reward after update: -5604.07, Optimal reward -4825.10
Iteration 11 took 2.43 seconds (mean sampled reward: -5772.35). Current reward after update: -4373.70, Optimal reward -4373.70
Iteration 12 took 2.47 seconds (mean sampled reward: -5723.80). Current reward after update: -4446.32, Optimal reward -4373.70
Iteration 13 took 2.28 seconds (mean sampled reward: -5633.91). Current reward after update: -4234.59, Optimal reward -4234.59
Iteration 14 took 2.53 seconds (mean sampled reward: -5949.68). Current reward after update: -4319.91, Optimal reward -4234.59
Iteration 15 took 2.36 seconds (mean sampled reward: -5179.70). Current reward after update: -4293.15, Optimal reward -4234.59
Iteration 16 took 2.55 seconds (mean sampled reward: -5499.54). Current reward after update: -4200.86, Optimal reward -4200.86
Iteration 17 took 2.51 seconds (mean sampled reward: -5266.54). Current reward after update: -4220.82, Optimal reward -4200.86
Iteration 18 took 2.36 seconds (mean sampled reward: -5501.24). Current reward after update: -4180.20, Optimal reward -4180.20
Iteration 19 took 2.38 seconds (mean sampled reward: -6330.35). Current reward after update: -4587.12, Optimal reward -4180.20
Iteration 20 took 2.46 seconds (mean sampled reward: -6195.33). Current reward after update: -4129.04, Optimal reward -4129.04
Iteration 21 took 2.58 seconds (mean sampled reward: -5501.88). Current reward after update: -3972.38, Optimal reward -3972.38
Iteration 22 took 2.53 seconds (mean sampled reward: -5712.96). Current reward after update: -3941.34, Optimal reward -3941.34
Iteration 23 took 2.45 seconds (mean sampled reward: -5799.88). Current reward after update: -4042.63, Optimal reward -3941.34
Iteration 24 took 2.51 seconds (mean sampled reward: -5308.31). Current reward after update: -3863.49, Optimal reward -3863.49
Iteration 25 took 2.40 seconds (mean sampled reward: -4965.41). Current reward after update: -3468.50, Optimal reward -3468.50
Iteration 26 took 2.40 seconds (mean sampled reward: -5426.40). Current reward after update: -3088.87, Optimal reward -3088.87
Iteration 27 took 2.33 seconds (mean sampled reward: -5089.25). Current reward after update: -3162.16, Optimal reward -3088.87
Iteration 28 took 2.54 seconds (mean sampled reward: -5248.50). Current reward after update: -2918.98, Optimal reward -2918.98
Iteration 29 took 2.33 seconds (mean sampled reward: -4985.44). Current reward after update: -3180.69, Optimal reward -2918.98
Iteration 30 took 2.45 seconds (mean sampled reward: -5251.25). Current reward after update: -3782.87, Optimal reward -2918.98
Iteration 31 took 2.43 seconds (mean sampled reward: -5739.50). Current reward after update: -2845.68, Optimal reward -2845.68
Iteration 32 took 2.44 seconds (mean sampled reward: -5617.84). Current reward after update: -2728.21, Optimal reward -2728.21
Iteration 33 took 2.31 seconds (mean sampled reward: -4153.24). Current reward after update: -2643.16, Optimal reward -2643.16
Iteration 34 took 2.50 seconds (mean sampled reward: -4491.00). Current reward after update: -2709.57, Optimal reward -2643.16
Iteration 35 took 2.32 seconds (mean sampled reward: -4676.43). Current reward after update: -2756.39, Optimal reward -2643.16
Iteration 36 took 2.38 seconds (mean sampled reward: -5046.48). Current reward after update: -3280.82, Optimal reward -2643.16
Iteration 37 took 2.23 seconds (mean sampled reward: -4933.49). Current reward after update: -2429.66, Optimal reward -2429.66
Iteration 38 took 2.18 seconds (mean sampled reward: -5954.17). Current reward after update: -2556.84, Optimal reward -2429.66
Iteration 39 took 2.13 seconds (mean sampled reward: -6476.33). Current reward after update: -2747.92, Optimal reward -2429.66
Iteration 40 took 2.16 seconds (mean sampled reward: -5768.23). Current reward after update: -2586.73, Optimal reward -2429.66
Iteration 41 took 2.15 seconds (mean sampled reward: -5451.84). Current reward after update: -3689.19, Optimal reward -2429.66
Iteration 42 took 2.25 seconds (mean sampled reward: -4599.55). Current reward after update: -2270.89, Optimal reward -2270.89
Iteration 43 took 2.33 seconds (mean sampled reward: -4512.57). Current reward after update: -2160.16, Optimal reward -2160.16
Iteration 44 took 2.24 seconds (mean sampled reward: -5617.89). Current reward after update: -1928.72, Optimal reward -1928.72
Iteration 45 took 2.36 seconds (mean sampled reward: -6192.87). Current reward after update: -2282.03, Optimal reward -1928.72
Iteration 46 took 2.31 seconds (mean sampled reward: -6039.18). Current reward after update: -2578.78, Optimal reward -1928.72
Iteration 47 took 2.31 seconds (mean sampled reward: -6178.87). Current reward after update: -2252.36, Optimal reward -1928.72
Iteration 48 took 2.42 seconds (mean sampled reward: -6043.91). Current reward after update: -2351.40, Optimal reward -1928.72
Iteration 49 took 2.44 seconds (mean sampled reward: -5717.25). Current reward after update: -2238.07, Optimal reward -1928.72
Iteration 50 took 2.42 seconds (mean sampled reward: -6228.69). Current reward after update: -2535.33, Optimal reward -1928.72
Iteration 51 took 2.42 seconds (mean sampled reward: -5559.89). Current reward after update: -2126.08, Optimal reward -1928.72
Iteration 52 took 2.42 seconds (mean sampled reward: -6286.83). Current reward after update: -2255.44, Optimal reward -1928.72
Iteration 53 took 2.41 seconds (mean sampled reward: -5374.57). Current reward after update: -2138.25, Optimal reward -1928.72
Iteration 54 took 2.60 seconds (mean sampled reward: -4711.76). Current reward after update: -1881.24, Optimal reward -1881.24
Iteration 55 took 2.73 seconds (mean sampled reward: -5133.29). Current reward after update: -2402.26, Optimal reward -1881.24
Iteration 56 took 2.33 seconds (mean sampled reward: -6172.10). Current reward after update: -6091.08, Optimal reward -1881.24
Iteration 57 took 2.38 seconds (mean sampled reward: -5764.24). Current reward after update: -2198.52, Optimal reward -1881.24
Iteration 58 took 2.38 seconds (mean sampled reward: -5575.60). Current reward after update: -2163.18, Optimal reward -1881.24
Iteration 59 took 2.35 seconds (mean sampled reward: -5428.88). Current reward after update: -2008.95, Optimal reward -1881.24
Iteration 60 took 2.48 seconds (mean sampled reward: -5232.88). Current reward after update: -2116.34, Optimal reward -1881.24
Iteration 61 took 2.33 seconds (mean sampled reward: -4501.90). Current reward after update: -2093.16, Optimal reward -1881.24
Iteration 62 took 2.46 seconds (mean sampled reward: -4649.78). Current reward after update: -1987.68, Optimal reward -1881.24
Iteration 63 took 2.32 seconds (mean sampled reward: -5345.21). Current reward after update: -2181.09, Optimal reward -1881.24
Iteration 64 took 2.24 seconds (mean sampled reward: -4858.32). Current reward after update: -5703.19, Optimal reward -1881.24
Iteration 65 took 2.21 seconds (mean sampled reward: -5150.21). Current reward after update: -2108.64, Optimal reward -1881.24
Iteration 66 took 2.23 seconds (mean sampled reward: -4788.28). Current reward after update: -2211.99, Optimal reward -1881.24
Iteration 67 took 2.25 seconds (mean sampled reward: -5011.10). Current reward after update: -2374.43, Optimal reward -1881.24
Iteration 68 took 2.36 seconds (mean sampled reward: -5096.24). Current reward after update: -2246.07, Optimal reward -1881.24
Iteration 69 took 2.29 seconds (mean sampled reward: -5523.18). Current reward after update: -1501.62, Optimal reward -1501.62
Iteration 70 took 2.36 seconds (mean sampled reward: -5398.50). Current reward after update: -1326.75, Optimal reward -1326.75
Iteration 71 took 2.27 seconds (mean sampled reward: -4379.26). Current reward after update: -1179.95, Optimal reward -1179.95
Iteration 72 took 2.28 seconds (mean sampled reward: -5064.44). Current reward after update: -1356.97, Optimal reward -1179.95
Iteration 73 took 2.47 seconds (mean sampled reward: -5251.49). Current reward after update: -1463.76, Optimal reward -1179.95
Iteration 74 took 2.30 seconds (mean sampled reward: -5763.91). Current reward after update: -1228.02, Optimal reward -1179.95
Iteration 75 took 2.31 seconds (mean sampled reward: -6175.80). Current reward after update: -2169.08, Optimal reward -1179.95
Iteration 76 took 2.33 seconds (mean sampled reward: -6747.95). Current reward after update: -1518.34, Optimal reward -1179.95
Iteration 77 took 2.29 seconds (mean sampled reward: -6432.36). Current reward after update: -1252.64, Optimal reward -1179.95
Iteration 78 took 2.44 seconds (mean sampled reward: -5994.19). Current reward after update: -1070.81, Optimal reward -1070.81
Iteration 79 took 2.36 seconds (mean sampled reward: -5329.60). Current reward after update: -1137.30, Optimal reward -1070.81
Iteration 80 took 2.28 seconds (mean sampled reward: -5091.00). Current reward after update: -1887.73, Optimal reward -1070.81
Iteration 81 took 2.39 seconds (mean sampled reward: -5563.11). Current reward after update: -1171.57, Optimal reward -1070.81
Iteration 82 took 2.52 seconds (mean sampled reward: -5882.77). Current reward after update: -1272.29, Optimal reward -1070.81
Iteration 83 took 2.46 seconds (mean sampled reward: -5975.11). Current reward after update: -1439.22, Optimal reward -1070.81
Iteration 84 took 2.49 seconds (mean sampled reward: -5583.56). Current reward after update: -1186.37, Optimal reward -1070.81
Iteration 85 took 2.45 seconds (mean sampled reward: -5104.69). Current reward after update: -1144.87, Optimal reward -1070.81
Iteration 86 took 2.54 seconds (mean sampled reward: -5582.17). Current reward after update: -1229.04, Optimal reward -1070.81
Iteration 87 took 2.45 seconds (mean sampled reward: -5486.03). Current reward after update: -1064.74, Optimal reward -1064.74
Iteration 88 took 2.48 seconds (mean sampled reward: -5454.08). Current reward after update: -986.60, Optimal reward -986.60
Iteration 89 took 2.42 seconds (mean sampled reward: -5743.55). Current reward after update: -1378.99, Optimal reward -986.60
Iteration 90 took 2.51 seconds (mean sampled reward: -5595.08). Current reward after update: -1054.38, Optimal reward -986.60
Iteration 91 took 2.43 seconds (mean sampled reward: -5515.67). Current reward after update: -1143.34, Optimal reward -986.60
Iteration 92 took 2.46 seconds (mean sampled reward: -5496.82). Current reward after update: -4028.62, Optimal reward -986.60
Iteration 93 took 2.49 seconds (mean sampled reward: -5325.54). Current reward after update: -1154.09, Optimal reward -986.60
Iteration 94 took 2.50 seconds (mean sampled reward: -4335.50). Current reward after update: -1170.25, Optimal reward -986.60
Iteration 95 took 2.56 seconds (mean sampled reward: -5409.99). Current reward after update: -1582.92, Optimal reward -986.60
Iteration 96 took 2.53 seconds (mean sampled reward: -4123.66). Current reward after update: -4776.14, Optimal reward -986.60
Iteration 97 took 2.51 seconds (mean sampled reward: -4320.07). Current reward after update: -1247.12, Optimal reward -986.60
Iteration 98 took 2.44 seconds (mean sampled reward: -4515.56). Current reward after update: -1400.16, Optimal reward -986.60
Iteration 99 took 2.52 seconds (mean sampled reward: -4150.38). Current reward after update: -1339.32, Optimal reward -986.60
Iteration 100 took 2.57 seconds (mean sampled reward: -4294.45). Current reward after update: -1517.13, Optimal reward -986.60
Iteration 101 took 2.56 seconds (mean sampled reward: -4465.55). Current reward after update: -5032.59, Optimal reward -986.60
Iteration 102 took 2.51 seconds (mean sampled reward: -4291.34). Current reward after update: -1268.28, Optimal reward -986.60
Iteration 103 took 2.52 seconds (mean sampled reward: -4856.02). Current reward after update: -1243.19, Optimal reward -986.60
Iteration 104 took 2.63 seconds (mean sampled reward: -5352.42). Current reward after update: -1124.29, Optimal reward -986.60
Iteration 105 took 2.54 seconds (mean sampled reward: -3309.89). Current reward after update: -1113.30, Optimal reward -986.60
Iteration 106 took 2.54 seconds (mean sampled reward: -2975.76). Current reward after update: -1003.49, Optimal reward -986.60
Iteration 107 took 2.73 seconds (mean sampled reward: -3571.81). Current reward after update: -1046.95, Optimal reward -986.60
Iteration 108 took 2.63 seconds (mean sampled reward: -3348.26). Current reward after update: -1559.62, Optimal reward -986.60
Iteration 109 took 2.62 seconds (mean sampled reward: -2978.68). Current reward after update: -896.25, Optimal reward -896.25
Iteration 110 took 2.54 seconds (mean sampled reward: -3856.12). Current reward after update: -839.70, Optimal reward -839.70
Iteration 111 took 2.48 seconds (mean sampled reward: -3375.60). Current reward after update: -792.05, Optimal reward -792.05
Iteration 112 took 2.46 seconds (mean sampled reward: -2505.08). Current reward after update: -826.15, Optimal reward -792.05
Iteration 113 took 2.64 seconds (mean sampled reward: -2631.95). Current reward after update: -828.30, Optimal reward -792.05
Iteration 114 took 2.65 seconds (mean sampled reward: -3076.32). Current reward after update: -829.53, Optimal reward -792.05
Iteration 115 took 2.43 seconds (mean sampled reward: -3918.79). Current reward after update: -827.45, Optimal reward -792.05
Iteration 116 took 2.46 seconds (mean sampled reward: -3968.45). Current reward after update: -1706.16, Optimal reward -792.05
Iteration 117 took 2.34 seconds (mean sampled reward: -3162.44). Current reward after update: -750.92, Optimal reward -750.92
Iteration 118 took 2.46 seconds (mean sampled reward: -3958.82). Current reward after update: -682.17, Optimal reward -682.17
Iteration 119 took 2.41 seconds (mean sampled reward: -3195.71). Current reward after update: -726.03, Optimal reward -682.17
Iteration 120 took 2.59 seconds (mean sampled reward: -4682.40). Current reward after update: -770.50, Optimal reward -682.17
Iteration 121 took 2.41 seconds (mean sampled reward: -3696.21). Current reward after update: -930.12, Optimal reward -682.17
Iteration 122 took 2.47 seconds (mean sampled reward: -3652.86). Current reward after update: -893.31, Optimal reward -682.17
Iteration 123 took 2.43 seconds (mean sampled reward: -3854.90). Current reward after update: -1295.35, Optimal reward -682.17
Iteration 124 took 2.34 seconds (mean sampled reward: -2886.27). Current reward after update: -1548.52, Optimal reward -682.17
Iteration 125 took 2.38 seconds (mean sampled reward: -2826.86). Current reward after update: -673.50, Optimal reward -673.50
Iteration 126 took 2.40 seconds (mean sampled reward: -2701.80). Current reward after update: -915.65, Optimal reward -673.50
Iteration 127 took 2.35 seconds (mean sampled reward: -3005.88). Current reward after update: -863.26, Optimal reward -673.50
Iteration 128 took 2.31 seconds (mean sampled reward: -2539.60). Current reward after update: -1808.52, Optimal reward -673.50
Iteration 129 took 2.30 seconds (mean sampled reward: -2483.55). Current reward after update: -846.04, Optimal reward -673.50
Iteration 130 took 2.32 seconds (mean sampled reward: -2198.08). Current reward after update: -677.20, Optimal reward -673.50
Iteration 131 took 2.30 seconds (mean sampled reward: -2841.26). Current reward after update: -841.52, Optimal reward -673.50
Iteration 132 took 2.24 seconds (mean sampled reward: -2609.99). Current reward after update: -848.41, Optimal reward -673.50
Iteration 133 took 2.43 seconds (mean sampled reward: -3170.15). Current reward after update: -733.11, Optimal reward -673.50
Iteration 134 took 2.33 seconds (mean sampled reward: -3673.12). Current reward after update: -711.76, Optimal reward -673.50
Iteration 135 took 2.28 seconds (mean sampled reward: -2399.28). Current reward after update: -646.71, Optimal reward -646.71
Iteration 136 took 2.26 seconds (mean sampled reward: -2731.44). Current reward after update: -650.92, Optimal reward -646.71
Iteration 137 took 2.33 seconds (mean sampled reward: -3164.78). Current reward after update: -748.95, Optimal reward -646.71
Iteration 138 took 2.29 seconds (mean sampled reward: -2331.61). Current reward after update: -1437.87, Optimal reward -646.71
Iteration 139 took 2.40 seconds (mean sampled reward: -2502.73). Current reward after update: -860.46, Optimal reward -646.71
Iteration 140 took 2.41 seconds (mean sampled reward: -2690.62). Current reward after update: -886.96, Optimal reward -646.71
Iteration 141 took 2.40 seconds (mean sampled reward: -2879.73). Current reward after update: -4459.60, Optimal reward -646.71
Iteration 142 took 2.37 seconds (mean sampled reward: -3106.07). Current reward after update: -884.28, Optimal reward -646.71
Iteration 143 took 2.49 seconds (mean sampled reward: -3049.69). Current reward after update: -1029.53, Optimal reward -646.71
Iteration 144 took 2.42 seconds (mean sampled reward: -3481.17). Current reward after update: -920.38, Optimal reward -646.71
Iteration 145 took 2.38 seconds (mean sampled reward: -2992.33). Current reward after update: -771.35, Optimal reward -646.71
Iteration 146 took 2.40 seconds (mean sampled reward: -3072.98). Current reward after update: -939.27, Optimal reward -646.71
Iteration 147 took 2.40 seconds (mean sampled reward: -3189.85). Current reward after update: -991.88, Optimal reward -646.71
Iteration 148 took 2.40 seconds (mean sampled reward: -3137.40). Current reward after update: -716.88, Optimal reward -646.71
Iteration 149 took 2.44 seconds (mean sampled reward: -3558.43). Current reward after update: -723.22, Optimal reward -646.71
Iteration 150 took 2.40 seconds (mean sampled reward: -3293.82). Current reward after update: -675.40, Optimal reward -646.71
Iteration 151 took 2.40 seconds (mean sampled reward: -3037.44). Current reward after update: -647.30, Optimal reward -646.71
Iteration 152 took 2.34 seconds (mean sampled reward: -2840.40). Current reward after update: -655.79, Optimal reward -646.71
Iteration 153 took 2.35 seconds (mean sampled reward: -2732.76). Current reward after update: -722.54, Optimal reward -646.71
Iteration 154 took 2.35 seconds (mean sampled reward: -2913.42). Current reward after update: -1450.00, Optimal reward -646.71
Iteration 155 took 2.47 seconds (mean sampled reward: -3094.01). Current reward after update: -738.25, Optimal reward -646.71
Iteration 156 took 2.31 seconds (mean sampled reward: -3488.71). Current reward after update: -846.90, Optimal reward -646.71
Iteration 157 took 2.37 seconds (mean sampled reward: -3315.19). Current reward after update: -1250.45, Optimal reward -646.71
Iteration 158 took 2.39 seconds (mean sampled reward: -3085.48). Current reward after update: -734.91, Optimal reward -646.71
Iteration 159 took 2.35 seconds (mean sampled reward: -3172.30). Current reward after update: -1160.95, Optimal reward -646.71
Iteration 160 took 2.49 seconds (mean sampled reward: -4325.90). Current reward after update: -6671.04, Optimal reward -646.71
Iteration 161 took 2.40 seconds (mean sampled reward: -4328.90). Current reward after update: -845.78, Optimal reward -646.71
Iteration 162 took 2.40 seconds (mean sampled reward: -4200.04). Current reward after update: -746.06, Optimal reward -646.71
Iteration 163 took 2.37 seconds (mean sampled reward: -3559.41). Current reward after update: -847.03, Optimal reward -646.71
Iteration 164 took 2.35 seconds (mean sampled reward: -3633.84). Current reward after update: -659.39, Optimal reward -646.71
Iteration 165 took 2.40 seconds (mean sampled reward: -3072.05). Current reward after update: -773.52, Optimal reward -646.71
Iteration 166 took 2.41 seconds (mean sampled reward: -3695.36). Current reward after update: -1178.17, Optimal reward -646.71
Iteration 167 took 2.45 seconds (mean sampled reward: -3342.78). Current reward after update: -629.25, Optimal reward -629.25
Iteration 168 took 2.39 seconds (mean sampled reward: -2766.35). Current reward after update: -1513.95, Optimal reward -629.25
Iteration 169 took 2.35 seconds (mean sampled reward: -2986.04). Current reward after update: -540.71, Optimal reward -540.71
Iteration 170 took 2.42 seconds (mean sampled reward: -3113.77). Current reward after update: -628.49, Optimal reward -540.71
Iteration 171 took 2.36 seconds (mean sampled reward: -2740.83). Current reward after update: -1341.58, Optimal reward -540.71
Iteration 172 took 2.38 seconds (mean sampled reward: -3690.32). Current reward after update: -554.27, Optimal reward -540.71
Iteration 173 took 2.50 seconds (mean sampled reward: -3983.82). Current reward after update: -738.37, Optimal reward -540.71
Iteration 174 took 2.40 seconds (mean sampled reward: -4305.25). Current reward after update: -747.28, Optimal reward -540.71
Iteration 175 took 2.42 seconds (mean sampled reward: -4770.71). Current reward after update: -725.96, Optimal reward -540.71
Iteration 176 took 2.53 seconds (mean sampled reward: -5129.40). Current reward after update: -777.93, Optimal reward -540.71
Iteration 177 took 2.39 seconds (mean sampled reward: -4587.96). Current reward after update: -749.37, Optimal reward -540.71
Iteration 178 took 2.39 seconds (mean sampled reward: -4126.93). Current reward after update: -785.58, Optimal reward -540.71
Iteration 179 took 2.47 seconds (mean sampled reward: -4805.89). Current reward after update: -2090.62, Optimal reward -540.71
Iteration 180 took 2.54 seconds (mean sampled reward: -5425.90). Current reward after update: -666.13, Optimal reward -540.71
Iteration 181 took 2.53 seconds (mean sampled reward: -5363.57). Current reward after update: -674.19, Optimal reward -540.71
Iteration 182 took 2.55 seconds (mean sampled reward: -6294.77). Current reward after update: -676.80, Optimal reward -540.71
Iteration 183 took 2.54 seconds (mean sampled reward: -6299.70). Current reward after update: -1030.24, Optimal reward -540.71
Iteration 184 took 2.45 seconds (mean sampled reward: -5040.44). Current reward after update: -1078.38, Optimal reward -540.71
Iteration 185 took 2.46 seconds (mean sampled reward: -4849.49). Current reward after update: -1770.59, Optimal reward -540.71
Iteration 186 took 2.40 seconds (mean sampled reward: -3625.28). Current reward after update: -675.81, Optimal reward -540.71
Iteration 187 took 2.48 seconds (mean sampled reward: -3350.58). Current reward after update: -2082.84, Optimal reward -540.71
Iteration 188 took 2.46 seconds (mean sampled reward: -4370.47). Current reward after update: -680.64, Optimal reward -540.71
Iteration 189 took 2.39 seconds (mean sampled reward: -3593.84). Current reward after update: -811.27, Optimal reward -540.71
Iteration 190 took 2.40 seconds (mean sampled reward: -3843.75). Current reward after update: -668.42, Optimal reward -540.71
Iteration 191 took 2.40 seconds (mean sampled reward: -3597.50). Current reward after update: -600.16, Optimal reward -540.71
Iteration 192 took 2.39 seconds (mean sampled reward: -3907.78). Current reward after update: -542.98, Optimal reward -540.71
Iteration 193 took 2.40 seconds (mean sampled reward: -4658.59). Current reward after update: -2787.47, Optimal reward -540.71
Iteration 194 took 2.40 seconds (mean sampled reward: -5052.30). Current reward after update: -719.63, Optimal reward -540.71
Iteration 195 took 2.47 seconds (mean sampled reward: -4948.62). Current reward after update: -743.83, Optimal reward -540.71
Iteration 196 took 2.38 seconds (mean sampled reward: -3755.77). Current reward after update: -755.95, Optimal reward -540.71
Iteration 197 took 2.58 seconds (mean sampled reward: -4168.31). Current reward after update: -699.61, Optimal reward -540.71
Iteration 198 took 2.43 seconds (mean sampled reward: -4381.57). Current reward after update: -685.52, Optimal reward -540.71
Iteration 199 took 2.49 seconds (mean sampled reward: -4213.88). Current reward after update: -757.55, Optimal reward -540.71
Iteration 200 took 2.46 seconds (mean sampled reward: -3569.93). Current reward after update: -943.77, Optimal reward -540.71
Iteration 1 took 2.42 seconds (mean sampled reward: -7628.74). Current reward after update: -7335.56, Optimal reward -7335.56
Iteration 2 took 2.40 seconds (mean sampled reward: -7586.97). Current reward after update: -7091.91, Optimal reward -7091.91
Iteration 3 took 2.40 seconds (mean sampled reward: -7427.26). Current reward after update: -6148.83, Optimal reward -6148.83
Iteration 4 took 2.36 seconds (mean sampled reward: -7188.55). Current reward after update: -3285.72, Optimal reward -3285.72
Iteration 5 took 2.57 seconds (mean sampled reward: -6315.88). Current reward after update: -2723.73, Optimal reward -2723.73
Iteration 6 took 2.40 seconds (mean sampled reward: -6329.46). Current reward after update: -2342.59, Optimal reward -2342.59
Iteration 7 took 2.36 seconds (mean sampled reward: -5835.33). Current reward after update: -2346.25, Optimal reward -2342.59
Iteration 8 took 2.41 seconds (mean sampled reward: -4220.84). Current reward after update: -1571.11, Optimal reward -1571.11
Iteration 9 took 2.25 seconds (mean sampled reward: -3795.90). Current reward after update: -1506.90, Optimal reward -1506.90
Iteration 10 took 2.33 seconds (mean sampled reward: -4059.20). Current reward after update: -1977.45, Optimal reward -1506.90
Iteration 11 took 2.31 seconds (mean sampled reward: -4035.90). Current reward after update: -1424.93, Optimal reward -1424.93
Iteration 12 took 2.30 seconds (mean sampled reward: -4296.71). Current reward after update: -1324.55, Optimal reward -1324.55
Iteration 13 took 2.39 seconds (mean sampled reward: -5218.15). Current reward after update: -1293.93, Optimal reward -1293.93
Iteration 14 took 2.30 seconds (mean sampled reward: -4320.62). Current reward after update: -1043.54, Optimal reward -1043.54
Iteration 15 took 2.35 seconds (mean sampled reward: -3575.30). Current reward after update: -1050.82, Optimal reward -1043.54
Iteration 16 took 2.22 seconds (mean sampled reward: -3451.04). Current reward after update: -993.62, Optimal reward -993.62
Iteration 17 took 2.33 seconds (mean sampled reward: -3072.70). Current reward after update: -1157.60, Optimal reward -993.62
Iteration 18 took 2.29 seconds (mean sampled reward: -2893.63). Current reward after update: -1086.58, Optimal reward -993.62
Iteration 19 took 2.46 seconds (mean sampled reward: -3037.33). Current reward after update: -1083.72, Optimal reward -993.62
Iteration 20 took 2.27 seconds (mean sampled reward: -3439.37). Current reward after update: -3171.81, Optimal reward -993.62
Iteration 21 took 2.26 seconds (mean sampled reward: -4075.40). Current reward after update: -785.87, Optimal reward -785.87
Iteration 22 took 2.40 seconds (mean sampled reward: -4232.13). Current reward after update: -908.55, Optimal reward -785.87
Iteration 23 took 2.38 seconds (mean sampled reward: -4166.31). Current reward after update: -802.82, Optimal reward -785.87
Iteration 24 took 2.31 seconds (mean sampled reward: -4061.49). Current reward after update: -633.89, Optimal reward -633.89
Iteration 25 took 2.43 seconds (mean sampled reward: -4241.94). Current reward after update: -798.82, Optimal reward -633.89
Iteration 26 took 2.27 seconds (mean sampled reward: -4047.03). Current reward after update: -747.17, Optimal reward -633.89
Iteration 27 took 2.39 seconds (mean sampled reward: -4163.82). Current reward after update: -6330.54, Optimal reward -633.89
Iteration 28 took 2.32 seconds (mean sampled reward: -4145.35). Current reward after update: -774.95, Optimal reward -633.89
Iteration 29 took 2.27 seconds (mean sampled reward: -3942.46). Current reward after update: -794.52, Optimal reward -633.89
Iteration 30 took 2.31 seconds (mean sampled reward: -3919.72). Current reward after update: -853.50, Optimal reward -633.89
Iteration 31 took 2.30 seconds (mean sampled reward: -2776.02). Current reward after update: -754.18, Optimal reward -633.89
Iteration 32 took 2.33 seconds (mean sampled reward: -3017.05). Current reward after update: -585.09, Optimal reward -585.09
Iteration 33 took 2.40 seconds (mean sampled reward: -3451.55). Current reward after update: -939.64, Optimal reward -585.09
Iteration 34 took 2.34 seconds (mean sampled reward: -3021.10). Current reward after update: -790.46, Optimal reward -585.09
Iteration 35 took 2.37 seconds (mean sampled reward: -4326.75). Current reward after update: -950.36, Optimal reward -585.09
Iteration 36 took 2.47 seconds (mean sampled reward: -3840.51). Current reward after update: -750.33, Optimal reward -585.09
Iteration 37 took 2.46 seconds (mean sampled reward: -3759.09). Current reward after update: -868.68, Optimal reward -585.09
Iteration 38 took 2.56 seconds (mean sampled reward: -4754.64). Current reward after update: -832.82, Optimal reward -585.09
Iteration 39 took 2.34 seconds (mean sampled reward: -3949.22). Current reward after update: -785.15, Optimal reward -585.09
Iteration 40 took 2.45 seconds (mean sampled reward: -3607.14). Current reward after update: -691.08, Optimal reward -585.09
Iteration 41 took 2.39 seconds (mean sampled reward: -3586.00). Current reward after update: -693.05, Optimal reward -585.09
Iteration 42 took 2.36 seconds (mean sampled reward: -4245.78). Current reward after update: -723.27, Optimal reward -585.09
Iteration 43 took 2.41 seconds (mean sampled reward: -4478.82). Current reward after update: -589.62, Optimal reward -585.09
Iteration 44 took 2.44 seconds (mean sampled reward: -4349.16). Current reward after update: -1401.75, Optimal reward -585.09
Iteration 45 took 2.39 seconds (mean sampled reward: -4162.75). Current reward after update: -600.33, Optimal reward -585.09
Iteration 46 took 2.31 seconds (mean sampled reward: -3706.24). Current reward after update: -946.01, Optimal reward -585.09
Iteration 47 took 2.35 seconds (mean sampled reward: -3881.82). Current reward after update: -561.50, Optimal reward -561.50
Iteration 48 took 2.39 seconds (mean sampled reward: -5238.77). Current reward after update: -601.07, Optimal reward -561.50
Iteration 49 took 2.33 seconds (mean sampled reward: -4885.79). Current reward after update: -596.13, Optimal reward -561.50
Iteration 50 took 2.27 seconds (mean sampled reward: -3828.60). Current reward after update: -514.28, Optimal reward -514.28
Iteration 51 took 2.20 seconds (mean sampled reward: -3796.67). Current reward after update: -701.72, Optimal reward -514.28
Iteration 52 took 2.24 seconds (mean sampled reward: -2844.56). Current reward after update: -572.04, Optimal reward -514.28
Iteration 53 took 2.27 seconds (mean sampled reward: -4421.40). Current reward after update: -514.52, Optimal reward -514.28
Iteration 54 took 2.32 seconds (mean sampled reward: -4739.35). Current reward after update: -464.79, Optimal reward -464.79
Iteration 55 took 2.37 seconds (mean sampled reward: -5297.67). Current reward after update: -542.34, Optimal reward -464.79
Iteration 56 took 2.38 seconds (mean sampled reward: -4449.44). Current reward after update: -540.53, Optimal reward -464.79
Iteration 57 took 2.48 seconds (mean sampled reward: -4684.42). Current reward after update: -463.74, Optimal reward -463.74
Iteration 58 took 2.40 seconds (mean sampled reward: -5738.73). Current reward after update: -460.70, Optimal reward -460.70
Iteration 59 took 2.34 seconds (mean sampled reward: -5779.12). Current reward after update: -572.82, Optimal reward -460.70
Iteration 60 took 2.38 seconds (mean sampled reward: -5894.49). Current reward after update: -424.65, Optimal reward -424.65
Iteration 61 took 2.26 seconds (mean sampled reward: -5612.68). Current reward after update: -499.86, Optimal reward -424.65
Iteration 62 took 2.24 seconds (mean sampled reward: -6292.62). Current reward after update: -744.00, Optimal reward -424.65
Iteration 63 took 2.35 seconds (mean sampled reward: -6386.84). Current reward after update: -827.49, Optimal reward -424.65
Iteration 64 took 2.37 seconds (mean sampled reward: -6417.38). Current reward after update: -1429.88, Optimal reward -424.65
Iteration 65 took 2.28 seconds (mean sampled reward: -4319.07). Current reward after update: -543.29, Optimal reward -424.65
Iteration 66 took 2.32 seconds (mean sampled reward: -4705.37). Current reward after update: -477.30, Optimal reward -424.65
Iteration 67 took 2.37 seconds (mean sampled reward: -4325.25). Current reward after update: -1301.78, Optimal reward -424.65
Iteration 68 took 2.32 seconds (mean sampled reward: -2976.43). Current reward after update: -528.24, Optimal reward -424.65
Iteration 69 took 2.35 seconds (mean sampled reward: -3437.04). Current reward after update: -542.10, Optimal reward -424.65
Iteration 70 took 2.34 seconds (mean sampled reward: -3570.29). Current reward after update: -496.36, Optimal reward -424.65
Iteration 71 took 2.46 seconds (mean sampled reward: -4764.21). Current reward after update: -642.22, Optimal reward -424.65
Iteration 72 took 2.38 seconds (mean sampled reward: -5495.42). Current reward after update: -684.53, Optimal reward -424.65
Iteration 73 took 2.50 seconds (mean sampled reward: -3168.32). Current reward after update: -535.40, Optimal reward -424.65
Iteration 74 took 2.43 seconds (mean sampled reward: -2849.20). Current reward after update: -913.49, Optimal reward -424.65
Iteration 75 took 2.30 seconds (mean sampled reward: -2113.95). Current reward after update: -433.86, Optimal reward -424.65
Iteration 76 took 2.27 seconds (mean sampled reward: -2536.14). Current reward after update: -664.98, Optimal reward -424.65
Iteration 77 took 2.32 seconds (mean sampled reward: -2816.23). Current reward after update: -539.37, Optimal reward -424.65
Iteration 78 took 2.27 seconds (mean sampled reward: -2413.21). Current reward after update: -558.95, Optimal reward -424.65
Iteration 79 took 2.30 seconds (mean sampled reward: -2783.66). Current reward after update: -791.73, Optimal reward -424.65
Iteration 80 took 2.36 seconds (mean sampled reward: -2404.38). Current reward after update: -489.32, Optimal reward -424.65
Iteration 81 took 2.36 seconds (mean sampled reward: -2272.95). Current reward after update: -587.16, Optimal reward -424.65
Iteration 82 took 2.38 seconds (mean sampled reward: -2701.42). Current reward after update: -1744.46, Optimal reward -424.65
Iteration 83 took 2.35 seconds (mean sampled reward: -5051.12). Current reward after update: -504.46, Optimal reward -424.65
Iteration 84 took 2.33 seconds (mean sampled reward: -5661.43). Current reward after update: -666.49, Optimal reward -424.65
Iteration 85 took 2.45 seconds (mean sampled reward: -4170.33). Current reward after update: -579.03, Optimal reward -424.65
Iteration 86 took 2.34 seconds (mean sampled reward: -3069.77). Current reward after update: -540.33, Optimal reward -424.65
Iteration 87 took 2.28 seconds (mean sampled reward: -3909.28). Current reward after update: -1091.12, Optimal reward -424.65
Iteration 88 took 2.38 seconds (mean sampled reward: -3535.94). Current reward after update: -2787.14, Optimal reward -424.65
Iteration 89 took 2.30 seconds (mean sampled reward: -3475.83). Current reward after update: -511.56, Optimal reward -424.65
Iteration 90 took 2.42 seconds (mean sampled reward: -3287.04). Current reward after update: -638.32, Optimal reward -424.65
Iteration 91 took 2.33 seconds (mean sampled reward: -2850.94). Current reward after update: -689.99, Optimal reward -424.65
Iteration 92 took 2.36 seconds (mean sampled reward: -2580.35). Current reward after update: -605.30, Optimal reward -424.65
Iteration 93 took 2.33 seconds (mean sampled reward: -2542.58). Current reward after update: -377.77, Optimal reward -377.77
Iteration 94 took 2.37 seconds (mean sampled reward: -2312.96). Current reward after update: -426.65, Optimal reward -377.77
Iteration 95 took 2.38 seconds (mean sampled reward: -2860.04). Current reward after update: -444.99, Optimal reward -377.77
Iteration 96 took 2.41 seconds (mean sampled reward: -2369.60). Current reward after update: -465.84, Optimal reward -377.77
Iteration 97 took 2.46 seconds (mean sampled reward: -2924.64). Current reward after update: -524.75, Optimal reward -377.77
Iteration 98 took 2.47 seconds (mean sampled reward: -2191.63). Current reward after update: -552.74, Optimal reward -377.77
Iteration 99 took 2.35 seconds (mean sampled reward: -2120.69). Current reward after update: -513.02, Optimal reward -377.77
Iteration 100 took 2.35 seconds (mean sampled reward: -3001.76). Current reward after update: -530.59, Optimal reward -377.77
Iteration 101 took 2.46 seconds (mean sampled reward: -2677.28). Current reward after update: -449.03, Optimal reward -377.77
Iteration 102 took 2.37 seconds (mean sampled reward: -2605.75). Current reward after update: -784.66, Optimal reward -377.77
Iteration 103 took 2.35 seconds (mean sampled reward: -4924.12). Current reward after update: -499.18, Optimal reward -377.77
Iteration 104 took 2.44 seconds (mean sampled reward: -4118.70). Current reward after update: -600.05, Optimal reward -377.77
Iteration 105 took 2.37 seconds (mean sampled reward: -4158.84). Current reward after update: -6383.71, Optimal reward -377.77
Iteration 106 took 2.44 seconds (mean sampled reward: -3960.47). Current reward after update: -472.79, Optimal reward -377.77
Iteration 107 took 2.39 seconds (mean sampled reward: -4162.79). Current reward after update: -480.07, Optimal reward -377.77
Iteration 108 took 2.58 seconds (mean sampled reward: -4197.69). Current reward after update: -5006.86, Optimal reward -377.77
Iteration 109 took 2.45 seconds (mean sampled reward: -3961.56). Current reward after update: -593.54, Optimal reward -377.77
Iteration 110 took 2.35 seconds (mean sampled reward: -3943.90). Current reward after update: -495.38, Optimal reward -377.77
Iteration 111 took 2.53 seconds (mean sampled reward: -4117.93). Current reward after update: -441.08, Optimal reward -377.77
Iteration 112 took 2.40 seconds (mean sampled reward: -3586.24). Current reward after update: -533.74, Optimal reward -377.77
Iteration 113 took 2.43 seconds (mean sampled reward: -3809.63). Current reward after update: -464.45, Optimal reward -377.77
Iteration 114 took 2.46 seconds (mean sampled reward: -3273.63). Current reward after update: -472.94, Optimal reward -377.77
Iteration 115 took 2.38 seconds (mean sampled reward: -2452.04). Current reward after update: -780.15, Optimal reward -377.77
Iteration 116 took 2.69 seconds (mean sampled reward: -3267.94). Current reward after update: -506.55, Optimal reward -377.77
Iteration 117 took 2.42 seconds (mean sampled reward: -2804.98). Current reward after update: -594.71, Optimal reward -377.77
Iteration 118 took 2.38 seconds (mean sampled reward: -2591.98). Current reward after update: -905.82, Optimal reward -377.77
Iteration 119 took 2.36 seconds (mean sampled reward: -3051.23). Current reward after update: -512.23, Optimal reward -377.77
Iteration 120 took 2.34 seconds (mean sampled reward: -3155.29). Current reward after update: -532.91, Optimal reward -377.77
Iteration 121 took 2.36 seconds (mean sampled reward: -3324.85). Current reward after update: -495.43, Optimal reward -377.77
Iteration 122 took 2.43 seconds (mean sampled reward: -3413.48). Current reward after update: -492.50, Optimal reward -377.77
Iteration 123 took 2.45 seconds (mean sampled reward: -4118.16). Current reward after update: -648.25, Optimal reward -377.77
Iteration 124 took 2.40 seconds (mean sampled reward: -3641.66). Current reward after update: -1835.34, Optimal reward -377.77
Iteration 125 took 2.36 seconds (mean sampled reward: -3463.06). Current reward after update: -486.79, Optimal reward -377.77
Iteration 126 took 2.32 seconds (mean sampled reward: -5520.29). Current reward after update: -570.14, Optimal reward -377.77
Iteration 127 took 2.38 seconds (mean sampled reward: -5562.59). Current reward after update: -646.32, Optimal reward -377.77
Iteration 128 took 2.37 seconds (mean sampled reward: -5327.15). Current reward after update: -551.97, Optimal reward -377.77
Iteration 129 took 2.36 seconds (mean sampled reward: -6097.80). Current reward after update: -605.38, Optimal reward -377.77
Iteration 130 took 2.32 seconds (mean sampled reward: -5316.64). Current reward after update: -580.32, Optimal reward -377.77
Iteration 131 took 2.43 seconds (mean sampled reward: -2815.28). Current reward after update: -1312.19, Optimal reward -377.77
Iteration 132 took 2.40 seconds (mean sampled reward: -3881.51). Current reward after update: -1328.00, Optimal reward -377.77
Iteration 133 took 2.35 seconds (mean sampled reward: -3057.88). Current reward after update: -441.53, Optimal reward -377.77
Iteration 134 took 2.36 seconds (mean sampled reward: -4528.38). Current reward after update: -668.11, Optimal reward -377.77
Iteration 135 took 2.33 seconds (mean sampled reward: -4982.83). Current reward after update: -616.60, Optimal reward -377.77
Iteration 136 took 2.34 seconds (mean sampled reward: -4818.49). Current reward after update: -4998.46, Optimal reward -377.77
Iteration 137 took 2.39 seconds (mean sampled reward: -6030.04). Current reward after update: -434.08, Optimal reward -377.77
Iteration 138 took 2.42 seconds (mean sampled reward: -4727.80). Current reward after update: -494.45, Optimal reward -377.77
Iteration 139 took 2.35 seconds (mean sampled reward: -4656.81). Current reward after update: -531.26, Optimal reward -377.77
Iteration 140 took 2.34 seconds (mean sampled reward: -4276.73). Current reward after update: -1527.97, Optimal reward -377.77
Iteration 141 took 2.36 seconds (mean sampled reward: -4059.57). Current reward after update: -499.04, Optimal reward -377.77
Iteration 142 took 2.36 seconds (mean sampled reward: -3434.50). Current reward after update: -476.58, Optimal reward -377.77
Iteration 143 took 2.40 seconds (mean sampled reward: -4622.19). Current reward after update: -580.85, Optimal reward -377.77
Iteration 144 took 2.39 seconds (mean sampled reward: -3319.28). Current reward after update: -1564.77, Optimal reward -377.77
Iteration 145 took 2.43 seconds (mean sampled reward: -2974.53). Current reward after update: -433.70, Optimal reward -377.77
Iteration 146 took 2.39 seconds (mean sampled reward: -2426.68). Current reward after update: -501.05, Optimal reward -377.77
Iteration 147 took 2.42 seconds (mean sampled reward: -2884.86). Current reward after update: -533.89, Optimal reward -377.77
Iteration 148 took 2.46 seconds (mean sampled reward: -2754.32). Current reward after update: -445.96, Optimal reward -377.77
Iteration 149 took 2.38 seconds (mean sampled reward: -3168.21). Current reward after update: -1027.24, Optimal reward -377.77
Iteration 150 took 2.41 seconds (mean sampled reward: -3096.75). Current reward after update: -1098.40, Optimal reward -377.77
Iteration 151 took 2.33 seconds (mean sampled reward: -4546.81). Current reward after update: -502.40, Optimal reward -377.77
Iteration 152 took 2.35 seconds (mean sampled reward: -3691.55). Current reward after update: -1192.61, Optimal reward -377.77
Iteration 153 took 2.37 seconds (mean sampled reward: -3695.56). Current reward after update: -521.92, Optimal reward -377.77
Iteration 154 took 2.41 seconds (mean sampled reward: -5183.72). Current reward after update: -597.96, Optimal reward -377.77
Iteration 155 took 2.36 seconds (mean sampled reward: -4267.23). Current reward after update: -645.07, Optimal reward -377.77
Iteration 156 took 2.25 seconds (mean sampled reward: -6007.46). Current reward after update: -593.63, Optimal reward -377.77
Iteration 157 took 2.33 seconds (mean sampled reward: -5520.81). Current reward after update: -599.42, Optimal reward -377.77
Iteration 158 took 2.35 seconds (mean sampled reward: -4608.48). Current reward after update: -547.07, Optimal reward -377.77
Iteration 159 took 2.36 seconds (mean sampled reward: -3458.64). Current reward after update: -597.19, Optimal reward -377.77
Iteration 160 took 2.35 seconds (mean sampled reward: -3037.80). Current reward after update: -609.61, Optimal reward -377.77
Iteration 161 took 2.41 seconds (mean sampled reward: -2292.46). Current reward after update: -594.31, Optimal reward -377.77
Iteration 162 took 2.36 seconds (mean sampled reward: -2495.80). Current reward after update: -532.82, Optimal reward -377.77
Iteration 163 took 2.44 seconds (mean sampled reward: -3004.37). Current reward after update: -549.56, Optimal reward -377.77
Iteration 164 took 2.44 seconds (mean sampled reward: -3177.53). Current reward after update: -698.85, Optimal reward -377.77
Iteration 165 took 2.41 seconds (mean sampled reward: -3459.06). Current reward after update: -556.66, Optimal reward -377.77
Iteration 166 took 2.36 seconds (mean sampled reward: -4167.51). Current reward after update: -498.38, Optimal reward -377.77
Iteration 167 took 2.34 seconds (mean sampled reward: -4957.63). Current reward after update: -2546.19, Optimal reward -377.77
Iteration 168 took 2.35 seconds (mean sampled reward: -5404.76). Current reward after update: -617.09, Optimal reward -377.77
Iteration 169 took 2.31 seconds (mean sampled reward: -4930.08). Current reward after update: -1234.76, Optimal reward -377.77
Iteration 170 took 2.36 seconds (mean sampled reward: -5324.57). Current reward after update: -586.91, Optimal reward -377.77
Iteration 171 took 2.28 seconds (mean sampled reward: -6258.93). Current reward after update: -615.13, Optimal reward -377.77
Iteration 172 took 2.33 seconds (mean sampled reward: -5659.26). Current reward after update: -595.18, Optimal reward -377.77
Iteration 173 took 2.31 seconds (mean sampled reward: -6061.12). Current reward after update: -647.72, Optimal reward -377.77
Iteration 174 took 2.41 seconds (mean sampled reward: -4838.00). Current reward after update: -606.68, Optimal reward -377.77
Iteration 175 took 2.35 seconds (mean sampled reward: -4901.29). Current reward after update: -901.69, Optimal reward -377.77
Iteration 176 took 2.34 seconds (mean sampled reward: -5059.46). Current reward after update: -554.79, Optimal reward -377.77
Iteration 177 took 2.37 seconds (mean sampled reward: -5659.56). Current reward after update: -7019.74, Optimal reward -377.77
Iteration 178 took 2.35 seconds (mean sampled reward: -4924.77). Current reward after update: -590.54, Optimal reward -377.77
Iteration 179 took 2.29 seconds (mean sampled reward: -5650.03). Current reward after update: -499.82, Optimal reward -377.77
Iteration 180 took 2.30 seconds (mean sampled reward: -5355.35). Current reward after update: -513.31, Optimal reward -377.77
Iteration 181 took 2.43 seconds (mean sampled reward: -5074.97). Current reward after update: -533.08, Optimal reward -377.77
Iteration 182 took 2.37 seconds (mean sampled reward: -4743.09). Current reward after update: -832.63, Optimal reward -377.77
Iteration 183 took 2.36 seconds (mean sampled reward: -4721.06). Current reward after update: -578.66, Optimal reward -377.77
Iteration 184 took 2.35 seconds (mean sampled reward: -4164.03). Current reward after update: -780.60, Optimal reward -377.77
Iteration 185 took 2.31 seconds (mean sampled reward: -4856.80). Current reward after update: -864.70, Optimal reward -377.77
Iteration 186 took 2.33 seconds (mean sampled reward: -5190.54). Current reward after update: -7065.60, Optimal reward -377.77
Iteration 187 took 2.33 seconds (mean sampled reward: -4867.85). Current reward after update: -782.88, Optimal reward -377.77
Iteration 188 took 2.29 seconds (mean sampled reward: -6471.91). Current reward after update: -843.17, Optimal reward -377.77
Iteration 189 took 2.35 seconds (mean sampled reward: -5664.90). Current reward after update: -725.64, Optimal reward -377.77
Iteration 190 took 2.31 seconds (mean sampled reward: -5827.07). Current reward after update: -803.18, Optimal reward -377.77
Iteration 191 took 2.29 seconds (mean sampled reward: -5304.70). Current reward after update: -674.80, Optimal reward -377.77
Iteration 192 took 2.31 seconds (mean sampled reward: -5298.48). Current reward after update: -593.38, Optimal reward -377.77
Iteration 193 took 2.46 seconds (mean sampled reward: -3501.09). Current reward after update: -596.72, Optimal reward -377.77
Iteration 194 took 2.47 seconds (mean sampled reward: -3503.85). Current reward after update: -2424.64, Optimal reward -377.77
Iteration 195 took 2.45 seconds (mean sampled reward: -3193.76). Current reward after update: -560.02, Optimal reward -377.77
Iteration 196 took 2.41 seconds (mean sampled reward: -3684.22). Current reward after update: -584.99, Optimal reward -377.77
Iteration 197 took 2.39 seconds (mean sampled reward: -3675.94). Current reward after update: -671.12, Optimal reward -377.77
Iteration 198 took 2.35 seconds (mean sampled reward: -5418.80). Current reward after update: -666.83, Optimal reward -377.77
Iteration 199 took 2.40 seconds (mean sampled reward: -4968.58). Current reward after update: -698.52, Optimal reward -377.77
Iteration 200 took 2.45 seconds (mean sampled reward: -4436.70). Current reward after update: -732.39, Optimal reward -377.77
Max force: 40 Sigma: 0.2 mean rewards: -468.33470912376106, best rewards:-377.77338287448634

Iteration 1 took 2.39 seconds (mean sampled reward: -7619.49). Current reward after update: -6636.65, Optimal reward -6636.65
Iteration 2 took 2.33 seconds (mean sampled reward: -7123.71). Current reward after update: -6070.77, Optimal reward -6070.77
Iteration 3 took 2.40 seconds (mean sampled reward: -6921.71). Current reward after update: -5197.63, Optimal reward -5197.63
Iteration 4 took 2.39 seconds (mean sampled reward: -6616.33). Current reward after update: -3360.27, Optimal reward -3360.27
Iteration 5 took 2.35 seconds (mean sampled reward: -6652.83). Current reward after update: -2486.08, Optimal reward -2486.08
Iteration 6 took 2.37 seconds (mean sampled reward: -6197.02). Current reward after update: -2549.03, Optimal reward -2486.08
Iteration 7 took 2.37 seconds (mean sampled reward: -6476.38). Current reward after update: -2314.39, Optimal reward -2314.39
Iteration 8 took 2.47 seconds (mean sampled reward: -6528.25). Current reward after update: -2064.19, Optimal reward -2064.19
Iteration 9 took 2.64 seconds (mean sampled reward: -6155.18). Current reward after update: -2155.62, Optimal reward -2064.19
Iteration 10 took 2.43 seconds (mean sampled reward: -6512.05). Current reward after update: -2409.59, Optimal reward -2064.19
Iteration 11 took 2.44 seconds (mean sampled reward: -6623.43). Current reward after update: -2336.78, Optimal reward -2064.19
Iteration 12 took 2.32 seconds (mean sampled reward: -6445.28). Current reward after update: -2144.00, Optimal reward -2064.19
Iteration 13 took 2.40 seconds (mean sampled reward: -4792.34). Current reward after update: -2029.03, Optimal reward -2029.03
Iteration 14 took 2.54 seconds (mean sampled reward: -4932.08). Current reward after update: -1977.32, Optimal reward -1977.32
Iteration 15 took 2.30 seconds (mean sampled reward: -4694.45). Current reward after update: -2791.19, Optimal reward -1977.32
Iteration 16 took 2.40 seconds (mean sampled reward: -4993.99). Current reward after update: -2033.72, Optimal reward -1977.32
Iteration 17 took 2.35 seconds (mean sampled reward: -5040.44). Current reward after update: -2067.82, Optimal reward -1977.32
Iteration 18 took 2.27 seconds (mean sampled reward: -5153.55). Current reward after update: -1464.26, Optimal reward -1464.26
Iteration 19 took 2.33 seconds (mean sampled reward: -5533.02). Current reward after update: -4698.94, Optimal reward -1464.26
Iteration 20 took 2.28 seconds (mean sampled reward: -5122.81). Current reward after update: -2442.51, Optimal reward -1464.26
Iteration 21 took 2.32 seconds (mean sampled reward: -5628.90). Current reward after update: -2123.28, Optimal reward -1464.26
Iteration 22 took 2.35 seconds (mean sampled reward: -6045.21). Current reward after update: -1869.00, Optimal reward -1464.26
Iteration 23 took 2.26 seconds (mean sampled reward: -5949.93). Current reward after update: -1856.32, Optimal reward -1464.26
Iteration 24 took 2.21 seconds (mean sampled reward: -5599.33). Current reward after update: -1819.39, Optimal reward -1464.26
Iteration 25 took 2.30 seconds (mean sampled reward: -5204.34). Current reward after update: -3739.35, Optimal reward -1464.26
Iteration 26 took 2.16 seconds (mean sampled reward: -5895.12). Current reward after update: -2137.96, Optimal reward -1464.26
Iteration 27 took 2.21 seconds (mean sampled reward: -5946.89). Current reward after update: -2163.44, Optimal reward -1464.26
Iteration 28 took 2.26 seconds (mean sampled reward: -6362.29). Current reward after update: -1938.09, Optimal reward -1464.26
Iteration 29 took 2.40 seconds (mean sampled reward: -6890.89). Current reward after update: -2039.56, Optimal reward -1464.26
Iteration 30 took 2.35 seconds (mean sampled reward: -6960.72). Current reward after update: -2058.80, Optimal reward -1464.26
Iteration 31 took 2.40 seconds (mean sampled reward: -7173.48). Current reward after update: -2666.54, Optimal reward -1464.26
Iteration 32 took 2.39 seconds (mean sampled reward: -6798.43). Current reward after update: -2441.20, Optimal reward -1464.26
Iteration 33 took 2.33 seconds (mean sampled reward: -6505.63). Current reward after update: -1952.80, Optimal reward -1464.26
Iteration 34 took 2.44 seconds (mean sampled reward: -6624.64). Current reward after update: -1920.33, Optimal reward -1464.26
Iteration 35 took 2.39 seconds (mean sampled reward: -6285.76). Current reward after update: -1881.40, Optimal reward -1464.26
Iteration 36 took 2.37 seconds (mean sampled reward: -6713.54). Current reward after update: -1898.04, Optimal reward -1464.26
Iteration 37 took 2.31 seconds (mean sampled reward: -6440.70). Current reward after update: -2459.32, Optimal reward -1464.26
Iteration 38 took 2.28 seconds (mean sampled reward: -6366.51). Current reward after update: -2370.38, Optimal reward -1464.26
Iteration 39 took 2.36 seconds (mean sampled reward: -6496.26). Current reward after update: -2525.35, Optimal reward -1464.26
Iteration 40 took 2.25 seconds (mean sampled reward: -6818.09). Current reward after update: -2087.62, Optimal reward -1464.26
Iteration 41 took 2.30 seconds (mean sampled reward: -6354.15). Current reward after update: -2327.81, Optimal reward -1464.26
Iteration 42 took 2.25 seconds (mean sampled reward: -6318.42). Current reward after update: -2228.78, Optimal reward -1464.26
Iteration 43 took 2.22 seconds (mean sampled reward: -6656.88). Current reward after update: -2356.17, Optimal reward -1464.26
Iteration 44 took 2.26 seconds (mean sampled reward: -5911.84). Current reward after update: -2093.04, Optimal reward -1464.26
Iteration 45 took 2.21 seconds (mean sampled reward: -5563.78). Current reward after update: -2091.92, Optimal reward -1464.26
Iteration 46 took 2.30 seconds (mean sampled reward: -6129.15). Current reward after update: -1655.04, Optimal reward -1464.26
Iteration 47 took 2.26 seconds (mean sampled reward: -6292.04). Current reward after update: -1854.33, Optimal reward -1464.26
Iteration 48 took 2.29 seconds (mean sampled reward: -5737.83). Current reward after update: -1358.69, Optimal reward -1358.69
Iteration 49 took 2.31 seconds (mean sampled reward: -6332.56). Current reward after update: -1530.36, Optimal reward -1358.69
Iteration 50 took 2.33 seconds (mean sampled reward: -6392.46). Current reward after update: -1966.59, Optimal reward -1358.69
Iteration 51 took 2.44 seconds (mean sampled reward: -5253.36). Current reward after update: -2044.01, Optimal reward -1358.69
Iteration 52 took 2.38 seconds (mean sampled reward: -5894.21). Current reward after update: -1593.98, Optimal reward -1358.69
Iteration 53 took 2.46 seconds (mean sampled reward: -6242.27). Current reward after update: -1810.85, Optimal reward -1358.69
Iteration 54 took 2.40 seconds (mean sampled reward: -6773.77). Current reward after update: -2046.32, Optimal reward -1358.69
Iteration 55 took 2.52 seconds (mean sampled reward: -7279.72). Current reward after update: -2114.81, Optimal reward -1358.69
Iteration 56 took 2.38 seconds (mean sampled reward: -6841.10). Current reward after update: -2068.19, Optimal reward -1358.69
Iteration 57 took 2.50 seconds (mean sampled reward: -7242.05). Current reward after update: -1355.83, Optimal reward -1355.83
Iteration 58 took 2.31 seconds (mean sampled reward: -6439.60). Current reward after update: -1385.85, Optimal reward -1355.83
Iteration 59 took 2.36 seconds (mean sampled reward: -6591.28). Current reward after update: -1652.26, Optimal reward -1355.83
Iteration 60 took 2.38 seconds (mean sampled reward: -6605.79). Current reward after update: -2147.07, Optimal reward -1355.83
Iteration 61 took 2.34 seconds (mean sampled reward: -6131.19). Current reward after update: -2024.75, Optimal reward -1355.83
Iteration 62 took 2.61 seconds (mean sampled reward: -6358.52). Current reward after update: -1587.85, Optimal reward -1355.83
Iteration 63 took 2.28 seconds (mean sampled reward: -6924.39). Current reward after update: -1869.11, Optimal reward -1355.83
Iteration 64 took 2.29 seconds (mean sampled reward: -6846.59). Current reward after update: -2542.16, Optimal reward -1355.83
Iteration 65 took 2.33 seconds (mean sampled reward: -6673.71). Current reward after update: -2159.07, Optimal reward -1355.83
Iteration 66 took 2.35 seconds (mean sampled reward: -5913.89). Current reward after update: -2153.54, Optimal reward -1355.83
Iteration 67 took 2.32 seconds (mean sampled reward: -6040.88). Current reward after update: -1913.09, Optimal reward -1355.83
Iteration 68 took 2.31 seconds (mean sampled reward: -6192.23). Current reward after update: -1700.64, Optimal reward -1355.83
Iteration 69 took 2.41 seconds (mean sampled reward: -6345.85). Current reward after update: -2086.78, Optimal reward -1355.83
Iteration 70 took 2.31 seconds (mean sampled reward: -5807.83). Current reward after update: -5281.31, Optimal reward -1355.83
Iteration 71 took 2.28 seconds (mean sampled reward: -6139.97). Current reward after update: -1527.65, Optimal reward -1355.83
Iteration 72 took 2.32 seconds (mean sampled reward: -4892.66). Current reward after update: -1242.76, Optimal reward -1242.76
Iteration 73 took 2.34 seconds (mean sampled reward: -4885.61). Current reward after update: -1667.79, Optimal reward -1242.76
Iteration 74 took 2.44 seconds (mean sampled reward: -5471.33). Current reward after update: -1330.51, Optimal reward -1242.76
Iteration 75 took 2.32 seconds (mean sampled reward: -5033.89). Current reward after update: -1402.74, Optimal reward -1242.76
Iteration 76 took 2.34 seconds (mean sampled reward: -5814.76). Current reward after update: -1624.37, Optimal reward -1242.76
Iteration 77 took 2.37 seconds (mean sampled reward: -5924.09). Current reward after update: -1503.87, Optimal reward -1242.76
Iteration 78 took 2.48 seconds (mean sampled reward: -6579.61). Current reward after update: -1864.22, Optimal reward -1242.76
Iteration 79 took 2.36 seconds (mean sampled reward: -5795.41). Current reward after update: -1647.15, Optimal reward -1242.76
Iteration 80 took 2.34 seconds (mean sampled reward: -5326.32). Current reward after update: -1786.54, Optimal reward -1242.76
Iteration 81 took 2.28 seconds (mean sampled reward: -5502.13). Current reward after update: -2087.09, Optimal reward -1242.76
Iteration 82 took 2.32 seconds (mean sampled reward: -5261.81). Current reward after update: -1571.90, Optimal reward -1242.76
Iteration 83 took 2.40 seconds (mean sampled reward: -5173.18). Current reward after update: -1777.73, Optimal reward -1242.76
Iteration 84 took 2.25 seconds (mean sampled reward: -5003.67). Current reward after update: -1872.20, Optimal reward -1242.76
Iteration 85 took 2.29 seconds (mean sampled reward: -5865.27). Current reward after update: -1909.32, Optimal reward -1242.76
Iteration 86 took 2.23 seconds (mean sampled reward: -4659.53). Current reward after update: -1793.49, Optimal reward -1242.76
Iteration 87 took 2.26 seconds (mean sampled reward: -4719.49). Current reward after update: -1858.42, Optimal reward -1242.76
Iteration 88 took 2.16 seconds (mean sampled reward: -4018.93). Current reward after update: -1664.66, Optimal reward -1242.76
Iteration 89 took 2.31 seconds (mean sampled reward: -3899.08). Current reward after update: -1556.70, Optimal reward -1242.76
Iteration 90 took 2.30 seconds (mean sampled reward: -3856.31). Current reward after update: -1578.61, Optimal reward -1242.76
Iteration 91 took 2.35 seconds (mean sampled reward: -4013.37). Current reward after update: -1494.18, Optimal reward -1242.76
Iteration 92 took 2.34 seconds (mean sampled reward: -4853.72). Current reward after update: -1455.31, Optimal reward -1242.76
Iteration 93 took 2.29 seconds (mean sampled reward: -4965.99). Current reward after update: -1615.76, Optimal reward -1242.76
Iteration 94 took 2.34 seconds (mean sampled reward: -5813.85). Current reward after update: -1556.85, Optimal reward -1242.76
Iteration 95 took 2.44 seconds (mean sampled reward: -6050.41). Current reward after update: -1901.70, Optimal reward -1242.76
Iteration 96 took 2.34 seconds (mean sampled reward: -5853.25). Current reward after update: -1348.01, Optimal reward -1242.76
Iteration 97 took 2.44 seconds (mean sampled reward: -5419.76). Current reward after update: -2533.02, Optimal reward -1242.76
Iteration 98 took 2.40 seconds (mean sampled reward: -5579.48). Current reward after update: -1874.20, Optimal reward -1242.76
Iteration 99 took 2.24 seconds (mean sampled reward: -5345.83). Current reward after update: -1868.28, Optimal reward -1242.76
Iteration 100 took 2.32 seconds (mean sampled reward: -5020.33). Current reward after update: -1683.31, Optimal reward -1242.76
Iteration 101 took 2.42 seconds (mean sampled reward: -5173.57). Current reward after update: -1626.62, Optimal reward -1242.76
Iteration 102 took 2.38 seconds (mean sampled reward: -5023.07). Current reward after update: -1342.32, Optimal reward -1242.76
Iteration 103 took 2.47 seconds (mean sampled reward: -5625.23). Current reward after update: -2156.81, Optimal reward -1242.76
Iteration 104 took 2.40 seconds (mean sampled reward: -6348.33). Current reward after update: -1817.11, Optimal reward -1242.76
Iteration 105 took 2.40 seconds (mean sampled reward: -6219.15). Current reward after update: -1889.53, Optimal reward -1242.76
Iteration 106 took 2.46 seconds (mean sampled reward: -5482.54). Current reward after update: -1820.89, Optimal reward -1242.76
Iteration 107 took 2.58 seconds (mean sampled reward: -5732.54). Current reward after update: -2502.93, Optimal reward -1242.76
Iteration 108 took 2.47 seconds (mean sampled reward: -5690.45). Current reward after update: -1935.50, Optimal reward -1242.76
Iteration 109 took 2.45 seconds (mean sampled reward: -6053.14). Current reward after update: -2021.37, Optimal reward -1242.76
Iteration 110 took 2.58 seconds (mean sampled reward: -5722.94). Current reward after update: -2189.84, Optimal reward -1242.76
Iteration 111 took 2.59 seconds (mean sampled reward: -5655.74). Current reward after update: -2076.75, Optimal reward -1242.76
Iteration 112 took 2.67 seconds (mean sampled reward: -5835.11). Current reward after update: -2171.28, Optimal reward -1242.76
Iteration 113 took 2.57 seconds (mean sampled reward: -5478.14). Current reward after update: -1675.62, Optimal reward -1242.76
Iteration 114 took 2.39 seconds (mean sampled reward: -5910.37). Current reward after update: -3001.76, Optimal reward -1242.76
Iteration 115 took 2.47 seconds (mean sampled reward: -5555.73). Current reward after update: -1997.26, Optimal reward -1242.76
Iteration 116 took 2.45 seconds (mean sampled reward: -5495.25). Current reward after update: -6732.64, Optimal reward -1242.76
Iteration 117 took 2.54 seconds (mean sampled reward: -5388.80). Current reward after update: -1586.76, Optimal reward -1242.76
Iteration 118 took 2.51 seconds (mean sampled reward: -5439.11). Current reward after update: -1911.31, Optimal reward -1242.76
Iteration 119 took 2.57 seconds (mean sampled reward: -5556.50). Current reward after update: -1466.47, Optimal reward -1242.76
Iteration 120 took 2.42 seconds (mean sampled reward: -5602.53). Current reward after update: -1883.84, Optimal reward -1242.76
Iteration 121 took 2.39 seconds (mean sampled reward: -5342.55). Current reward after update: -1429.94, Optimal reward -1242.76
Iteration 122 took 2.43 seconds (mean sampled reward: -5473.93). Current reward after update: -1508.41, Optimal reward -1242.76
Iteration 123 took 2.43 seconds (mean sampled reward: -5511.48). Current reward after update: -1796.30, Optimal reward -1242.76
Iteration 124 took 2.38 seconds (mean sampled reward: -5879.19). Current reward after update: -1227.02, Optimal reward -1227.02
Iteration 125 took 2.39 seconds (mean sampled reward: -5398.10). Current reward after update: -1116.88, Optimal reward -1116.88
Iteration 126 took 2.38 seconds (mean sampled reward: -5364.77). Current reward after update: -981.98, Optimal reward -981.98
Iteration 127 took 2.35 seconds (mean sampled reward: -5666.93). Current reward after update: -1151.98, Optimal reward -981.98
Iteration 128 took 2.40 seconds (mean sampled reward: -5721.26). Current reward after update: -1337.00, Optimal reward -981.98
Iteration 129 took 2.37 seconds (mean sampled reward: -5369.19). Current reward after update: -1402.95, Optimal reward -981.98
Iteration 130 took 2.49 seconds (mean sampled reward: -5090.54). Current reward after update: -1254.48, Optimal reward -981.98
Iteration 131 took 2.41 seconds (mean sampled reward: -5333.22). Current reward after update: -1331.87, Optimal reward -981.98
Iteration 132 took 2.36 seconds (mean sampled reward: -5360.44). Current reward after update: -1452.53, Optimal reward -981.98
Iteration 133 took 2.33 seconds (mean sampled reward: -5122.14). Current reward after update: -1750.68, Optimal reward -981.98
Iteration 134 took 2.39 seconds (mean sampled reward: -5486.92). Current reward after update: -1534.33, Optimal reward -981.98
Iteration 135 took 2.39 seconds (mean sampled reward: -5196.32). Current reward after update: -1703.04, Optimal reward -981.98
Iteration 136 took 2.38 seconds (mean sampled reward: -5024.02). Current reward after update: -1315.96, Optimal reward -981.98
Iteration 137 took 2.36 seconds (mean sampled reward: -5110.34). Current reward after update: -1225.01, Optimal reward -981.98
Iteration 138 took 2.39 seconds (mean sampled reward: -5205.18). Current reward after update: -1501.61, Optimal reward -981.98
Iteration 139 took 2.38 seconds (mean sampled reward: -4790.83). Current reward after update: -1531.19, Optimal reward -981.98
Iteration 140 took 2.34 seconds (mean sampled reward: -4513.88). Current reward after update: -1184.75, Optimal reward -981.98
Iteration 141 took 2.33 seconds (mean sampled reward: -4520.82). Current reward after update: -1348.96, Optimal reward -981.98
Iteration 142 took 2.31 seconds (mean sampled reward: -4754.85). Current reward after update: -1509.01, Optimal reward -981.98
Iteration 143 took 2.33 seconds (mean sampled reward: -4673.40). Current reward after update: -1535.28, Optimal reward -981.98
Iteration 144 took 2.35 seconds (mean sampled reward: -4570.87). Current reward after update: -1527.88, Optimal reward -981.98
Iteration 145 took 2.30 seconds (mean sampled reward: -4369.34). Current reward after update: -1372.75, Optimal reward -981.98
Iteration 146 took 2.27 seconds (mean sampled reward: -4419.50). Current reward after update: -4950.38, Optimal reward -981.98
Iteration 147 took 2.27 seconds (mean sampled reward: -4594.05). Current reward after update: -1566.54, Optimal reward -981.98
Iteration 148 took 2.30 seconds (mean sampled reward: -4451.09). Current reward after update: -1512.14, Optimal reward -981.98
Iteration 149 took 2.32 seconds (mean sampled reward: -4749.79). Current reward after update: -1404.28, Optimal reward -981.98
Iteration 150 took 2.38 seconds (mean sampled reward: -5830.93). Current reward after update: -1468.45, Optimal reward -981.98
Iteration 151 took 2.46 seconds (mean sampled reward: -5039.71). Current reward after update: -6700.64, Optimal reward -981.98
Iteration 152 took 2.43 seconds (mean sampled reward: -5124.85). Current reward after update: -1625.59, Optimal reward -981.98
Iteration 153 took 2.39 seconds (mean sampled reward: -4851.70). Current reward after update: -1640.73, Optimal reward -981.98
Iteration 154 took 2.42 seconds (mean sampled reward: -4792.49). Current reward after update: -1840.47, Optimal reward -981.98
Iteration 155 took 2.46 seconds (mean sampled reward: -5074.63). Current reward after update: -5039.45, Optimal reward -981.98
Iteration 156 took 2.44 seconds (mean sampled reward: -4939.28). Current reward after update: -1550.97, Optimal reward -981.98
Iteration 157 took 2.35 seconds (mean sampled reward: -4916.00). Current reward after update: -1826.34, Optimal reward -981.98
Iteration 158 took 2.32 seconds (mean sampled reward: -4937.57). Current reward after update: -1886.11, Optimal reward -981.98
Iteration 159 took 2.32 seconds (mean sampled reward: -5239.03). Current reward after update: -1719.02, Optimal reward -981.98
Iteration 160 took 2.33 seconds (mean sampled reward: -5220.62). Current reward after update: -1357.04, Optimal reward -981.98
Iteration 161 took 2.39 seconds (mean sampled reward: -5062.89). Current reward after update: -1889.17, Optimal reward -981.98
Iteration 162 took 2.42 seconds (mean sampled reward: -5758.47). Current reward after update: -1550.14, Optimal reward -981.98
Iteration 163 took 2.37 seconds (mean sampled reward: -5558.06). Current reward after update: -2048.42, Optimal reward -981.98
Iteration 164 took 2.40 seconds (mean sampled reward: -5967.40). Current reward after update: -2066.57, Optimal reward -981.98
Iteration 165 took 2.43 seconds (mean sampled reward: -5061.81). Current reward after update: -1769.00, Optimal reward -981.98
Iteration 166 took 2.33 seconds (mean sampled reward: -4448.96). Current reward after update: -5170.14, Optimal reward -981.98
Iteration 167 took 2.43 seconds (mean sampled reward: -5036.20). Current reward after update: -3448.13, Optimal reward -981.98
Iteration 168 took 2.44 seconds (mean sampled reward: -5325.26). Current reward after update: -1497.54, Optimal reward -981.98
Iteration 169 took 2.29 seconds (mean sampled reward: -5111.80). Current reward after update: -1567.17, Optimal reward -981.98
Iteration 170 took 2.40 seconds (mean sampled reward: -5440.95). Current reward after update: -1522.79, Optimal reward -981.98
Iteration 171 took 2.34 seconds (mean sampled reward: -4899.27). Current reward after update: -1921.99, Optimal reward -981.98
Iteration 172 took 2.29 seconds (mean sampled reward: -4940.32). Current reward after update: -3053.04, Optimal reward -981.98
Iteration 173 took 2.35 seconds (mean sampled reward: -5146.32). Current reward after update: -1728.63, Optimal reward -981.98
Iteration 174 took 2.36 seconds (mean sampled reward: -4998.64). Current reward after update: -1520.20, Optimal reward -981.98
Iteration 175 took 2.30 seconds (mean sampled reward: -5034.43). Current reward after update: -3223.74, Optimal reward -981.98
Iteration 176 took 2.30 seconds (mean sampled reward: -5062.75). Current reward after update: -1362.23, Optimal reward -981.98
Iteration 177 took 2.39 seconds (mean sampled reward: -4765.58). Current reward after update: -1520.40, Optimal reward -981.98
Iteration 178 took 2.44 seconds (mean sampled reward: -4742.02). Current reward after update: -1598.54, Optimal reward -981.98
Iteration 179 took 2.32 seconds (mean sampled reward: -4937.30). Current reward after update: -1737.88, Optimal reward -981.98
Iteration 180 took 2.32 seconds (mean sampled reward: -5108.34). Current reward after update: -1466.39, Optimal reward -981.98
Iteration 181 took 2.33 seconds (mean sampled reward: -5397.44). Current reward after update: -1850.88, Optimal reward -981.98
Iteration 182 took 2.30 seconds (mean sampled reward: -5018.02). Current reward after update: -1572.52, Optimal reward -981.98
Iteration 183 took 2.31 seconds (mean sampled reward: -5043.26). Current reward after update: -6073.63, Optimal reward -981.98
Iteration 184 took 2.34 seconds (mean sampled reward: -5691.38). Current reward after update: -1437.38, Optimal reward -981.98
Iteration 185 took 2.34 seconds (mean sampled reward: -5260.63). Current reward after update: -1286.35, Optimal reward -981.98
Iteration 186 took 2.33 seconds (mean sampled reward: -4888.89). Current reward after update: -1428.49, Optimal reward -981.98
Iteration 187 took 2.46 seconds (mean sampled reward: -5070.86). Current reward after update: -1425.85, Optimal reward -981.98
Iteration 188 took 2.36 seconds (mean sampled reward: -4795.53). Current reward after update: -1451.13, Optimal reward -981.98
Iteration 189 took 2.40 seconds (mean sampled reward: -4406.32). Current reward after update: -1713.32, Optimal reward -981.98
Iteration 190 took 2.52 seconds (mean sampled reward: -4461.43). Current reward after update: -1473.28, Optimal reward -981.98
Iteration 191 took 2.37 seconds (mean sampled reward: -4438.94). Current reward after update: -1557.57, Optimal reward -981.98
Iteration 192 took 2.38 seconds (mean sampled reward: -4647.11). Current reward after update: -1413.86, Optimal reward -981.98
Iteration 193 took 2.37 seconds (mean sampled reward: -4589.37). Current reward after update: -1364.48, Optimal reward -981.98
Iteration 194 took 2.39 seconds (mean sampled reward: -5787.02). Current reward after update: -1436.26, Optimal reward -981.98
Iteration 195 took 2.46 seconds (mean sampled reward: -5161.90). Current reward after update: -1447.87, Optimal reward -981.98
Iteration 196 took 2.41 seconds (mean sampled reward: -5459.41). Current reward after update: -1492.81, Optimal reward -981.98
Iteration 197 took 2.42 seconds (mean sampled reward: -5588.86). Current reward after update: -1557.91, Optimal reward -981.98
Iteration 198 took 2.28 seconds (mean sampled reward: -5411.52). Current reward after update: -1378.82, Optimal reward -981.98
Iteration 199 took 2.34 seconds (mean sampled reward: -5272.57). Current reward after update: -1236.29, Optimal reward -981.98
Iteration 200 took 2.34 seconds (mean sampled reward: -4976.26). Current reward after update: -1229.98, Optimal reward -981.98
Iteration 1 took 2.45 seconds (mean sampled reward: -7636.00). Current reward after update: -6771.41, Optimal reward -6771.41
Iteration 2 took 2.37 seconds (mean sampled reward: -7239.77). Current reward after update: -3597.45, Optimal reward -3597.45
Iteration 3 took 2.26 seconds (mean sampled reward: -6881.25). Current reward after update: -3104.61, Optimal reward -3104.61
Iteration 4 took 2.35 seconds (mean sampled reward: -6938.87). Current reward after update: -2848.80, Optimal reward -2848.80
Iteration 5 took 2.37 seconds (mean sampled reward: -7049.30). Current reward after update: -2909.93, Optimal reward -2848.80
Iteration 6 took 2.41 seconds (mean sampled reward: -6723.03). Current reward after update: -2688.04, Optimal reward -2688.04
Iteration 7 took 2.44 seconds (mean sampled reward: -6770.48). Current reward after update: -2722.67, Optimal reward -2688.04
Iteration 8 took 2.37 seconds (mean sampled reward: -7156.23). Current reward after update: -2672.49, Optimal reward -2672.49
Iteration 9 took 2.54 seconds (mean sampled reward: -6675.70). Current reward after update: -2794.58, Optimal reward -2672.49
Iteration 10 took 2.52 seconds (mean sampled reward: -6233.18). Current reward after update: -2751.22, Optimal reward -2672.49
Iteration 11 took 2.44 seconds (mean sampled reward: -5011.13). Current reward after update: -2175.53, Optimal reward -2175.53
Iteration 12 took 2.37 seconds (mean sampled reward: -4245.10). Current reward after update: -2009.79, Optimal reward -2009.79
Iteration 13 took 2.30 seconds (mean sampled reward: -3471.81). Current reward after update: -1695.39, Optimal reward -1695.39
Iteration 14 took 2.42 seconds (mean sampled reward: -4018.36). Current reward after update: -3034.65, Optimal reward -1695.39
Iteration 15 took 2.27 seconds (mean sampled reward: -4020.55). Current reward after update: -2144.35, Optimal reward -1695.39
Iteration 16 took 2.31 seconds (mean sampled reward: -3657.59). Current reward after update: -1669.42, Optimal reward -1669.42
Iteration 17 took 2.34 seconds (mean sampled reward: -3711.56). Current reward after update: -1489.14, Optimal reward -1489.14
Iteration 18 took 2.33 seconds (mean sampled reward: -3945.84). Current reward after update: -1706.07, Optimal reward -1489.14
Iteration 19 took 2.42 seconds (mean sampled reward: -4819.67). Current reward after update: -1535.90, Optimal reward -1489.14
Iteration 20 took 2.37 seconds (mean sampled reward: -5132.04). Current reward after update: -1377.38, Optimal reward -1377.38
Iteration 21 took 2.32 seconds (mean sampled reward: -5459.74). Current reward after update: -1402.49, Optimal reward -1377.38
Iteration 22 took 2.34 seconds (mean sampled reward: -5215.18). Current reward after update: -1598.81, Optimal reward -1377.38
Iteration 23 took 2.37 seconds (mean sampled reward: -5654.33). Current reward after update: -1530.65, Optimal reward -1377.38
Iteration 24 took 2.34 seconds (mean sampled reward: -5127.73). Current reward after update: -1341.71, Optimal reward -1341.71
Iteration 25 took 2.32 seconds (mean sampled reward: -4024.50). Current reward after update: -1254.05, Optimal reward -1254.05
Iteration 26 took 2.31 seconds (mean sampled reward: -4579.42). Current reward after update: -1424.60, Optimal reward -1254.05
Iteration 27 took 2.45 seconds (mean sampled reward: -5602.67). Current reward after update: -1406.81, Optimal reward -1254.05
Iteration 28 took 2.50 seconds (mean sampled reward: -5202.93). Current reward after update: -1495.83, Optimal reward -1254.05
Iteration 29 took 2.31 seconds (mean sampled reward: -4422.85). Current reward after update: -1284.85, Optimal reward -1254.05
Iteration 30 took 2.36 seconds (mean sampled reward: -3730.19). Current reward after update: -1309.36, Optimal reward -1254.05
Iteration 31 took 2.40 seconds (mean sampled reward: -4320.54). Current reward after update: -6526.17, Optimal reward -1254.05
Iteration 32 took 2.41 seconds (mean sampled reward: -5271.36). Current reward after update: -1289.35, Optimal reward -1254.05
Iteration 33 took 2.46 seconds (mean sampled reward: -4893.60). Current reward after update: -1309.62, Optimal reward -1254.05
Iteration 34 took 2.43 seconds (mean sampled reward: -4278.12). Current reward after update: -1323.92, Optimal reward -1254.05
Iteration 35 took 2.49 seconds (mean sampled reward: -4094.99). Current reward after update: -1242.34, Optimal reward -1242.34
Iteration 36 took 2.40 seconds (mean sampled reward: -3566.53). Current reward after update: -1062.32, Optimal reward -1062.32
Iteration 37 took 2.27 seconds (mean sampled reward: -3148.74). Current reward after update: -1113.88, Optimal reward -1062.32
Iteration 38 took 2.24 seconds (mean sampled reward: -3416.81). Current reward after update: -1048.35, Optimal reward -1048.35
Iteration 39 took 2.39 seconds (mean sampled reward: -3310.97). Current reward after update: -1021.58, Optimal reward -1021.58
Iteration 40 took 2.29 seconds (mean sampled reward: -3282.83). Current reward after update: -1089.42, Optimal reward -1021.58
Iteration 41 took 2.31 seconds (mean sampled reward: -3881.38). Current reward after update: -1098.80, Optimal reward -1021.58
Iteration 42 took 2.32 seconds (mean sampled reward: -3364.75). Current reward after update: -1131.85, Optimal reward -1021.58
Iteration 43 took 2.27 seconds (mean sampled reward: -3382.87). Current reward after update: -2207.72, Optimal reward -1021.58
Iteration 44 took 2.38 seconds (mean sampled reward: -3285.64). Current reward after update: -1237.86, Optimal reward -1021.58
Iteration 45 took 2.37 seconds (mean sampled reward: -3013.84). Current reward after update: -1095.54, Optimal reward -1021.58
Iteration 46 took 2.28 seconds (mean sampled reward: -3278.26). Current reward after update: -1064.20, Optimal reward -1021.58
Iteration 47 took 2.34 seconds (mean sampled reward: -3508.60). Current reward after update: -1117.65, Optimal reward -1021.58
Iteration 48 took 2.30 seconds (mean sampled reward: -3681.29). Current reward after update: -1236.70, Optimal reward -1021.58
Iteration 49 took 2.35 seconds (mean sampled reward: -3594.70). Current reward after update: -3024.48, Optimal reward -1021.58
Iteration 50 took 2.25 seconds (mean sampled reward: -3914.65). Current reward after update: -1311.68, Optimal reward -1021.58
Iteration 51 took 2.37 seconds (mean sampled reward: -3863.64). Current reward after update: -1307.00, Optimal reward -1021.58
Iteration 52 took 2.32 seconds (mean sampled reward: -3413.86). Current reward after update: -3151.62, Optimal reward -1021.58
Iteration 53 took 2.37 seconds (mean sampled reward: -3320.23). Current reward after update: -2509.70, Optimal reward -1021.58
Iteration 54 took 2.29 seconds (mean sampled reward: -3654.11). Current reward after update: -1215.67, Optimal reward -1021.58
Iteration 55 took 2.28 seconds (mean sampled reward: -4332.60). Current reward after update: -1382.51, Optimal reward -1021.58
Iteration 56 took 2.40 seconds (mean sampled reward: -4304.92). Current reward after update: -1294.87, Optimal reward -1021.58
Iteration 57 took 2.60 seconds (mean sampled reward: -4290.94). Current reward after update: -1314.08, Optimal reward -1021.58
Iteration 58 took 2.29 seconds (mean sampled reward: -4154.93). Current reward after update: -1276.78, Optimal reward -1021.58
Iteration 59 took 2.33 seconds (mean sampled reward: -3689.44). Current reward after update: -3375.80, Optimal reward -1021.58
Iteration 60 took 2.33 seconds (mean sampled reward: -3666.44). Current reward after update: -1258.58, Optimal reward -1021.58
Iteration 61 took 2.31 seconds (mean sampled reward: -3802.76). Current reward after update: -1224.05, Optimal reward -1021.58
Iteration 62 took 2.32 seconds (mean sampled reward: -3950.70). Current reward after update: -1215.32, Optimal reward -1021.58
Iteration 63 took 2.34 seconds (mean sampled reward: -4542.92). Current reward after update: -1166.37, Optimal reward -1021.58
Iteration 64 took 2.39 seconds (mean sampled reward: -4895.98). Current reward after update: -1203.15, Optimal reward -1021.58
Iteration 65 took 2.35 seconds (mean sampled reward: -5687.71). Current reward after update: -1393.24, Optimal reward -1021.58
Iteration 66 took 2.41 seconds (mean sampled reward: -5075.45). Current reward after update: -1386.18, Optimal reward -1021.58
Iteration 67 took 2.46 seconds (mean sampled reward: -4701.87). Current reward after update: -1332.52, Optimal reward -1021.58
Iteration 68 took 2.52 seconds (mean sampled reward: -4637.52). Current reward after update: -1244.03, Optimal reward -1021.58
Iteration 69 took 2.50 seconds (mean sampled reward: -4292.66). Current reward after update: -1544.11, Optimal reward -1021.58
Iteration 70 took 2.45 seconds (mean sampled reward: -4575.18). Current reward after update: -1290.30, Optimal reward -1021.58
Iteration 71 took 2.36 seconds (mean sampled reward: -4037.98). Current reward after update: -1186.34, Optimal reward -1021.58
Iteration 72 took 2.38 seconds (mean sampled reward: -4275.47). Current reward after update: -1375.72, Optimal reward -1021.58
Iteration 73 took 2.35 seconds (mean sampled reward: -3700.22). Current reward after update: -1181.30, Optimal reward -1021.58
Iteration 74 took 2.32 seconds (mean sampled reward: -3520.04). Current reward after update: -1244.11, Optimal reward -1021.58
Iteration 75 took 2.26 seconds (mean sampled reward: -3471.62). Current reward after update: -1171.20, Optimal reward -1021.58
Iteration 76 took 2.40 seconds (mean sampled reward: -4034.44). Current reward after update: -1219.90, Optimal reward -1021.58
Iteration 77 took 2.41 seconds (mean sampled reward: -3993.18). Current reward after update: -1126.47, Optimal reward -1021.58
Iteration 78 took 2.35 seconds (mean sampled reward: -4664.99). Current reward after update: -1390.25, Optimal reward -1021.58
Iteration 79 took 2.32 seconds (mean sampled reward: -4207.22). Current reward after update: -1247.83, Optimal reward -1021.58
Iteration 80 took 2.37 seconds (mean sampled reward: -4440.54). Current reward after update: -3032.92, Optimal reward -1021.58
Iteration 81 took 2.41 seconds (mean sampled reward: -3876.89). Current reward after update: -1234.70, Optimal reward -1021.58
Iteration 82 took 2.43 seconds (mean sampled reward: -4128.68). Current reward after update: -1283.43, Optimal reward -1021.58
Iteration 83 took 2.39 seconds (mean sampled reward: -3945.33). Current reward after update: -1003.09, Optimal reward -1003.09
Iteration 84 took 2.34 seconds (mean sampled reward: -4210.93). Current reward after update: -1253.32, Optimal reward -1003.09
Iteration 85 took 2.38 seconds (mean sampled reward: -4071.90). Current reward after update: -2110.01, Optimal reward -1003.09
Iteration 86 took 2.30 seconds (mean sampled reward: -4195.87). Current reward after update: -1217.19, Optimal reward -1003.09
Iteration 87 took 2.35 seconds (mean sampled reward: -4369.61). Current reward after update: -1243.64, Optimal reward -1003.09
Iteration 88 took 2.40 seconds (mean sampled reward: -3674.98). Current reward after update: -1186.59, Optimal reward -1003.09
Iteration 89 took 2.25 seconds (mean sampled reward: -4439.82). Current reward after update: -1177.03, Optimal reward -1003.09
Iteration 90 took 2.26 seconds (mean sampled reward: -4200.57). Current reward after update: -1100.50, Optimal reward -1003.09
Iteration 91 took 2.29 seconds (mean sampled reward: -4481.34). Current reward after update: -1064.16, Optimal reward -1003.09
Iteration 92 took 2.27 seconds (mean sampled reward: -4187.23). Current reward after update: -1131.99, Optimal reward -1003.09
Iteration 93 took 2.33 seconds (mean sampled reward: -3436.01). Current reward after update: -1062.80, Optimal reward -1003.09
Iteration 94 took 2.28 seconds (mean sampled reward: -4137.40). Current reward after update: -1117.45, Optimal reward -1003.09
Iteration 95 took 2.33 seconds (mean sampled reward: -4504.38). Current reward after update: -1705.45, Optimal reward -1003.09
Iteration 96 took 2.32 seconds (mean sampled reward: -4058.40). Current reward after update: -1271.87, Optimal reward -1003.09
Iteration 97 took 2.32 seconds (mean sampled reward: -4380.47). Current reward after update: -1218.65, Optimal reward -1003.09
Iteration 98 took 2.28 seconds (mean sampled reward: -3596.76). Current reward after update: -1147.98, Optimal reward -1003.09
Iteration 99 took 2.24 seconds (mean sampled reward: -4506.50). Current reward after update: -1228.97, Optimal reward -1003.09
Iteration 100 took 2.31 seconds (mean sampled reward: -4515.19). Current reward after update: -1139.43, Optimal reward -1003.09
Iteration 101 took 2.35 seconds (mean sampled reward: -4615.80). Current reward after update: -1233.04, Optimal reward -1003.09
Iteration 102 took 2.26 seconds (mean sampled reward: -4538.55). Current reward after update: -1074.85, Optimal reward -1003.09
Iteration 103 took 2.33 seconds (mean sampled reward: -4345.01). Current reward after update: -1279.64, Optimal reward -1003.09
Iteration 104 took 2.31 seconds (mean sampled reward: -4269.15). Current reward after update: -1118.83, Optimal reward -1003.09
Iteration 105 took 2.31 seconds (mean sampled reward: -3901.80). Current reward after update: -1038.31, Optimal reward -1003.09
Iteration 106 took 2.32 seconds (mean sampled reward: -3996.05). Current reward after update: -1131.28, Optimal reward -1003.09
Iteration 107 took 2.24 seconds (mean sampled reward: -4179.70). Current reward after update: -1244.46, Optimal reward -1003.09
Iteration 108 took 2.26 seconds (mean sampled reward: -4050.96). Current reward after update: -1193.50, Optimal reward -1003.09
Iteration 109 took 2.39 seconds (mean sampled reward: -3888.12). Current reward after update: -1101.49, Optimal reward -1003.09
Iteration 110 took 2.24 seconds (mean sampled reward: -3486.09). Current reward after update: -1041.29, Optimal reward -1003.09
Iteration 111 took 2.38 seconds (mean sampled reward: -3675.04). Current reward after update: -1274.42, Optimal reward -1003.09
Iteration 112 took 2.25 seconds (mean sampled reward: -3766.62). Current reward after update: -1103.02, Optimal reward -1003.09
Iteration 113 took 2.58 seconds (mean sampled reward: -4320.40). Current reward after update: -1274.49, Optimal reward -1003.09
Iteration 114 took 2.42 seconds (mean sampled reward: -4606.81). Current reward after update: -1273.45, Optimal reward -1003.09
Iteration 115 took 2.75 seconds (mean sampled reward: -4661.43). Current reward after update: -1280.88, Optimal reward -1003.09
Iteration 116 took 2.29 seconds (mean sampled reward: -4688.87). Current reward after update: -1379.70, Optimal reward -1003.09
Iteration 117 took 2.24 seconds (mean sampled reward: -4703.65). Current reward after update: -1235.36, Optimal reward -1003.09
Iteration 118 took 2.22 seconds (mean sampled reward: -4147.05). Current reward after update: -1329.10, Optimal reward -1003.09
Iteration 119 took 2.45 seconds (mean sampled reward: -4130.15). Current reward after update: -1276.37, Optimal reward -1003.09
Iteration 120 took 2.34 seconds (mean sampled reward: -3892.21). Current reward after update: -1135.51, Optimal reward -1003.09
Iteration 121 took 2.34 seconds (mean sampled reward: -4045.20). Current reward after update: -4846.16, Optimal reward -1003.09
Iteration 122 took 2.26 seconds (mean sampled reward: -3995.44). Current reward after update: -1214.66, Optimal reward -1003.09
Iteration 123 took 2.31 seconds (mean sampled reward: -3651.32). Current reward after update: -2558.26, Optimal reward -1003.09
Iteration 124 took 2.35 seconds (mean sampled reward: -3926.70). Current reward after update: -1431.01, Optimal reward -1003.09
Iteration 125 took 2.31 seconds (mean sampled reward: -3634.43). Current reward after update: -1235.88, Optimal reward -1003.09
Iteration 126 took 2.39 seconds (mean sampled reward: -3558.64). Current reward after update: -1260.93, Optimal reward -1003.09
Iteration 127 took 2.27 seconds (mean sampled reward: -3491.75). Current reward after update: -1142.75, Optimal reward -1003.09
Iteration 128 took 2.33 seconds (mean sampled reward: -3551.33). Current reward after update: -1185.33, Optimal reward -1003.09
Iteration 129 took 2.32 seconds (mean sampled reward: -3547.69). Current reward after update: -1169.32, Optimal reward -1003.09
Iteration 130 took 2.25 seconds (mean sampled reward: -3113.76). Current reward after update: -1172.03, Optimal reward -1003.09
Iteration 131 took 2.30 seconds (mean sampled reward: -3732.99). Current reward after update: -1218.88, Optimal reward -1003.09
Iteration 132 took 2.28 seconds (mean sampled reward: -3561.64). Current reward after update: -1459.29, Optimal reward -1003.09
Iteration 133 took 2.31 seconds (mean sampled reward: -4337.16). Current reward after update: -1135.10, Optimal reward -1003.09
Iteration 134 took 2.28 seconds (mean sampled reward: -3523.36). Current reward after update: -1197.55, Optimal reward -1003.09
Iteration 135 took 2.35 seconds (mean sampled reward: -4114.27). Current reward after update: -6234.29, Optimal reward -1003.09
Iteration 136 took 2.31 seconds (mean sampled reward: -4106.49). Current reward after update: -1143.57, Optimal reward -1003.09
Iteration 137 took 2.32 seconds (mean sampled reward: -4681.80). Current reward after update: -1317.91, Optimal reward -1003.09
Iteration 138 took 2.38 seconds (mean sampled reward: -3699.79). Current reward after update: -1338.46, Optimal reward -1003.09
Iteration 139 took 2.56 seconds (mean sampled reward: -4124.81). Current reward after update: -1256.76, Optimal reward -1003.09
Iteration 140 took 2.41 seconds (mean sampled reward: -4591.33). Current reward after update: -1304.01, Optimal reward -1003.09
Iteration 141 took 2.40 seconds (mean sampled reward: -5806.38). Current reward after update: -1486.70, Optimal reward -1003.09
Iteration 142 took 2.47 seconds (mean sampled reward: -4818.80). Current reward after update: -1545.79, Optimal reward -1003.09
Iteration 143 took 2.45 seconds (mean sampled reward: -4826.26). Current reward after update: -1365.94, Optimal reward -1003.09
Iteration 144 took 2.43 seconds (mean sampled reward: -4438.52). Current reward after update: -1333.89, Optimal reward -1003.09
Iteration 145 took 2.42 seconds (mean sampled reward: -5010.23). Current reward after update: -1316.28, Optimal reward -1003.09
Iteration 146 took 2.55 seconds (mean sampled reward: -4523.09). Current reward after update: -1492.64, Optimal reward -1003.09
Iteration 147 took 2.36 seconds (mean sampled reward: -3779.13). Current reward after update: -1278.43, Optimal reward -1003.09
Iteration 148 took 2.39 seconds (mean sampled reward: -3181.52). Current reward after update: -1144.63, Optimal reward -1003.09
Iteration 149 took 2.37 seconds (mean sampled reward: -3344.28). Current reward after update: -1703.44, Optimal reward -1003.09
Iteration 150 took 2.36 seconds (mean sampled reward: -3010.64). Current reward after update: -1087.05, Optimal reward -1003.09
Iteration 151 took 2.34 seconds (mean sampled reward: -3507.10). Current reward after update: -2151.93, Optimal reward -1003.09
Iteration 152 took 2.38 seconds (mean sampled reward: -3012.72). Current reward after update: -1036.19, Optimal reward -1003.09
Iteration 153 took 2.32 seconds (mean sampled reward: -2714.70). Current reward after update: -993.32, Optimal reward -993.32
Iteration 154 took 2.35 seconds (mean sampled reward: -2819.41). Current reward after update: -1949.79, Optimal reward -993.32
Iteration 155 took 2.41 seconds (mean sampled reward: -2954.51). Current reward after update: -1500.89, Optimal reward -993.32
Iteration 156 took 2.40 seconds (mean sampled reward: -2985.24). Current reward after update: -961.20, Optimal reward -961.20
Iteration 157 took 2.39 seconds (mean sampled reward: -2910.62). Current reward after update: -1012.62, Optimal reward -961.20
Iteration 158 took 2.34 seconds (mean sampled reward: -3449.55). Current reward after update: -993.37, Optimal reward -961.20
Iteration 159 took 2.38 seconds (mean sampled reward: -3294.38). Current reward after update: -1030.38, Optimal reward -961.20
Iteration 160 took 2.41 seconds (mean sampled reward: -2998.77). Current reward after update: -3275.76, Optimal reward -961.20
Iteration 161 took 2.33 seconds (mean sampled reward: -3245.97). Current reward after update: -999.73, Optimal reward -961.20
Iteration 162 took 2.43 seconds (mean sampled reward: -2906.35). Current reward after update: -2630.61, Optimal reward -961.20
Iteration 163 took 2.38 seconds (mean sampled reward: -3434.92). Current reward after update: -1682.49, Optimal reward -961.20
Iteration 164 took 2.35 seconds (mean sampled reward: -3334.73). Current reward after update: -962.05, Optimal reward -961.20
Iteration 165 took 2.37 seconds (mean sampled reward: -3022.69). Current reward after update: -884.13, Optimal reward -884.13
Iteration 166 took 2.34 seconds (mean sampled reward: -4234.61). Current reward after update: -1017.23, Optimal reward -884.13
Iteration 167 took 2.32 seconds (mean sampled reward: -3369.82). Current reward after update: -932.25, Optimal reward -884.13
Iteration 168 took 2.37 seconds (mean sampled reward: -3222.48). Current reward after update: -1001.08, Optimal reward -884.13
Iteration 169 took 2.33 seconds (mean sampled reward: -2851.04). Current reward after update: -986.39, Optimal reward -884.13
Iteration 170 took 2.29 seconds (mean sampled reward: -2758.75). Current reward after update: -1464.00, Optimal reward -884.13
Iteration 171 took 2.35 seconds (mean sampled reward: -3103.38). Current reward after update: -1497.47, Optimal reward -884.13
Iteration 172 took 2.31 seconds (mean sampled reward: -3006.30). Current reward after update: -1033.24, Optimal reward -884.13
Iteration 173 took 2.40 seconds (mean sampled reward: -3712.01). Current reward after update: -2929.12, Optimal reward -884.13
Iteration 174 took 2.37 seconds (mean sampled reward: -3682.53). Current reward after update: -1070.41, Optimal reward -884.13
Iteration 175 took 2.42 seconds (mean sampled reward: -4398.45). Current reward after update: -1034.19, Optimal reward -884.13
Iteration 176 took 2.42 seconds (mean sampled reward: -4234.24). Current reward after update: -5334.72, Optimal reward -884.13
Iteration 177 took 2.34 seconds (mean sampled reward: -4705.52). Current reward after update: -1050.16, Optimal reward -884.13
Iteration 178 took 2.38 seconds (mean sampled reward: -4099.88). Current reward after update: -1020.71, Optimal reward -884.13
Iteration 179 took 2.31 seconds (mean sampled reward: -4172.55). Current reward after update: -1072.56, Optimal reward -884.13
Iteration 180 took 2.37 seconds (mean sampled reward: -2835.26). Current reward after update: -1015.00, Optimal reward -884.13
Iteration 181 took 2.35 seconds (mean sampled reward: -2897.72). Current reward after update: -963.79, Optimal reward -884.13
Iteration 182 took 2.34 seconds (mean sampled reward: -3152.56). Current reward after update: -5202.86, Optimal reward -884.13
Iteration 183 took 2.33 seconds (mean sampled reward: -3090.67). Current reward after update: -1033.36, Optimal reward -884.13
Iteration 184 took 2.34 seconds (mean sampled reward: -2610.35). Current reward after update: -2837.11, Optimal reward -884.13
Iteration 185 took 2.34 seconds (mean sampled reward: -2853.86). Current reward after update: -1051.19, Optimal reward -884.13
Iteration 186 took 2.42 seconds (mean sampled reward: -2829.00). Current reward after update: -1028.25, Optimal reward -884.13
Iteration 187 took 2.33 seconds (mean sampled reward: -2780.88). Current reward after update: -1145.83, Optimal reward -884.13
Iteration 188 took 2.25 seconds (mean sampled reward: -3091.68). Current reward after update: -981.04, Optimal reward -884.13
Iteration 189 took 2.27 seconds (mean sampled reward: -4039.18). Current reward after update: -968.04, Optimal reward -884.13
Iteration 190 took 2.32 seconds (mean sampled reward: -4617.23). Current reward after update: -1069.33, Optimal reward -884.13
Iteration 191 took 2.31 seconds (mean sampled reward: -4046.82). Current reward after update: -977.82, Optimal reward -884.13
Iteration 192 took 2.32 seconds (mean sampled reward: -3231.92). Current reward after update: -1019.51, Optimal reward -884.13
Iteration 193 took 2.31 seconds (mean sampled reward: -3593.55). Current reward after update: -1052.66, Optimal reward -884.13
Iteration 194 took 2.33 seconds (mean sampled reward: -3181.14). Current reward after update: -855.54, Optimal reward -855.54
Iteration 195 took 2.34 seconds (mean sampled reward: -3157.56). Current reward after update: -989.61, Optimal reward -855.54
Iteration 196 took 2.23 seconds (mean sampled reward: -3129.59). Current reward after update: -2064.21, Optimal reward -855.54
Iteration 197 took 2.27 seconds (mean sampled reward: -2752.52). Current reward after update: -980.88, Optimal reward -855.54
Iteration 198 took 2.31 seconds (mean sampled reward: -2745.27). Current reward after update: -1014.02, Optimal reward -855.54
Iteration 199 took 2.28 seconds (mean sampled reward: -3280.24). Current reward after update: -961.52, Optimal reward -855.54
Iteration 200 took 2.28 seconds (mean sampled reward: -3039.95). Current reward after update: -1078.90, Optimal reward -855.54
Iteration 1 took 2.37 seconds (mean sampled reward: -7613.83). Current reward after update: -6356.63, Optimal reward -6356.63
Iteration 2 took 2.21 seconds (mean sampled reward: -7304.72). Current reward after update: -3557.35, Optimal reward -3557.35
Iteration 3 took 2.25 seconds (mean sampled reward: -6856.02). Current reward after update: -3235.61, Optimal reward -3235.61
Iteration 4 took 2.64 seconds (mean sampled reward: -6705.13). Current reward after update: -2983.22, Optimal reward -2983.22
Iteration 5 took 2.25 seconds (mean sampled reward: -6645.76). Current reward after update: -2804.07, Optimal reward -2804.07
Iteration 6 took 2.48 seconds (mean sampled reward: -6269.63). Current reward after update: -2523.71, Optimal reward -2523.71
Iteration 7 took 2.41 seconds (mean sampled reward: -6210.16). Current reward after update: -2541.68, Optimal reward -2523.71
Iteration 8 took 2.47 seconds (mean sampled reward: -6152.90). Current reward after update: -2619.15, Optimal reward -2523.71
Iteration 9 took 2.33 seconds (mean sampled reward: -5729.43). Current reward after update: -2373.32, Optimal reward -2373.32
Iteration 10 took 2.30 seconds (mean sampled reward: -5504.48). Current reward after update: -2482.62, Optimal reward -2373.32
Iteration 11 took 2.27 seconds (mean sampled reward: -5462.46). Current reward after update: -1927.11, Optimal reward -1927.11
Iteration 12 took 2.43 seconds (mean sampled reward: -5347.93). Current reward after update: -1591.89, Optimal reward -1591.89
Iteration 13 took 2.19 seconds (mean sampled reward: -5164.63). Current reward after update: -1542.77, Optimal reward -1542.77
Iteration 14 took 2.20 seconds (mean sampled reward: -5358.45). Current reward after update: -1748.93, Optimal reward -1542.77
Iteration 15 took 2.16 seconds (mean sampled reward: -6038.34). Current reward after update: -2114.70, Optimal reward -1542.77
Iteration 16 took 2.35 seconds (mean sampled reward: -5164.34). Current reward after update: -1686.63, Optimal reward -1542.77
Iteration 17 took 2.16 seconds (mean sampled reward: -5226.44). Current reward after update: -1986.99, Optimal reward -1542.77
Iteration 18 took 2.17 seconds (mean sampled reward: -4426.16). Current reward after update: -1792.09, Optimal reward -1542.77
Iteration 19 took 2.28 seconds (mean sampled reward: -4943.47). Current reward after update: -1690.66, Optimal reward -1542.77
Iteration 20 took 2.23 seconds (mean sampled reward: -5403.44). Current reward after update: -1987.64, Optimal reward -1542.77
Iteration 21 took 2.23 seconds (mean sampled reward: -5347.08). Current reward after update: -1820.27, Optimal reward -1542.77
Iteration 22 took 2.18 seconds (mean sampled reward: -5186.05). Current reward after update: -1561.22, Optimal reward -1542.77
Iteration 23 took 2.37 seconds (mean sampled reward: -3598.40). Current reward after update: -1814.54, Optimal reward -1542.77
Iteration 24 took 2.28 seconds (mean sampled reward: -5471.76). Current reward after update: -1541.80, Optimal reward -1541.80
Iteration 25 took 2.23 seconds (mean sampled reward: -6050.98). Current reward after update: -2225.36, Optimal reward -1541.80
Iteration 26 took 2.19 seconds (mean sampled reward: -6117.71). Current reward after update: -1860.69, Optimal reward -1541.80
Iteration 27 took 2.23 seconds (mean sampled reward: -6717.09). Current reward after update: -2597.34, Optimal reward -1541.80
Iteration 28 took 2.15 seconds (mean sampled reward: -6522.63). Current reward after update: -2768.53, Optimal reward -1541.80
Iteration 29 took 2.47 seconds (mean sampled reward: -5792.00). Current reward after update: -1469.02, Optimal reward -1469.02
Iteration 30 took 2.44 seconds (mean sampled reward: -6169.88). Current reward after update: -2495.67, Optimal reward -1469.02
Iteration 31 took 2.42 seconds (mean sampled reward: -5927.50). Current reward after update: -2479.56, Optimal reward -1469.02
Iteration 32 took 2.21 seconds (mean sampled reward: -6153.00). Current reward after update: -2081.52, Optimal reward -1469.02
Iteration 33 took 2.35 seconds (mean sampled reward: -6094.99). Current reward after update: -1777.59, Optimal reward -1469.02
Iteration 34 took 2.22 seconds (mean sampled reward: -5122.25). Current reward after update: -1656.17, Optimal reward -1469.02
Iteration 35 took 2.27 seconds (mean sampled reward: -5788.20). Current reward after update: -1348.05, Optimal reward -1348.05
Iteration 36 took 2.30 seconds (mean sampled reward: -5663.92). Current reward after update: -1325.96, Optimal reward -1325.96
Iteration 37 took 2.18 seconds (mean sampled reward: -5139.02). Current reward after update: -1565.36, Optimal reward -1325.96
Iteration 38 took 2.20 seconds (mean sampled reward: -5382.54). Current reward after update: -1171.03, Optimal reward -1171.03
Iteration 39 took 2.27 seconds (mean sampled reward: -4989.57). Current reward after update: -1333.15, Optimal reward -1171.03
Iteration 40 took 2.19 seconds (mean sampled reward: -4650.30). Current reward after update: -1409.09, Optimal reward -1171.03
Iteration 41 took 2.34 seconds (mean sampled reward: -4767.26). Current reward after update: -1183.20, Optimal reward -1171.03
Iteration 42 took 2.23 seconds (mean sampled reward: -3817.60). Current reward after update: -1055.07, Optimal reward -1055.07
Iteration 43 took 2.17 seconds (mean sampled reward: -3464.16). Current reward after update: -1130.18, Optimal reward -1055.07
Iteration 44 took 2.29 seconds (mean sampled reward: -3732.95). Current reward after update: -1274.87, Optimal reward -1055.07
Iteration 45 took 2.26 seconds (mean sampled reward: -3778.22). Current reward after update: -1301.74, Optimal reward -1055.07
Iteration 46 took 2.25 seconds (mean sampled reward: -4102.42). Current reward after update: -1122.51, Optimal reward -1055.07
Iteration 47 took 2.41 seconds (mean sampled reward: -4207.68). Current reward after update: -988.41, Optimal reward -988.41
Iteration 48 took 2.29 seconds (mean sampled reward: -4072.30). Current reward after update: -1063.93, Optimal reward -988.41
Iteration 49 took 2.24 seconds (mean sampled reward: -3743.49). Current reward after update: -2137.29, Optimal reward -988.41
Iteration 50 took 2.23 seconds (mean sampled reward: -3787.74). Current reward after update: -1122.46, Optimal reward -988.41
Iteration 51 took 2.22 seconds (mean sampled reward: -3661.69). Current reward after update: -933.57, Optimal reward -933.57
Iteration 52 took 2.20 seconds (mean sampled reward: -3993.00). Current reward after update: -1886.52, Optimal reward -933.57
Iteration 53 took 2.43 seconds (mean sampled reward: -4252.18). Current reward after update: -1120.73, Optimal reward -933.57
Iteration 54 took 2.32 seconds (mean sampled reward: -3808.58). Current reward after update: -1062.24, Optimal reward -933.57
Iteration 55 took 2.20 seconds (mean sampled reward: -3801.75). Current reward after update: -2520.32, Optimal reward -933.57
Iteration 56 took 2.38 seconds (mean sampled reward: -4302.28). Current reward after update: -1081.57, Optimal reward -933.57
Iteration 57 took 2.31 seconds (mean sampled reward: -4029.89). Current reward after update: -1191.54, Optimal reward -933.57
Iteration 58 took 2.19 seconds (mean sampled reward: -4033.46). Current reward after update: -1243.67, Optimal reward -933.57
Iteration 59 took 2.29 seconds (mean sampled reward: -5202.59). Current reward after update: -1084.32, Optimal reward -933.57
Iteration 60 took 2.28 seconds (mean sampled reward: -4393.42). Current reward after update: -1189.26, Optimal reward -933.57
Iteration 61 took 2.22 seconds (mean sampled reward: -4480.56). Current reward after update: -1080.38, Optimal reward -933.57
Iteration 62 took 2.48 seconds (mean sampled reward: -4755.15). Current reward after update: -1098.45, Optimal reward -933.57
Iteration 63 took 2.28 seconds (mean sampled reward: -5153.39). Current reward after update: -1137.30, Optimal reward -933.57
Iteration 64 took 2.17 seconds (mean sampled reward: -4383.13). Current reward after update: -1055.65, Optimal reward -933.57
Iteration 65 took 2.17 seconds (mean sampled reward: -3385.72). Current reward after update: -865.70, Optimal reward -865.70
Iteration 66 took 2.14 seconds (mean sampled reward: -4267.52). Current reward after update: -1017.25, Optimal reward -865.70
Iteration 67 took 2.17 seconds (mean sampled reward: -4654.05). Current reward after update: -915.95, Optimal reward -865.70
Iteration 68 took 2.43 seconds (mean sampled reward: -4594.04). Current reward after update: -1111.90, Optimal reward -865.70
Iteration 69 took 2.20 seconds (mean sampled reward: -4278.38). Current reward after update: -1093.10, Optimal reward -865.70
Iteration 70 took 2.14 seconds (mean sampled reward: -4211.87). Current reward after update: -990.15, Optimal reward -865.70
Iteration 71 took 2.19 seconds (mean sampled reward: -3977.57). Current reward after update: -874.76, Optimal reward -865.70
Iteration 72 took 2.14 seconds (mean sampled reward: -5610.82). Current reward after update: -762.41, Optimal reward -762.41
Iteration 73 took 2.16 seconds (mean sampled reward: -3616.67). Current reward after update: -808.99, Optimal reward -762.41
Iteration 74 took 2.16 seconds (mean sampled reward: -3931.77). Current reward after update: -854.28, Optimal reward -762.41
Iteration 75 took 2.20 seconds (mean sampled reward: -3599.46). Current reward after update: -858.11, Optimal reward -762.41
Iteration 76 took 2.18 seconds (mean sampled reward: -4006.37). Current reward after update: -7208.21, Optimal reward -762.41
Iteration 77 took 2.17 seconds (mean sampled reward: -4498.63). Current reward after update: -690.27, Optimal reward -690.27
Iteration 78 took 2.22 seconds (mean sampled reward: -4606.81). Current reward after update: -618.07, Optimal reward -618.07
Iteration 79 took 2.25 seconds (mean sampled reward: -5505.27). Current reward after update: -701.16, Optimal reward -618.07
Iteration 80 took 2.21 seconds (mean sampled reward: -5646.29). Current reward after update: -878.01, Optimal reward -618.07
Iteration 81 took 2.18 seconds (mean sampled reward: -5894.13). Current reward after update: -962.04, Optimal reward -618.07
Iteration 82 took 2.24 seconds (mean sampled reward: -4997.20). Current reward after update: -659.40, Optimal reward -618.07
Iteration 83 took 2.24 seconds (mean sampled reward: -4498.69). Current reward after update: -753.16, Optimal reward -618.07
Iteration 84 took 2.22 seconds (mean sampled reward: -5062.91). Current reward after update: -2325.97, Optimal reward -618.07
Iteration 85 took 2.25 seconds (mean sampled reward: -5509.86). Current reward after update: -639.93, Optimal reward -618.07
Iteration 86 took 2.19 seconds (mean sampled reward: -3930.59). Current reward after update: -632.58, Optimal reward -618.07
Iteration 87 took 2.30 seconds (mean sampled reward: -3847.38). Current reward after update: -1176.88, Optimal reward -618.07
Iteration 88 took 2.19 seconds (mean sampled reward: -4934.57). Current reward after update: -923.75, Optimal reward -618.07
Iteration 89 took 2.28 seconds (mean sampled reward: -5805.33). Current reward after update: -1456.83, Optimal reward -618.07
Iteration 90 took 2.20 seconds (mean sampled reward: -5237.18). Current reward after update: -823.98, Optimal reward -618.07
Iteration 91 took 2.24 seconds (mean sampled reward: -4955.67). Current reward after update: -843.47, Optimal reward -618.07
Iteration 92 took 2.29 seconds (mean sampled reward: -4193.93). Current reward after update: -712.94, Optimal reward -618.07
Iteration 93 took 2.31 seconds (mean sampled reward: -4956.58). Current reward after update: -962.90, Optimal reward -618.07
Iteration 94 took 2.29 seconds (mean sampled reward: -4053.07). Current reward after update: -960.06, Optimal reward -618.07
Iteration 95 took 2.22 seconds (mean sampled reward: -4038.66). Current reward after update: -767.16, Optimal reward -618.07
Iteration 96 took 2.18 seconds (mean sampled reward: -3964.84). Current reward after update: -2530.50, Optimal reward -618.07
Iteration 97 took 2.24 seconds (mean sampled reward: -3827.78). Current reward after update: -814.67, Optimal reward -618.07
Iteration 98 took 2.24 seconds (mean sampled reward: -4329.09). Current reward after update: -5855.72, Optimal reward -618.07
Iteration 99 took 2.24 seconds (mean sampled reward: -4035.90). Current reward after update: -833.61, Optimal reward -618.07
Iteration 100 took 2.20 seconds (mean sampled reward: -3779.45). Current reward after update: -834.77, Optimal reward -618.07
Iteration 101 took 2.25 seconds (mean sampled reward: -3463.36). Current reward after update: -868.47, Optimal reward -618.07
Iteration 102 took 2.26 seconds (mean sampled reward: -3792.51). Current reward after update: -820.02, Optimal reward -618.07
Iteration 103 took 2.21 seconds (mean sampled reward: -4119.49). Current reward after update: -880.24, Optimal reward -618.07
Iteration 104 took 2.22 seconds (mean sampled reward: -3371.52). Current reward after update: -791.04, Optimal reward -618.07
Iteration 105 took 2.20 seconds (mean sampled reward: -2975.28). Current reward after update: -853.63, Optimal reward -618.07
Iteration 106 took 2.20 seconds (mean sampled reward: -4093.48). Current reward after update: -836.03, Optimal reward -618.07
Iteration 107 took 2.53 seconds (mean sampled reward: -3988.65). Current reward after update: -1279.79, Optimal reward -618.07
Iteration 108 took 2.14 seconds (mean sampled reward: -4201.58). Current reward after update: -1551.76, Optimal reward -618.07
Iteration 109 took 2.19 seconds (mean sampled reward: -4809.08). Current reward after update: -929.44, Optimal reward -618.07
Iteration 110 took 2.31 seconds (mean sampled reward: -4191.91). Current reward after update: -795.26, Optimal reward -618.07
Iteration 111 took 2.22 seconds (mean sampled reward: -3943.59). Current reward after update: -880.00, Optimal reward -618.07
Iteration 112 took 2.27 seconds (mean sampled reward: -4096.38). Current reward after update: -932.54, Optimal reward -618.07
Iteration 113 took 2.19 seconds (mean sampled reward: -3185.14). Current reward after update: -633.99, Optimal reward -618.07
Iteration 114 took 2.17 seconds (mean sampled reward: -2778.38). Current reward after update: -913.41, Optimal reward -618.07
Iteration 115 took 2.29 seconds (mean sampled reward: -2873.09). Current reward after update: -768.96, Optimal reward -618.07
Iteration 116 took 2.16 seconds (mean sampled reward: -2773.35). Current reward after update: -1059.91, Optimal reward -618.07
Iteration 117 took 2.14 seconds (mean sampled reward: -3056.69). Current reward after update: -866.40, Optimal reward -618.07
Iteration 118 took 2.23 seconds (mean sampled reward: -4578.58). Current reward after update: -688.21, Optimal reward -618.07
Iteration 119 took 2.14 seconds (mean sampled reward: -5132.16). Current reward after update: -802.12, Optimal reward -618.07
Iteration 120 took 2.16 seconds (mean sampled reward: -4373.96). Current reward after update: -820.84, Optimal reward -618.07
Iteration 121 took 2.15 seconds (mean sampled reward: -4822.41). Current reward after update: -1098.42, Optimal reward -618.07
Iteration 122 took 2.28 seconds (mean sampled reward: -5397.58). Current reward after update: -1004.44, Optimal reward -618.07
Iteration 123 took 2.21 seconds (mean sampled reward: -3468.30). Current reward after update: -767.25, Optimal reward -618.07
Iteration 124 took 2.23 seconds (mean sampled reward: -3419.86). Current reward after update: -1228.50, Optimal reward -618.07
Iteration 125 took 2.13 seconds (mean sampled reward: -3816.11). Current reward after update: -1156.81, Optimal reward -618.07
Iteration 126 took 2.14 seconds (mean sampled reward: -4556.58). Current reward after update: -742.54, Optimal reward -618.07
Iteration 127 took 2.13 seconds (mean sampled reward: -3583.21). Current reward after update: -927.67, Optimal reward -618.07
Iteration 128 took 2.14 seconds (mean sampled reward: -5063.73). Current reward after update: -1347.64, Optimal reward -618.07
Iteration 129 took 2.10 seconds (mean sampled reward: -5457.28). Current reward after update: -751.09, Optimal reward -618.07
Iteration 130 took 2.12 seconds (mean sampled reward: -4412.75). Current reward after update: -528.45, Optimal reward -528.45
Iteration 131 took 2.14 seconds (mean sampled reward: -3879.20). Current reward after update: -476.46, Optimal reward -476.46
Iteration 132 took 2.10 seconds (mean sampled reward: -4472.79). Current reward after update: -539.68, Optimal reward -476.46
Iteration 133 took 2.19 seconds (mean sampled reward: -4301.80). Current reward after update: -561.63, Optimal reward -476.46
Iteration 134 took 2.15 seconds (mean sampled reward: -4548.88). Current reward after update: -477.19, Optimal reward -476.46
Iteration 135 took 2.10 seconds (mean sampled reward: -3587.62). Current reward after update: -532.88, Optimal reward -476.46
Iteration 136 took 2.14 seconds (mean sampled reward: -3658.68). Current reward after update: -443.64, Optimal reward -443.64
Iteration 137 took 2.15 seconds (mean sampled reward: -3660.43). Current reward after update: -598.70, Optimal reward -443.64
Iteration 138 took 2.19 seconds (mean sampled reward: -3214.32). Current reward after update: -462.94, Optimal reward -443.64
Iteration 139 took 2.13 seconds (mean sampled reward: -3499.02). Current reward after update: -1234.56, Optimal reward -443.64
Iteration 140 took 2.14 seconds (mean sampled reward: -4293.26). Current reward after update: -589.03, Optimal reward -443.64
Iteration 141 took 2.21 seconds (mean sampled reward: -2873.89). Current reward after update: -349.53, Optimal reward -349.53
Iteration 142 took 2.14 seconds (mean sampled reward: -3337.29). Current reward after update: -467.24, Optimal reward -349.53
Iteration 143 took 2.13 seconds (mean sampled reward: -4288.37). Current reward after update: -706.80, Optimal reward -349.53
Iteration 144 took 2.14 seconds (mean sampled reward: -2911.15). Current reward after update: -469.41, Optimal reward -349.53
Iteration 145 took 2.10 seconds (mean sampled reward: -5579.95). Current reward after update: -679.91, Optimal reward -349.53
Iteration 146 took 2.14 seconds (mean sampled reward: -5544.02). Current reward after update: -719.66, Optimal reward -349.53
Iteration 147 took 2.19 seconds (mean sampled reward: -4693.21). Current reward after update: -734.22, Optimal reward -349.53
Iteration 148 took 2.11 seconds (mean sampled reward: -4286.31). Current reward after update: -537.65, Optimal reward -349.53
Iteration 149 took 2.10 seconds (mean sampled reward: -4846.94). Current reward after update: -658.10, Optimal reward -349.53
Iteration 150 took 2.13 seconds (mean sampled reward: -4803.90). Current reward after update: -722.41, Optimal reward -349.53
Iteration 151 took 2.15 seconds (mean sampled reward: -4109.77). Current reward after update: -591.53, Optimal reward -349.53
Iteration 152 took 2.12 seconds (mean sampled reward: -3529.26). Current reward after update: -573.52, Optimal reward -349.53
Iteration 153 took 2.13 seconds (mean sampled reward: -4008.14). Current reward after update: -569.46, Optimal reward -349.53
Iteration 154 took 2.15 seconds (mean sampled reward: -3432.46). Current reward after update: -635.11, Optimal reward -349.53
Iteration 155 took 2.14 seconds (mean sampled reward: -3896.65). Current reward after update: -672.44, Optimal reward -349.53
Iteration 156 took 2.25 seconds (mean sampled reward: -4663.09). Current reward after update: -557.70, Optimal reward -349.53
Iteration 157 took 2.19 seconds (mean sampled reward: -2862.06). Current reward after update: -617.08, Optimal reward -349.53
Iteration 158 took 2.22 seconds (mean sampled reward: -2796.42). Current reward after update: -861.90, Optimal reward -349.53
Iteration 159 took 2.17 seconds (mean sampled reward: -3176.39). Current reward after update: -705.07, Optimal reward -349.53
Iteration 160 took 2.13 seconds (mean sampled reward: -5222.08). Current reward after update: -723.99, Optimal reward -349.53
Iteration 161 took 2.14 seconds (mean sampled reward: -4055.95). Current reward after update: -525.68, Optimal reward -349.53
Iteration 162 took 2.19 seconds (mean sampled reward: -5253.55). Current reward after update: -587.53, Optimal reward -349.53
Iteration 163 took 2.23 seconds (mean sampled reward: -5237.98). Current reward after update: -1034.44, Optimal reward -349.53
Iteration 164 took 2.19 seconds (mean sampled reward: -4591.67). Current reward after update: -347.25, Optimal reward -347.25
Iteration 165 took 2.25 seconds (mean sampled reward: -4769.11). Current reward after update: -477.54, Optimal reward -347.25
Iteration 166 took 2.17 seconds (mean sampled reward: -4122.23). Current reward after update: -406.66, Optimal reward -347.25
Iteration 167 took 2.15 seconds (mean sampled reward: -3331.81). Current reward after update: -509.49, Optimal reward -347.25
Iteration 168 took 2.15 seconds (mean sampled reward: -3151.23). Current reward after update: -326.64, Optimal reward -326.64
Iteration 169 took 2.16 seconds (mean sampled reward: -3523.53). Current reward after update: -457.94, Optimal reward -326.64
Iteration 170 took 2.12 seconds (mean sampled reward: -3740.21). Current reward after update: -567.27, Optimal reward -326.64
Iteration 171 took 2.18 seconds (mean sampled reward: -3420.68). Current reward after update: -583.04, Optimal reward -326.64
Iteration 172 took 2.20 seconds (mean sampled reward: -2903.05). Current reward after update: -340.18, Optimal reward -326.64
Iteration 173 took 2.14 seconds (mean sampled reward: -2867.07). Current reward after update: -409.18, Optimal reward -326.64
Iteration 174 took 2.16 seconds (mean sampled reward: -3267.59). Current reward after update: -601.84, Optimal reward -326.64
Iteration 175 took 2.17 seconds (mean sampled reward: -3149.09). Current reward after update: -427.14, Optimal reward -326.64
Iteration 176 took 2.15 seconds (mean sampled reward: -2522.31). Current reward after update: -520.03, Optimal reward -326.64
Iteration 177 took 2.15 seconds (mean sampled reward: -4111.80). Current reward after update: -493.98, Optimal reward -326.64
Iteration 178 took 2.13 seconds (mean sampled reward: -3405.21). Current reward after update: -771.45, Optimal reward -326.64
Iteration 179 took 2.14 seconds (mean sampled reward: -3875.92). Current reward after update: -914.62, Optimal reward -326.64
Iteration 180 took 2.16 seconds (mean sampled reward: -3260.74). Current reward after update: -892.33, Optimal reward -326.64
Iteration 181 took 2.12 seconds (mean sampled reward: -2806.37). Current reward after update: -717.71, Optimal reward -326.64
Iteration 182 took 2.23 seconds (mean sampled reward: -2657.51). Current reward after update: -618.93, Optimal reward -326.64
Iteration 183 took 2.29 seconds (mean sampled reward: -3189.22). Current reward after update: -419.35, Optimal reward -326.64
Iteration 184 took 2.24 seconds (mean sampled reward: -2854.60). Current reward after update: -681.88, Optimal reward -326.64
Iteration 185 took 2.22 seconds (mean sampled reward: -2862.60). Current reward after update: -590.76, Optimal reward -326.64
Iteration 186 took 2.11 seconds (mean sampled reward: -2416.62). Current reward after update: -582.40, Optimal reward -326.64
Iteration 187 took 2.10 seconds (mean sampled reward: -2518.28). Current reward after update: -779.15, Optimal reward -326.64
Iteration 188 took 2.11 seconds (mean sampled reward: -2489.82). Current reward after update: -809.64, Optimal reward -326.64
Iteration 189 took 2.16 seconds (mean sampled reward: -2210.25). Current reward after update: -1832.28, Optimal reward -326.64
Iteration 190 took 2.13 seconds (mean sampled reward: -2396.67). Current reward after update: -640.92, Optimal reward -326.64
Iteration 191 took 2.13 seconds (mean sampled reward: -2452.02). Current reward after update: -629.95, Optimal reward -326.64
Iteration 192 took 2.18 seconds (mean sampled reward: -2518.29). Current reward after update: -610.83, Optimal reward -326.64
Iteration 193 took 2.19 seconds (mean sampled reward: -2863.27). Current reward after update: -378.89, Optimal reward -326.64
Iteration 194 took 2.19 seconds (mean sampled reward: -2602.74). Current reward after update: -366.87, Optimal reward -326.64
Iteration 195 took 2.21 seconds (mean sampled reward: -2679.02). Current reward after update: -461.21, Optimal reward -326.64
Iteration 196 took 2.16 seconds (mean sampled reward: -2478.28). Current reward after update: -476.54, Optimal reward -326.64
Iteration 197 took 2.17 seconds (mean sampled reward: -2201.45). Current reward after update: -451.33, Optimal reward -326.64
Iteration 198 took 2.18 seconds (mean sampled reward: -3471.38). Current reward after update: -651.25, Optimal reward -326.64
Iteration 199 took 2.21 seconds (mean sampled reward: -2342.83). Current reward after update: -456.37, Optimal reward -326.64
Iteration 200 took 2.15 seconds (mean sampled reward: -2662.00). Current reward after update: -415.55, Optimal reward -326.64
Max force: 40 Sigma: 0.4 mean rewards: -721.3888118601252, best rewards:-326.6430081766932

Iteration 1 took 2.42 seconds (mean sampled reward: -7463.94). Current reward after update: -3601.80, Optimal reward -3601.80
Iteration 2 took 2.32 seconds (mean sampled reward: -7311.04). Current reward after update: -2971.90, Optimal reward -2971.90
Iteration 3 took 2.28 seconds (mean sampled reward: -7336.69). Current reward after update: -3122.07, Optimal reward -2971.90
Iteration 4 took 2.42 seconds (mean sampled reward: -6609.02). Current reward after update: -2751.26, Optimal reward -2751.26
Iteration 5 took 2.36 seconds (mean sampled reward: -6698.12). Current reward after update: -2180.35, Optimal reward -2180.35
Iteration 6 took 2.38 seconds (mean sampled reward: -6621.46). Current reward after update: -2670.02, Optimal reward -2180.35
Iteration 7 took 2.41 seconds (mean sampled reward: -6428.21). Current reward after update: -6001.34, Optimal reward -2180.35
Iteration 8 took 2.39 seconds (mean sampled reward: -5943.00). Current reward after update: -2766.24, Optimal reward -2180.35
Iteration 9 took 2.46 seconds (mean sampled reward: -6980.76). Current reward after update: -7256.08, Optimal reward -2180.35
Iteration 10 took 2.32 seconds (mean sampled reward: -6683.75). Current reward after update: -2805.30, Optimal reward -2180.35
Iteration 11 took 2.57 seconds (mean sampled reward: -6262.17). Current reward after update: -3430.78, Optimal reward -2180.35
Iteration 12 took 2.51 seconds (mean sampled reward: -6670.57). Current reward after update: -3057.35, Optimal reward -2180.35
Iteration 13 took 2.25 seconds (mean sampled reward: -6005.71). Current reward after update: -2810.80, Optimal reward -2180.35
Iteration 14 took 2.29 seconds (mean sampled reward: -6024.04). Current reward after update: -2809.81, Optimal reward -2180.35
Iteration 15 took 2.42 seconds (mean sampled reward: -5227.94). Current reward after update: -6710.49, Optimal reward -2180.35
Iteration 16 took 2.40 seconds (mean sampled reward: -5527.10). Current reward after update: -3045.64, Optimal reward -2180.35
Iteration 17 took 2.32 seconds (mean sampled reward: -5717.39). Current reward after update: -2353.22, Optimal reward -2180.35
Iteration 18 took 2.32 seconds (mean sampled reward: -5800.85). Current reward after update: -2228.52, Optimal reward -2180.35
Iteration 19 took 2.39 seconds (mean sampled reward: -5859.76). Current reward after update: -2204.96, Optimal reward -2180.35
Iteration 20 took 2.36 seconds (mean sampled reward: -5699.72). Current reward after update: -2117.36, Optimal reward -2117.36
Iteration 21 took 2.17 seconds (mean sampled reward: -5567.45). Current reward after update: -2109.02, Optimal reward -2109.02
Iteration 22 took 2.35 seconds (mean sampled reward: -4746.36). Current reward after update: -1938.06, Optimal reward -1938.06
Iteration 23 took 2.25 seconds (mean sampled reward: -4829.93). Current reward after update: -1986.76, Optimal reward -1938.06
Iteration 24 took 2.28 seconds (mean sampled reward: -5762.10). Current reward after update: -1758.81, Optimal reward -1758.81
Iteration 25 took 2.27 seconds (mean sampled reward: -6533.76). Current reward after update: -1731.58, Optimal reward -1731.58
Iteration 26 took 2.27 seconds (mean sampled reward: -6058.47). Current reward after update: -1495.30, Optimal reward -1495.30
Iteration 27 took 2.34 seconds (mean sampled reward: -6118.74). Current reward after update: -1744.10, Optimal reward -1495.30
Iteration 28 took 2.44 seconds (mean sampled reward: -6366.52). Current reward after update: -1858.90, Optimal reward -1495.30
Iteration 29 took 2.38 seconds (mean sampled reward: -5616.72). Current reward after update: -1329.43, Optimal reward -1329.43
Iteration 30 took 2.53 seconds (mean sampled reward: -5698.91). Current reward after update: -1577.57, Optimal reward -1329.43
Iteration 31 took 2.42 seconds (mean sampled reward: -6081.56). Current reward after update: -1901.56, Optimal reward -1329.43
Iteration 32 took 2.40 seconds (mean sampled reward: -6371.98). Current reward after update: -7227.15, Optimal reward -1329.43
Iteration 33 took 2.51 seconds (mean sampled reward: -5615.52). Current reward after update: -1351.35, Optimal reward -1329.43
Iteration 34 took 2.45 seconds (mean sampled reward: -6007.28). Current reward after update: -1702.82, Optimal reward -1329.43
Iteration 35 took 2.33 seconds (mean sampled reward: -5826.43). Current reward after update: -1330.51, Optimal reward -1329.43
Iteration 36 took 2.52 seconds (mean sampled reward: -6627.88). Current reward after update: -1603.15, Optimal reward -1329.43
Iteration 37 took 2.37 seconds (mean sampled reward: -6391.11). Current reward after update: -1860.63, Optimal reward -1329.43
Iteration 38 took 2.44 seconds (mean sampled reward: -6761.70). Current reward after update: -1485.09, Optimal reward -1329.43
Iteration 39 took 2.34 seconds (mean sampled reward: -6588.59). Current reward after update: -1629.25, Optimal reward -1329.43
Iteration 40 took 2.43 seconds (mean sampled reward: -7095.75). Current reward after update: -1381.37, Optimal reward -1329.43
Iteration 41 took 2.48 seconds (mean sampled reward: -7027.87). Current reward after update: -1364.90, Optimal reward -1329.43
Iteration 42 took 2.44 seconds (mean sampled reward: -6770.91). Current reward after update: -1129.35, Optimal reward -1129.35
Iteration 43 took 2.46 seconds (mean sampled reward: -7111.82). Current reward after update: -1267.72, Optimal reward -1129.35
Iteration 44 took 2.58 seconds (mean sampled reward: -6962.67). Current reward after update: -1820.48, Optimal reward -1129.35
Iteration 45 took 2.62 seconds (mean sampled reward: -6755.09). Current reward after update: -1447.97, Optimal reward -1129.35
Iteration 46 took 2.55 seconds (mean sampled reward: -6611.89). Current reward after update: -1495.95, Optimal reward -1129.35
Iteration 47 took 2.33 seconds (mean sampled reward: -4993.26). Current reward after update: -1440.47, Optimal reward -1129.35
Iteration 48 took 2.32 seconds (mean sampled reward: -4577.33). Current reward after update: -1428.95, Optimal reward -1129.35
Iteration 49 took 2.41 seconds (mean sampled reward: -4977.01). Current reward after update: -1362.07, Optimal reward -1129.35
Iteration 50 took 2.34 seconds (mean sampled reward: -4779.80). Current reward after update: -1364.18, Optimal reward -1129.35
Iteration 51 took 2.32 seconds (mean sampled reward: -5483.26). Current reward after update: -1400.88, Optimal reward -1129.35
Iteration 52 took 2.32 seconds (mean sampled reward: -4277.89). Current reward after update: -1358.16, Optimal reward -1129.35
Iteration 53 took 2.37 seconds (mean sampled reward: -4742.87). Current reward after update: -1469.02, Optimal reward -1129.35
Iteration 54 took 2.35 seconds (mean sampled reward: -4601.48). Current reward after update: -1224.16, Optimal reward -1129.35
Iteration 55 took 2.42 seconds (mean sampled reward: -4109.61). Current reward after update: -1314.57, Optimal reward -1129.35
Iteration 56 took 2.27 seconds (mean sampled reward: -3712.91). Current reward after update: -1311.84, Optimal reward -1129.35
Iteration 57 took 2.39 seconds (mean sampled reward: -4345.46). Current reward after update: -1185.04, Optimal reward -1129.35
Iteration 58 took 2.37 seconds (mean sampled reward: -4574.99). Current reward after update: -1715.49, Optimal reward -1129.35
Iteration 59 took 2.27 seconds (mean sampled reward: -3846.20). Current reward after update: -1260.75, Optimal reward -1129.35
Iteration 60 took 2.29 seconds (mean sampled reward: -4464.35). Current reward after update: -1332.34, Optimal reward -1129.35
Iteration 61 took 2.26 seconds (mean sampled reward: -4610.03). Current reward after update: -1400.03, Optimal reward -1129.35
Iteration 62 took 2.28 seconds (mean sampled reward: -4914.14). Current reward after update: -1272.37, Optimal reward -1129.35
Iteration 63 took 2.25 seconds (mean sampled reward: -3725.68). Current reward after update: -1131.31, Optimal reward -1129.35
Iteration 64 took 2.30 seconds (mean sampled reward: -2901.86). Current reward after update: -1054.75, Optimal reward -1054.75
Iteration 65 took 2.57 seconds (mean sampled reward: -3046.16). Current reward after update: -826.34, Optimal reward -826.34
Iteration 66 took 2.28 seconds (mean sampled reward: -3078.89). Current reward after update: -862.28, Optimal reward -826.34
Iteration 67 took 2.34 seconds (mean sampled reward: -3711.96). Current reward after update: -975.92, Optimal reward -826.34
Iteration 68 took 2.41 seconds (mean sampled reward: -4341.30). Current reward after update: -1115.09, Optimal reward -826.34
Iteration 69 took 2.25 seconds (mean sampled reward: -5192.17). Current reward after update: -1158.92, Optimal reward -826.34
Iteration 70 took 2.20 seconds (mean sampled reward: -5033.17). Current reward after update: -1317.74, Optimal reward -826.34
Iteration 71 took 2.29 seconds (mean sampled reward: -5562.23). Current reward after update: -1207.78, Optimal reward -826.34
Iteration 72 took 2.25 seconds (mean sampled reward: -5329.25). Current reward after update: -1226.76, Optimal reward -826.34
Iteration 73 took 2.36 seconds (mean sampled reward: -5847.45). Current reward after update: -1314.62, Optimal reward -826.34
Iteration 74 took 2.35 seconds (mean sampled reward: -5729.56). Current reward after update: -935.00, Optimal reward -826.34
Iteration 75 took 2.43 seconds (mean sampled reward: -6014.42). Current reward after update: -1140.25, Optimal reward -826.34
Iteration 76 took 2.38 seconds (mean sampled reward: -5351.50). Current reward after update: -1343.31, Optimal reward -826.34
Iteration 77 took 2.45 seconds (mean sampled reward: -6539.72). Current reward after update: -1411.51, Optimal reward -826.34
Iteration 78 took 2.43 seconds (mean sampled reward: -5823.59). Current reward after update: -1460.42, Optimal reward -826.34
Iteration 79 took 2.45 seconds (mean sampled reward: -5118.41). Current reward after update: -1361.71, Optimal reward -826.34
Iteration 80 took 2.40 seconds (mean sampled reward: -5251.53). Current reward after update: -1720.59, Optimal reward -826.34
Iteration 81 took 2.41 seconds (mean sampled reward: -4891.29). Current reward after update: -1295.70, Optimal reward -826.34
Iteration 82 took 2.42 seconds (mean sampled reward: -4711.47). Current reward after update: -1431.77, Optimal reward -826.34
Iteration 83 took 2.39 seconds (mean sampled reward: -4243.83). Current reward after update: -1119.32, Optimal reward -826.34
Iteration 84 took 2.39 seconds (mean sampled reward: -4508.86). Current reward after update: -1211.78, Optimal reward -826.34
Iteration 85 took 2.27 seconds (mean sampled reward: -4472.51). Current reward after update: -1627.24, Optimal reward -826.34
Iteration 86 took 2.42 seconds (mean sampled reward: -5254.17). Current reward after update: -1288.32, Optimal reward -826.34
Iteration 87 took 2.35 seconds (mean sampled reward: -5276.29). Current reward after update: -1259.85, Optimal reward -826.34
Iteration 88 took 2.18 seconds (mean sampled reward: -4914.27). Current reward after update: -1446.25, Optimal reward -826.34
Iteration 89 took 2.22 seconds (mean sampled reward: -4768.63). Current reward after update: -1606.15, Optimal reward -826.34
Iteration 90 took 2.14 seconds (mean sampled reward: -4364.71). Current reward after update: -1159.60, Optimal reward -826.34
Iteration 91 took 2.18 seconds (mean sampled reward: -5786.39). Current reward after update: -1395.09, Optimal reward -826.34
Iteration 92 took 2.22 seconds (mean sampled reward: -5984.48). Current reward after update: -1104.20, Optimal reward -826.34
Iteration 93 took 2.22 seconds (mean sampled reward: -5881.61). Current reward after update: -1378.67, Optimal reward -826.34
Iteration 94 took 2.15 seconds (mean sampled reward: -5798.02). Current reward after update: -7005.50, Optimal reward -826.34
Iteration 95 took 2.18 seconds (mean sampled reward: -6325.41). Current reward after update: -1501.84, Optimal reward -826.34
Iteration 96 took 2.14 seconds (mean sampled reward: -5956.79). Current reward after update: -1947.42, Optimal reward -826.34
Iteration 97 took 2.22 seconds (mean sampled reward: -5840.23). Current reward after update: -1116.15, Optimal reward -826.34
Iteration 98 took 2.22 seconds (mean sampled reward: -5828.20). Current reward after update: -1061.40, Optimal reward -826.34
Iteration 99 took 2.24 seconds (mean sampled reward: -5778.15). Current reward after update: -1544.58, Optimal reward -826.34
Iteration 100 took 2.28 seconds (mean sampled reward: -5625.91). Current reward after update: -1388.33, Optimal reward -826.34
Iteration 101 took 2.24 seconds (mean sampled reward: -5226.36). Current reward after update: -1359.35, Optimal reward -826.34
Iteration 102 took 2.33 seconds (mean sampled reward: -5218.01). Current reward after update: -1162.64, Optimal reward -826.34
Iteration 103 took 2.23 seconds (mean sampled reward: -5102.75). Current reward after update: -980.19, Optimal reward -826.34
Iteration 104 took 2.11 seconds (mean sampled reward: -4948.19). Current reward after update: -1374.64, Optimal reward -826.34
Iteration 105 took 2.12 seconds (mean sampled reward: -4742.74). Current reward after update: -1093.43, Optimal reward -826.34
Iteration 106 took 2.28 seconds (mean sampled reward: -4883.15). Current reward after update: -1009.82, Optimal reward -826.34
Iteration 107 took 2.30 seconds (mean sampled reward: -4925.35). Current reward after update: -1917.33, Optimal reward -826.34
Iteration 108 took 2.31 seconds (mean sampled reward: -5173.93). Current reward after update: -1148.34, Optimal reward -826.34
Iteration 109 took 2.33 seconds (mean sampled reward: -5114.26). Current reward after update: -1376.22, Optimal reward -826.34
Iteration 110 took 2.21 seconds (mean sampled reward: -5692.12). Current reward after update: -1360.60, Optimal reward -826.34
Iteration 111 took 2.34 seconds (mean sampled reward: -6098.70). Current reward after update: -1139.88, Optimal reward -826.34
Iteration 112 took 2.50 seconds (mean sampled reward: -6316.26). Current reward after update: -1328.44, Optimal reward -826.34
Iteration 113 took 2.22 seconds (mean sampled reward: -6398.85). Current reward after update: -1120.46, Optimal reward -826.34
Iteration 114 took 2.47 seconds (mean sampled reward: -5915.07). Current reward after update: -1199.23, Optimal reward -826.34
Iteration 115 took 2.42 seconds (mean sampled reward: -6044.57). Current reward after update: -809.71, Optimal reward -809.71
Iteration 116 took 2.50 seconds (mean sampled reward: -5431.15). Current reward after update: -875.54, Optimal reward -809.71
Iteration 117 took 2.50 seconds (mean sampled reward: -5847.53). Current reward after update: -1029.19, Optimal reward -809.71
Iteration 118 took 2.44 seconds (mean sampled reward: -5797.23). Current reward after update: -950.04, Optimal reward -809.71
Iteration 119 took 2.42 seconds (mean sampled reward: -4906.71). Current reward after update: -898.60, Optimal reward -809.71
Iteration 120 took 2.45 seconds (mean sampled reward: -5008.70). Current reward after update: -871.56, Optimal reward -809.71
Iteration 121 took 2.54 seconds (mean sampled reward: -5371.39). Current reward after update: -1038.87, Optimal reward -809.71
Iteration 122 took 2.50 seconds (mean sampled reward: -5234.53). Current reward after update: -937.03, Optimal reward -809.71
Iteration 123 took 2.53 seconds (mean sampled reward: -6322.05). Current reward after update: -1010.99, Optimal reward -809.71
Iteration 124 took 2.38 seconds (mean sampled reward: -5316.90). Current reward after update: -2175.11, Optimal reward -809.71
Iteration 125 took 2.50 seconds (mean sampled reward: -5001.22). Current reward after update: -1093.34, Optimal reward -809.71
Iteration 126 took 2.41 seconds (mean sampled reward: -5377.73). Current reward after update: -1074.67, Optimal reward -809.71
Iteration 127 took 2.43 seconds (mean sampled reward: -5320.62). Current reward after update: -936.77, Optimal reward -809.71
Iteration 128 took 2.43 seconds (mean sampled reward: -5659.71). Current reward after update: -1081.94, Optimal reward -809.71
Iteration 129 took 2.43 seconds (mean sampled reward: -6429.09). Current reward after update: -2003.26, Optimal reward -809.71
Iteration 130 took 2.39 seconds (mean sampled reward: -6288.42). Current reward after update: -4088.80, Optimal reward -809.71
Iteration 131 took 2.45 seconds (mean sampled reward: -6625.06). Current reward after update: -1167.65, Optimal reward -809.71
Iteration 132 took 2.42 seconds (mean sampled reward: -6664.87). Current reward after update: -1001.57, Optimal reward -809.71
Iteration 133 took 2.39 seconds (mean sampled reward: -6052.95). Current reward after update: -1236.33, Optimal reward -809.71
Iteration 134 took 2.34 seconds (mean sampled reward: -5980.06). Current reward after update: -1033.27, Optimal reward -809.71
Iteration 135 took 2.29 seconds (mean sampled reward: -4640.07). Current reward after update: -1739.05, Optimal reward -809.71
Iteration 136 took 2.38 seconds (mean sampled reward: -5837.82). Current reward after update: -1055.68, Optimal reward -809.71
Iteration 137 took 2.31 seconds (mean sampled reward: -5369.17). Current reward after update: -1042.72, Optimal reward -809.71
Iteration 138 took 2.32 seconds (mean sampled reward: -5322.54). Current reward after update: -1075.29, Optimal reward -809.71
Iteration 139 took 2.26 seconds (mean sampled reward: -5763.76). Current reward after update: -1020.08, Optimal reward -809.71
Iteration 140 took 2.35 seconds (mean sampled reward: -6296.58). Current reward after update: -1331.70, Optimal reward -809.71
Iteration 141 took 2.39 seconds (mean sampled reward: -6125.30). Current reward after update: -1192.28, Optimal reward -809.71
Iteration 142 took 2.44 seconds (mean sampled reward: -6473.96). Current reward after update: -1410.30, Optimal reward -809.71
Iteration 143 took 2.47 seconds (mean sampled reward: -6387.60). Current reward after update: -1254.22, Optimal reward -809.71
Iteration 144 took 2.41 seconds (mean sampled reward: -6405.80). Current reward after update: -1280.90, Optimal reward -809.71
Iteration 145 took 2.40 seconds (mean sampled reward: -5950.44). Current reward after update: -1368.25, Optimal reward -809.71
Iteration 146 took 2.40 seconds (mean sampled reward: -6257.84). Current reward after update: -1282.20, Optimal reward -809.71
Iteration 147 took 2.42 seconds (mean sampled reward: -6262.89). Current reward after update: -1309.63, Optimal reward -809.71
Iteration 148 took 2.34 seconds (mean sampled reward: -5468.69). Current reward after update: -2017.07, Optimal reward -809.71
Iteration 149 took 2.28 seconds (mean sampled reward: -5591.36). Current reward after update: -2589.93, Optimal reward -809.71
Iteration 150 took 2.25 seconds (mean sampled reward: -5860.79). Current reward after update: -1114.76, Optimal reward -809.71
Iteration 151 took 2.41 seconds (mean sampled reward: -5959.32). Current reward after update: -1318.50, Optimal reward -809.71
Iteration 152 took 2.27 seconds (mean sampled reward: -5518.39). Current reward after update: -1211.86, Optimal reward -809.71
Iteration 153 took 2.38 seconds (mean sampled reward: -5992.95). Current reward after update: -1440.48, Optimal reward -809.71
Iteration 154 took 2.29 seconds (mean sampled reward: -6143.18). Current reward after update: -1321.87, Optimal reward -809.71
Iteration 155 took 2.39 seconds (mean sampled reward: -6150.98). Current reward after update: -1081.99, Optimal reward -809.71
Iteration 156 took 2.38 seconds (mean sampled reward: -6549.44). Current reward after update: -1216.05, Optimal reward -809.71
Iteration 157 took 2.57 seconds (mean sampled reward: -6607.68). Current reward after update: -1184.23, Optimal reward -809.71
Iteration 158 took 2.48 seconds (mean sampled reward: -5470.13). Current reward after update: -1261.49, Optimal reward -809.71
Iteration 159 took 2.44 seconds (mean sampled reward: -5444.06). Current reward after update: -1256.24, Optimal reward -809.71
Iteration 160 took 2.37 seconds (mean sampled reward: -5752.90). Current reward after update: -1083.88, Optimal reward -809.71
Iteration 161 took 2.43 seconds (mean sampled reward: -6113.21). Current reward after update: -1160.65, Optimal reward -809.71
Iteration 162 took 2.44 seconds (mean sampled reward: -6086.19). Current reward after update: -2058.43, Optimal reward -809.71
Iteration 163 took 2.49 seconds (mean sampled reward: -6367.71). Current reward after update: -1082.34, Optimal reward -809.71
Iteration 164 took 2.46 seconds (mean sampled reward: -6144.56). Current reward after update: -7041.43, Optimal reward -809.71
Iteration 165 took 2.56 seconds (mean sampled reward: -5741.93). Current reward after update: -1115.03, Optimal reward -809.71
Iteration 166 took 2.49 seconds (mean sampled reward: -5164.84). Current reward after update: -1037.63, Optimal reward -809.71
Iteration 167 took 2.57 seconds (mean sampled reward: -5261.16). Current reward after update: -875.52, Optimal reward -809.71
Iteration 168 took 2.60 seconds (mean sampled reward: -6481.82). Current reward after update: -1202.02, Optimal reward -809.71
Iteration 169 took 2.48 seconds (mean sampled reward: -5401.10). Current reward after update: -1098.41, Optimal reward -809.71
Iteration 170 took 2.53 seconds (mean sampled reward: -5541.16). Current reward after update: -843.29, Optimal reward -809.71
Iteration 171 took 2.51 seconds (mean sampled reward: -6490.42). Current reward after update: -999.85, Optimal reward -809.71
Iteration 172 took 2.55 seconds (mean sampled reward: -6755.64). Current reward after update: -987.99, Optimal reward -809.71
Iteration 173 took 2.54 seconds (mean sampled reward: -6889.73). Current reward after update: -934.51, Optimal reward -809.71
Iteration 174 took 2.56 seconds (mean sampled reward: -6702.59). Current reward after update: -844.52, Optimal reward -809.71
Iteration 175 took 2.60 seconds (mean sampled reward: -6751.16). Current reward after update: -908.02, Optimal reward -809.71
Iteration 176 took 2.55 seconds (mean sampled reward: -6458.36). Current reward after update: -984.15, Optimal reward -809.71
Iteration 177 took 2.55 seconds (mean sampled reward: -6596.40). Current reward after update: -601.29, Optimal reward -601.29
Iteration 178 took 2.58 seconds (mean sampled reward: -5591.89). Current reward after update: -710.92, Optimal reward -601.29
Iteration 179 took 2.48 seconds (mean sampled reward: -4402.56). Current reward after update: -721.30, Optimal reward -601.29
Iteration 180 took 2.55 seconds (mean sampled reward: -5395.06). Current reward after update: -647.70, Optimal reward -601.29
Iteration 181 took 2.55 seconds (mean sampled reward: -4602.39). Current reward after update: -637.76, Optimal reward -601.29
Iteration 182 took 2.48 seconds (mean sampled reward: -4762.72). Current reward after update: -630.16, Optimal reward -601.29
Iteration 183 took 2.46 seconds (mean sampled reward: -4407.19). Current reward after update: -607.44, Optimal reward -601.29
Iteration 184 took 2.45 seconds (mean sampled reward: -4628.65). Current reward after update: -989.10, Optimal reward -601.29
Iteration 185 took 2.46 seconds (mean sampled reward: -5032.08). Current reward after update: -699.51, Optimal reward -601.29
Iteration 186 took 2.62 seconds (mean sampled reward: -5347.85). Current reward after update: -1557.84, Optimal reward -601.29
Iteration 187 took 2.56 seconds (mean sampled reward: -5717.16). Current reward after update: -632.79, Optimal reward -601.29
Iteration 188 took 2.53 seconds (mean sampled reward: -6043.70). Current reward after update: -590.21, Optimal reward -590.21
Iteration 189 took 2.54 seconds (mean sampled reward: -6631.95). Current reward after update: -517.22, Optimal reward -517.22
Iteration 190 took 2.50 seconds (mean sampled reward: -6787.43). Current reward after update: -1015.48, Optimal reward -517.22
Iteration 191 took 2.51 seconds (mean sampled reward: -5677.39). Current reward after update: -663.09, Optimal reward -517.22
Iteration 192 took 2.53 seconds (mean sampled reward: -6059.84). Current reward after update: -1360.53, Optimal reward -517.22
Iteration 193 took 2.57 seconds (mean sampled reward: -6346.25). Current reward after update: -563.75, Optimal reward -517.22
Iteration 194 took 2.59 seconds (mean sampled reward: -5750.95). Current reward after update: -672.11, Optimal reward -517.22
Iteration 195 took 2.67 seconds (mean sampled reward: -6212.37). Current reward after update: -3403.77, Optimal reward -517.22
Iteration 196 took 2.58 seconds (mean sampled reward: -6279.15). Current reward after update: -926.49, Optimal reward -517.22
Iteration 197 took 2.58 seconds (mean sampled reward: -5628.01). Current reward after update: -661.33, Optimal reward -517.22
Iteration 198 took 2.49 seconds (mean sampled reward: -5368.37). Current reward after update: -594.09, Optimal reward -517.22
Iteration 199 took 2.54 seconds (mean sampled reward: -5433.30). Current reward after update: -7062.06, Optimal reward -517.22
Iteration 200 took 2.45 seconds (mean sampled reward: -5131.38). Current reward after update: -739.68, Optimal reward -517.22
Iteration 1 took 2.44 seconds (mean sampled reward: -7421.02). Current reward after update: -3182.80, Optimal reward -3182.80
Iteration 2 took 2.33 seconds (mean sampled reward: -7019.32). Current reward after update: -2219.92, Optimal reward -2219.92
Iteration 3 took 2.30 seconds (mean sampled reward: -6608.54). Current reward after update: -1690.25, Optimal reward -1690.25
Iteration 4 took 2.46 seconds (mean sampled reward: -6212.27). Current reward after update: -1929.69, Optimal reward -1690.25
Iteration 5 took 2.33 seconds (mean sampled reward: -6453.96). Current reward after update: -1482.54, Optimal reward -1482.54
Iteration 6 took 2.27 seconds (mean sampled reward: -6109.07). Current reward after update: -1585.26, Optimal reward -1482.54
Iteration 7 took 2.27 seconds (mean sampled reward: -4534.75). Current reward after update: -1253.91, Optimal reward -1253.91
Iteration 8 took 2.54 seconds (mean sampled reward: -5264.90). Current reward after update: -791.28, Optimal reward -791.28
Iteration 9 took 2.56 seconds (mean sampled reward: -5924.63). Current reward after update: -987.80, Optimal reward -791.28
Iteration 10 took 2.40 seconds (mean sampled reward: -5551.58). Current reward after update: -1167.33, Optimal reward -791.28
Iteration 11 took 2.26 seconds (mean sampled reward: -6540.21). Current reward after update: -1234.01, Optimal reward -791.28
Iteration 12 took 2.27 seconds (mean sampled reward: -4835.12). Current reward after update: -978.26, Optimal reward -791.28
Iteration 13 took 2.20 seconds (mean sampled reward: -4305.15). Current reward after update: -909.19, Optimal reward -791.28
Iteration 14 took 2.59 seconds (mean sampled reward: -4227.81). Current reward after update: -918.08, Optimal reward -791.28
Iteration 15 took 2.40 seconds (mean sampled reward: -3900.39). Current reward after update: -837.49, Optimal reward -791.28
Iteration 16 took 2.53 seconds (mean sampled reward: -5402.92). Current reward after update: -959.46, Optimal reward -791.28
Iteration 17 took 2.40 seconds (mean sampled reward: -5541.80). Current reward after update: -2791.45, Optimal reward -791.28
Iteration 18 took 2.47 seconds (mean sampled reward: -5390.14). Current reward after update: -866.58, Optimal reward -791.28
Iteration 19 took 2.30 seconds (mean sampled reward: -5348.46). Current reward after update: -913.95, Optimal reward -791.28
Iteration 20 took 2.40 seconds (mean sampled reward: -5329.09). Current reward after update: -1169.77, Optimal reward -791.28
Iteration 21 took 2.45 seconds (mean sampled reward: -5576.58). Current reward after update: -805.46, Optimal reward -791.28
Iteration 22 took 2.56 seconds (mean sampled reward: -5327.28). Current reward after update: -855.96, Optimal reward -791.28
Iteration 23 took 2.48 seconds (mean sampled reward: -5733.42). Current reward after update: -948.13, Optimal reward -791.28
Iteration 24 took 2.33 seconds (mean sampled reward: -4827.74). Current reward after update: -895.97, Optimal reward -791.28
Iteration 25 took 2.48 seconds (mean sampled reward: -4961.73). Current reward after update: -898.84, Optimal reward -791.28
Iteration 26 took 2.35 seconds (mean sampled reward: -4500.52). Current reward after update: -769.80, Optimal reward -769.80
Iteration 27 took 2.60 seconds (mean sampled reward: -5160.39). Current reward after update: -426.68, Optimal reward -426.68
Iteration 28 took 2.61 seconds (mean sampled reward: -4729.12). Current reward after update: -518.62, Optimal reward -426.68
Iteration 29 took 2.43 seconds (mean sampled reward: -4385.47). Current reward after update: -244.91, Optimal reward -244.91
Iteration 30 took 2.29 seconds (mean sampled reward: -3587.22). Current reward after update: -367.60, Optimal reward -244.91
Iteration 31 took 2.26 seconds (mean sampled reward: -3420.61). Current reward after update: -323.14, Optimal reward -244.91
Iteration 32 took 2.28 seconds (mean sampled reward: -3410.83). Current reward after update: -586.51, Optimal reward -244.91
Iteration 33 took 2.37 seconds (mean sampled reward: -4243.87). Current reward after update: -494.57, Optimal reward -244.91
Iteration 34 took 2.50 seconds (mean sampled reward: -5520.92). Current reward after update: -824.91, Optimal reward -244.91
Iteration 35 took 2.31 seconds (mean sampled reward: -3717.90). Current reward after update: -624.82, Optimal reward -244.91
Iteration 36 took 2.28 seconds (mean sampled reward: -4755.88). Current reward after update: -1165.90, Optimal reward -244.91
Iteration 37 took 2.25 seconds (mean sampled reward: -4629.01). Current reward after update: -806.08, Optimal reward -244.91
Iteration 38 took 2.29 seconds (mean sampled reward: -3076.82). Current reward after update: -631.53, Optimal reward -244.91
Iteration 39 took 2.26 seconds (mean sampled reward: -3662.94). Current reward after update: -564.54, Optimal reward -244.91
Iteration 40 took 2.22 seconds (mean sampled reward: -3607.99). Current reward after update: -2959.56, Optimal reward -244.91
Iteration 41 took 2.28 seconds (mean sampled reward: -4411.55). Current reward after update: -716.83, Optimal reward -244.91
Iteration 42 took 2.28 seconds (mean sampled reward: -4020.97). Current reward after update: -808.70, Optimal reward -244.91
Iteration 43 took 2.29 seconds (mean sampled reward: -4078.33). Current reward after update: -891.58, Optimal reward -244.91
Iteration 44 took 2.33 seconds (mean sampled reward: -4105.66). Current reward after update: -971.79, Optimal reward -244.91
Iteration 45 took 2.41 seconds (mean sampled reward: -4641.00). Current reward after update: -1243.76, Optimal reward -244.91
Iteration 46 took 2.29 seconds (mean sampled reward: -5487.89). Current reward after update: -1086.60, Optimal reward -244.91
Iteration 47 took 2.30 seconds (mean sampled reward: -4964.34). Current reward after update: -6837.82, Optimal reward -244.91
Iteration 48 took 2.31 seconds (mean sampled reward: -5352.49). Current reward after update: -621.57, Optimal reward -244.91
Iteration 49 took 2.34 seconds (mean sampled reward: -5885.73). Current reward after update: -854.43, Optimal reward -244.91
Iteration 50 took 2.23 seconds (mean sampled reward: -4919.23). Current reward after update: -762.77, Optimal reward -244.91
Iteration 51 took 2.21 seconds (mean sampled reward: -4782.06). Current reward after update: -635.40, Optimal reward -244.91
Iteration 52 took 2.29 seconds (mean sampled reward: -4740.41). Current reward after update: -1694.90, Optimal reward -244.91
Iteration 53 took 2.36 seconds (mean sampled reward: -4184.26). Current reward after update: -703.45, Optimal reward -244.91
Iteration 54 took 2.22 seconds (mean sampled reward: -4665.02). Current reward after update: -447.82, Optimal reward -244.91
Iteration 55 took 2.44 seconds (mean sampled reward: -4377.98). Current reward after update: -826.24, Optimal reward -244.91
Iteration 56 took 2.33 seconds (mean sampled reward: -3467.47). Current reward after update: -643.75, Optimal reward -244.91
Iteration 57 took 2.45 seconds (mean sampled reward: -5244.15). Current reward after update: -590.69, Optimal reward -244.91
Iteration 58 took 2.35 seconds (mean sampled reward: -5929.03). Current reward after update: -420.92, Optimal reward -244.91
Iteration 59 took 2.29 seconds (mean sampled reward: -4993.14). Current reward after update: -592.05, Optimal reward -244.91
Iteration 60 took 2.26 seconds (mean sampled reward: -4219.19). Current reward after update: -583.60, Optimal reward -244.91
Iteration 61 took 2.20 seconds (mean sampled reward: -5498.27). Current reward after update: -1058.81, Optimal reward -244.91
Iteration 62 took 2.40 seconds (mean sampled reward: -6887.96). Current reward after update: -633.95, Optimal reward -244.91
Iteration 63 took 2.35 seconds (mean sampled reward: -5818.42). Current reward after update: -482.35, Optimal reward -244.91
Iteration 64 took 2.32 seconds (mean sampled reward: -4818.89). Current reward after update: -348.55, Optimal reward -244.91
Iteration 65 took 2.26 seconds (mean sampled reward: -5809.01). Current reward after update: -591.49, Optimal reward -244.91
Iteration 66 took 2.28 seconds (mean sampled reward: -5023.61). Current reward after update: -431.55, Optimal reward -244.91
Iteration 67 took 2.25 seconds (mean sampled reward: -5033.49). Current reward after update: -402.97, Optimal reward -244.91
Iteration 68 took 2.37 seconds (mean sampled reward: -6731.02). Current reward after update: -624.40, Optimal reward -244.91
Iteration 69 took 2.21 seconds (mean sampled reward: -4743.35). Current reward after update: -736.23, Optimal reward -244.91
Iteration 70 took 2.26 seconds (mean sampled reward: -4572.81). Current reward after update: -612.89, Optimal reward -244.91
Iteration 71 took 2.20 seconds (mean sampled reward: -4498.54). Current reward after update: -557.05, Optimal reward -244.91
Iteration 72 took 2.21 seconds (mean sampled reward: -4327.20). Current reward after update: -470.70, Optimal reward -244.91
Iteration 73 took 2.20 seconds (mean sampled reward: -2999.87). Current reward after update: -349.37, Optimal reward -244.91
Iteration 74 took 2.18 seconds (mean sampled reward: -4441.37). Current reward after update: -509.86, Optimal reward -244.91
Iteration 75 took 2.20 seconds (mean sampled reward: -4100.23). Current reward after update: -467.60, Optimal reward -244.91
Iteration 76 took 2.26 seconds (mean sampled reward: -4777.76). Current reward after update: -537.82, Optimal reward -244.91
Iteration 77 took 2.20 seconds (mean sampled reward: -4466.09). Current reward after update: -482.65, Optimal reward -244.91
Iteration 78 took 2.28 seconds (mean sampled reward: -5126.87). Current reward after update: -565.67, Optimal reward -244.91
Iteration 79 took 2.25 seconds (mean sampled reward: -4175.04). Current reward after update: -454.80, Optimal reward -244.91
Iteration 80 took 2.32 seconds (mean sampled reward: -4755.37). Current reward after update: -814.75, Optimal reward -244.91
Iteration 81 took 2.29 seconds (mean sampled reward: -4170.75). Current reward after update: -689.58, Optimal reward -244.91
Iteration 82 took 2.27 seconds (mean sampled reward: -4735.79). Current reward after update: -598.60, Optimal reward -244.91
Iteration 83 took 2.30 seconds (mean sampled reward: -2843.67). Current reward after update: -6943.69, Optimal reward -244.91
Iteration 84 took 2.28 seconds (mean sampled reward: -3119.87). Current reward after update: -200.96, Optimal reward -200.96
Iteration 85 took 2.30 seconds (mean sampled reward: -3267.76). Current reward after update: -199.52, Optimal reward -199.52
Iteration 86 took 2.25 seconds (mean sampled reward: -3476.16). Current reward after update: -327.48, Optimal reward -199.52
Iteration 87 took 2.22 seconds (mean sampled reward: -3483.51). Current reward after update: -337.92, Optimal reward -199.52
Iteration 88 took 2.30 seconds (mean sampled reward: -3327.40). Current reward after update: -427.30, Optimal reward -199.52
Iteration 89 took 2.26 seconds (mean sampled reward: -3722.14). Current reward after update: -329.25, Optimal reward -199.52
Iteration 90 took 2.28 seconds (mean sampled reward: -3586.69). Current reward after update: -299.06, Optimal reward -199.52
Iteration 91 took 2.28 seconds (mean sampled reward: -4555.60). Current reward after update: -227.93, Optimal reward -199.52
Iteration 92 took 2.24 seconds (mean sampled reward: -4651.12). Current reward after update: -364.29, Optimal reward -199.52
Iteration 93 took 2.23 seconds (mean sampled reward: -5629.32). Current reward after update: -221.35, Optimal reward -199.52
Iteration 94 took 2.21 seconds (mean sampled reward: -4948.01). Current reward after update: -418.96, Optimal reward -199.52
Iteration 95 took 2.24 seconds (mean sampled reward: -5706.42). Current reward after update: -1019.05, Optimal reward -199.52
Iteration 96 took 2.24 seconds (mean sampled reward: -4582.27). Current reward after update: -723.80, Optimal reward -199.52
Iteration 97 took 2.29 seconds (mean sampled reward: -4093.60). Current reward after update: -966.25, Optimal reward -199.52
Iteration 98 took 2.23 seconds (mean sampled reward: -3828.84). Current reward after update: -613.63, Optimal reward -199.52
Iteration 99 took 2.28 seconds (mean sampled reward: -2969.77). Current reward after update: -910.33, Optimal reward -199.52
Iteration 100 took 2.22 seconds (mean sampled reward: -3780.00). Current reward after update: -1839.18, Optimal reward -199.52
Iteration 101 took 2.27 seconds (mean sampled reward: -3000.16). Current reward after update: -3454.01, Optimal reward -199.52
Iteration 102 took 2.26 seconds (mean sampled reward: -3712.77). Current reward after update: -370.42, Optimal reward -199.52
Iteration 103 took 2.23 seconds (mean sampled reward: -4323.52). Current reward after update: -566.60, Optimal reward -199.52
Iteration 104 took 2.27 seconds (mean sampled reward: -4532.16). Current reward after update: -540.07, Optimal reward -199.52
Iteration 105 took 2.26 seconds (mean sampled reward: -3660.63). Current reward after update: -626.15, Optimal reward -199.52
Iteration 106 took 2.39 seconds (mean sampled reward: -3510.96). Current reward after update: -326.29, Optimal reward -199.52
Iteration 107 took 2.24 seconds (mean sampled reward: -2981.48). Current reward after update: -333.53, Optimal reward -199.52
Iteration 108 took 2.28 seconds (mean sampled reward: -2586.44). Current reward after update: -282.67, Optimal reward -199.52
Iteration 109 took 2.33 seconds (mean sampled reward: -3249.96). Current reward after update: -330.92, Optimal reward -199.52
Iteration 110 took 2.39 seconds (mean sampled reward: -3365.11). Current reward after update: -369.80, Optimal reward -199.52
Iteration 111 took 2.38 seconds (mean sampled reward: -3293.70). Current reward after update: -227.21, Optimal reward -199.52
Iteration 112 took 2.22 seconds (mean sampled reward: -3009.89). Current reward after update: -411.37, Optimal reward -199.52
Iteration 113 took 2.25 seconds (mean sampled reward: -3947.65). Current reward after update: -814.46, Optimal reward -199.52
Iteration 114 took 2.36 seconds (mean sampled reward: -4042.62). Current reward after update: -558.06, Optimal reward -199.52
Iteration 115 took 2.33 seconds (mean sampled reward: -5558.26). Current reward after update: -884.51, Optimal reward -199.52
Iteration 116 took 2.34 seconds (mean sampled reward: -4089.76). Current reward after update: -852.54, Optimal reward -199.52
Iteration 117 took 2.25 seconds (mean sampled reward: -4091.21). Current reward after update: -325.06, Optimal reward -199.52
Iteration 118 took 2.39 seconds (mean sampled reward: -4257.26). Current reward after update: -389.39, Optimal reward -199.52
Iteration 119 took 2.23 seconds (mean sampled reward: -3830.38). Current reward after update: -368.92, Optimal reward -199.52
Iteration 120 took 2.26 seconds (mean sampled reward: -3563.59). Current reward after update: -220.90, Optimal reward -199.52
Iteration 121 took 2.34 seconds (mean sampled reward: -3548.33). Current reward after update: -457.76, Optimal reward -199.52
Iteration 122 took 2.27 seconds (mean sampled reward: -4611.98). Current reward after update: -420.74, Optimal reward -199.52
Iteration 123 took 2.31 seconds (mean sampled reward: -4835.36). Current reward after update: -701.16, Optimal reward -199.52
Iteration 124 took 2.34 seconds (mean sampled reward: -3828.96). Current reward after update: -333.32, Optimal reward -199.52
Iteration 125 took 2.36 seconds (mean sampled reward: -4842.24). Current reward after update: -403.49, Optimal reward -199.52
Iteration 126 took 2.29 seconds (mean sampled reward: -2850.63). Current reward after update: -801.94, Optimal reward -199.52
Iteration 127 took 2.29 seconds (mean sampled reward: -3349.02). Current reward after update: -528.72, Optimal reward -199.52
Iteration 128 took 2.22 seconds (mean sampled reward: -3654.10). Current reward after update: -191.93, Optimal reward -191.93
Iteration 129 took 2.23 seconds (mean sampled reward: -3468.55). Current reward after update: -323.87, Optimal reward -191.93
Iteration 130 took 2.24 seconds (mean sampled reward: -4688.14). Current reward after update: -403.22, Optimal reward -191.93
Iteration 131 took 2.25 seconds (mean sampled reward: -4049.91). Current reward after update: -545.06, Optimal reward -191.93
Iteration 132 took 2.22 seconds (mean sampled reward: -4096.17). Current reward after update: -430.78, Optimal reward -191.93
Iteration 133 took 2.20 seconds (mean sampled reward: -3435.22). Current reward after update: -224.50, Optimal reward -191.93
Iteration 134 took 2.22 seconds (mean sampled reward: -2947.67). Current reward after update: -224.61, Optimal reward -191.93
Iteration 135 took 2.21 seconds (mean sampled reward: -3787.37). Current reward after update: -254.47, Optimal reward -191.93
Iteration 136 took 2.23 seconds (mean sampled reward: -4777.11). Current reward after update: -376.66, Optimal reward -191.93
Iteration 137 took 2.20 seconds (mean sampled reward: -3801.98). Current reward after update: -227.67, Optimal reward -191.93
Iteration 138 took 2.21 seconds (mean sampled reward: -4401.78). Current reward after update: -1219.09, Optimal reward -191.93
Iteration 139 took 2.19 seconds (mean sampled reward: -4502.43). Current reward after update: -422.90, Optimal reward -191.93
Iteration 140 took 2.28 seconds (mean sampled reward: -4257.59). Current reward after update: -364.93, Optimal reward -191.93
Iteration 141 took 2.27 seconds (mean sampled reward: -4530.02). Current reward after update: -377.47, Optimal reward -191.93
Iteration 142 took 2.23 seconds (mean sampled reward: -4408.87). Current reward after update: -301.55, Optimal reward -191.93
Iteration 143 took 2.22 seconds (mean sampled reward: -4555.39). Current reward after update: -467.09, Optimal reward -191.93
Iteration 144 took 2.23 seconds (mean sampled reward: -4858.47). Current reward after update: -371.89, Optimal reward -191.93
Iteration 145 took 2.24 seconds (mean sampled reward: -4245.61). Current reward after update: -246.87, Optimal reward -191.93
Iteration 146 took 2.27 seconds (mean sampled reward: -4841.73). Current reward after update: -529.12, Optimal reward -191.93
Iteration 147 took 2.34 seconds (mean sampled reward: -6303.50). Current reward after update: -442.09, Optimal reward -191.93
Iteration 148 took 2.26 seconds (mean sampled reward: -4047.73). Current reward after update: -347.47, Optimal reward -191.93
Iteration 149 took 2.29 seconds (mean sampled reward: -4363.49). Current reward after update: -495.39, Optimal reward -191.93
Iteration 150 took 2.30 seconds (mean sampled reward: -4612.62). Current reward after update: -413.68, Optimal reward -191.93
Iteration 151 took 2.28 seconds (mean sampled reward: -4690.38). Current reward after update: -340.48, Optimal reward -191.93
Iteration 152 took 2.25 seconds (mean sampled reward: -3920.48). Current reward after update: -475.46, Optimal reward -191.93
Iteration 153 took 2.26 seconds (mean sampled reward: -3967.37). Current reward after update: -390.19, Optimal reward -191.93
Iteration 154 took 2.26 seconds (mean sampled reward: -4285.75). Current reward after update: -876.57, Optimal reward -191.93
Iteration 155 took 2.30 seconds (mean sampled reward: -4662.24). Current reward after update: -525.23, Optimal reward -191.93
Iteration 156 took 2.30 seconds (mean sampled reward: -5470.46). Current reward after update: -519.57, Optimal reward -191.93
Iteration 157 took 2.35 seconds (mean sampled reward: -5774.18). Current reward after update: -397.64, Optimal reward -191.93
Iteration 158 took 2.25 seconds (mean sampled reward: -3901.31). Current reward after update: -6589.75, Optimal reward -191.93
Iteration 159 took 2.37 seconds (mean sampled reward: -4483.25). Current reward after update: -462.07, Optimal reward -191.93
Iteration 160 took 2.28 seconds (mean sampled reward: -4483.65). Current reward after update: -543.39, Optimal reward -191.93
Iteration 161 took 2.34 seconds (mean sampled reward: -4853.68). Current reward after update: -487.28, Optimal reward -191.93
Iteration 162 took 2.37 seconds (mean sampled reward: -5413.88). Current reward after update: -424.95, Optimal reward -191.93
Iteration 163 took 2.37 seconds (mean sampled reward: -4185.14). Current reward after update: -461.75, Optimal reward -191.93
Iteration 164 took 2.35 seconds (mean sampled reward: -4428.00). Current reward after update: -709.28, Optimal reward -191.93
Iteration 165 took 2.34 seconds (mean sampled reward: -5259.63). Current reward after update: -481.10, Optimal reward -191.93
Iteration 166 took 2.36 seconds (mean sampled reward: -4874.34). Current reward after update: -354.84, Optimal reward -191.93
Iteration 167 took 2.35 seconds (mean sampled reward: -5266.46). Current reward after update: -507.22, Optimal reward -191.93
Iteration 168 took 2.30 seconds (mean sampled reward: -4843.90). Current reward after update: -375.00, Optimal reward -191.93
Iteration 169 took 2.24 seconds (mean sampled reward: -5132.51). Current reward after update: -313.23, Optimal reward -191.93
Iteration 170 took 2.27 seconds (mean sampled reward: -3429.16). Current reward after update: -317.20, Optimal reward -191.93
Iteration 171 took 2.26 seconds (mean sampled reward: -5013.02). Current reward after update: -1724.96, Optimal reward -191.93
Iteration 172 took 2.35 seconds (mean sampled reward: -3717.22). Current reward after update: -461.66, Optimal reward -191.93
Iteration 173 took 2.37 seconds (mean sampled reward: -5435.28). Current reward after update: -360.00, Optimal reward -191.93
Iteration 174 took 2.29 seconds (mean sampled reward: -2599.27). Current reward after update: -245.49, Optimal reward -191.93
Iteration 175 took 2.30 seconds (mean sampled reward: -2324.97). Current reward after update: -508.32, Optimal reward -191.93
Iteration 176 took 2.27 seconds (mean sampled reward: -2505.88). Current reward after update: -383.56, Optimal reward -191.93
Iteration 177 took 2.27 seconds (mean sampled reward: -2698.53). Current reward after update: -968.51, Optimal reward -191.93
Iteration 178 took 2.27 seconds (mean sampled reward: -2263.81). Current reward after update: -328.01, Optimal reward -191.93
Iteration 179 took 2.27 seconds (mean sampled reward: -2811.30). Current reward after update: -323.04, Optimal reward -191.93
Iteration 180 took 2.29 seconds (mean sampled reward: -3061.13). Current reward after update: -586.72, Optimal reward -191.93
Iteration 181 took 2.37 seconds (mean sampled reward: -3298.80). Current reward after update: -506.53, Optimal reward -191.93
Iteration 182 took 2.44 seconds (mean sampled reward: -5487.57). Current reward after update: -731.70, Optimal reward -191.93
Iteration 183 took 2.26 seconds (mean sampled reward: -4473.32). Current reward after update: -2662.90, Optimal reward -191.93
Iteration 184 took 2.34 seconds (mean sampled reward: -3627.36). Current reward after update: -470.55, Optimal reward -191.93
Iteration 185 took 2.42 seconds (mean sampled reward: -5512.82). Current reward after update: -363.89, Optimal reward -191.93
Iteration 186 took 2.35 seconds (mean sampled reward: -2540.29). Current reward after update: -368.42, Optimal reward -191.93
Iteration 187 took 2.32 seconds (mean sampled reward: -4044.84). Current reward after update: -319.70, Optimal reward -191.93
Iteration 188 took 2.30 seconds (mean sampled reward: -4289.64). Current reward after update: -478.63, Optimal reward -191.93
Iteration 189 took 2.29 seconds (mean sampled reward: -4604.42). Current reward after update: -557.31, Optimal reward -191.93
Iteration 190 took 2.30 seconds (mean sampled reward: -4918.96). Current reward after update: -400.77, Optimal reward -191.93
Iteration 191 took 2.31 seconds (mean sampled reward: -5805.64). Current reward after update: -443.83, Optimal reward -191.93
Iteration 192 took 2.36 seconds (mean sampled reward: -5372.14). Current reward after update: -2375.48, Optimal reward -191.93
Iteration 193 took 2.35 seconds (mean sampled reward: -5525.99). Current reward after update: -361.63, Optimal reward -191.93
Iteration 194 took 2.34 seconds (mean sampled reward: -5297.36). Current reward after update: -428.60, Optimal reward -191.93
Iteration 195 took 2.27 seconds (mean sampled reward: -4751.76). Current reward after update: -477.55, Optimal reward -191.93
Iteration 196 took 2.27 seconds (mean sampled reward: -3316.57). Current reward after update: -273.06, Optimal reward -191.93
Iteration 197 took 2.38 seconds (mean sampled reward: -5054.01). Current reward after update: -304.04, Optimal reward -191.93
Iteration 198 took 2.34 seconds (mean sampled reward: -4830.92). Current reward after update: -515.99, Optimal reward -191.93
Iteration 199 took 2.29 seconds (mean sampled reward: -4529.22). Current reward after update: -309.89, Optimal reward -191.93
Iteration 200 took 2.25 seconds (mean sampled reward: -4132.08). Current reward after update: -419.58, Optimal reward -191.93
Iteration 1 took 2.39 seconds (mean sampled reward: -7508.15). Current reward after update: -5023.53, Optimal reward -5023.53
Iteration 2 took 2.36 seconds (mean sampled reward: -7109.77). Current reward after update: -1383.96, Optimal reward -1383.96
Iteration 3 took 2.33 seconds (mean sampled reward: -7356.55). Current reward after update: -2334.85, Optimal reward -1383.96
Iteration 4 took 2.54 seconds (mean sampled reward: -6701.24). Current reward after update: -2105.49, Optimal reward -1383.96
Iteration 5 took 2.54 seconds (mean sampled reward: -6197.14). Current reward after update: -1599.18, Optimal reward -1383.96
Iteration 6 took 2.30 seconds (mean sampled reward: -5746.76). Current reward after update: -1340.71, Optimal reward -1340.71
Iteration 7 took 2.59 seconds (mean sampled reward: -6034.87). Current reward after update: -1269.90, Optimal reward -1269.90
Iteration 8 took 2.26 seconds (mean sampled reward: -5566.18). Current reward after update: -1086.67, Optimal reward -1086.67
Iteration 9 took 2.51 seconds (mean sampled reward: -5927.22). Current reward after update: -1325.60, Optimal reward -1086.67
Iteration 10 took 2.42 seconds (mean sampled reward: -5965.96). Current reward after update: -1811.19, Optimal reward -1086.67
Iteration 11 took 2.39 seconds (mean sampled reward: -5471.41). Current reward after update: -1520.90, Optimal reward -1086.67
Iteration 12 took 2.29 seconds (mean sampled reward: -5011.22). Current reward after update: -1142.13, Optimal reward -1086.67
Iteration 13 took 2.39 seconds (mean sampled reward: -5609.17). Current reward after update: -843.48, Optimal reward -843.48
Iteration 14 took 2.27 seconds (mean sampled reward: -5890.56). Current reward after update: -1134.67, Optimal reward -843.48
Iteration 15 took 2.66 seconds (mean sampled reward: -4154.79). Current reward after update: -985.27, Optimal reward -843.48
Iteration 16 took 2.46 seconds (mean sampled reward: -5395.35). Current reward after update: -944.69, Optimal reward -843.48
Iteration 17 took 2.37 seconds (mean sampled reward: -6213.62). Current reward after update: -878.72, Optimal reward -843.48
Iteration 18 took 2.41 seconds (mean sampled reward: -4570.26). Current reward after update: -1005.01, Optimal reward -843.48
Iteration 19 took 2.48 seconds (mean sampled reward: -4249.29). Current reward after update: -806.84, Optimal reward -806.84
Iteration 20 took 2.27 seconds (mean sampled reward: -3694.85). Current reward after update: -1074.32, Optimal reward -806.84
Iteration 21 took 2.40 seconds (mean sampled reward: -3564.25). Current reward after update: -823.95, Optimal reward -806.84
Iteration 22 took 2.30 seconds (mean sampled reward: -4157.00). Current reward after update: -1240.02, Optimal reward -806.84
Iteration 23 took 2.25 seconds (mean sampled reward: -4863.55). Current reward after update: -764.00, Optimal reward -764.00
Iteration 24 took 2.32 seconds (mean sampled reward: -4871.30). Current reward after update: -902.20, Optimal reward -764.00
Iteration 25 took 2.30 seconds (mean sampled reward: -4551.69). Current reward after update: -1017.88, Optimal reward -764.00
Iteration 26 took 2.24 seconds (mean sampled reward: -4273.54). Current reward after update: -893.03, Optimal reward -764.00
Iteration 27 took 2.42 seconds (mean sampled reward: -4366.78). Current reward after update: -890.82, Optimal reward -764.00
Iteration 28 took 2.33 seconds (mean sampled reward: -5954.23). Current reward after update: -6945.26, Optimal reward -764.00
Iteration 29 took 2.28 seconds (mean sampled reward: -6355.26). Current reward after update: -1089.36, Optimal reward -764.00
Iteration 30 took 2.22 seconds (mean sampled reward: -5975.61). Current reward after update: -1150.84, Optimal reward -764.00
Iteration 31 took 2.30 seconds (mean sampled reward: -4140.11). Current reward after update: -842.69, Optimal reward -764.00
Iteration 32 took 2.33 seconds (mean sampled reward: -3491.58). Current reward after update: -927.49, Optimal reward -764.00
Iteration 33 took 2.24 seconds (mean sampled reward: -4187.71). Current reward after update: -800.06, Optimal reward -764.00
Iteration 34 took 2.35 seconds (mean sampled reward: -3582.47). Current reward after update: -1311.62, Optimal reward -764.00
Iteration 35 took 2.33 seconds (mean sampled reward: -4561.52). Current reward after update: -925.26, Optimal reward -764.00
Iteration 36 took 2.25 seconds (mean sampled reward: -4023.06). Current reward after update: -1042.71, Optimal reward -764.00
Iteration 37 took 2.24 seconds (mean sampled reward: -4343.91). Current reward after update: -907.05, Optimal reward -764.00
Iteration 38 took 2.28 seconds (mean sampled reward: -3857.80). Current reward after update: -805.21, Optimal reward -764.00
Iteration 39 took 2.36 seconds (mean sampled reward: -4310.96). Current reward after update: -846.14, Optimal reward -764.00
Iteration 40 took 2.26 seconds (mean sampled reward: -3421.06). Current reward after update: -859.13, Optimal reward -764.00
Iteration 41 took 2.32 seconds (mean sampled reward: -5019.20). Current reward after update: -1009.26, Optimal reward -764.00
Iteration 42 took 2.25 seconds (mean sampled reward: -4566.31). Current reward after update: -802.68, Optimal reward -764.00
Iteration 43 took 2.26 seconds (mean sampled reward: -3698.63). Current reward after update: -839.31, Optimal reward -764.00
Iteration 44 took 2.33 seconds (mean sampled reward: -4018.40). Current reward after update: -647.54, Optimal reward -647.54
Iteration 45 took 2.24 seconds (mean sampled reward: -4754.19). Current reward after update: -907.18, Optimal reward -647.54
Iteration 46 took 2.30 seconds (mean sampled reward: -3831.22). Current reward after update: -865.60, Optimal reward -647.54
Iteration 47 took 2.26 seconds (mean sampled reward: -3259.05). Current reward after update: -693.44, Optimal reward -647.54
Iteration 48 took 2.25 seconds (mean sampled reward: -3027.17). Current reward after update: -673.19, Optimal reward -647.54
Iteration 49 took 2.26 seconds (mean sampled reward: -4855.81). Current reward after update: -951.16, Optimal reward -647.54
Iteration 50 took 2.26 seconds (mean sampled reward: -3830.72). Current reward after update: -800.49, Optimal reward -647.54
Iteration 51 took 2.28 seconds (mean sampled reward: -5069.41). Current reward after update: -729.15, Optimal reward -647.54
Iteration 52 took 2.24 seconds (mean sampled reward: -4425.92). Current reward after update: -790.18, Optimal reward -647.54
Iteration 53 took 2.46 seconds (mean sampled reward: -4884.94). Current reward after update: -943.33, Optimal reward -647.54
Iteration 54 took 2.52 seconds (mean sampled reward: -3649.55). Current reward after update: -696.52, Optimal reward -647.54
Iteration 55 took 2.34 seconds (mean sampled reward: -3609.77). Current reward after update: -975.15, Optimal reward -647.54
Iteration 56 took 2.57 seconds (mean sampled reward: -2792.64). Current reward after update: -635.68, Optimal reward -635.68
Iteration 57 took 2.49 seconds (mean sampled reward: -4524.09). Current reward after update: -993.39, Optimal reward -635.68
Iteration 58 took 2.34 seconds (mean sampled reward: -3240.29). Current reward after update: -2355.36, Optimal reward -635.68
Iteration 59 took 2.32 seconds (mean sampled reward: -2973.85). Current reward after update: -817.51, Optimal reward -635.68
Iteration 60 took 2.40 seconds (mean sampled reward: -3157.24). Current reward after update: -758.19, Optimal reward -635.68
Iteration 61 took 2.36 seconds (mean sampled reward: -2552.10). Current reward after update: -643.80, Optimal reward -635.68
Iteration 62 took 2.41 seconds (mean sampled reward: -3636.64). Current reward after update: -618.17, Optimal reward -618.17
Iteration 63 took 2.43 seconds (mean sampled reward: -2740.96). Current reward after update: -1237.69, Optimal reward -618.17
Iteration 64 took 2.20 seconds (mean sampled reward: -2979.70). Current reward after update: -644.46, Optimal reward -618.17
Iteration 65 took 2.26 seconds (mean sampled reward: -2893.12). Current reward after update: -780.99, Optimal reward -618.17
Iteration 66 took 2.28 seconds (mean sampled reward: -2793.77). Current reward after update: -763.99, Optimal reward -618.17
Iteration 67 took 2.29 seconds (mean sampled reward: -2885.65). Current reward after update: -1303.86, Optimal reward -618.17
Iteration 68 took 2.32 seconds (mean sampled reward: -2998.59). Current reward after update: -780.00, Optimal reward -618.17
Iteration 69 took 2.33 seconds (mean sampled reward: -2348.69). Current reward after update: -779.03, Optimal reward -618.17
Iteration 70 took 2.26 seconds (mean sampled reward: -2560.41). Current reward after update: -733.38, Optimal reward -618.17
Iteration 71 took 2.28 seconds (mean sampled reward: -2705.75). Current reward after update: -723.88, Optimal reward -618.17
Iteration 72 took 2.23 seconds (mean sampled reward: -3061.91). Current reward after update: -750.91, Optimal reward -618.17
Iteration 73 took 2.21 seconds (mean sampled reward: -2368.70). Current reward after update: -1270.01, Optimal reward -618.17
Iteration 74 took 2.28 seconds (mean sampled reward: -2438.52). Current reward after update: -773.14, Optimal reward -618.17
Iteration 75 took 2.30 seconds (mean sampled reward: -3200.95). Current reward after update: -844.27, Optimal reward -618.17
Iteration 76 took 2.18 seconds (mean sampled reward: -2831.52). Current reward after update: -682.55, Optimal reward -618.17
Iteration 77 took 2.25 seconds (mean sampled reward: -3168.33). Current reward after update: -557.58, Optimal reward -557.58
Iteration 78 took 2.23 seconds (mean sampled reward: -2713.11). Current reward after update: -626.13, Optimal reward -557.58
Iteration 79 took 2.21 seconds (mean sampled reward: -2612.30). Current reward after update: -599.71, Optimal reward -557.58
Iteration 80 took 2.22 seconds (mean sampled reward: -2531.33). Current reward after update: -889.86, Optimal reward -557.58
Iteration 81 took 2.29 seconds (mean sampled reward: -2651.32). Current reward after update: -618.01, Optimal reward -557.58
Iteration 82 took 2.21 seconds (mean sampled reward: -2377.22). Current reward after update: -542.78, Optimal reward -542.78
Iteration 83 took 2.32 seconds (mean sampled reward: -2739.19). Current reward after update: -636.87, Optimal reward -542.78
Iteration 84 took 2.23 seconds (mean sampled reward: -2635.49). Current reward after update: -636.11, Optimal reward -542.78
Iteration 85 took 2.26 seconds (mean sampled reward: -2220.41). Current reward after update: -518.92, Optimal reward -518.92
Iteration 86 took 2.25 seconds (mean sampled reward: -2491.14). Current reward after update: -1040.36, Optimal reward -518.92
Iteration 87 took 2.28 seconds (mean sampled reward: -2225.41). Current reward after update: -655.35, Optimal reward -518.92
Iteration 88 took 2.24 seconds (mean sampled reward: -2292.98). Current reward after update: -594.71, Optimal reward -518.92
Iteration 89 took 2.37 seconds (mean sampled reward: -2082.60). Current reward after update: -528.75, Optimal reward -518.92
Iteration 90 took 2.27 seconds (mean sampled reward: -2557.19). Current reward after update: -609.89, Optimal reward -518.92
Iteration 91 took 2.20 seconds (mean sampled reward: -2973.03). Current reward after update: -625.69, Optimal reward -518.92
Iteration 92 took 2.21 seconds (mean sampled reward: -3114.93). Current reward after update: -736.50, Optimal reward -518.92
Iteration 93 took 2.31 seconds (mean sampled reward: -2620.48). Current reward after update: -2389.47, Optimal reward -518.92
Iteration 94 took 2.23 seconds (mean sampled reward: -2248.36). Current reward after update: -900.64, Optimal reward -518.92
Iteration 95 took 2.20 seconds (mean sampled reward: -1920.52). Current reward after update: -653.36, Optimal reward -518.92
Iteration 96 took 2.18 seconds (mean sampled reward: -2165.95). Current reward after update: -584.85, Optimal reward -518.92
Iteration 97 took 2.25 seconds (mean sampled reward: -2848.18). Current reward after update: -634.15, Optimal reward -518.92
Iteration 98 took 2.26 seconds (mean sampled reward: -2642.41). Current reward after update: -615.55, Optimal reward -518.92
Iteration 99 took 2.23 seconds (mean sampled reward: -2772.20). Current reward after update: -695.49, Optimal reward -518.92
Iteration 100 took 2.25 seconds (mean sampled reward: -2749.12). Current reward after update: -732.62, Optimal reward -518.92
Iteration 101 took 2.30 seconds (mean sampled reward: -2485.95). Current reward after update: -549.33, Optimal reward -518.92
Iteration 102 took 2.36 seconds (mean sampled reward: -2966.61). Current reward after update: -564.66, Optimal reward -518.92
Iteration 103 took 2.31 seconds (mean sampled reward: -2661.95). Current reward after update: -635.33, Optimal reward -518.92
Iteration 104 took 2.27 seconds (mean sampled reward: -3056.00). Current reward after update: -519.59, Optimal reward -518.92
Iteration 105 took 2.21 seconds (mean sampled reward: -3299.47). Current reward after update: -530.38, Optimal reward -518.92
Iteration 106 took 2.34 seconds (mean sampled reward: -3596.95). Current reward after update: -844.62, Optimal reward -518.92
Iteration 107 took 2.36 seconds (mean sampled reward: -3820.26). Current reward after update: -968.49, Optimal reward -518.92
Iteration 108 took 2.33 seconds (mean sampled reward: -3451.41). Current reward after update: -614.65, Optimal reward -518.92
Iteration 109 took 2.46 seconds (mean sampled reward: -3936.71). Current reward after update: -589.45, Optimal reward -518.92
Iteration 110 took 2.46 seconds (mean sampled reward: -3884.91). Current reward after update: -715.53, Optimal reward -518.92
Iteration 111 took 2.36 seconds (mean sampled reward: -3725.10). Current reward after update: -585.05, Optimal reward -518.92
Iteration 112 took 2.52 seconds (mean sampled reward: -3686.86). Current reward after update: -7077.69, Optimal reward -518.92
Iteration 113 took 2.31 seconds (mean sampled reward: -4284.54). Current reward after update: -567.18, Optimal reward -518.92
Iteration 114 took 2.40 seconds (mean sampled reward: -3728.60). Current reward after update: -613.71, Optimal reward -518.92
Iteration 115 took 2.37 seconds (mean sampled reward: -2997.15). Current reward after update: -515.33, Optimal reward -515.33
Iteration 116 took 2.33 seconds (mean sampled reward: -2898.86). Current reward after update: -1809.76, Optimal reward -515.33
Iteration 117 took 2.32 seconds (mean sampled reward: -3735.68). Current reward after update: -724.06, Optimal reward -515.33
Iteration 118 took 2.42 seconds (mean sampled reward: -3766.93). Current reward after update: -552.13, Optimal reward -515.33
Iteration 119 took 2.28 seconds (mean sampled reward: -4030.23). Current reward after update: -564.40, Optimal reward -515.33
Iteration 120 took 2.35 seconds (mean sampled reward: -4204.49). Current reward after update: -1834.22, Optimal reward -515.33
Iteration 121 took 2.30 seconds (mean sampled reward: -3485.03). Current reward after update: -558.11, Optimal reward -515.33
Iteration 122 took 2.31 seconds (mean sampled reward: -4303.78). Current reward after update: -636.29, Optimal reward -515.33
Iteration 123 took 2.34 seconds (mean sampled reward: -4007.18). Current reward after update: -7051.36, Optimal reward -515.33
Iteration 124 took 2.36 seconds (mean sampled reward: -4202.96). Current reward after update: -586.51, Optimal reward -515.33
Iteration 125 took 2.35 seconds (mean sampled reward: -4430.92). Current reward after update: -7035.76, Optimal reward -515.33
Iteration 126 took 2.28 seconds (mean sampled reward: -3580.14). Current reward after update: -398.69, Optimal reward -398.69
Iteration 127 took 2.27 seconds (mean sampled reward: -3086.10). Current reward after update: -344.87, Optimal reward -344.87
Iteration 128 took 2.29 seconds (mean sampled reward: -3364.93). Current reward after update: -943.46, Optimal reward -344.87
Iteration 129 took 2.29 seconds (mean sampled reward: -3287.04). Current reward after update: -436.04, Optimal reward -344.87
Iteration 130 took 2.27 seconds (mean sampled reward: -2939.63). Current reward after update: -450.22, Optimal reward -344.87
Iteration 131 took 2.31 seconds (mean sampled reward: -3603.28). Current reward after update: -372.43, Optimal reward -344.87
Iteration 132 took 2.35 seconds (mean sampled reward: -3502.92). Current reward after update: -502.74, Optimal reward -344.87
Iteration 133 took 2.25 seconds (mean sampled reward: -3626.85). Current reward after update: -396.98, Optimal reward -344.87
Iteration 134 took 2.27 seconds (mean sampled reward: -3908.88). Current reward after update: -485.21, Optimal reward -344.87
Iteration 135 took 2.30 seconds (mean sampled reward: -3911.57). Current reward after update: -561.00, Optimal reward -344.87
Iteration 136 took 2.43 seconds (mean sampled reward: -3843.89). Current reward after update: -409.56, Optimal reward -344.87
Iteration 137 took 2.28 seconds (mean sampled reward: -4073.96). Current reward after update: -402.33, Optimal reward -344.87
Iteration 138 took 2.42 seconds (mean sampled reward: -3767.89). Current reward after update: -426.07, Optimal reward -344.87
Iteration 139 took 2.23 seconds (mean sampled reward: -2898.64). Current reward after update: -1520.18, Optimal reward -344.87
Iteration 140 took 2.24 seconds (mean sampled reward: -3258.69). Current reward after update: -7046.61, Optimal reward -344.87
Iteration 141 took 2.32 seconds (mean sampled reward: -3761.84). Current reward after update: -332.31, Optimal reward -332.31
Iteration 142 took 2.34 seconds (mean sampled reward: -3403.93). Current reward after update: -7046.12, Optimal reward -332.31
Iteration 143 took 2.30 seconds (mean sampled reward: -3796.92). Current reward after update: -514.98, Optimal reward -332.31
Iteration 144 took 2.30 seconds (mean sampled reward: -3976.31). Current reward after update: -482.30, Optimal reward -332.31
Iteration 145 took 2.27 seconds (mean sampled reward: -3399.22). Current reward after update: -507.44, Optimal reward -332.31
Iteration 146 took 2.34 seconds (mean sampled reward: -4150.59). Current reward after update: -6975.04, Optimal reward -332.31
Iteration 147 took 2.28 seconds (mean sampled reward: -3499.41). Current reward after update: -430.11, Optimal reward -332.31
Iteration 148 took 2.26 seconds (mean sampled reward: -3271.13). Current reward after update: -785.92, Optimal reward -332.31
Iteration 149 took 2.23 seconds (mean sampled reward: -2958.45). Current reward after update: -684.86, Optimal reward -332.31
Iteration 150 took 2.26 seconds (mean sampled reward: -3242.51). Current reward after update: -7068.58, Optimal reward -332.31
Iteration 151 took 2.22 seconds (mean sampled reward: -2199.79). Current reward after update: -2201.54, Optimal reward -332.31
Iteration 152 took 2.22 seconds (mean sampled reward: -2016.56). Current reward after update: -1374.46, Optimal reward -332.31
Iteration 153 took 2.21 seconds (mean sampled reward: -1931.85). Current reward after update: -564.97, Optimal reward -332.31
Iteration 154 took 2.26 seconds (mean sampled reward: -2379.37). Current reward after update: -587.36, Optimal reward -332.31
Iteration 155 took 2.34 seconds (mean sampled reward: -3500.88). Current reward after update: -586.93, Optimal reward -332.31
Iteration 156 took 2.42 seconds (mean sampled reward: -5059.24). Current reward after update: -1378.41, Optimal reward -332.31
Iteration 157 took 2.29 seconds (mean sampled reward: -3990.31). Current reward after update: -634.28, Optimal reward -332.31
Iteration 158 took 2.22 seconds (mean sampled reward: -1974.38). Current reward after update: -493.62, Optimal reward -332.31
Iteration 159 took 2.28 seconds (mean sampled reward: -2259.22). Current reward after update: -6408.15, Optimal reward -332.31
Iteration 160 took 2.28 seconds (mean sampled reward: -3676.58). Current reward after update: -555.19, Optimal reward -332.31
Iteration 161 took 2.43 seconds (mean sampled reward: -5160.06). Current reward after update: -493.89, Optimal reward -332.31
Iteration 162 took 2.25 seconds (mean sampled reward: -2277.52). Current reward after update: -775.37, Optimal reward -332.31
Iteration 163 took 2.33 seconds (mean sampled reward: -3725.08). Current reward after update: -490.32, Optimal reward -332.31
Iteration 164 took 2.25 seconds (mean sampled reward: -2336.08). Current reward after update: -449.37, Optimal reward -332.31
Iteration 165 took 2.19 seconds (mean sampled reward: -1776.31). Current reward after update: -461.01, Optimal reward -332.31
Iteration 166 took 2.27 seconds (mean sampled reward: -3018.15). Current reward after update: -456.89, Optimal reward -332.31
Iteration 167 took 2.24 seconds (mean sampled reward: -2473.67). Current reward after update: -1082.81, Optimal reward -332.31
Iteration 168 took 2.19 seconds (mean sampled reward: -1836.98). Current reward after update: -642.47, Optimal reward -332.31
Iteration 169 took 2.24 seconds (mean sampled reward: -2799.70). Current reward after update: -471.26, Optimal reward -332.31
Iteration 170 took 2.25 seconds (mean sampled reward: -2587.19). Current reward after update: -385.97, Optimal reward -332.31
Iteration 171 took 2.38 seconds (mean sampled reward: -4383.44). Current reward after update: -377.70, Optimal reward -332.31
Iteration 172 took 2.31 seconds (mean sampled reward: -2716.86). Current reward after update: -1271.20, Optimal reward -332.31
Iteration 173 took 2.28 seconds (mean sampled reward: -3881.78). Current reward after update: -467.74, Optimal reward -332.31
Iteration 174 took 2.30 seconds (mean sampled reward: -3145.91). Current reward after update: -489.73, Optimal reward -332.31
Iteration 175 took 2.21 seconds (mean sampled reward: -2154.12). Current reward after update: -488.16, Optimal reward -332.31
Iteration 176 took 2.22 seconds (mean sampled reward: -1771.73). Current reward after update: -415.52, Optimal reward -332.31
Iteration 177 took 2.23 seconds (mean sampled reward: -1350.09). Current reward after update: -432.26, Optimal reward -332.31
Iteration 178 took 2.26 seconds (mean sampled reward: -1401.94). Current reward after update: -509.15, Optimal reward -332.31
Iteration 179 took 2.22 seconds (mean sampled reward: -1457.34). Current reward after update: -427.05, Optimal reward -332.31
Iteration 180 took 2.22 seconds (mean sampled reward: -1081.20). Current reward after update: -334.60, Optimal reward -332.31
Iteration 181 took 2.19 seconds (mean sampled reward: -1467.67). Current reward after update: -921.33, Optimal reward -332.31
Iteration 182 took 2.22 seconds (mean sampled reward: -1211.86). Current reward after update: -535.84, Optimal reward -332.31
Iteration 183 took 2.20 seconds (mean sampled reward: -1293.96). Current reward after update: -332.97, Optimal reward -332.31
Iteration 184 took 2.19 seconds (mean sampled reward: -1476.84). Current reward after update: -643.73, Optimal reward -332.31
Iteration 185 took 2.22 seconds (mean sampled reward: -1558.00). Current reward after update: -519.17, Optimal reward -332.31
Iteration 186 took 2.20 seconds (mean sampled reward: -1695.33). Current reward after update: -466.97, Optimal reward -332.31
Iteration 187 took 2.23 seconds (mean sampled reward: -1767.87). Current reward after update: -518.47, Optimal reward -332.31
Iteration 188 took 2.22 seconds (mean sampled reward: -1357.59). Current reward after update: -506.77, Optimal reward -332.31
Iteration 189 took 2.18 seconds (mean sampled reward: -1289.05). Current reward after update: -1185.45, Optimal reward -332.31
Iteration 190 took 2.21 seconds (mean sampled reward: -1328.87). Current reward after update: -1255.02, Optimal reward -332.31
Iteration 191 took 2.15 seconds (mean sampled reward: -1427.79). Current reward after update: -471.56, Optimal reward -332.31
Iteration 192 took 2.20 seconds (mean sampled reward: -1347.83). Current reward after update: -417.79, Optimal reward -332.31
Iteration 193 took 2.20 seconds (mean sampled reward: -1347.03). Current reward after update: -466.25, Optimal reward -332.31
Iteration 194 took 2.20 seconds (mean sampled reward: -1296.75). Current reward after update: -988.27, Optimal reward -332.31
Iteration 195 took 2.21 seconds (mean sampled reward: -1188.84). Current reward after update: -742.04, Optimal reward -332.31
Iteration 196 took 2.21 seconds (mean sampled reward: -1298.79). Current reward after update: -1797.51, Optimal reward -332.31
Iteration 197 took 2.26 seconds (mean sampled reward: -1233.12). Current reward after update: -597.86, Optimal reward -332.31
Iteration 198 took 2.31 seconds (mean sampled reward: -1456.23). Current reward after update: -410.29, Optimal reward -332.31
Iteration 199 took 2.29 seconds (mean sampled reward: -1466.02). Current reward after update: -399.64, Optimal reward -332.31
Iteration 200 took 2.29 seconds (mean sampled reward: -1716.83). Current reward after update: -514.92, Optimal reward -332.31
Max force: 40 Sigma: 0.8 mean rewards: -347.15311927258836, best rewards:-191.93255880908225

Iteration 1 took 2.40 seconds (mean sampled reward: -7624.46). Current reward after update: -7303.59, Optimal reward -7303.59
Iteration 2 took 2.32 seconds (mean sampled reward: -7636.19). Current reward after update: -7346.46, Optimal reward -7303.59
Iteration 3 took 2.30 seconds (mean sampled reward: -7637.28). Current reward after update: -7340.38, Optimal reward -7303.59
Iteration 4 took 2.40 seconds (mean sampled reward: -7607.49). Current reward after update: -7605.64, Optimal reward -7303.59
Iteration 5 took 2.67 seconds (mean sampled reward: -7605.12). Current reward after update: -7616.71, Optimal reward -7303.59
Iteration 6 took 2.50 seconds (mean sampled reward: -7609.56). Current reward after update: -7305.12, Optimal reward -7303.59
Iteration 7 took 2.61 seconds (mean sampled reward: -7587.31). Current reward after update: -7206.67, Optimal reward -7206.67
Iteration 8 took 2.86 seconds (mean sampled reward: -7605.90). Current reward after update: -7196.50, Optimal reward -7196.50
Iteration 9 took 2.66 seconds (mean sampled reward: -7500.38). Current reward after update: -6530.52, Optimal reward -6530.52
Iteration 10 took 2.81 seconds (mean sampled reward: -7425.28). Current reward after update: -6493.88, Optimal reward -6493.88
Iteration 11 took 2.66 seconds (mean sampled reward: -7530.95). Current reward after update: -6405.12, Optimal reward -6405.12
Iteration 12 took 2.39 seconds (mean sampled reward: -7244.91). Current reward after update: -6053.74, Optimal reward -6053.74
Iteration 13 took 2.34 seconds (mean sampled reward: -7132.22). Current reward after update: -5549.91, Optimal reward -5549.91
Iteration 14 took 2.37 seconds (mean sampled reward: -6950.81). Current reward after update: -4990.35, Optimal reward -4990.35
Iteration 15 took 2.32 seconds (mean sampled reward: -6480.57). Current reward after update: -4931.76, Optimal reward -4931.76
Iteration 16 took 2.39 seconds (mean sampled reward: -6718.80). Current reward after update: -4879.93, Optimal reward -4879.93
Iteration 17 took 2.24 seconds (mean sampled reward: -6196.22). Current reward after update: -4126.84, Optimal reward -4126.84
Iteration 18 took 2.33 seconds (mean sampled reward: -5805.00). Current reward after update: -4060.19, Optimal reward -4060.19
Iteration 19 took 2.40 seconds (mean sampled reward: -5203.90). Current reward after update: -4016.98, Optimal reward -4016.98
Iteration 20 took 2.34 seconds (mean sampled reward: -4869.12). Current reward after update: -3700.97, Optimal reward -3700.97
Iteration 21 took 2.63 seconds (mean sampled reward: -5241.32). Current reward after update: -3307.23, Optimal reward -3307.23
Iteration 22 took 2.65 seconds (mean sampled reward: -5302.35). Current reward after update: -3266.08, Optimal reward -3266.08
Iteration 23 took 2.28 seconds (mean sampled reward: -4889.66). Current reward after update: -3274.93, Optimal reward -3266.08
Iteration 24 took 2.30 seconds (mean sampled reward: -4333.11). Current reward after update: -3208.34, Optimal reward -3208.34
Iteration 25 took 2.54 seconds (mean sampled reward: -4766.73). Current reward after update: -2875.91, Optimal reward -2875.91
Iteration 26 took 2.51 seconds (mean sampled reward: -4121.93). Current reward after update: -3165.90, Optimal reward -2875.91
Iteration 27 took 2.31 seconds (mean sampled reward: -5003.78). Current reward after update: -3119.16, Optimal reward -2875.91
Iteration 28 took 2.38 seconds (mean sampled reward: -5695.69). Current reward after update: -3118.90, Optimal reward -2875.91
Iteration 29 took 2.47 seconds (mean sampled reward: -5986.49). Current reward after update: -3050.26, Optimal reward -2875.91
Iteration 30 took 2.42 seconds (mean sampled reward: -6092.71). Current reward after update: -3114.63, Optimal reward -2875.91
Iteration 31 took 2.25 seconds (mean sampled reward: -6002.24). Current reward after update: -3091.69, Optimal reward -2875.91
Iteration 32 took 2.26 seconds (mean sampled reward: -5222.65). Current reward after update: -2935.51, Optimal reward -2875.91
Iteration 33 took 2.41 seconds (mean sampled reward: -5381.00). Current reward after update: -2703.60, Optimal reward -2703.60
Iteration 34 took 2.22 seconds (mean sampled reward: -5677.26). Current reward after update: -2966.52, Optimal reward -2703.60
Iteration 35 took 2.42 seconds (mean sampled reward: -5253.41). Current reward after update: -2843.09, Optimal reward -2703.60
Iteration 36 took 2.23 seconds (mean sampled reward: -6311.13). Current reward after update: -2692.21, Optimal reward -2692.21
Iteration 37 took 2.21 seconds (mean sampled reward: -5939.00). Current reward after update: -2584.87, Optimal reward -2584.87
Iteration 38 took 2.46 seconds (mean sampled reward: -5460.42). Current reward after update: -2496.68, Optimal reward -2496.68
Iteration 39 took 2.40 seconds (mean sampled reward: -5463.49). Current reward after update: -2593.69, Optimal reward -2496.68
Iteration 40 took 2.37 seconds (mean sampled reward: -5554.82). Current reward after update: -2480.58, Optimal reward -2480.58
Iteration 41 took 2.40 seconds (mean sampled reward: -5017.06). Current reward after update: -2526.46, Optimal reward -2480.58
Iteration 42 took 2.33 seconds (mean sampled reward: -4390.80). Current reward after update: -2333.32, Optimal reward -2333.32
Iteration 43 took 2.34 seconds (mean sampled reward: -4459.58). Current reward after update: -2272.02, Optimal reward -2272.02
Iteration 44 took 2.34 seconds (mean sampled reward: -4076.25). Current reward after update: -2146.18, Optimal reward -2146.18
Iteration 45 took 2.35 seconds (mean sampled reward: -4431.07). Current reward after update: -2339.50, Optimal reward -2146.18
Iteration 46 took 2.27 seconds (mean sampled reward: -4462.20). Current reward after update: -2974.90, Optimal reward -2146.18
Iteration 47 took 2.24 seconds (mean sampled reward: -4310.46). Current reward after update: -2246.36, Optimal reward -2146.18
Iteration 48 took 2.26 seconds (mean sampled reward: -3739.80). Current reward after update: -2514.37, Optimal reward -2146.18
Iteration 49 took 2.25 seconds (mean sampled reward: -3855.33). Current reward after update: -2383.10, Optimal reward -2146.18
Iteration 50 took 2.24 seconds (mean sampled reward: -4026.99). Current reward after update: -2331.91, Optimal reward -2146.18
Iteration 51 took 2.26 seconds (mean sampled reward: -3986.72). Current reward after update: -2292.29, Optimal reward -2146.18
Iteration 52 took 2.36 seconds (mean sampled reward: -4325.05). Current reward after update: -2217.84, Optimal reward -2146.18
Iteration 53 took 2.23 seconds (mean sampled reward: -4696.56). Current reward after update: -2284.39, Optimal reward -2146.18
Iteration 54 took 2.51 seconds (mean sampled reward: -4468.40). Current reward after update: -2264.23, Optimal reward -2146.18
Iteration 55 took 2.35 seconds (mean sampled reward: -5061.36). Current reward after update: -3572.55, Optimal reward -2146.18
Iteration 56 took 2.46 seconds (mean sampled reward: -5527.81). Current reward after update: -2217.50, Optimal reward -2146.18
Iteration 57 took 2.46 seconds (mean sampled reward: -4994.60). Current reward after update: -2201.35, Optimal reward -2146.18
Iteration 58 took 2.39 seconds (mean sampled reward: -5557.01). Current reward after update: -2048.55, Optimal reward -2048.55
Iteration 59 took 2.49 seconds (mean sampled reward: -4239.98). Current reward after update: -3021.71, Optimal reward -2048.55
Iteration 60 took 2.29 seconds (mean sampled reward: -3920.73). Current reward after update: -1930.80, Optimal reward -1930.80
Iteration 61 took 2.36 seconds (mean sampled reward: -3058.18). Current reward after update: -1796.50, Optimal reward -1796.50
Iteration 62 took 2.31 seconds (mean sampled reward: -3581.28). Current reward after update: -1877.60, Optimal reward -1796.50
Iteration 63 took 2.15 seconds (mean sampled reward: -4000.65). Current reward after update: -1769.82, Optimal reward -1769.82
Iteration 64 took 2.31 seconds (mean sampled reward: -3906.20). Current reward after update: -1739.11, Optimal reward -1739.11
Iteration 65 took 2.24 seconds (mean sampled reward: -3515.80). Current reward after update: -1783.25, Optimal reward -1739.11
Iteration 66 took 2.25 seconds (mean sampled reward: -3084.60). Current reward after update: -1640.13, Optimal reward -1640.13
Iteration 67 took 2.22 seconds (mean sampled reward: -3490.69). Current reward after update: -1208.05, Optimal reward -1208.05
Iteration 68 took 2.27 seconds (mean sampled reward: -5456.00). Current reward after update: -1376.19, Optimal reward -1208.05
Iteration 69 took 2.28 seconds (mean sampled reward: -5256.54). Current reward after update: -1187.58, Optimal reward -1187.58
Iteration 70 took 2.24 seconds (mean sampled reward: -5138.32). Current reward after update: -1336.05, Optimal reward -1187.58
Iteration 71 took 2.21 seconds (mean sampled reward: -3971.68). Current reward after update: -1287.94, Optimal reward -1187.58
Iteration 72 took 2.23 seconds (mean sampled reward: -4084.86). Current reward after update: -2042.97, Optimal reward -1187.58
Iteration 73 took 2.15 seconds (mean sampled reward: -2361.34). Current reward after update: -1527.80, Optimal reward -1187.58
Iteration 74 took 2.14 seconds (mean sampled reward: -2207.01). Current reward after update: -1190.15, Optimal reward -1187.58
Iteration 75 took 2.18 seconds (mean sampled reward: -2222.99). Current reward after update: -1078.33, Optimal reward -1078.33
Iteration 76 took 2.17 seconds (mean sampled reward: -2380.23). Current reward after update: -1063.83, Optimal reward -1063.83
Iteration 77 took 2.18 seconds (mean sampled reward: -2116.46). Current reward after update: -1147.64, Optimal reward -1063.83
Iteration 78 took 2.29 seconds (mean sampled reward: -3629.22). Current reward after update: -1026.13, Optimal reward -1026.13
Iteration 79 took 2.20 seconds (mean sampled reward: -4061.15). Current reward after update: -981.42, Optimal reward -981.42
Iteration 80 took 2.19 seconds (mean sampled reward: -4922.73). Current reward after update: -1044.16, Optimal reward -981.42
Iteration 81 took 2.24 seconds (mean sampled reward: -4513.26). Current reward after update: -996.55, Optimal reward -981.42
Iteration 82 took 2.15 seconds (mean sampled reward: -4406.82). Current reward after update: -979.43, Optimal reward -979.43
Iteration 83 took 2.14 seconds (mean sampled reward: -2913.39). Current reward after update: -945.48, Optimal reward -945.48
Iteration 84 took 2.11 seconds (mean sampled reward: -2174.87). Current reward after update: -843.92, Optimal reward -843.92
Iteration 85 took 2.20 seconds (mean sampled reward: -2585.47). Current reward after update: -895.31, Optimal reward -843.92
Iteration 86 took 2.13 seconds (mean sampled reward: -3941.43). Current reward after update: -868.83, Optimal reward -843.92
Iteration 87 took 2.22 seconds (mean sampled reward: -4735.42). Current reward after update: -1053.82, Optimal reward -843.92
Iteration 88 took 2.23 seconds (mean sampled reward: -2990.17). Current reward after update: -1100.94, Optimal reward -843.92
Iteration 89 took 2.12 seconds (mean sampled reward: -2481.65). Current reward after update: -788.36, Optimal reward -788.36
Iteration 90 took 2.24 seconds (mean sampled reward: -2897.28). Current reward after update: -954.12, Optimal reward -788.36
Iteration 91 took 2.23 seconds (mean sampled reward: -4912.72). Current reward after update: -829.83, Optimal reward -788.36
Iteration 92 took 2.25 seconds (mean sampled reward: -3362.12). Current reward after update: -866.97, Optimal reward -788.36
Iteration 93 took 2.19 seconds (mean sampled reward: -2657.35). Current reward after update: -824.04, Optimal reward -788.36
Iteration 94 took 2.21 seconds (mean sampled reward: -3941.89). Current reward after update: -868.30, Optimal reward -788.36
Iteration 95 took 2.25 seconds (mean sampled reward: -2307.95). Current reward after update: -1274.31, Optimal reward -788.36
Iteration 96 took 2.22 seconds (mean sampled reward: -3789.57). Current reward after update: -785.10, Optimal reward -785.10
Iteration 97 took 2.20 seconds (mean sampled reward: -5336.70). Current reward after update: -900.66, Optimal reward -785.10
Iteration 98 took 2.20 seconds (mean sampled reward: -3418.22). Current reward after update: -678.21, Optimal reward -678.21
Iteration 99 took 2.20 seconds (mean sampled reward: -2649.25). Current reward after update: -750.89, Optimal reward -678.21
Iteration 100 took 2.17 seconds (mean sampled reward: -2365.45). Current reward after update: -667.23, Optimal reward -667.23
Iteration 101 took 2.18 seconds (mean sampled reward: -2722.49). Current reward after update: -680.56, Optimal reward -667.23
Iteration 102 took 2.20 seconds (mean sampled reward: -2062.93). Current reward after update: -593.88, Optimal reward -593.88
Iteration 103 took 2.18 seconds (mean sampled reward: -2069.10). Current reward after update: -826.56, Optimal reward -593.88
Iteration 104 took 2.19 seconds (mean sampled reward: -3338.24). Current reward after update: -601.16, Optimal reward -593.88
Iteration 105 took 2.14 seconds (mean sampled reward: -1556.08). Current reward after update: -929.38, Optimal reward -593.88
Iteration 106 took 2.26 seconds (mean sampled reward: -1707.07). Current reward after update: -496.32, Optimal reward -496.32
Iteration 107 took 2.11 seconds (mean sampled reward: -1812.62). Current reward after update: -541.18, Optimal reward -496.32
Iteration 108 took 2.29 seconds (mean sampled reward: -2064.77). Current reward after update: -534.79, Optimal reward -496.32
Iteration 109 took 2.21 seconds (mean sampled reward: -2203.93). Current reward after update: -513.31, Optimal reward -496.32
Iteration 110 took 2.27 seconds (mean sampled reward: -1678.03). Current reward after update: -480.67, Optimal reward -480.67
Iteration 111 took 2.25 seconds (mean sampled reward: -2098.20). Current reward after update: -582.57, Optimal reward -480.67
Iteration 112 took 2.24 seconds (mean sampled reward: -3748.37). Current reward after update: -920.38, Optimal reward -480.67
Iteration 113 took 2.16 seconds (mean sampled reward: -3569.00). Current reward after update: -525.52, Optimal reward -480.67
Iteration 114 took 2.21 seconds (mean sampled reward: -1560.34). Current reward after update: -546.49, Optimal reward -480.67
Iteration 115 took 2.14 seconds (mean sampled reward: -2338.13). Current reward after update: -530.87, Optimal reward -480.67
Iteration 116 took 2.20 seconds (mean sampled reward: -2140.32). Current reward after update: -598.44, Optimal reward -480.67
Iteration 117 took 2.27 seconds (mean sampled reward: -1938.76). Current reward after update: -571.40, Optimal reward -480.67
Iteration 118 took 2.23 seconds (mean sampled reward: -1780.19). Current reward after update: -480.36, Optimal reward -480.36
Iteration 119 took 2.25 seconds (mean sampled reward: -2029.28). Current reward after update: -423.63, Optimal reward -423.63
Iteration 120 took 2.16 seconds (mean sampled reward: -1483.85). Current reward after update: -1055.22, Optimal reward -423.63
Iteration 121 took 2.18 seconds (mean sampled reward: -1450.57). Current reward after update: -774.82, Optimal reward -423.63
Iteration 122 took 2.18 seconds (mean sampled reward: -1432.30). Current reward after update: -482.44, Optimal reward -423.63
Iteration 123 took 2.16 seconds (mean sampled reward: -1616.55). Current reward after update: -824.46, Optimal reward -423.63
Iteration 124 took 2.18 seconds (mean sampled reward: -1829.97). Current reward after update: -487.08, Optimal reward -423.63
Iteration 125 took 2.17 seconds (mean sampled reward: -1648.98). Current reward after update: -1429.66, Optimal reward -423.63
Iteration 126 took 2.17 seconds (mean sampled reward: -1956.84). Current reward after update: -478.03, Optimal reward -423.63
Iteration 127 took 2.19 seconds (mean sampled reward: -3358.76). Current reward after update: -501.24, Optimal reward -423.63
Iteration 128 took 2.15 seconds (mean sampled reward: -1934.94). Current reward after update: -1487.92, Optimal reward -423.63
Iteration 129 took 2.19 seconds (mean sampled reward: -4746.88). Current reward after update: -509.96, Optimal reward -423.63
Iteration 130 took 2.19 seconds (mean sampled reward: -4755.17). Current reward after update: -1822.90, Optimal reward -423.63
Iteration 131 took 2.17 seconds (mean sampled reward: -2708.67). Current reward after update: -2103.21, Optimal reward -423.63
Iteration 132 took 2.21 seconds (mean sampled reward: -2358.64). Current reward after update: -2359.66, Optimal reward -423.63
Iteration 133 took 2.19 seconds (mean sampled reward: -2769.55). Current reward after update: -619.69, Optimal reward -423.63
Iteration 134 took 2.17 seconds (mean sampled reward: -3359.50). Current reward after update: -587.30, Optimal reward -423.63
Iteration 135 took 2.20 seconds (mean sampled reward: -2520.16). Current reward after update: -1456.74, Optimal reward -423.63
Iteration 136 took 2.16 seconds (mean sampled reward: -2300.92). Current reward after update: -511.85, Optimal reward -423.63
Iteration 137 took 2.17 seconds (mean sampled reward: -2319.75). Current reward after update: -504.38, Optimal reward -423.63
Iteration 138 took 2.17 seconds (mean sampled reward: -2543.26). Current reward after update: -550.87, Optimal reward -423.63
Iteration 139 took 2.17 seconds (mean sampled reward: -3751.80). Current reward after update: -738.38, Optimal reward -423.63
Iteration 140 took 2.24 seconds (mean sampled reward: -1875.95). Current reward after update: -602.97, Optimal reward -423.63
Iteration 141 took 2.26 seconds (mean sampled reward: -2326.32). Current reward after update: -619.04, Optimal reward -423.63
Iteration 142 took 2.19 seconds (mean sampled reward: -3417.21). Current reward after update: -1625.15, Optimal reward -423.63
Iteration 143 took 2.19 seconds (mean sampled reward: -3699.49). Current reward after update: -476.30, Optimal reward -423.63
Iteration 144 took 2.14 seconds (mean sampled reward: -2331.69). Current reward after update: -1771.18, Optimal reward -423.63
Iteration 145 took 2.16 seconds (mean sampled reward: -1959.91). Current reward after update: -637.55, Optimal reward -423.63
Iteration 146 took 2.15 seconds (mean sampled reward: -2282.63). Current reward after update: -522.64, Optimal reward -423.63
Iteration 147 took 2.32 seconds (mean sampled reward: -2261.58). Current reward after update: -480.50, Optimal reward -423.63
Iteration 148 took 2.11 seconds (mean sampled reward: -2000.49). Current reward after update: -469.56, Optimal reward -423.63
Iteration 149 took 2.15 seconds (mean sampled reward: -4278.50). Current reward after update: -458.93, Optimal reward -423.63
Iteration 150 took 2.12 seconds (mean sampled reward: -2077.16). Current reward after update: -474.08, Optimal reward -423.63
Iteration 151 took 2.14 seconds (mean sampled reward: -1889.79). Current reward after update: -503.29, Optimal reward -423.63
Iteration 152 took 2.18 seconds (mean sampled reward: -1977.47). Current reward after update: -564.52, Optimal reward -423.63
Iteration 153 took 2.19 seconds (mean sampled reward: -4604.48). Current reward after update: -589.66, Optimal reward -423.63
Iteration 154 took 2.17 seconds (mean sampled reward: -1954.59). Current reward after update: -646.29, Optimal reward -423.63
Iteration 155 took 2.15 seconds (mean sampled reward: -2145.59). Current reward after update: -475.24, Optimal reward -423.63
Iteration 156 took 2.21 seconds (mean sampled reward: -2176.57). Current reward after update: -462.79, Optimal reward -423.63
Iteration 157 took 2.16 seconds (mean sampled reward: -2628.80). Current reward after update: -1955.20, Optimal reward -423.63
Iteration 158 took 2.26 seconds (mean sampled reward: -2101.70). Current reward after update: -510.63, Optimal reward -423.63
Iteration 159 took 2.14 seconds (mean sampled reward: -2087.69). Current reward after update: -933.52, Optimal reward -423.63
Iteration 160 took 2.18 seconds (mean sampled reward: -2460.62). Current reward after update: -571.05, Optimal reward -423.63
Iteration 161 took 2.15 seconds (mean sampled reward: -2358.18). Current reward after update: -763.23, Optimal reward -423.63
Iteration 162 took 2.17 seconds (mean sampled reward: -2285.37). Current reward after update: -520.04, Optimal reward -423.63
Iteration 163 took 2.15 seconds (mean sampled reward: -2077.57). Current reward after update: -556.78, Optimal reward -423.63
Iteration 164 took 2.13 seconds (mean sampled reward: -2216.07). Current reward after update: -1516.24, Optimal reward -423.63
Iteration 165 took 2.27 seconds (mean sampled reward: -2223.67). Current reward after update: -540.50, Optimal reward -423.63
Iteration 166 took 2.17 seconds (mean sampled reward: -2450.73). Current reward after update: -561.91, Optimal reward -423.63
Iteration 167 took 2.22 seconds (mean sampled reward: -2062.41). Current reward after update: -502.35, Optimal reward -423.63
Iteration 168 took 2.17 seconds (mean sampled reward: -2110.98). Current reward after update: -1394.30, Optimal reward -423.63
Iteration 169 took 2.13 seconds (mean sampled reward: -2430.27). Current reward after update: -1130.30, Optimal reward -423.63
Iteration 170 took 2.19 seconds (mean sampled reward: -1879.56). Current reward after update: -1705.83, Optimal reward -423.63
Iteration 171 took 2.18 seconds (mean sampled reward: -2688.83). Current reward after update: -1176.36, Optimal reward -423.63
Iteration 172 took 2.16 seconds (mean sampled reward: -3074.76). Current reward after update: -535.00, Optimal reward -423.63
Iteration 173 took 2.15 seconds (mean sampled reward: -2705.91). Current reward after update: -681.61, Optimal reward -423.63
Iteration 174 took 2.16 seconds (mean sampled reward: -2542.95). Current reward after update: -773.79, Optimal reward -423.63
Iteration 175 took 2.20 seconds (mean sampled reward: -3491.59). Current reward after update: -418.84, Optimal reward -418.84
Iteration 176 took 2.24 seconds (mean sampled reward: -2667.68). Current reward after update: -1223.01, Optimal reward -418.84
Iteration 177 took 2.27 seconds (mean sampled reward: -1884.05). Current reward after update: -485.06, Optimal reward -418.84
Iteration 178 took 2.18 seconds (mean sampled reward: -2368.50). Current reward after update: -1134.74, Optimal reward -418.84
Iteration 179 took 2.28 seconds (mean sampled reward: -3424.30). Current reward after update: -1541.74, Optimal reward -418.84
Iteration 180 took 2.26 seconds (mean sampled reward: -3141.12). Current reward after update: -589.38, Optimal reward -418.84
Iteration 181 took 2.27 seconds (mean sampled reward: -4622.39). Current reward after update: -523.52, Optimal reward -418.84
Iteration 182 took 2.24 seconds (mean sampled reward: -3899.70). Current reward after update: -712.88, Optimal reward -418.84
Iteration 183 took 2.19 seconds (mean sampled reward: -3641.03). Current reward after update: -547.50, Optimal reward -418.84
Iteration 184 took 2.09 seconds (mean sampled reward: -3242.23). Current reward after update: -677.97, Optimal reward -418.84
Iteration 185 took 2.13 seconds (mean sampled reward: -3544.47). Current reward after update: -739.30, Optimal reward -418.84
Iteration 186 took 2.21 seconds (mean sampled reward: -4199.94). Current reward after update: -625.07, Optimal reward -418.84
Iteration 187 took 2.13 seconds (mean sampled reward: -3923.62). Current reward after update: -1871.68, Optimal reward -418.84
Iteration 188 took 2.15 seconds (mean sampled reward: -2198.10). Current reward after update: -610.29, Optimal reward -418.84
Iteration 189 took 2.17 seconds (mean sampled reward: -2218.02). Current reward after update: -573.39, Optimal reward -418.84
Iteration 190 took 2.18 seconds (mean sampled reward: -2119.25). Current reward after update: -3114.57, Optimal reward -418.84
Iteration 191 took 2.20 seconds (mean sampled reward: -1975.09). Current reward after update: -562.44, Optimal reward -418.84
Iteration 192 took 2.17 seconds (mean sampled reward: -2416.82). Current reward after update: -1510.67, Optimal reward -418.84
Iteration 193 took 2.18 seconds (mean sampled reward: -2456.83). Current reward after update: -532.00, Optimal reward -418.84
Iteration 194 took 2.16 seconds (mean sampled reward: -2675.72). Current reward after update: -577.54, Optimal reward -418.84
Iteration 195 took 2.14 seconds (mean sampled reward: -2838.39). Current reward after update: -693.08, Optimal reward -418.84
Iteration 196 took 2.11 seconds (mean sampled reward: -2374.85). Current reward after update: -569.49, Optimal reward -418.84
Iteration 197 took 2.15 seconds (mean sampled reward: -2258.69). Current reward after update: -1694.87, Optimal reward -418.84
Iteration 198 took 2.20 seconds (mean sampled reward: -2498.88). Current reward after update: -859.21, Optimal reward -418.84
Iteration 199 took 2.18 seconds (mean sampled reward: -1794.74). Current reward after update: -1276.10, Optimal reward -418.84
Iteration 200 took 2.15 seconds (mean sampled reward: -3170.96). Current reward after update: -565.88, Optimal reward -418.84
Iteration 1 took 2.40 seconds (mean sampled reward: -7619.86). Current reward after update: -7414.52, Optimal reward -7414.52
Iteration 2 took 2.37 seconds (mean sampled reward: -7616.39). Current reward after update: -7492.20, Optimal reward -7414.52
Iteration 3 took 2.38 seconds (mean sampled reward: -7603.70). Current reward after update: -7307.18, Optimal reward -7307.18
Iteration 4 took 2.38 seconds (mean sampled reward: -7591.66). Current reward after update: -7568.06, Optimal reward -7307.18
Iteration 5 took 2.49 seconds (mean sampled reward: -7581.27). Current reward after update: -7321.52, Optimal reward -7307.18
Iteration 6 took 2.61 seconds (mean sampled reward: -7570.96). Current reward after update: -7289.01, Optimal reward -7289.01
Iteration 7 took 2.30 seconds (mean sampled reward: -7585.83). Current reward after update: -7270.67, Optimal reward -7270.67
Iteration 8 took 2.25 seconds (mean sampled reward: -7596.36). Current reward after update: -7253.25, Optimal reward -7253.25
Iteration 9 took 2.68 seconds (mean sampled reward: -7619.92). Current reward after update: -7423.84, Optimal reward -7253.25
Iteration 10 took 2.38 seconds (mean sampled reward: -7603.60). Current reward after update: -7324.81, Optimal reward -7253.25
Iteration 11 took 2.22 seconds (mean sampled reward: -7616.02). Current reward after update: -7666.20, Optimal reward -7253.25
Iteration 12 took 2.35 seconds (mean sampled reward: -7661.28). Current reward after update: -7330.87, Optimal reward -7253.25
Iteration 13 took 2.32 seconds (mean sampled reward: -7640.43). Current reward after update: -7313.15, Optimal reward -7253.25
Iteration 14 took 2.41 seconds (mean sampled reward: -7616.82). Current reward after update: -7323.02, Optimal reward -7253.25
Iteration 15 took 2.48 seconds (mean sampled reward: -7645.08). Current reward after update: -7408.94, Optimal reward -7253.25
Iteration 16 took 2.42 seconds (mean sampled reward: -7653.29). Current reward after update: -7368.61, Optimal reward -7253.25
Iteration 17 took 2.25 seconds (mean sampled reward: -7627.13). Current reward after update: -7377.16, Optimal reward -7253.25
Iteration 18 took 2.36 seconds (mean sampled reward: -7637.04). Current reward after update: -7390.73, Optimal reward -7253.25
Iteration 19 took 2.30 seconds (mean sampled reward: -7635.67). Current reward after update: -7357.22, Optimal reward -7253.25
Iteration 20 took 2.32 seconds (mean sampled reward: -7616.16). Current reward after update: -7330.42, Optimal reward -7253.25
Iteration 21 took 2.29 seconds (mean sampled reward: -7607.00). Current reward after update: -7306.04, Optimal reward -7253.25
Iteration 22 took 2.32 seconds (mean sampled reward: -7605.82). Current reward after update: -7287.27, Optimal reward -7253.25
Iteration 23 took 2.31 seconds (mean sampled reward: -7603.17). Current reward after update: -7326.69, Optimal reward -7253.25
Iteration 24 took 2.27 seconds (mean sampled reward: -7612.34). Current reward after update: -7229.82, Optimal reward -7229.82
Iteration 25 took 2.28 seconds (mean sampled reward: -7622.65). Current reward after update: -7594.99, Optimal reward -7229.82
Iteration 26 took 2.34 seconds (mean sampled reward: -7610.42). Current reward after update: -7329.12, Optimal reward -7229.82
Iteration 27 took 2.54 seconds (mean sampled reward: -7615.43). Current reward after update: -7356.54, Optimal reward -7229.82
Iteration 28 took 2.55 seconds (mean sampled reward: -7594.78). Current reward after update: -7234.92, Optimal reward -7229.82
Iteration 29 took 2.36 seconds (mean sampled reward: -7594.38). Current reward after update: -7286.23, Optimal reward -7229.82
Iteration 30 took 2.56 seconds (mean sampled reward: -7602.92). Current reward after update: -7348.85, Optimal reward -7229.82
Iteration 31 took 2.46 seconds (mean sampled reward: -7587.83). Current reward after update: -6992.93, Optimal reward -6992.93
Iteration 32 took 2.70 seconds (mean sampled reward: -7533.57). Current reward after update: -6722.86, Optimal reward -6722.86
Iteration 33 took 2.45 seconds (mean sampled reward: -7543.15). Current reward after update: -7488.11, Optimal reward -6722.86
Iteration 34 took 2.48 seconds (mean sampled reward: -7528.19). Current reward after update: -7488.48, Optimal reward -6722.86
Iteration 35 took 2.53 seconds (mean sampled reward: -7498.41). Current reward after update: -6813.74, Optimal reward -6722.86
Iteration 36 took 2.57 seconds (mean sampled reward: -7452.09). Current reward after update: -6602.29, Optimal reward -6602.29
Iteration 37 took 2.55 seconds (mean sampled reward: -7439.57). Current reward after update: -6593.79, Optimal reward -6593.79
Iteration 38 took 2.38 seconds (mean sampled reward: -7522.42). Current reward after update: -6597.42, Optimal reward -6593.79
Iteration 39 took 2.44 seconds (mean sampled reward: -7473.58). Current reward after update: -6588.79, Optimal reward -6588.79
Iteration 40 took 2.42 seconds (mean sampled reward: -7385.49). Current reward after update: -6569.86, Optimal reward -6569.86
Iteration 41 took 2.41 seconds (mean sampled reward: -7007.06). Current reward after update: -6562.54, Optimal reward -6562.54
Iteration 42 took 2.40 seconds (mean sampled reward: -7209.14). Current reward after update: -6488.96, Optimal reward -6488.96
Iteration 43 took 2.36 seconds (mean sampled reward: -7263.91). Current reward after update: -6495.57, Optimal reward -6488.96
Iteration 44 took 2.40 seconds (mean sampled reward: -7242.41). Current reward after update: -6509.92, Optimal reward -6488.96
Iteration 45 took 2.35 seconds (mean sampled reward: -7398.57). Current reward after update: -6493.39, Optimal reward -6488.96
Iteration 46 took 2.42 seconds (mean sampled reward: -7338.45). Current reward after update: -7263.12, Optimal reward -6488.96
Iteration 47 took 2.46 seconds (mean sampled reward: -7477.83). Current reward after update: -6536.81, Optimal reward -6488.96
Iteration 48 took 2.49 seconds (mean sampled reward: -7477.99). Current reward after update: -6557.33, Optimal reward -6488.96
Iteration 49 took 2.38 seconds (mean sampled reward: -7404.53). Current reward after update: -6450.95, Optimal reward -6450.95
Iteration 50 took 2.43 seconds (mean sampled reward: -7483.10). Current reward after update: -6519.77, Optimal reward -6450.95
Iteration 51 took 2.45 seconds (mean sampled reward: -7458.61). Current reward after update: -6599.13, Optimal reward -6450.95
Iteration 52 took 2.56 seconds (mean sampled reward: -7463.25). Current reward after update: -6521.26, Optimal reward -6450.95
Iteration 53 took 2.65 seconds (mean sampled reward: -7430.79). Current reward after update: -6503.17, Optimal reward -6450.95
Iteration 54 took 2.71 seconds (mean sampled reward: -7054.20). Current reward after update: -7019.41, Optimal reward -6450.95
Iteration 55 took 2.39 seconds (mean sampled reward: -7152.28). Current reward after update: -6500.58, Optimal reward -6450.95
Iteration 56 took 2.48 seconds (mean sampled reward: -7028.15). Current reward after update: -6500.29, Optimal reward -6450.95
Iteration 57 took 2.54 seconds (mean sampled reward: -7069.98). Current reward after update: -7436.99, Optimal reward -6450.95
Iteration 58 took 2.52 seconds (mean sampled reward: -7248.71). Current reward after update: -7500.68, Optimal reward -6450.95
Iteration 59 took 2.44 seconds (mean sampled reward: -7261.24). Current reward after update: -7503.08, Optimal reward -6450.95
Iteration 60 took 2.39 seconds (mean sampled reward: -7052.86). Current reward after update: -6155.66, Optimal reward -6155.66
Iteration 61 took 2.43 seconds (mean sampled reward: -6942.88). Current reward after update: -6103.39, Optimal reward -6103.39
Iteration 62 took 2.37 seconds (mean sampled reward: -6850.93). Current reward after update: -6986.06, Optimal reward -6103.39
Iteration 63 took 2.39 seconds (mean sampled reward: -6972.04). Current reward after update: -5955.66, Optimal reward -5955.66
Iteration 64 took 2.32 seconds (mean sampled reward: -6924.83). Current reward after update: -6022.50, Optimal reward -5955.66
Iteration 65 took 2.35 seconds (mean sampled reward: -6618.95). Current reward after update: -5875.17, Optimal reward -5875.17
Iteration 66 took 2.34 seconds (mean sampled reward: -6486.51). Current reward after update: -5945.33, Optimal reward -5875.17
Iteration 67 took 2.74 seconds (mean sampled reward: -6499.01). Current reward after update: -5887.82, Optimal reward -5875.17
Iteration 68 took 2.98 seconds (mean sampled reward: -6624.30). Current reward after update: -6082.36, Optimal reward -5875.17
Iteration 69 took 2.33 seconds (mean sampled reward: -6490.82). Current reward after update: -5901.96, Optimal reward -5875.17
Iteration 70 took 2.35 seconds (mean sampled reward: -6482.04). Current reward after update: -5926.33, Optimal reward -5875.17
Iteration 71 took 2.33 seconds (mean sampled reward: -6453.72). Current reward after update: -5824.04, Optimal reward -5824.04
Iteration 72 took 2.33 seconds (mean sampled reward: -6450.90). Current reward after update: -5791.18, Optimal reward -5791.18
Iteration 73 took 2.36 seconds (mean sampled reward: -6683.72). Current reward after update: -5731.34, Optimal reward -5731.34
Iteration 74 took 2.34 seconds (mean sampled reward: -6680.13). Current reward after update: -5654.52, Optimal reward -5654.52
Iteration 75 took 2.43 seconds (mean sampled reward: -6563.38). Current reward after update: -5524.50, Optimal reward -5524.50
Iteration 76 took 2.43 seconds (mean sampled reward: -6532.64). Current reward after update: -5512.78, Optimal reward -5512.78
Iteration 77 took 2.47 seconds (mean sampled reward: -6420.23). Current reward after update: -5441.78, Optimal reward -5441.78
Iteration 78 took 2.45 seconds (mean sampled reward: -6311.06). Current reward after update: -5407.94, Optimal reward -5407.94
Iteration 79 took 2.47 seconds (mean sampled reward: -6076.29). Current reward after update: -5322.47, Optimal reward -5322.47
Iteration 80 took 2.49 seconds (mean sampled reward: -5897.72). Current reward after update: -5515.09, Optimal reward -5322.47
Iteration 81 took 2.54 seconds (mean sampled reward: -6002.36). Current reward after update: -5131.81, Optimal reward -5131.81
Iteration 82 took 2.52 seconds (mean sampled reward: -5906.24). Current reward after update: -5945.15, Optimal reward -5131.81
Iteration 83 took 2.47 seconds (mean sampled reward: -6285.33). Current reward after update: -6335.32, Optimal reward -5131.81
Iteration 84 took 2.43 seconds (mean sampled reward: -6270.90). Current reward after update: -5196.30, Optimal reward -5131.81
Iteration 85 took 2.57 seconds (mean sampled reward: -6244.32). Current reward after update: -5138.64, Optimal reward -5131.81
Iteration 86 took 2.43 seconds (mean sampled reward: -6333.51). Current reward after update: -5055.93, Optimal reward -5055.93
Iteration 87 took 2.51 seconds (mean sampled reward: -6360.16). Current reward after update: -5327.69, Optimal reward -5055.93
Iteration 88 took 2.49 seconds (mean sampled reward: -6230.28). Current reward after update: -5165.65, Optimal reward -5055.93
Iteration 89 took 2.47 seconds (mean sampled reward: -6062.83). Current reward after update: -5127.36, Optimal reward -5055.93
Iteration 90 took 2.45 seconds (mean sampled reward: -6014.62). Current reward after update: -5059.19, Optimal reward -5055.93
Iteration 91 took 2.37 seconds (mean sampled reward: -6246.41). Current reward after update: -4768.92, Optimal reward -4768.92
Iteration 92 took 2.37 seconds (mean sampled reward: -6087.36). Current reward after update: -4696.99, Optimal reward -4696.99
Iteration 93 took 2.34 seconds (mean sampled reward: -5976.78). Current reward after update: -4570.34, Optimal reward -4570.34
Iteration 94 took 2.43 seconds (mean sampled reward: -5973.98). Current reward after update: -4538.07, Optimal reward -4538.07
Iteration 95 took 2.42 seconds (mean sampled reward: -5968.87). Current reward after update: -4513.06, Optimal reward -4513.06
Iteration 96 took 2.39 seconds (mean sampled reward: -6242.60). Current reward after update: -4704.42, Optimal reward -4513.06
Iteration 97 took 2.40 seconds (mean sampled reward: -6455.84). Current reward after update: -4562.03, Optimal reward -4513.06
Iteration 98 took 2.39 seconds (mean sampled reward: -6563.31). Current reward after update: -4502.22, Optimal reward -4502.22
Iteration 99 took 2.39 seconds (mean sampled reward: -6156.94). Current reward after update: -4432.66, Optimal reward -4432.66
Iteration 100 took 2.49 seconds (mean sampled reward: -5852.65). Current reward after update: -4848.27, Optimal reward -4432.66
Iteration 101 took 2.37 seconds (mean sampled reward: -6364.25). Current reward after update: -4346.57, Optimal reward -4346.57
Iteration 102 took 2.43 seconds (mean sampled reward: -5935.75). Current reward after update: -4388.00, Optimal reward -4346.57
Iteration 103 took 2.49 seconds (mean sampled reward: -5404.13). Current reward after update: -4298.23, Optimal reward -4298.23
Iteration 104 took 2.40 seconds (mean sampled reward: -5295.79). Current reward after update: -5260.75, Optimal reward -4298.23
Iteration 105 took 2.47 seconds (mean sampled reward: -5729.50). Current reward after update: -4042.35, Optimal reward -4042.35
Iteration 106 took 2.59 seconds (mean sampled reward: -5434.13). Current reward after update: -4301.42, Optimal reward -4042.35
Iteration 107 took 2.39 seconds (mean sampled reward: -5382.14). Current reward after update: -4144.66, Optimal reward -4042.35
Iteration 108 took 2.48 seconds (mean sampled reward: -5578.07). Current reward after update: -4788.84, Optimal reward -4042.35
Iteration 109 took 2.41 seconds (mean sampled reward: -5465.73). Current reward after update: -4354.02, Optimal reward -4042.35
Iteration 110 took 2.51 seconds (mean sampled reward: -5372.47). Current reward after update: -4290.51, Optimal reward -4042.35
Iteration 111 took 2.34 seconds (mean sampled reward: -6003.76). Current reward after update: -4701.62, Optimal reward -4042.35
Iteration 112 took 2.41 seconds (mean sampled reward: -5427.18). Current reward after update: -4176.95, Optimal reward -4042.35
Iteration 113 took 2.37 seconds (mean sampled reward: -5613.95). Current reward after update: -4205.81, Optimal reward -4042.35
Iteration 114 took 2.49 seconds (mean sampled reward: -5906.87). Current reward after update: -4101.28, Optimal reward -4042.35
Iteration 115 took 2.35 seconds (mean sampled reward: -5440.89). Current reward after update: -4083.21, Optimal reward -4042.35
Iteration 116 took 2.39 seconds (mean sampled reward: -5070.03). Current reward after update: -4119.31, Optimal reward -4042.35
Iteration 117 took 2.30 seconds (mean sampled reward: -5659.99). Current reward after update: -5110.70, Optimal reward -4042.35
Iteration 118 took 2.29 seconds (mean sampled reward: -5811.60). Current reward after update: -4133.57, Optimal reward -4042.35
Iteration 119 took 2.42 seconds (mean sampled reward: -5730.07). Current reward after update: -4271.67, Optimal reward -4042.35
Iteration 120 took 2.34 seconds (mean sampled reward: -5990.79). Current reward after update: -4614.13, Optimal reward -4042.35
Iteration 121 took 2.27 seconds (mean sampled reward: -5781.84). Current reward after update: -4199.91, Optimal reward -4042.35
Iteration 122 took 2.30 seconds (mean sampled reward: -5986.81). Current reward after update: -4690.77, Optimal reward -4042.35
Iteration 123 took 2.26 seconds (mean sampled reward: -6549.68). Current reward after update: -4407.00, Optimal reward -4042.35
Iteration 124 took 2.30 seconds (mean sampled reward: -6099.40). Current reward after update: -4337.46, Optimal reward -4042.35
Iteration 125 took 2.32 seconds (mean sampled reward: -5876.85). Current reward after update: -4724.06, Optimal reward -4042.35
Iteration 126 took 2.40 seconds (mean sampled reward: -5707.87). Current reward after update: -4302.13, Optimal reward -4042.35
Iteration 127 took 2.32 seconds (mean sampled reward: -5404.46). Current reward after update: -4600.87, Optimal reward -4042.35
Iteration 128 took 2.37 seconds (mean sampled reward: -5239.76). Current reward after update: -4091.30, Optimal reward -4042.35
Iteration 129 took 2.32 seconds (mean sampled reward: -5920.04). Current reward after update: -4171.01, Optimal reward -4042.35
Iteration 130 took 2.29 seconds (mean sampled reward: -5817.62). Current reward after update: -4312.80, Optimal reward -4042.35
Iteration 131 took 2.31 seconds (mean sampled reward: -5931.81). Current reward after update: -4448.27, Optimal reward -4042.35
Iteration 132 took 2.30 seconds (mean sampled reward: -5384.95). Current reward after update: -4243.24, Optimal reward -4042.35
Iteration 133 took 2.31 seconds (mean sampled reward: -5674.88). Current reward after update: -4257.23, Optimal reward -4042.35
Iteration 134 took 2.24 seconds (mean sampled reward: -6221.00). Current reward after update: -3866.17, Optimal reward -3866.17
Iteration 135 took 2.24 seconds (mean sampled reward: -6086.28). Current reward after update: -3945.28, Optimal reward -3866.17
Iteration 136 took 2.27 seconds (mean sampled reward: -6265.46). Current reward after update: -5624.31, Optimal reward -3866.17
Iteration 137 took 2.28 seconds (mean sampled reward: -5930.72). Current reward after update: -4275.95, Optimal reward -3866.17
Iteration 138 took 2.20 seconds (mean sampled reward: -6125.30). Current reward after update: -5572.43, Optimal reward -3866.17
Iteration 139 took 2.20 seconds (mean sampled reward: -6147.86). Current reward after update: -3682.86, Optimal reward -3682.86
Iteration 140 took 2.26 seconds (mean sampled reward: -6050.32). Current reward after update: -3728.00, Optimal reward -3682.86
Iteration 141 took 2.20 seconds (mean sampled reward: -5367.11). Current reward after update: -3537.88, Optimal reward -3537.88
Iteration 142 took 2.22 seconds (mean sampled reward: -5353.63). Current reward after update: -3694.20, Optimal reward -3537.88
Iteration 143 took 2.20 seconds (mean sampled reward: -5064.82). Current reward after update: -5485.06, Optimal reward -3537.88
Iteration 144 took 2.29 seconds (mean sampled reward: -5542.00). Current reward after update: -3618.89, Optimal reward -3537.88
Iteration 145 took 2.23 seconds (mean sampled reward: -5608.75). Current reward after update: -3713.82, Optimal reward -3537.88
Iteration 146 took 2.31 seconds (mean sampled reward: -5944.64). Current reward after update: -3580.90, Optimal reward -3537.88
Iteration 147 took 2.25 seconds (mean sampled reward: -5320.02). Current reward after update: -3566.28, Optimal reward -3537.88
Iteration 148 took 2.26 seconds (mean sampled reward: -5782.76). Current reward after update: -3588.28, Optimal reward -3537.88
Iteration 149 took 2.31 seconds (mean sampled reward: -6057.20). Current reward after update: -5479.79, Optimal reward -3537.88
Iteration 150 took 2.26 seconds (mean sampled reward: -5353.77). Current reward after update: -3519.81, Optimal reward -3519.81
Iteration 151 took 2.34 seconds (mean sampled reward: -5921.64). Current reward after update: -3526.09, Optimal reward -3519.81
Iteration 152 took 2.32 seconds (mean sampled reward: -6086.33). Current reward after update: -3551.17, Optimal reward -3519.81
Iteration 153 took 2.31 seconds (mean sampled reward: -6297.52). Current reward after update: -3541.96, Optimal reward -3519.81
Iteration 154 took 2.29 seconds (mean sampled reward: -5937.64). Current reward after update: -3546.32, Optimal reward -3519.81
Iteration 155 took 2.36 seconds (mean sampled reward: -5857.25). Current reward after update: -3515.34, Optimal reward -3515.34
Iteration 156 took 2.35 seconds (mean sampled reward: -6294.74). Current reward after update: -3564.63, Optimal reward -3515.34
Iteration 157 took 2.31 seconds (mean sampled reward: -5833.66). Current reward after update: -3518.48, Optimal reward -3515.34
Iteration 158 took 2.39 seconds (mean sampled reward: -5966.99). Current reward after update: -4001.41, Optimal reward -3515.34
Iteration 159 took 2.32 seconds (mean sampled reward: -6007.01). Current reward after update: -3466.12, Optimal reward -3466.12
Iteration 160 took 2.30 seconds (mean sampled reward: -5689.49). Current reward after update: -3449.43, Optimal reward -3449.43
Iteration 161 took 2.33 seconds (mean sampled reward: -5312.99). Current reward after update: -3557.03, Optimal reward -3449.43
Iteration 162 took 2.34 seconds (mean sampled reward: -5207.05). Current reward after update: -3539.88, Optimal reward -3449.43
Iteration 163 took 2.26 seconds (mean sampled reward: -5277.27). Current reward after update: -3569.86, Optimal reward -3449.43
Iteration 164 took 2.30 seconds (mean sampled reward: -5627.02). Current reward after update: -3599.47, Optimal reward -3449.43
Iteration 165 took 2.38 seconds (mean sampled reward: -5829.21). Current reward after update: -3589.94, Optimal reward -3449.43
Iteration 166 took 2.36 seconds (mean sampled reward: -6239.48). Current reward after update: -3686.20, Optimal reward -3449.43
Iteration 167 took 2.37 seconds (mean sampled reward: -5817.79). Current reward after update: -5490.55, Optimal reward -3449.43
Iteration 168 took 2.33 seconds (mean sampled reward: -5843.71). Current reward after update: -5443.29, Optimal reward -3449.43
Iteration 169 took 2.31 seconds (mean sampled reward: -5691.30). Current reward after update: -5472.80, Optimal reward -3449.43
Iteration 170 took 2.32 seconds (mean sampled reward: -5778.60). Current reward after update: -3563.69, Optimal reward -3449.43
Iteration 171 took 2.35 seconds (mean sampled reward: -5099.71). Current reward after update: -4909.79, Optimal reward -3449.43
Iteration 172 took 2.27 seconds (mean sampled reward: -5550.51). Current reward after update: -3340.70, Optimal reward -3340.70
Iteration 173 took 2.23 seconds (mean sampled reward: -5435.66). Current reward after update: -3476.92, Optimal reward -3340.70
Iteration 174 took 2.32 seconds (mean sampled reward: -5145.95). Current reward after update: -3517.05, Optimal reward -3340.70
Iteration 175 took 2.21 seconds (mean sampled reward: -5071.66). Current reward after update: -3674.54, Optimal reward -3340.70
Iteration 176 took 2.28 seconds (mean sampled reward: -5061.89). Current reward after update: -3367.27, Optimal reward -3340.70
Iteration 177 took 2.27 seconds (mean sampled reward: -5138.53). Current reward after update: -3433.31, Optimal reward -3340.70
Iteration 178 took 2.28 seconds (mean sampled reward: -4954.03). Current reward after update: -3439.33, Optimal reward -3340.70
Iteration 179 took 2.28 seconds (mean sampled reward: -4357.77). Current reward after update: -3209.87, Optimal reward -3209.87
Iteration 180 took 2.20 seconds (mean sampled reward: -4154.16). Current reward after update: -3105.30, Optimal reward -3105.30
Iteration 181 took 2.20 seconds (mean sampled reward: -4110.26). Current reward after update: -3097.32, Optimal reward -3097.32
Iteration 182 took 2.28 seconds (mean sampled reward: -3830.43). Current reward after update: -4008.30, Optimal reward -3097.32
Iteration 183 took 2.20 seconds (mean sampled reward: -4529.34). Current reward after update: -3165.95, Optimal reward -3097.32
Iteration 184 took 2.21 seconds (mean sampled reward: -3900.93). Current reward after update: -3136.59, Optimal reward -3097.32
Iteration 185 took 2.21 seconds (mean sampled reward: -4356.93). Current reward after update: -3201.04, Optimal reward -3097.32
Iteration 186 took 2.24 seconds (mean sampled reward: -4395.84). Current reward after update: -3190.76, Optimal reward -3097.32
Iteration 187 took 2.24 seconds (mean sampled reward: -4714.35). Current reward after update: -3213.78, Optimal reward -3097.32
Iteration 188 took 2.21 seconds (mean sampled reward: -3965.44). Current reward after update: -3182.75, Optimal reward -3097.32
Iteration 189 took 2.26 seconds (mean sampled reward: -5916.44). Current reward after update: -3248.89, Optimal reward -3097.32
Iteration 190 took 2.24 seconds (mean sampled reward: -6001.11). Current reward after update: -3210.77, Optimal reward -3097.32
Iteration 191 took 2.21 seconds (mean sampled reward: -5034.35). Current reward after update: -3682.47, Optimal reward -3097.32
Iteration 192 took 2.31 seconds (mean sampled reward: -5348.83). Current reward after update: -3235.28, Optimal reward -3097.32
Iteration 193 took 2.18 seconds (mean sampled reward: -5066.71). Current reward after update: -3131.93, Optimal reward -3097.32
Iteration 194 took 2.35 seconds (mean sampled reward: -4776.57). Current reward after update: -3201.75, Optimal reward -3097.32
Iteration 195 took 2.26 seconds (mean sampled reward: -4080.48). Current reward after update: -3847.31, Optimal reward -3097.32
Iteration 196 took 2.28 seconds (mean sampled reward: -5217.84). Current reward after update: -3169.55, Optimal reward -3097.32
Iteration 197 took 2.23 seconds (mean sampled reward: -4503.95). Current reward after update: -3161.77, Optimal reward -3097.32
Iteration 198 took 2.26 seconds (mean sampled reward: -4095.69). Current reward after update: -3078.65, Optimal reward -3078.65
Iteration 199 took 2.33 seconds (mean sampled reward: -4847.14). Current reward after update: -3077.77, Optimal reward -3077.77
Iteration 200 took 2.28 seconds (mean sampled reward: -4505.36). Current reward after update: -3094.63, Optimal reward -3077.77
Iteration 1 took 2.39 seconds (mean sampled reward: -7619.68). Current reward after update: -7355.08, Optimal reward -7355.08
Iteration 2 took 2.34 seconds (mean sampled reward: -7638.42). Current reward after update: -7369.30, Optimal reward -7355.08
Iteration 3 took 2.36 seconds (mean sampled reward: -7647.53). Current reward after update: -7425.57, Optimal reward -7355.08
Iteration 4 took 2.41 seconds (mean sampled reward: -7667.66). Current reward after update: -7320.50, Optimal reward -7320.50
Iteration 5 took 2.39 seconds (mean sampled reward: -7665.63). Current reward after update: -7297.83, Optimal reward -7297.83
Iteration 6 took 2.47 seconds (mean sampled reward: -7643.90). Current reward after update: -7565.40, Optimal reward -7297.83
Iteration 7 took 2.37 seconds (mean sampled reward: -7620.28). Current reward after update: -7206.44, Optimal reward -7206.44
Iteration 8 took 2.45 seconds (mean sampled reward: -7621.54). Current reward after update: -7123.45, Optimal reward -7123.45
Iteration 9 took 2.36 seconds (mean sampled reward: -7594.07). Current reward after update: -6666.13, Optimal reward -6666.13
Iteration 10 took 2.32 seconds (mean sampled reward: -7471.87). Current reward after update: -6161.92, Optimal reward -6161.92
Iteration 11 took 2.38 seconds (mean sampled reward: -7252.00). Current reward after update: -5902.99, Optimal reward -5902.99
Iteration 12 took 2.42 seconds (mean sampled reward: -6895.53). Current reward after update: -5274.17, Optimal reward -5274.17
Iteration 13 took 2.31 seconds (mean sampled reward: -7149.39). Current reward after update: -5006.32, Optimal reward -5006.32
Iteration 14 took 2.13 seconds (mean sampled reward: -6786.61). Current reward after update: -4936.87, Optimal reward -4936.87
Iteration 15 took 2.19 seconds (mean sampled reward: -6375.75). Current reward after update: -4794.65, Optimal reward -4794.65
Iteration 16 took 2.35 seconds (mean sampled reward: -6521.43). Current reward after update: -4756.54, Optimal reward -4756.54
Iteration 17 took 2.38 seconds (mean sampled reward: -6518.80). Current reward after update: -4709.85, Optimal reward -4709.85
Iteration 18 took 2.51 seconds (mean sampled reward: -5958.75). Current reward after update: -3960.93, Optimal reward -3960.93
Iteration 19 took 2.27 seconds (mean sampled reward: -6665.15). Current reward after update: -3748.68, Optimal reward -3748.68
Iteration 20 took 2.17 seconds (mean sampled reward: -6067.08). Current reward after update: -3643.78, Optimal reward -3643.78
Iteration 21 took 2.22 seconds (mean sampled reward: -5569.34). Current reward after update: -3160.49, Optimal reward -3160.49
Iteration 22 took 2.26 seconds (mean sampled reward: -6373.14). Current reward after update: -2791.70, Optimal reward -2791.70
Iteration 23 took 2.19 seconds (mean sampled reward: -5939.94). Current reward after update: -2674.87, Optimal reward -2674.87
Iteration 24 took 2.20 seconds (mean sampled reward: -5036.01). Current reward after update: -3918.00, Optimal reward -2674.87
Iteration 25 took 2.22 seconds (mean sampled reward: -6132.42). Current reward after update: -2682.87, Optimal reward -2674.87
Iteration 26 took 2.18 seconds (mean sampled reward: -5850.41). Current reward after update: -2989.99, Optimal reward -2674.87
Iteration 27 took 2.25 seconds (mean sampled reward: -6697.14). Current reward after update: -2786.55, Optimal reward -2674.87
Iteration 28 took 2.17 seconds (mean sampled reward: -6812.44). Current reward after update: -2683.43, Optimal reward -2674.87
Iteration 29 took 2.34 seconds (mean sampled reward: -6449.99). Current reward after update: -2911.63, Optimal reward -2674.87
Iteration 30 took 2.50 seconds (mean sampled reward: -5956.78). Current reward after update: -2791.31, Optimal reward -2674.87
Iteration 31 took 2.56 seconds (mean sampled reward: -6108.44). Current reward after update: -2217.46, Optimal reward -2217.46
Iteration 32 took 2.32 seconds (mean sampled reward: -7038.86). Current reward after update: -2916.59, Optimal reward -2217.46
Iteration 33 took 2.14 seconds (mean sampled reward: -6053.61). Current reward after update: -2591.45, Optimal reward -2217.46
Iteration 34 took 2.19 seconds (mean sampled reward: -6662.78). Current reward after update: -2516.44, Optimal reward -2217.46
Iteration 35 took 2.22 seconds (mean sampled reward: -5873.58). Current reward after update: -2360.42, Optimal reward -2217.46
Iteration 36 took 2.13 seconds (mean sampled reward: -6521.90). Current reward after update: -2764.63, Optimal reward -2217.46
Iteration 37 took 2.17 seconds (mean sampled reward: -6515.47). Current reward after update: -2369.91, Optimal reward -2217.46
Iteration 38 took 2.24 seconds (mean sampled reward: -5662.69). Current reward after update: -2197.96, Optimal reward -2197.96
Iteration 39 took 2.19 seconds (mean sampled reward: -5850.79). Current reward after update: -2328.78, Optimal reward -2197.96
Iteration 40 took 2.27 seconds (mean sampled reward: -5359.06). Current reward after update: -2258.01, Optimal reward -2197.96
Iteration 41 took 2.08 seconds (mean sampled reward: -5837.04). Current reward after update: -2190.07, Optimal reward -2190.07
Iteration 42 took 2.12 seconds (mean sampled reward: -5401.57). Current reward after update: -2115.07, Optimal reward -2115.07
Iteration 43 took 2.09 seconds (mean sampled reward: -4411.61). Current reward after update: -1976.79, Optimal reward -1976.79
Iteration 44 took 2.20 seconds (mean sampled reward: -4433.11). Current reward after update: -2063.77, Optimal reward -1976.79
Iteration 45 took 2.19 seconds (mean sampled reward: -5676.95). Current reward after update: -2135.66, Optimal reward -1976.79
Iteration 46 took 2.15 seconds (mean sampled reward: -3682.88). Current reward after update: -1936.01, Optimal reward -1936.01
Iteration 47 took 2.16 seconds (mean sampled reward: -4853.67). Current reward after update: -1922.75, Optimal reward -1922.75
Iteration 48 took 2.21 seconds (mean sampled reward: -5220.72). Current reward after update: -2013.20, Optimal reward -1922.75
Iteration 49 took 2.17 seconds (mean sampled reward: -3790.70). Current reward after update: -1894.23, Optimal reward -1894.23
Iteration 50 took 2.22 seconds (mean sampled reward: -5669.34). Current reward after update: -1884.45, Optimal reward -1884.45
Iteration 51 took 2.29 seconds (mean sampled reward: -4896.16). Current reward after update: -1892.18, Optimal reward -1884.45
Iteration 52 took 2.18 seconds (mean sampled reward: -5574.83). Current reward after update: -1926.36, Optimal reward -1884.45
Iteration 53 took 2.23 seconds (mean sampled reward: -5508.24). Current reward after update: -1886.19, Optimal reward -1884.45
Iteration 54 took 2.23 seconds (mean sampled reward: -5290.20). Current reward after update: -1968.21, Optimal reward -1884.45
Iteration 55 took 2.18 seconds (mean sampled reward: -4903.66). Current reward after update: -1899.19, Optimal reward -1884.45
Iteration 56 took 2.42 seconds (mean sampled reward: -5089.47). Current reward after update: -1971.87, Optimal reward -1884.45
Iteration 57 took 2.48 seconds (mean sampled reward: -5278.13). Current reward after update: -1923.75, Optimal reward -1884.45
Iteration 58 took 2.15 seconds (mean sampled reward: -4015.35). Current reward after update: -6088.77, Optimal reward -1884.45
Iteration 59 took 2.12 seconds (mean sampled reward: -5912.12). Current reward after update: -1945.18, Optimal reward -1884.45
Iteration 60 took 2.16 seconds (mean sampled reward: -5157.75). Current reward after update: -2520.86, Optimal reward -1884.45
Iteration 61 took 2.14 seconds (mean sampled reward: -5236.37). Current reward after update: -1778.69, Optimal reward -1778.69
Iteration 62 took 2.16 seconds (mean sampled reward: -4410.66). Current reward after update: -1949.55, Optimal reward -1778.69
Iteration 63 took 2.18 seconds (mean sampled reward: -4870.66). Current reward after update: -2405.70, Optimal reward -1778.69
Iteration 64 took 2.31 seconds (mean sampled reward: -4877.61). Current reward after update: -1926.79, Optimal reward -1778.69
Iteration 65 took 2.19 seconds (mean sampled reward: -6711.77). Current reward after update: -1901.37, Optimal reward -1778.69
Iteration 66 took 2.20 seconds (mean sampled reward: -5980.22). Current reward after update: -1963.37, Optimal reward -1778.69
Iteration 67 took 2.22 seconds (mean sampled reward: -3716.30). Current reward after update: -1868.35, Optimal reward -1778.69
Iteration 68 took 2.22 seconds (mean sampled reward: -4928.98). Current reward after update: -1912.70, Optimal reward -1778.69
Iteration 69 took 2.23 seconds (mean sampled reward: -5734.44). Current reward after update: -1851.01, Optimal reward -1778.69
Iteration 70 took 2.40 seconds (mean sampled reward: -5788.77). Current reward after update: -1826.77, Optimal reward -1778.69
Iteration 71 took 2.15 seconds (mean sampled reward: -5361.51). Current reward after update: -1936.88, Optimal reward -1778.69
Iteration 72 took 2.28 seconds (mean sampled reward: -5561.07). Current reward after update: -1758.17, Optimal reward -1758.17
Iteration 73 took 2.22 seconds (mean sampled reward: -5725.86). Current reward after update: -1764.23, Optimal reward -1758.17
Iteration 74 took 2.18 seconds (mean sampled reward: -4322.92). Current reward after update: -2157.79, Optimal reward -1758.17
Iteration 75 took 2.15 seconds (mean sampled reward: -4019.32). Current reward after update: -1827.16, Optimal reward -1758.17
Iteration 76 took 2.16 seconds (mean sampled reward: -3992.41). Current reward after update: -1788.82, Optimal reward -1758.17
Iteration 77 took 2.19 seconds (mean sampled reward: -3593.37). Current reward after update: -1721.03, Optimal reward -1721.03
Iteration 78 took 2.27 seconds (mean sampled reward: -4259.50). Current reward after update: -1733.34, Optimal reward -1721.03
Iteration 79 took 2.18 seconds (mean sampled reward: -4503.81). Current reward after update: -1756.06, Optimal reward -1721.03
Iteration 80 took 2.16 seconds (mean sampled reward: -5125.52). Current reward after update: -1865.08, Optimal reward -1721.03
Iteration 81 took 2.22 seconds (mean sampled reward: -4508.16). Current reward after update: -1841.38, Optimal reward -1721.03
Iteration 82 took 2.14 seconds (mean sampled reward: -4217.51). Current reward after update: -5985.51, Optimal reward -1721.03
Iteration 83 took 2.21 seconds (mean sampled reward: -5226.65). Current reward after update: -1781.70, Optimal reward -1721.03
Iteration 84 took 2.13 seconds (mean sampled reward: -4474.76). Current reward after update: -1831.95, Optimal reward -1721.03
Iteration 85 took 2.13 seconds (mean sampled reward: -3896.36). Current reward after update: -2103.70, Optimal reward -1721.03
Iteration 86 took 2.16 seconds (mean sampled reward: -3268.42). Current reward after update: -2142.25, Optimal reward -1721.03
Iteration 87 took 2.18 seconds (mean sampled reward: -3647.25). Current reward after update: -1767.44, Optimal reward -1721.03
Iteration 88 took 2.21 seconds (mean sampled reward: -3524.66). Current reward after update: -1751.24, Optimal reward -1721.03
Iteration 89 took 2.19 seconds (mean sampled reward: -3763.14). Current reward after update: -2037.46, Optimal reward -1721.03
Iteration 90 took 2.22 seconds (mean sampled reward: -3922.30). Current reward after update: -1775.17, Optimal reward -1721.03
Iteration 91 took 2.21 seconds (mean sampled reward: -3494.65). Current reward after update: -1705.38, Optimal reward -1705.38
Iteration 92 took 2.13 seconds (mean sampled reward: -3844.29). Current reward after update: -1961.01, Optimal reward -1705.38
Iteration 93 took 2.14 seconds (mean sampled reward: -3872.09). Current reward after update: -1715.79, Optimal reward -1705.38
Iteration 94 took 2.08 seconds (mean sampled reward: -4651.97). Current reward after update: -1712.25, Optimal reward -1705.38
Iteration 95 took 2.06 seconds (mean sampled reward: -4521.29). Current reward after update: -1737.19, Optimal reward -1705.38
Iteration 96 took 2.11 seconds (mean sampled reward: -3367.05). Current reward after update: -1699.88, Optimal reward -1699.88
Iteration 97 took 2.18 seconds (mean sampled reward: -3498.65). Current reward after update: -1606.56, Optimal reward -1606.56
Iteration 98 took 2.15 seconds (mean sampled reward: -4817.92). Current reward after update: -1653.94, Optimal reward -1606.56
Iteration 99 took 2.20 seconds (mean sampled reward: -5931.03). Current reward after update: -1733.75, Optimal reward -1606.56
Iteration 100 took 2.18 seconds (mean sampled reward: -5413.15). Current reward after update: -1700.96, Optimal reward -1606.56
Iteration 101 took 2.22 seconds (mean sampled reward: -5617.02). Current reward after update: -1637.15, Optimal reward -1606.56
Iteration 102 took 2.30 seconds (mean sampled reward: -4493.43). Current reward after update: -1681.37, Optimal reward -1606.56
Iteration 103 took 2.41 seconds (mean sampled reward: -5661.29). Current reward after update: -1653.51, Optimal reward -1606.56
Iteration 104 took 2.28 seconds (mean sampled reward: -4603.72). Current reward after update: -1661.31, Optimal reward -1606.56
Iteration 105 took 2.30 seconds (mean sampled reward: -5363.95). Current reward after update: -1774.22, Optimal reward -1606.56
Iteration 106 took 2.38 seconds (mean sampled reward: -5608.05). Current reward after update: -1815.91, Optimal reward -1606.56
Iteration 107 took 2.29 seconds (mean sampled reward: -4986.91). Current reward after update: -1737.55, Optimal reward -1606.56
Iteration 108 took 2.62 seconds (mean sampled reward: -5708.38). Current reward after update: -1675.42, Optimal reward -1606.56
Iteration 109 took 2.40 seconds (mean sampled reward: -4334.09). Current reward after update: -1728.84, Optimal reward -1606.56
Iteration 110 took 2.40 seconds (mean sampled reward: -4599.50). Current reward after update: -1849.80, Optimal reward -1606.56
Iteration 111 took 2.31 seconds (mean sampled reward: -5097.61). Current reward after update: -1695.64, Optimal reward -1606.56
Iteration 112 took 2.28 seconds (mean sampled reward: -4641.78). Current reward after update: -1641.79, Optimal reward -1606.56
Iteration 113 took 2.43 seconds (mean sampled reward: -5039.43). Current reward after update: -1560.68, Optimal reward -1560.68
Iteration 114 took 2.32 seconds (mean sampled reward: -6216.36). Current reward after update: -1646.14, Optimal reward -1560.68
Iteration 115 took 2.41 seconds (mean sampled reward: -6000.12). Current reward after update: -2564.84, Optimal reward -1560.68
Iteration 116 took 2.31 seconds (mean sampled reward: -4657.60). Current reward after update: -1570.41, Optimal reward -1560.68
Iteration 117 took 2.26 seconds (mean sampled reward: -4249.84). Current reward after update: -1622.30, Optimal reward -1560.68
Iteration 118 took 2.23 seconds (mean sampled reward: -3081.63). Current reward after update: -6416.74, Optimal reward -1560.68
Iteration 119 took 2.40 seconds (mean sampled reward: -5444.31). Current reward after update: -1634.13, Optimal reward -1560.68
Iteration 120 took 2.20 seconds (mean sampled reward: -4712.46). Current reward after update: -1623.54, Optimal reward -1560.68
Iteration 121 took 2.23 seconds (mean sampled reward: -3562.76). Current reward after update: -1702.17, Optimal reward -1560.68
Iteration 122 took 2.26 seconds (mean sampled reward: -3969.80). Current reward after update: -1928.11, Optimal reward -1560.68
Iteration 123 took 2.21 seconds (mean sampled reward: -3173.62). Current reward after update: -1625.96, Optimal reward -1560.68
Iteration 124 took 2.20 seconds (mean sampled reward: -3330.94). Current reward after update: -1701.51, Optimal reward -1560.68
Iteration 125 took 2.17 seconds (mean sampled reward: -5040.26). Current reward after update: -1735.35, Optimal reward -1560.68
Iteration 126 took 2.17 seconds (mean sampled reward: -3376.82). Current reward after update: -1634.78, Optimal reward -1560.68
Iteration 127 took 2.22 seconds (mean sampled reward: -3089.13). Current reward after update: -1630.62, Optimal reward -1560.68
Iteration 128 took 2.46 seconds (mean sampled reward: -2365.45). Current reward after update: -1627.19, Optimal reward -1560.68
Iteration 129 took 2.20 seconds (mean sampled reward: -2977.89). Current reward after update: -1609.83, Optimal reward -1560.68
Iteration 130 took 2.25 seconds (mean sampled reward: -3485.26). Current reward after update: -1810.63, Optimal reward -1560.68
Iteration 131 took 2.35 seconds (mean sampled reward: -5384.79). Current reward after update: -1639.62, Optimal reward -1560.68
Iteration 132 took 2.52 seconds (mean sampled reward: -4841.66). Current reward after update: -1638.09, Optimal reward -1560.68
Iteration 133 took 2.37 seconds (mean sampled reward: -4712.82). Current reward after update: -1643.99, Optimal reward -1560.68
Iteration 134 took 2.35 seconds (mean sampled reward: -6110.28). Current reward after update: -1734.54, Optimal reward -1560.68
Iteration 135 took 2.31 seconds (mean sampled reward: -5104.66). Current reward after update: -1644.38, Optimal reward -1560.68
Iteration 136 took 2.25 seconds (mean sampled reward: -4097.03). Current reward after update: -1642.47, Optimal reward -1560.68
Iteration 137 took 2.26 seconds (mean sampled reward: -3258.12). Current reward after update: -1786.55, Optimal reward -1560.68
Iteration 138 took 2.20 seconds (mean sampled reward: -3971.05). Current reward after update: -1609.71, Optimal reward -1560.68
Iteration 139 took 2.21 seconds (mean sampled reward: -3711.78). Current reward after update: -1649.16, Optimal reward -1560.68
Iteration 140 took 2.35 seconds (mean sampled reward: -5650.78). Current reward after update: -1643.58, Optimal reward -1560.68
Iteration 141 took 2.25 seconds (mean sampled reward: -4813.59). Current reward after update: -1641.02, Optimal reward -1560.68
Iteration 142 took 2.24 seconds (mean sampled reward: -5353.21). Current reward after update: -1692.13, Optimal reward -1560.68
Iteration 143 took 2.24 seconds (mean sampled reward: -4232.47). Current reward after update: -1656.66, Optimal reward -1560.68
Iteration 144 took 2.24 seconds (mean sampled reward: -4376.98). Current reward after update: -1639.26, Optimal reward -1560.68
Iteration 145 took 2.22 seconds (mean sampled reward: -4892.43). Current reward after update: -1707.10, Optimal reward -1560.68
Iteration 146 took 2.20 seconds (mean sampled reward: -5185.40). Current reward after update: -1812.91, Optimal reward -1560.68
Iteration 147 took 2.20 seconds (mean sampled reward: -4327.03). Current reward after update: -1848.11, Optimal reward -1560.68
Iteration 148 took 2.15 seconds (mean sampled reward: -3816.02). Current reward after update: -1795.21, Optimal reward -1560.68
Iteration 149 took 2.23 seconds (mean sampled reward: -3774.57). Current reward after update: -1755.64, Optimal reward -1560.68
Iteration 150 took 2.13 seconds (mean sampled reward: -2926.02). Current reward after update: -1742.74, Optimal reward -1560.68
Iteration 151 took 2.11 seconds (mean sampled reward: -3027.18). Current reward after update: -2979.30, Optimal reward -1560.68
Iteration 152 took 2.18 seconds (mean sampled reward: -4320.15). Current reward after update: -1736.42, Optimal reward -1560.68
Iteration 153 took 2.19 seconds (mean sampled reward: -4587.55). Current reward after update: -1724.61, Optimal reward -1560.68
Iteration 154 took 2.22 seconds (mean sampled reward: -3390.51). Current reward after update: -1677.51, Optimal reward -1560.68
Iteration 155 took 2.18 seconds (mean sampled reward: -3576.90). Current reward after update: -1929.96, Optimal reward -1560.68
Iteration 156 took 2.30 seconds (mean sampled reward: -4070.60). Current reward after update: -1684.23, Optimal reward -1560.68
Iteration 157 took 2.21 seconds (mean sampled reward: -3546.71). Current reward after update: -2281.84, Optimal reward -1560.68
Iteration 158 took 2.19 seconds (mean sampled reward: -3402.78). Current reward after update: -1713.68, Optimal reward -1560.68
Iteration 159 took 2.26 seconds (mean sampled reward: -3732.55). Current reward after update: -4600.32, Optimal reward -1560.68
Iteration 160 took 2.30 seconds (mean sampled reward: -4237.78). Current reward after update: -1675.20, Optimal reward -1560.68
Iteration 161 took 2.21 seconds (mean sampled reward: -3498.00). Current reward after update: -1610.33, Optimal reward -1560.68
Iteration 162 took 2.28 seconds (mean sampled reward: -4189.87). Current reward after update: -1601.71, Optimal reward -1560.68
Iteration 163 took 2.21 seconds (mean sampled reward: -4961.40). Current reward after update: -1624.61, Optimal reward -1560.68
Iteration 164 took 2.33 seconds (mean sampled reward: -4839.99). Current reward after update: -1826.72, Optimal reward -1560.68
Iteration 165 took 2.27 seconds (mean sampled reward: -3685.13). Current reward after update: -2077.95, Optimal reward -1560.68
Iteration 166 took 2.30 seconds (mean sampled reward: -5476.34). Current reward after update: -6272.99, Optimal reward -1560.68
Iteration 167 took 2.33 seconds (mean sampled reward: -4968.24). Current reward after update: -1673.86, Optimal reward -1560.68
Iteration 168 took 2.25 seconds (mean sampled reward: -4463.34). Current reward after update: -1659.45, Optimal reward -1560.68
Iteration 169 took 2.30 seconds (mean sampled reward: -5274.55). Current reward after update: -1801.86, Optimal reward -1560.68
Iteration 170 took 2.23 seconds (mean sampled reward: -5816.47). Current reward after update: -1709.47, Optimal reward -1560.68
Iteration 171 took 2.24 seconds (mean sampled reward: -4571.86). Current reward after update: -1704.61, Optimal reward -1560.68
Iteration 172 took 2.23 seconds (mean sampled reward: -4650.93). Current reward after update: -1720.02, Optimal reward -1560.68
Iteration 173 took 2.30 seconds (mean sampled reward: -4270.92). Current reward after update: -1823.74, Optimal reward -1560.68
Iteration 174 took 2.27 seconds (mean sampled reward: -4141.53). Current reward after update: -1744.42, Optimal reward -1560.68
Iteration 175 took 2.26 seconds (mean sampled reward: -4783.12). Current reward after update: -1680.68, Optimal reward -1560.68
Iteration 176 took 2.42 seconds (mean sampled reward: -5309.82). Current reward after update: -1695.80, Optimal reward -1560.68
Iteration 177 took 2.23 seconds (mean sampled reward: -5772.11). Current reward after update: -1736.35, Optimal reward -1560.68
Iteration 178 took 2.20 seconds (mean sampled reward: -5276.51). Current reward after update: -1695.39, Optimal reward -1560.68
Iteration 179 took 2.27 seconds (mean sampled reward: -4051.19). Current reward after update: -1682.02, Optimal reward -1560.68
Iteration 180 took 2.26 seconds (mean sampled reward: -3865.39). Current reward after update: -1716.35, Optimal reward -1560.68
Iteration 181 took 2.27 seconds (mean sampled reward: -4931.32). Current reward after update: -1821.16, Optimal reward -1560.68
Iteration 182 took 2.24 seconds (mean sampled reward: -4602.36). Current reward after update: -1864.91, Optimal reward -1560.68
Iteration 183 took 2.22 seconds (mean sampled reward: -4965.23). Current reward after update: -1618.35, Optimal reward -1560.68
Iteration 184 took 2.23 seconds (mean sampled reward: -3304.56). Current reward after update: -1616.68, Optimal reward -1560.68
Iteration 185 took 2.22 seconds (mean sampled reward: -3197.67). Current reward after update: -1609.67, Optimal reward -1560.68
Iteration 186 took 2.23 seconds (mean sampled reward: -2986.89). Current reward after update: -1911.81, Optimal reward -1560.68
Iteration 187 took 2.20 seconds (mean sampled reward: -2913.49). Current reward after update: -1648.85, Optimal reward -1560.68
Iteration 188 took 2.23 seconds (mean sampled reward: -3325.98). Current reward after update: -1954.37, Optimal reward -1560.68
Iteration 189 took 2.19 seconds (mean sampled reward: -3041.74). Current reward after update: -1781.42, Optimal reward -1560.68
Iteration 190 took 2.22 seconds (mean sampled reward: -3209.49). Current reward after update: -1595.60, Optimal reward -1560.68
Iteration 191 took 2.23 seconds (mean sampled reward: -3206.25). Current reward after update: -2726.15, Optimal reward -1560.68
Iteration 192 took 2.32 seconds (mean sampled reward: -2881.52). Current reward after update: -1884.27, Optimal reward -1560.68
Iteration 193 took 2.25 seconds (mean sampled reward: -4442.61). Current reward after update: -1597.60, Optimal reward -1560.68
Iteration 194 took 2.24 seconds (mean sampled reward: -3079.57). Current reward after update: -1611.91, Optimal reward -1560.68
Iteration 195 took 2.24 seconds (mean sampled reward: -3166.25). Current reward after update: -6286.25, Optimal reward -1560.68
Iteration 196 took 2.25 seconds (mean sampled reward: -3393.25). Current reward after update: -2327.54, Optimal reward -1560.68
Iteration 197 took 2.22 seconds (mean sampled reward: -3235.68). Current reward after update: -1774.69, Optimal reward -1560.68
Iteration 198 took 2.25 seconds (mean sampled reward: -3192.86). Current reward after update: -1572.99, Optimal reward -1560.68
Iteration 199 took 2.26 seconds (mean sampled reward: -3986.86). Current reward after update: -1570.57, Optimal reward -1560.68
Iteration 200 took 2.24 seconds (mean sampled reward: -4058.44). Current reward after update: -1537.84, Optimal reward -1537.84
Max force: 50 Sigma: 0.1 mean rewards: -1678.1497957414322, best rewards:-418.83616111734807

Iteration 1 took 2.41 seconds (mean sampled reward: -7625.69). Current reward after update: -7373.45, Optimal reward -7373.45
Iteration 2 took 2.25 seconds (mean sampled reward: -7658.75). Current reward after update: -6796.48, Optimal reward -6796.48
Iteration 3 took 2.40 seconds (mean sampled reward: -7568.27). Current reward after update: -6413.71, Optimal reward -6413.71
Iteration 4 took 2.40 seconds (mean sampled reward: -7407.16). Current reward after update: -5824.61, Optimal reward -5824.61
Iteration 5 took 2.59 seconds (mean sampled reward: -7115.90). Current reward after update: -4935.43, Optimal reward -4935.43
Iteration 6 took 2.41 seconds (mean sampled reward: -6683.52). Current reward after update: -4681.49, Optimal reward -4681.49
Iteration 7 took 2.33 seconds (mean sampled reward: -6832.62). Current reward after update: -4583.56, Optimal reward -4583.56
Iteration 8 took 2.53 seconds (mean sampled reward: -7011.07). Current reward after update: -4581.32, Optimal reward -4581.32
Iteration 9 took 2.31 seconds (mean sampled reward: -6661.45). Current reward after update: -4380.84, Optimal reward -4380.84
Iteration 10 took 2.44 seconds (mean sampled reward: -6760.11). Current reward after update: -3392.76, Optimal reward -3392.76
Iteration 11 took 2.52 seconds (mean sampled reward: -5855.71). Current reward after update: -3663.26, Optimal reward -3392.76
Iteration 12 took 2.36 seconds (mean sampled reward: -5720.97). Current reward after update: -3259.16, Optimal reward -3259.16
Iteration 13 took 2.43 seconds (mean sampled reward: -6056.91). Current reward after update: -3583.63, Optimal reward -3259.16
Iteration 14 took 2.30 seconds (mean sampled reward: -5722.79). Current reward after update: -3307.42, Optimal reward -3259.16
Iteration 15 took 2.61 seconds (mean sampled reward: -5959.79). Current reward after update: -2215.84, Optimal reward -2215.84
Iteration 16 took 2.50 seconds (mean sampled reward: -6479.24). Current reward after update: -2178.86, Optimal reward -2178.86
Iteration 17 took 2.37 seconds (mean sampled reward: -6696.22). Current reward after update: -2512.01, Optimal reward -2178.86
Iteration 18 took 2.27 seconds (mean sampled reward: -7116.30). Current reward after update: -2511.73, Optimal reward -2178.86
Iteration 19 took 2.31 seconds (mean sampled reward: -7226.28). Current reward after update: -2442.67, Optimal reward -2178.86
Iteration 20 took 2.37 seconds (mean sampled reward: -7237.50). Current reward after update: -2254.17, Optimal reward -2178.86
Iteration 21 took 2.36 seconds (mean sampled reward: -6278.86). Current reward after update: -1885.06, Optimal reward -1885.06
Iteration 22 took 2.29 seconds (mean sampled reward: -6625.48). Current reward after update: -2074.88, Optimal reward -1885.06
Iteration 23 took 2.29 seconds (mean sampled reward: -6718.66). Current reward after update: -1899.89, Optimal reward -1885.06
Iteration 24 took 2.37 seconds (mean sampled reward: -5868.75). Current reward after update: -1952.94, Optimal reward -1885.06
Iteration 25 took 2.20 seconds (mean sampled reward: -5954.56). Current reward after update: -1986.02, Optimal reward -1885.06
Iteration 26 took 2.33 seconds (mean sampled reward: -6432.62). Current reward after update: -1876.06, Optimal reward -1876.06
Iteration 27 took 2.18 seconds (mean sampled reward: -6548.41). Current reward after update: -2091.80, Optimal reward -1876.06
Iteration 28 took 2.17 seconds (mean sampled reward: -6804.92). Current reward after update: -2138.66, Optimal reward -1876.06
Iteration 29 took 2.25 seconds (mean sampled reward: -7213.47). Current reward after update: -1796.78, Optimal reward -1796.78
Iteration 30 took 2.31 seconds (mean sampled reward: -7296.60). Current reward after update: -2269.99, Optimal reward -1796.78
Iteration 31 took 2.35 seconds (mean sampled reward: -6385.21). Current reward after update: -2626.35, Optimal reward -1796.78
Iteration 32 took 2.27 seconds (mean sampled reward: -7007.21). Current reward after update: -1808.47, Optimal reward -1796.78
Iteration 33 took 2.30 seconds (mean sampled reward: -6891.09). Current reward after update: -1605.35, Optimal reward -1605.35
Iteration 34 took 2.23 seconds (mean sampled reward: -6475.40). Current reward after update: -2022.96, Optimal reward -1605.35
Iteration 35 took 2.25 seconds (mean sampled reward: -6791.31). Current reward after update: -1691.53, Optimal reward -1605.35
Iteration 36 took 2.21 seconds (mean sampled reward: -6616.57). Current reward after update: -1585.24, Optimal reward -1585.24
Iteration 37 took 2.31 seconds (mean sampled reward: -6478.86). Current reward after update: -1719.36, Optimal reward -1585.24
Iteration 38 took 2.31 seconds (mean sampled reward: -5682.06). Current reward after update: -1456.23, Optimal reward -1456.23
Iteration 39 took 2.16 seconds (mean sampled reward: -6323.74). Current reward after update: -5432.84, Optimal reward -1456.23
Iteration 40 took 2.30 seconds (mean sampled reward: -6317.80). Current reward after update: -1308.00, Optimal reward -1308.00
Iteration 41 took 2.14 seconds (mean sampled reward: -5991.44). Current reward after update: -1362.55, Optimal reward -1308.00
Iteration 42 took 2.40 seconds (mean sampled reward: -5672.16). Current reward after update: -1396.48, Optimal reward -1308.00
Iteration 43 took 2.23 seconds (mean sampled reward: -5896.26). Current reward after update: -1251.10, Optimal reward -1251.10
Iteration 44 took 2.11 seconds (mean sampled reward: -5752.72). Current reward after update: -2149.30, Optimal reward -1251.10
Iteration 45 took 2.16 seconds (mean sampled reward: -6603.99). Current reward after update: -1558.14, Optimal reward -1251.10
Iteration 46 took 2.19 seconds (mean sampled reward: -5635.58). Current reward after update: -1363.38, Optimal reward -1251.10
Iteration 47 took 2.14 seconds (mean sampled reward: -6275.23). Current reward after update: -1412.14, Optimal reward -1251.10
Iteration 48 took 2.16 seconds (mean sampled reward: -6934.90). Current reward after update: -1403.50, Optimal reward -1251.10
Iteration 49 took 2.15 seconds (mean sampled reward: -6884.10). Current reward after update: -1697.68, Optimal reward -1251.10
Iteration 50 took 2.30 seconds (mean sampled reward: -5986.16). Current reward after update: -1338.74, Optimal reward -1251.10
Iteration 51 took 2.25 seconds (mean sampled reward: -6280.38). Current reward after update: -1591.51, Optimal reward -1251.10
Iteration 52 took 2.16 seconds (mean sampled reward: -5580.21). Current reward after update: -1369.40, Optimal reward -1251.10
Iteration 53 took 2.28 seconds (mean sampled reward: -6480.72). Current reward after update: -1370.89, Optimal reward -1251.10
Iteration 54 took 2.14 seconds (mean sampled reward: -5480.26). Current reward after update: -1375.85, Optimal reward -1251.10
Iteration 55 took 2.27 seconds (mean sampled reward: -5812.16). Current reward after update: -1437.53, Optimal reward -1251.10
Iteration 56 took 2.20 seconds (mean sampled reward: -6144.71). Current reward after update: -1429.82, Optimal reward -1251.10
Iteration 57 took 2.13 seconds (mean sampled reward: -6941.11). Current reward after update: -1542.84, Optimal reward -1251.10
Iteration 58 took 2.11 seconds (mean sampled reward: -6405.78). Current reward after update: -1506.01, Optimal reward -1251.10
Iteration 59 took 2.26 seconds (mean sampled reward: -7151.33). Current reward after update: -1887.93, Optimal reward -1251.10
Iteration 60 took 2.19 seconds (mean sampled reward: -6815.08). Current reward after update: -1855.14, Optimal reward -1251.10
Iteration 61 took 2.12 seconds (mean sampled reward: -6925.17). Current reward after update: -1733.50, Optimal reward -1251.10
Iteration 62 took 2.25 seconds (mean sampled reward: -6512.88). Current reward after update: -1769.75, Optimal reward -1251.10
Iteration 63 took 2.19 seconds (mean sampled reward: -6216.93). Current reward after update: -1884.49, Optimal reward -1251.10
Iteration 64 took 2.14 seconds (mean sampled reward: -5222.92). Current reward after update: -1492.32, Optimal reward -1251.10
Iteration 65 took 2.17 seconds (mean sampled reward: -5718.53). Current reward after update: -1292.84, Optimal reward -1251.10
Iteration 66 took 2.26 seconds (mean sampled reward: -4918.83). Current reward after update: -1318.94, Optimal reward -1251.10
Iteration 67 took 2.22 seconds (mean sampled reward: -5181.02). Current reward after update: -2793.60, Optimal reward -1251.10
Iteration 68 took 2.40 seconds (mean sampled reward: -4121.59). Current reward after update: -1077.94, Optimal reward -1077.94
Iteration 69 took 2.19 seconds (mean sampled reward: -3701.03). Current reward after update: -2461.09, Optimal reward -1077.94
Iteration 70 took 2.25 seconds (mean sampled reward: -4113.45). Current reward after update: -1016.55, Optimal reward -1016.55
Iteration 71 took 2.20 seconds (mean sampled reward: -3465.01). Current reward after update: -1080.21, Optimal reward -1016.55
Iteration 72 took 2.16 seconds (mean sampled reward: -3623.20). Current reward after update: -997.92, Optimal reward -997.92
Iteration 73 took 2.25 seconds (mean sampled reward: -5239.47). Current reward after update: -980.60, Optimal reward -980.60
Iteration 74 took 2.23 seconds (mean sampled reward: -5410.12). Current reward after update: -976.77, Optimal reward -976.77
Iteration 75 took 2.22 seconds (mean sampled reward: -4908.01). Current reward after update: -866.54, Optimal reward -866.54
Iteration 76 took 2.24 seconds (mean sampled reward: -5787.42). Current reward after update: -811.45, Optimal reward -811.45
Iteration 77 took 2.19 seconds (mean sampled reward: -4116.31). Current reward after update: -754.49, Optimal reward -754.49
Iteration 78 took 2.23 seconds (mean sampled reward: -4921.48). Current reward after update: -779.57, Optimal reward -754.49
Iteration 79 took 2.23 seconds (mean sampled reward: -4648.49). Current reward after update: -739.58, Optimal reward -739.58
Iteration 80 took 2.21 seconds (mean sampled reward: -4706.40). Current reward after update: -855.66, Optimal reward -739.58
Iteration 81 took 2.24 seconds (mean sampled reward: -3799.13). Current reward after update: -721.59, Optimal reward -721.59
Iteration 82 took 2.18 seconds (mean sampled reward: -3894.43). Current reward after update: -662.60, Optimal reward -662.60
Iteration 83 took 2.23 seconds (mean sampled reward: -3922.29). Current reward after update: -1045.79, Optimal reward -662.60
Iteration 84 took 2.23 seconds (mean sampled reward: -4797.10). Current reward after update: -809.81, Optimal reward -662.60
Iteration 85 took 2.20 seconds (mean sampled reward: -4221.32). Current reward after update: -710.14, Optimal reward -662.60
Iteration 86 took 2.29 seconds (mean sampled reward: -5124.72). Current reward after update: -642.90, Optimal reward -642.90
Iteration 87 took 2.32 seconds (mean sampled reward: -5210.06). Current reward after update: -641.34, Optimal reward -641.34
Iteration 88 took 2.26 seconds (mean sampled reward: -5106.04). Current reward after update: -682.93, Optimal reward -641.34
Iteration 89 took 2.31 seconds (mean sampled reward: -5888.98). Current reward after update: -1266.22, Optimal reward -641.34
Iteration 90 took 2.26 seconds (mean sampled reward: -5253.66). Current reward after update: -644.62, Optimal reward -641.34
Iteration 91 took 2.29 seconds (mean sampled reward: -4985.83). Current reward after update: -720.77, Optimal reward -641.34
Iteration 92 took 2.17 seconds (mean sampled reward: -3287.86). Current reward after update: -1549.32, Optimal reward -641.34
Iteration 93 took 2.22 seconds (mean sampled reward: -3776.82). Current reward after update: -665.12, Optimal reward -641.34
Iteration 94 took 2.17 seconds (mean sampled reward: -4226.04). Current reward after update: -691.05, Optimal reward -641.34
Iteration 95 took 2.29 seconds (mean sampled reward: -3665.09). Current reward after update: -675.41, Optimal reward -641.34
Iteration 96 took 2.32 seconds (mean sampled reward: -5060.96). Current reward after update: -1099.18, Optimal reward -641.34
Iteration 97 took 2.29 seconds (mean sampled reward: -5517.96). Current reward after update: -727.05, Optimal reward -641.34
Iteration 98 took 2.22 seconds (mean sampled reward: -4854.94). Current reward after update: -656.66, Optimal reward -641.34
Iteration 99 took 2.21 seconds (mean sampled reward: -4020.18). Current reward after update: -766.40, Optimal reward -641.34
Iteration 100 took 2.15 seconds (mean sampled reward: -3209.70). Current reward after update: -691.62, Optimal reward -641.34
Iteration 101 took 2.22 seconds (mean sampled reward: -3595.55). Current reward after update: -787.20, Optimal reward -641.34
Iteration 102 took 2.24 seconds (mean sampled reward: -3294.39). Current reward after update: -773.03, Optimal reward -641.34
Iteration 103 took 2.22 seconds (mean sampled reward: -4138.84). Current reward after update: -775.11, Optimal reward -641.34
Iteration 104 took 2.17 seconds (mean sampled reward: -2991.21). Current reward after update: -2044.54, Optimal reward -641.34
Iteration 105 took 2.17 seconds (mean sampled reward: -3114.53). Current reward after update: -645.06, Optimal reward -641.34
Iteration 106 took 2.18 seconds (mean sampled reward: -2608.75). Current reward after update: -691.79, Optimal reward -641.34
Iteration 107 took 2.10 seconds (mean sampled reward: -2988.56). Current reward after update: -1145.83, Optimal reward -641.34
Iteration 108 took 2.17 seconds (mean sampled reward: -2343.36). Current reward after update: -636.39, Optimal reward -636.39
Iteration 109 took 2.22 seconds (mean sampled reward: -2977.15). Current reward after update: -1506.43, Optimal reward -636.39
Iteration 110 took 2.12 seconds (mean sampled reward: -3155.75). Current reward after update: -1562.66, Optimal reward -636.39
Iteration 111 took 2.22 seconds (mean sampled reward: -3089.56). Current reward after update: -698.64, Optimal reward -636.39
Iteration 112 took 2.21 seconds (mean sampled reward: -2510.14). Current reward after update: -621.64, Optimal reward -621.64
Iteration 113 took 2.16 seconds (mean sampled reward: -3095.64). Current reward after update: -809.10, Optimal reward -621.64
Iteration 114 took 2.23 seconds (mean sampled reward: -4591.30). Current reward after update: -1336.65, Optimal reward -621.64
Iteration 115 took 2.31 seconds (mean sampled reward: -4598.17). Current reward after update: -783.27, Optimal reward -621.64
Iteration 116 took 2.14 seconds (mean sampled reward: -2968.31). Current reward after update: -810.69, Optimal reward -621.64
Iteration 117 took 2.24 seconds (mean sampled reward: -2727.15). Current reward after update: -802.26, Optimal reward -621.64
Iteration 118 took 2.17 seconds (mean sampled reward: -4048.26). Current reward after update: -1369.11, Optimal reward -621.64
Iteration 119 took 2.17 seconds (mean sampled reward: -5509.84). Current reward after update: -815.11, Optimal reward -621.64
Iteration 120 took 2.32 seconds (mean sampled reward: -5189.10). Current reward after update: -854.32, Optimal reward -621.64
Iteration 121 took 2.16 seconds (mean sampled reward: -4473.44). Current reward after update: -1186.26, Optimal reward -621.64
Iteration 122 took 2.08 seconds (mean sampled reward: -4260.41). Current reward after update: -648.17, Optimal reward -621.64
Iteration 123 took 2.11 seconds (mean sampled reward: -3414.20). Current reward after update: -736.01, Optimal reward -621.64
Iteration 124 took 2.13 seconds (mean sampled reward: -4938.50). Current reward after update: -718.94, Optimal reward -621.64
Iteration 125 took 2.14 seconds (mean sampled reward: -4879.04). Current reward after update: -644.61, Optimal reward -621.64
Iteration 126 took 2.13 seconds (mean sampled reward: -3579.06). Current reward after update: -878.07, Optimal reward -621.64
Iteration 127 took 2.11 seconds (mean sampled reward: -3388.96). Current reward after update: -6831.81, Optimal reward -621.64
Iteration 128 took 2.14 seconds (mean sampled reward: -4629.85). Current reward after update: -710.66, Optimal reward -621.64
Iteration 129 took 2.13 seconds (mean sampled reward: -4815.78). Current reward after update: -858.98, Optimal reward -621.64
Iteration 130 took 2.07 seconds (mean sampled reward: -4216.65). Current reward after update: -861.87, Optimal reward -621.64
Iteration 131 took 2.10 seconds (mean sampled reward: -4479.01). Current reward after update: -935.48, Optimal reward -621.64
Iteration 132 took 2.12 seconds (mean sampled reward: -3787.94). Current reward after update: -860.40, Optimal reward -621.64
Iteration 133 took 2.22 seconds (mean sampled reward: -4557.60). Current reward after update: -879.89, Optimal reward -621.64
Iteration 134 took 2.15 seconds (mean sampled reward: -5260.96). Current reward after update: -897.40, Optimal reward -621.64
Iteration 135 took 2.21 seconds (mean sampled reward: -5733.47). Current reward after update: -902.66, Optimal reward -621.64
Iteration 136 took 2.15 seconds (mean sampled reward: -4111.61). Current reward after update: -776.69, Optimal reward -621.64
Iteration 137 took 2.15 seconds (mean sampled reward: -4628.75). Current reward after update: -846.67, Optimal reward -621.64
Iteration 138 took 2.14 seconds (mean sampled reward: -4487.64). Current reward after update: -1537.86, Optimal reward -621.64
Iteration 139 took 2.12 seconds (mean sampled reward: -3632.11). Current reward after update: -810.08, Optimal reward -621.64
Iteration 140 took 2.12 seconds (mean sampled reward: -2933.38). Current reward after update: -1421.30, Optimal reward -621.64
Iteration 141 took 2.09 seconds (mean sampled reward: -3841.10). Current reward after update: -670.07, Optimal reward -621.64
Iteration 142 took 2.14 seconds (mean sampled reward: -4824.00). Current reward after update: -839.42, Optimal reward -621.64
Iteration 143 took 2.18 seconds (mean sampled reward: -5447.68). Current reward after update: -784.19, Optimal reward -621.64
Iteration 144 took 2.14 seconds (mean sampled reward: -5832.81). Current reward after update: -766.91, Optimal reward -621.64
Iteration 145 took 2.17 seconds (mean sampled reward: -5736.37). Current reward after update: -1615.45, Optimal reward -621.64
Iteration 146 took 2.16 seconds (mean sampled reward: -5642.36). Current reward after update: -795.81, Optimal reward -621.64
Iteration 147 took 2.15 seconds (mean sampled reward: -5486.60). Current reward after update: -836.55, Optimal reward -621.64
Iteration 148 took 2.13 seconds (mean sampled reward: -4888.05). Current reward after update: -883.47, Optimal reward -621.64
Iteration 149 took 2.19 seconds (mean sampled reward: -5683.86). Current reward after update: -1000.55, Optimal reward -621.64
Iteration 150 took 2.20 seconds (mean sampled reward: -4980.44). Current reward after update: -1000.33, Optimal reward -621.64
Iteration 151 took 2.15 seconds (mean sampled reward: -3171.33). Current reward after update: -843.14, Optimal reward -621.64
Iteration 152 took 2.06 seconds (mean sampled reward: -2714.44). Current reward after update: -806.87, Optimal reward -621.64
Iteration 153 took 2.14 seconds (mean sampled reward: -4332.54). Current reward after update: -707.18, Optimal reward -621.64
Iteration 154 took 2.21 seconds (mean sampled reward: -4467.20). Current reward after update: -984.57, Optimal reward -621.64
Iteration 155 took 2.26 seconds (mean sampled reward: -5053.51). Current reward after update: -597.25, Optimal reward -597.25
Iteration 156 took 2.26 seconds (mean sampled reward: -4972.99). Current reward after update: -680.27, Optimal reward -597.25
Iteration 157 took 2.20 seconds (mean sampled reward: -3892.91). Current reward after update: -701.94, Optimal reward -597.25
Iteration 158 took 2.23 seconds (mean sampled reward: -4514.08). Current reward after update: -653.43, Optimal reward -597.25
Iteration 159 took 2.28 seconds (mean sampled reward: -5610.72). Current reward after update: -787.68, Optimal reward -597.25
Iteration 160 took 2.31 seconds (mean sampled reward: -5679.39). Current reward after update: -6856.22, Optimal reward -597.25
Iteration 161 took 2.25 seconds (mean sampled reward: -4024.47). Current reward after update: -654.08, Optimal reward -597.25
Iteration 162 took 2.19 seconds (mean sampled reward: -3552.73). Current reward after update: -706.36, Optimal reward -597.25
Iteration 163 took 2.16 seconds (mean sampled reward: -3391.42). Current reward after update: -881.57, Optimal reward -597.25
Iteration 164 took 2.19 seconds (mean sampled reward: -4282.13). Current reward after update: -715.20, Optimal reward -597.25
Iteration 165 took 2.11 seconds (mean sampled reward: -3627.58). Current reward after update: -701.27, Optimal reward -597.25
Iteration 166 took 2.24 seconds (mean sampled reward: -4911.12). Current reward after update: -687.31, Optimal reward -597.25
Iteration 167 took 2.26 seconds (mean sampled reward: -4502.01). Current reward after update: -2468.46, Optimal reward -597.25
Iteration 168 took 2.22 seconds (mean sampled reward: -4795.56). Current reward after update: -755.11, Optimal reward -597.25
Iteration 169 took 2.25 seconds (mean sampled reward: -5256.91). Current reward after update: -776.92, Optimal reward -597.25
Iteration 170 took 2.27 seconds (mean sampled reward: -5475.60). Current reward after update: -720.17, Optimal reward -597.25
Iteration 171 took 2.26 seconds (mean sampled reward: -6159.75). Current reward after update: -779.39, Optimal reward -597.25
Iteration 172 took 2.29 seconds (mean sampled reward: -5031.92). Current reward after update: -2595.07, Optimal reward -597.25
Iteration 173 took 2.21 seconds (mean sampled reward: -4093.85). Current reward after update: -604.75, Optimal reward -597.25
Iteration 174 took 2.27 seconds (mean sampled reward: -4614.96). Current reward after update: -678.89, Optimal reward -597.25
Iteration 175 took 2.19 seconds (mean sampled reward: -4667.03). Current reward after update: -689.39, Optimal reward -597.25
Iteration 176 took 2.25 seconds (mean sampled reward: -4658.84). Current reward after update: -715.52, Optimal reward -597.25
Iteration 177 took 2.31 seconds (mean sampled reward: -4967.97). Current reward after update: -759.34, Optimal reward -597.25
Iteration 178 took 2.25 seconds (mean sampled reward: -4462.33). Current reward after update: -694.48, Optimal reward -597.25
Iteration 179 took 2.24 seconds (mean sampled reward: -5465.31). Current reward after update: -2829.35, Optimal reward -597.25
Iteration 180 took 2.13 seconds (mean sampled reward: -2868.66). Current reward after update: -927.60, Optimal reward -597.25
Iteration 181 took 2.16 seconds (mean sampled reward: -2443.44). Current reward after update: -1467.83, Optimal reward -597.25
Iteration 182 took 2.11 seconds (mean sampled reward: -2654.78). Current reward after update: -757.58, Optimal reward -597.25
Iteration 183 took 2.14 seconds (mean sampled reward: -3204.02). Current reward after update: -700.80, Optimal reward -597.25
Iteration 184 took 2.22 seconds (mean sampled reward: -4624.45). Current reward after update: -679.07, Optimal reward -597.25
Iteration 185 took 2.23 seconds (mean sampled reward: -4103.79). Current reward after update: -798.98, Optimal reward -597.25
Iteration 186 took 2.17 seconds (mean sampled reward: -2830.70). Current reward after update: -792.06, Optimal reward -597.25
Iteration 187 took 2.25 seconds (mean sampled reward: -2592.26). Current reward after update: -858.06, Optimal reward -597.25
Iteration 188 took 2.17 seconds (mean sampled reward: -3795.39). Current reward after update: -840.85, Optimal reward -597.25
Iteration 189 took 2.15 seconds (mean sampled reward: -4743.78). Current reward after update: -681.66, Optimal reward -597.25
Iteration 190 took 2.13 seconds (mean sampled reward: -3883.20). Current reward after update: -904.54, Optimal reward -597.25
Iteration 191 took 2.19 seconds (mean sampled reward: -3397.86). Current reward after update: -3335.07, Optimal reward -597.25
Iteration 192 took 2.14 seconds (mean sampled reward: -3182.93). Current reward after update: -777.43, Optimal reward -597.25
Iteration 193 took 2.12 seconds (mean sampled reward: -3561.51). Current reward after update: -940.65, Optimal reward -597.25
Iteration 194 took 2.21 seconds (mean sampled reward: -5994.91). Current reward after update: -1598.79, Optimal reward -597.25
Iteration 195 took 2.20 seconds (mean sampled reward: -4572.12). Current reward after update: -721.64, Optimal reward -597.25
Iteration 196 took 2.22 seconds (mean sampled reward: -3977.61). Current reward after update: -715.20, Optimal reward -597.25
Iteration 197 took 2.20 seconds (mean sampled reward: -4623.94). Current reward after update: -2255.19, Optimal reward -597.25
Iteration 198 took 2.23 seconds (mean sampled reward: -4922.48). Current reward after update: -936.71, Optimal reward -597.25
Iteration 199 took 2.25 seconds (mean sampled reward: -4917.73). Current reward after update: -1093.28, Optimal reward -597.25
Iteration 200 took 2.17 seconds (mean sampled reward: -2499.81). Current reward after update: -837.64, Optimal reward -597.25
Iteration 1 took 2.45 seconds (mean sampled reward: -7632.41). Current reward after update: -7630.63, Optimal reward -7630.63
Iteration 2 took 2.31 seconds (mean sampled reward: -7615.45). Current reward after update: -7386.22, Optimal reward -7386.22
Iteration 3 took 2.35 seconds (mean sampled reward: -7537.44). Current reward after update: -6716.36, Optimal reward -6716.36
Iteration 4 took 2.18 seconds (mean sampled reward: -7477.22). Current reward after update: -6451.58, Optimal reward -6451.58
Iteration 5 took 2.23 seconds (mean sampled reward: -7385.80). Current reward after update: -4376.93, Optimal reward -4376.93
Iteration 6 took 2.27 seconds (mean sampled reward: -7136.64). Current reward after update: -3679.17, Optimal reward -3679.17
Iteration 7 took 2.27 seconds (mean sampled reward: -6837.09). Current reward after update: -3283.46, Optimal reward -3283.46
Iteration 8 took 2.25 seconds (mean sampled reward: -5773.88). Current reward after update: -2901.87, Optimal reward -2901.87
Iteration 9 took 2.20 seconds (mean sampled reward: -4808.93). Current reward after update: -2650.55, Optimal reward -2650.55
Iteration 10 took 2.22 seconds (mean sampled reward: -4836.61). Current reward after update: -2380.16, Optimal reward -2380.16
Iteration 11 took 2.30 seconds (mean sampled reward: -4302.56). Current reward after update: -2240.15, Optimal reward -2240.15
Iteration 12 took 2.34 seconds (mean sampled reward: -3874.62). Current reward after update: -2200.16, Optimal reward -2200.16
Iteration 13 took 2.23 seconds (mean sampled reward: -4711.55). Current reward after update: -2338.09, Optimal reward -2200.16
Iteration 14 took 2.31 seconds (mean sampled reward: -5235.05). Current reward after update: -2068.59, Optimal reward -2068.59
Iteration 15 took 2.33 seconds (mean sampled reward: -6366.55). Current reward after update: -2258.03, Optimal reward -2068.59
Iteration 16 took 2.37 seconds (mean sampled reward: -6326.77). Current reward after update: -2064.73, Optimal reward -2064.73
Iteration 17 took 2.23 seconds (mean sampled reward: -6028.90). Current reward after update: -2064.25, Optimal reward -2064.25
Iteration 18 took 2.26 seconds (mean sampled reward: -5514.56). Current reward after update: -2096.43, Optimal reward -2064.25
Iteration 19 took 2.21 seconds (mean sampled reward: -5635.27). Current reward after update: -1727.81, Optimal reward -1727.81
Iteration 20 took 2.21 seconds (mean sampled reward: -4659.66). Current reward after update: -1461.39, Optimal reward -1461.39
Iteration 21 took 2.13 seconds (mean sampled reward: -4223.48). Current reward after update: -1370.05, Optimal reward -1370.05
Iteration 22 took 2.16 seconds (mean sampled reward: -4867.49). Current reward after update: -1193.87, Optimal reward -1193.87
Iteration 23 took 2.16 seconds (mean sampled reward: -4651.16). Current reward after update: -1214.98, Optimal reward -1193.87
Iteration 24 took 2.25 seconds (mean sampled reward: -4694.01). Current reward after update: -1336.31, Optimal reward -1193.87
Iteration 25 took 2.28 seconds (mean sampled reward: -4210.45). Current reward after update: -1463.93, Optimal reward -1193.87
Iteration 26 took 2.31 seconds (mean sampled reward: -4370.61). Current reward after update: -1574.06, Optimal reward -1193.87
Iteration 27 took 2.21 seconds (mean sampled reward: -4453.13). Current reward after update: -1342.24, Optimal reward -1193.87
Iteration 28 took 2.24 seconds (mean sampled reward: -4303.92). Current reward after update: -1495.57, Optimal reward -1193.87
Iteration 29 took 2.31 seconds (mean sampled reward: -4762.32). Current reward after update: -1449.62, Optimal reward -1193.87
Iteration 30 took 2.20 seconds (mean sampled reward: -4635.45). Current reward after update: -1360.36, Optimal reward -1193.87
Iteration 31 took 2.17 seconds (mean sampled reward: -5041.00). Current reward after update: -1210.49, Optimal reward -1193.87
Iteration 32 took 2.13 seconds (mean sampled reward: -5415.67). Current reward after update: -1245.21, Optimal reward -1193.87
Iteration 33 took 2.41 seconds (mean sampled reward: -4504.81). Current reward after update: -1110.75, Optimal reward -1110.75
Iteration 34 took 2.24 seconds (mean sampled reward: -4413.08). Current reward after update: -2175.86, Optimal reward -1110.75
Iteration 35 took 2.21 seconds (mean sampled reward: -4687.59). Current reward after update: -1106.55, Optimal reward -1106.55
Iteration 36 took 2.22 seconds (mean sampled reward: -3632.60). Current reward after update: -1081.39, Optimal reward -1081.39
Iteration 37 took 2.15 seconds (mean sampled reward: -2917.00). Current reward after update: -1094.45, Optimal reward -1081.39
Iteration 38 took 2.20 seconds (mean sampled reward: -2942.94). Current reward after update: -2229.18, Optimal reward -1081.39
Iteration 39 took 2.15 seconds (mean sampled reward: -3723.03). Current reward after update: -867.11, Optimal reward -867.11
Iteration 40 took 2.17 seconds (mean sampled reward: -3192.04). Current reward after update: -1064.96, Optimal reward -867.11
Iteration 41 took 2.20 seconds (mean sampled reward: -4025.02). Current reward after update: -982.57, Optimal reward -867.11
Iteration 42 took 2.21 seconds (mean sampled reward: -3706.87). Current reward after update: -1051.98, Optimal reward -867.11
Iteration 43 took 2.17 seconds (mean sampled reward: -3962.73). Current reward after update: -932.36, Optimal reward -867.11
Iteration 44 took 2.16 seconds (mean sampled reward: -4428.13). Current reward after update: -1144.79, Optimal reward -867.11
Iteration 45 took 2.20 seconds (mean sampled reward: -5099.53). Current reward after update: -1254.85, Optimal reward -867.11
Iteration 46 took 2.21 seconds (mean sampled reward: -5252.56). Current reward after update: -1090.95, Optimal reward -867.11
Iteration 47 took 2.35 seconds (mean sampled reward: -4616.55). Current reward after update: -1079.88, Optimal reward -867.11
Iteration 48 took 2.23 seconds (mean sampled reward: -4477.41). Current reward after update: -1041.15, Optimal reward -867.11
Iteration 49 took 2.26 seconds (mean sampled reward: -4210.30). Current reward after update: -869.38, Optimal reward -867.11
Iteration 50 took 2.20 seconds (mean sampled reward: -4418.44). Current reward after update: -742.66, Optimal reward -742.66
Iteration 51 took 2.23 seconds (mean sampled reward: -4684.81). Current reward after update: -862.58, Optimal reward -742.66
Iteration 52 took 2.24 seconds (mean sampled reward: -4983.57). Current reward after update: -1031.90, Optimal reward -742.66
Iteration 53 took 2.28 seconds (mean sampled reward: -5224.12). Current reward after update: -1033.12, Optimal reward -742.66
Iteration 54 took 2.22 seconds (mean sampled reward: -5146.23). Current reward after update: -1140.99, Optimal reward -742.66
Iteration 55 took 2.17 seconds (mean sampled reward: -5073.59). Current reward after update: -1382.54, Optimal reward -742.66
Iteration 56 took 2.35 seconds (mean sampled reward: -5449.96). Current reward after update: -1550.76, Optimal reward -742.66
Iteration 57 took 2.46 seconds (mean sampled reward: -5725.95). Current reward after update: -1465.31, Optimal reward -742.66
Iteration 58 took 2.23 seconds (mean sampled reward: -4073.90). Current reward after update: -1273.63, Optimal reward -742.66
Iteration 59 took 2.35 seconds (mean sampled reward: -4538.55). Current reward after update: -2022.54, Optimal reward -742.66
Iteration 60 took 2.30 seconds (mean sampled reward: -4547.21). Current reward after update: -1299.41, Optimal reward -742.66
Iteration 61 took 2.35 seconds (mean sampled reward: -4412.39). Current reward after update: -1468.50, Optimal reward -742.66
Iteration 62 took 2.29 seconds (mean sampled reward: -3787.12). Current reward after update: -2046.49, Optimal reward -742.66
Iteration 63 took 2.18 seconds (mean sampled reward: -4001.60). Current reward after update: -1487.21, Optimal reward -742.66
Iteration 64 took 2.23 seconds (mean sampled reward: -4376.80). Current reward after update: -1241.69, Optimal reward -742.66
Iteration 65 took 2.23 seconds (mean sampled reward: -4312.80). Current reward after update: -1390.97, Optimal reward -742.66
Iteration 66 took 2.26 seconds (mean sampled reward: -4124.03). Current reward after update: -1180.50, Optimal reward -742.66
Iteration 67 took 2.37 seconds (mean sampled reward: -4302.07). Current reward after update: -1379.21, Optimal reward -742.66
Iteration 68 took 2.23 seconds (mean sampled reward: -4515.76). Current reward after update: -1209.21, Optimal reward -742.66
Iteration 69 took 2.48 seconds (mean sampled reward: -3242.15). Current reward after update: -1078.14, Optimal reward -742.66
Iteration 70 took 2.23 seconds (mean sampled reward: -3585.16). Current reward after update: -964.44, Optimal reward -742.66
Iteration 71 took 2.28 seconds (mean sampled reward: -4037.65). Current reward after update: -980.04, Optimal reward -742.66
Iteration 72 took 2.22 seconds (mean sampled reward: -4293.37). Current reward after update: -1152.84, Optimal reward -742.66
Iteration 73 took 2.24 seconds (mean sampled reward: -5156.48). Current reward after update: -937.90, Optimal reward -742.66
Iteration 74 took 2.21 seconds (mean sampled reward: -5385.01). Current reward after update: -1167.82, Optimal reward -742.66
Iteration 75 took 2.25 seconds (mean sampled reward: -5147.71). Current reward after update: -1017.53, Optimal reward -742.66
Iteration 76 took 2.25 seconds (mean sampled reward: -5593.35). Current reward after update: -1577.87, Optimal reward -742.66
Iteration 77 took 2.27 seconds (mean sampled reward: -4812.57). Current reward after update: -1195.03, Optimal reward -742.66
Iteration 78 took 2.31 seconds (mean sampled reward: -4899.81). Current reward after update: -1321.23, Optimal reward -742.66
Iteration 79 took 2.27 seconds (mean sampled reward: -4920.59). Current reward after update: -1296.01, Optimal reward -742.66
Iteration 80 took 2.24 seconds (mean sampled reward: -4755.82). Current reward after update: -1793.82, Optimal reward -742.66
Iteration 81 took 2.27 seconds (mean sampled reward: -5457.56). Current reward after update: -1336.72, Optimal reward -742.66
Iteration 82 took 2.29 seconds (mean sampled reward: -5336.44). Current reward after update: -1618.64, Optimal reward -742.66
Iteration 83 took 2.21 seconds (mean sampled reward: -4752.77). Current reward after update: -1486.22, Optimal reward -742.66
Iteration 84 took 2.33 seconds (mean sampled reward: -4618.45). Current reward after update: -1545.12, Optimal reward -742.66
Iteration 85 took 2.28 seconds (mean sampled reward: -4689.51). Current reward after update: -1612.89, Optimal reward -742.66
Iteration 86 took 2.30 seconds (mean sampled reward: -4611.60). Current reward after update: -1949.25, Optimal reward -742.66
Iteration 87 took 2.33 seconds (mean sampled reward: -4150.29). Current reward after update: -1377.68, Optimal reward -742.66
Iteration 88 took 2.28 seconds (mean sampled reward: -4400.99). Current reward after update: -1434.69, Optimal reward -742.66
Iteration 89 took 2.31 seconds (mean sampled reward: -4455.51). Current reward after update: -2056.79, Optimal reward -742.66
Iteration 90 took 2.33 seconds (mean sampled reward: -3871.39). Current reward after update: -1363.35, Optimal reward -742.66
Iteration 91 took 2.37 seconds (mean sampled reward: -3561.60). Current reward after update: -1414.44, Optimal reward -742.66
Iteration 92 took 2.38 seconds (mean sampled reward: -3126.03). Current reward after update: -1797.00, Optimal reward -742.66
Iteration 93 took 2.41 seconds (mean sampled reward: -4414.48). Current reward after update: -1458.65, Optimal reward -742.66
Iteration 94 took 2.44 seconds (mean sampled reward: -3314.07). Current reward after update: -1517.61, Optimal reward -742.66
Iteration 95 took 2.28 seconds (mean sampled reward: -2754.07). Current reward after update: -1383.43, Optimal reward -742.66
Iteration 96 took 2.24 seconds (mean sampled reward: -2823.48). Current reward after update: -2930.27, Optimal reward -742.66
Iteration 97 took 2.36 seconds (mean sampled reward: -4171.40). Current reward after update: -1333.37, Optimal reward -742.66
Iteration 98 took 2.27 seconds (mean sampled reward: -3313.47). Current reward after update: -1633.94, Optimal reward -742.66
Iteration 99 took 2.34 seconds (mean sampled reward: -2836.21). Current reward after update: -1315.79, Optimal reward -742.66
Iteration 100 took 2.34 seconds (mean sampled reward: -2888.70). Current reward after update: -2515.35, Optimal reward -742.66
Iteration 101 took 2.21 seconds (mean sampled reward: -3255.77). Current reward after update: -1371.50, Optimal reward -742.66
Iteration 102 took 2.26 seconds (mean sampled reward: -3121.82). Current reward after update: -1284.83, Optimal reward -742.66
Iteration 103 took 2.31 seconds (mean sampled reward: -3499.57). Current reward after update: -1212.96, Optimal reward -742.66
Iteration 104 took 2.30 seconds (mean sampled reward: -4139.58). Current reward after update: -1329.21, Optimal reward -742.66
Iteration 105 took 2.30 seconds (mean sampled reward: -3736.93). Current reward after update: -1303.08, Optimal reward -742.66
Iteration 106 took 2.41 seconds (mean sampled reward: -4001.73). Current reward after update: -1286.27, Optimal reward -742.66
Iteration 107 took 2.53 seconds (mean sampled reward: -4125.22). Current reward after update: -1270.01, Optimal reward -742.66
Iteration 108 took 2.47 seconds (mean sampled reward: -3957.07). Current reward after update: -1376.55, Optimal reward -742.66
Iteration 109 took 2.26 seconds (mean sampled reward: -3472.76). Current reward after update: -1441.04, Optimal reward -742.66
Iteration 110 took 2.40 seconds (mean sampled reward: -3102.31). Current reward after update: -1317.11, Optimal reward -742.66
Iteration 111 took 2.40 seconds (mean sampled reward: -3389.51). Current reward after update: -1323.40, Optimal reward -742.66
Iteration 112 took 2.39 seconds (mean sampled reward: -3005.71). Current reward after update: -5364.28, Optimal reward -742.66
Iteration 113 took 2.37 seconds (mean sampled reward: -3722.10). Current reward after update: -1087.24, Optimal reward -742.66
Iteration 114 took 2.57 seconds (mean sampled reward: -3919.60). Current reward after update: -5318.97, Optimal reward -742.66
Iteration 115 took 2.39 seconds (mean sampled reward: -3976.18). Current reward after update: -1214.10, Optimal reward -742.66
Iteration 116 took 2.41 seconds (mean sampled reward: -4311.92). Current reward after update: -5495.41, Optimal reward -742.66
Iteration 117 took 2.37 seconds (mean sampled reward: -4239.06). Current reward after update: -1230.89, Optimal reward -742.66
Iteration 118 took 2.40 seconds (mean sampled reward: -4803.71). Current reward after update: -1197.64, Optimal reward -742.66
Iteration 119 took 2.34 seconds (mean sampled reward: -4983.09). Current reward after update: -5457.51, Optimal reward -742.66
Iteration 120 took 2.30 seconds (mean sampled reward: -5003.75). Current reward after update: -2125.79, Optimal reward -742.66
Iteration 121 took 2.30 seconds (mean sampled reward: -4746.72). Current reward after update: -5632.63, Optimal reward -742.66
Iteration 122 took 2.29 seconds (mean sampled reward: -5141.49). Current reward after update: -1169.61, Optimal reward -742.66
Iteration 123 took 2.42 seconds (mean sampled reward: -5121.57). Current reward after update: -1299.37, Optimal reward -742.66
Iteration 124 took 2.38 seconds (mean sampled reward: -5190.50). Current reward after update: -1189.68, Optimal reward -742.66
Iteration 125 took 2.31 seconds (mean sampled reward: -4958.21). Current reward after update: -1257.98, Optimal reward -742.66
Iteration 126 took 2.29 seconds (mean sampled reward: -4521.09). Current reward after update: -5125.04, Optimal reward -742.66
Iteration 127 took 2.37 seconds (mean sampled reward: -4529.78). Current reward after update: -1044.96, Optimal reward -742.66
Iteration 128 took 2.34 seconds (mean sampled reward: -4009.04). Current reward after update: -1004.55, Optimal reward -742.66
Iteration 129 took 2.31 seconds (mean sampled reward: -4110.41). Current reward after update: -1060.63, Optimal reward -742.66
Iteration 130 took 2.29 seconds (mean sampled reward: -4115.69). Current reward after update: -1019.15, Optimal reward -742.66
Iteration 131 took 2.29 seconds (mean sampled reward: -4248.97). Current reward after update: -5714.62, Optimal reward -742.66
Iteration 132 took 2.32 seconds (mean sampled reward: -4038.92). Current reward after update: -1076.94, Optimal reward -742.66
Iteration 133 took 2.31 seconds (mean sampled reward: -4013.37). Current reward after update: -4982.97, Optimal reward -742.66
Iteration 134 took 2.25 seconds (mean sampled reward: -3775.00). Current reward after update: -1005.60, Optimal reward -742.66
Iteration 135 took 2.23 seconds (mean sampled reward: -3773.66). Current reward after update: -5794.33, Optimal reward -742.66
Iteration 136 took 2.31 seconds (mean sampled reward: -4106.69). Current reward after update: -949.46, Optimal reward -742.66
Iteration 137 took 2.33 seconds (mean sampled reward: -4187.91). Current reward after update: -970.60, Optimal reward -742.66
Iteration 138 took 2.26 seconds (mean sampled reward: -4233.70). Current reward after update: -974.19, Optimal reward -742.66
Iteration 139 took 2.36 seconds (mean sampled reward: -4685.42). Current reward after update: -873.24, Optimal reward -742.66
Iteration 140 took 2.32 seconds (mean sampled reward: -4412.17). Current reward after update: -978.32, Optimal reward -742.66
Iteration 141 took 2.31 seconds (mean sampled reward: -4022.91). Current reward after update: -849.88, Optimal reward -742.66
Iteration 142 took 2.28 seconds (mean sampled reward: -3988.08). Current reward after update: -918.65, Optimal reward -742.66
Iteration 143 took 2.26 seconds (mean sampled reward: -3858.48). Current reward after update: -864.32, Optimal reward -742.66
Iteration 144 took 2.30 seconds (mean sampled reward: -3811.69). Current reward after update: -897.31, Optimal reward -742.66
Iteration 145 took 2.29 seconds (mean sampled reward: -4006.13). Current reward after update: -5631.75, Optimal reward -742.66
Iteration 146 took 2.37 seconds (mean sampled reward: -3785.83). Current reward after update: -5492.58, Optimal reward -742.66
Iteration 147 took 2.29 seconds (mean sampled reward: -3768.13). Current reward after update: -809.45, Optimal reward -742.66
Iteration 148 took 2.26 seconds (mean sampled reward: -3504.11). Current reward after update: -739.48, Optimal reward -739.48
Iteration 149 took 2.28 seconds (mean sampled reward: -3404.74). Current reward after update: -5925.32, Optimal reward -739.48
Iteration 150 took 2.26 seconds (mean sampled reward: -3425.25). Current reward after update: -706.88, Optimal reward -706.88
Iteration 151 took 2.24 seconds (mean sampled reward: -3601.16). Current reward after update: -2051.97, Optimal reward -706.88
Iteration 152 took 2.28 seconds (mean sampled reward: -3397.79). Current reward after update: -773.02, Optimal reward -706.88
Iteration 153 took 2.27 seconds (mean sampled reward: -3745.86). Current reward after update: -763.29, Optimal reward -706.88
Iteration 154 took 2.30 seconds (mean sampled reward: -3952.29). Current reward after update: -807.40, Optimal reward -706.88
Iteration 155 took 2.22 seconds (mean sampled reward: -3455.55). Current reward after update: -755.93, Optimal reward -706.88
Iteration 156 took 2.31 seconds (mean sampled reward: -3366.69). Current reward after update: -754.87, Optimal reward -706.88
Iteration 157 took 2.35 seconds (mean sampled reward: -3757.94). Current reward after update: -2912.43, Optimal reward -706.88
Iteration 158 took 2.24 seconds (mean sampled reward: -3560.19). Current reward after update: -639.88, Optimal reward -639.88
Iteration 159 took 2.33 seconds (mean sampled reward: -3846.18). Current reward after update: -691.36, Optimal reward -639.88
Iteration 160 took 2.32 seconds (mean sampled reward: -4170.93). Current reward after update: -842.80, Optimal reward -639.88
Iteration 161 took 2.24 seconds (mean sampled reward: -3992.32). Current reward after update: -771.32, Optimal reward -639.88
Iteration 162 took 2.17 seconds (mean sampled reward: -4124.11). Current reward after update: -817.40, Optimal reward -639.88
Iteration 163 took 2.22 seconds (mean sampled reward: -4232.26). Current reward after update: -822.36, Optimal reward -639.88
Iteration 164 took 2.26 seconds (mean sampled reward: -3878.48). Current reward after update: -669.24, Optimal reward -639.88
Iteration 165 took 2.24 seconds (mean sampled reward: -3822.56). Current reward after update: -792.41, Optimal reward -639.88
Iteration 166 took 2.26 seconds (mean sampled reward: -3653.05). Current reward after update: -756.94, Optimal reward -639.88
Iteration 167 took 2.28 seconds (mean sampled reward: -3491.45). Current reward after update: -719.07, Optimal reward -639.88
Iteration 168 took 2.24 seconds (mean sampled reward: -3493.87). Current reward after update: -737.62, Optimal reward -639.88
Iteration 169 took 2.24 seconds (mean sampled reward: -3728.00). Current reward after update: -2758.51, Optimal reward -639.88
Iteration 170 took 2.30 seconds (mean sampled reward: -3911.25). Current reward after update: -746.95, Optimal reward -639.88
Iteration 171 took 2.26 seconds (mean sampled reward: -4325.60). Current reward after update: -887.49, Optimal reward -639.88
Iteration 172 took 2.29 seconds (mean sampled reward: -4274.04). Current reward after update: -886.24, Optimal reward -639.88
Iteration 173 took 2.25 seconds (mean sampled reward: -4673.66). Current reward after update: -886.15, Optimal reward -639.88
Iteration 174 took 2.28 seconds (mean sampled reward: -4520.17). Current reward after update: -720.75, Optimal reward -639.88
Iteration 175 took 2.33 seconds (mean sampled reward: -5059.87). Current reward after update: -781.19, Optimal reward -639.88
Iteration 176 took 2.43 seconds (mean sampled reward: -5297.04). Current reward after update: -973.59, Optimal reward -639.88
Iteration 177 took 2.28 seconds (mean sampled reward: -4414.13). Current reward after update: -905.53, Optimal reward -639.88
Iteration 178 took 2.26 seconds (mean sampled reward: -4556.70). Current reward after update: -833.05, Optimal reward -639.88
Iteration 179 took 2.26 seconds (mean sampled reward: -4054.45). Current reward after update: -739.19, Optimal reward -639.88
Iteration 180 took 2.31 seconds (mean sampled reward: -4871.50). Current reward after update: -857.01, Optimal reward -639.88
Iteration 181 took 2.33 seconds (mean sampled reward: -4710.07). Current reward after update: -1054.02, Optimal reward -639.88
Iteration 182 took 2.31 seconds (mean sampled reward: -4562.58). Current reward after update: -914.66, Optimal reward -639.88
Iteration 183 took 2.31 seconds (mean sampled reward: -4550.79). Current reward after update: -963.25, Optimal reward -639.88
Iteration 184 took 2.23 seconds (mean sampled reward: -4374.44). Current reward after update: -858.98, Optimal reward -639.88
Iteration 185 took 2.21 seconds (mean sampled reward: -5313.12). Current reward after update: -1049.16, Optimal reward -639.88
Iteration 186 took 2.19 seconds (mean sampled reward: -4822.89). Current reward after update: -1027.24, Optimal reward -639.88
Iteration 187 took 2.24 seconds (mean sampled reward: -4150.47). Current reward after update: -1259.18, Optimal reward -639.88
Iteration 188 took 2.24 seconds (mean sampled reward: -4461.14). Current reward after update: -1048.45, Optimal reward -639.88
Iteration 189 took 2.21 seconds (mean sampled reward: -3972.05). Current reward after update: -999.98, Optimal reward -639.88
Iteration 190 took 2.30 seconds (mean sampled reward: -4556.63). Current reward after update: -1003.77, Optimal reward -639.88
Iteration 191 took 2.26 seconds (mean sampled reward: -4626.64). Current reward after update: -5479.55, Optimal reward -639.88
Iteration 192 took 2.26 seconds (mean sampled reward: -5031.22). Current reward after update: -1352.88, Optimal reward -639.88
Iteration 193 took 2.23 seconds (mean sampled reward: -4782.05). Current reward after update: -993.74, Optimal reward -639.88
Iteration 194 took 2.22 seconds (mean sampled reward: -3826.82). Current reward after update: -1077.89, Optimal reward -639.88
Iteration 195 took 2.22 seconds (mean sampled reward: -4043.84). Current reward after update: -968.43, Optimal reward -639.88
Iteration 196 took 2.32 seconds (mean sampled reward: -3970.72). Current reward after update: -862.54, Optimal reward -639.88
Iteration 197 took 2.19 seconds (mean sampled reward: -3864.35). Current reward after update: -1096.80, Optimal reward -639.88
Iteration 198 took 2.16 seconds (mean sampled reward: -4031.76). Current reward after update: -889.02, Optimal reward -639.88
Iteration 199 took 2.27 seconds (mean sampled reward: -3367.42). Current reward after update: -905.37, Optimal reward -639.88
Iteration 200 took 2.19 seconds (mean sampled reward: -3427.10). Current reward after update: -2236.07, Optimal reward -639.88
Iteration 1 took 2.38 seconds (mean sampled reward: -7630.87). Current reward after update: -7413.69, Optimal reward -7413.69
Iteration 2 took 2.38 seconds (mean sampled reward: -7598.15). Current reward after update: -6147.44, Optimal reward -6147.44
Iteration 3 took 2.30 seconds (mean sampled reward: -7359.34). Current reward after update: -6097.35, Optimal reward -6097.35
Iteration 4 took 2.32 seconds (mean sampled reward: -7301.43). Current reward after update: -5980.28, Optimal reward -5980.28
Iteration 5 took 2.33 seconds (mean sampled reward: -7187.02). Current reward after update: -5645.06, Optimal reward -5645.06
Iteration 6 took 2.34 seconds (mean sampled reward: -7045.17). Current reward after update: -5721.16, Optimal reward -5645.06
Iteration 7 took 2.31 seconds (mean sampled reward: -6951.28). Current reward after update: -5669.28, Optimal reward -5645.06
Iteration 8 took 2.39 seconds (mean sampled reward: -6889.26). Current reward after update: -5551.23, Optimal reward -5551.23
Iteration 9 took 2.32 seconds (mean sampled reward: -6797.25). Current reward after update: -5468.22, Optimal reward -5468.22
Iteration 10 took 2.37 seconds (mean sampled reward: -6512.02). Current reward after update: -5324.28, Optimal reward -5324.28
Iteration 11 took 2.29 seconds (mean sampled reward: -6434.98). Current reward after update: -5371.56, Optimal reward -5324.28
Iteration 12 took 2.35 seconds (mean sampled reward: -6700.46). Current reward after update: -4948.17, Optimal reward -4948.17
Iteration 13 took 2.27 seconds (mean sampled reward: -6605.23). Current reward after update: -3928.67, Optimal reward -3928.67
Iteration 14 took 2.25 seconds (mean sampled reward: -6423.83). Current reward after update: -3555.78, Optimal reward -3555.78
Iteration 15 took 2.27 seconds (mean sampled reward: -6423.19). Current reward after update: -3349.49, Optimal reward -3349.49
Iteration 16 took 2.27 seconds (mean sampled reward: -5933.30). Current reward after update: -3100.29, Optimal reward -3100.29
Iteration 17 took 2.22 seconds (mean sampled reward: -6008.26). Current reward after update: -3002.67, Optimal reward -3002.67
Iteration 18 took 2.60 seconds (mean sampled reward: -6054.17). Current reward after update: -2918.57, Optimal reward -2918.57
Iteration 19 took 2.21 seconds (mean sampled reward: -6776.46). Current reward after update: -2911.54, Optimal reward -2911.54
Iteration 20 took 2.17 seconds (mean sampled reward: -5928.25). Current reward after update: -3094.89, Optimal reward -2911.54
Iteration 21 took 2.16 seconds (mean sampled reward: -5461.73). Current reward after update: -3168.20, Optimal reward -2911.54
Iteration 22 took 2.23 seconds (mean sampled reward: -4484.10). Current reward after update: -3848.83, Optimal reward -2911.54
Iteration 23 took 2.21 seconds (mean sampled reward: -4685.12). Current reward after update: -3010.27, Optimal reward -2911.54
Iteration 24 took 2.30 seconds (mean sampled reward: -4831.64). Current reward after update: -2757.49, Optimal reward -2757.49
Iteration 25 took 2.28 seconds (mean sampled reward: -5007.49). Current reward after update: -2751.39, Optimal reward -2751.39
Iteration 26 took 2.40 seconds (mean sampled reward: -4495.37). Current reward after update: -2814.34, Optimal reward -2751.39
Iteration 27 took 2.36 seconds (mean sampled reward: -5788.17). Current reward after update: -2493.65, Optimal reward -2493.65
Iteration 28 took 2.28 seconds (mean sampled reward: -5508.85). Current reward after update: -2650.86, Optimal reward -2493.65
Iteration 29 took 2.32 seconds (mean sampled reward: -5801.33). Current reward after update: -2661.11, Optimal reward -2493.65
Iteration 30 took 2.36 seconds (mean sampled reward: -5608.63). Current reward after update: -2611.91, Optimal reward -2493.65
Iteration 31 took 2.25 seconds (mean sampled reward: -5497.49). Current reward after update: -2622.89, Optimal reward -2493.65
Iteration 32 took 2.28 seconds (mean sampled reward: -5623.07). Current reward after update: -2698.08, Optimal reward -2493.65
Iteration 33 took 2.22 seconds (mean sampled reward: -4411.56). Current reward after update: -3002.21, Optimal reward -2493.65
Iteration 34 took 2.18 seconds (mean sampled reward: -4398.55). Current reward after update: -2676.99, Optimal reward -2493.65
Iteration 35 took 2.39 seconds (mean sampled reward: -4465.73). Current reward after update: -2948.33, Optimal reward -2493.65
Iteration 36 took 2.23 seconds (mean sampled reward: -4666.57). Current reward after update: -2668.80, Optimal reward -2493.65
Iteration 37 took 2.16 seconds (mean sampled reward: -4945.71). Current reward after update: -2088.99, Optimal reward -2088.99
Iteration 38 took 2.54 seconds (mean sampled reward: -5214.32). Current reward after update: -2049.57, Optimal reward -2049.57
Iteration 39 took 2.34 seconds (mean sampled reward: -4725.81). Current reward after update: -1814.33, Optimal reward -1814.33
Iteration 40 took 2.26 seconds (mean sampled reward: -5118.44). Current reward after update: -2015.83, Optimal reward -1814.33
Iteration 41 took 2.48 seconds (mean sampled reward: -5205.72). Current reward after update: -1962.36, Optimal reward -1814.33
Iteration 42 took 2.21 seconds (mean sampled reward: -5019.65). Current reward after update: -2104.02, Optimal reward -1814.33
Iteration 43 took 2.28 seconds (mean sampled reward: -4966.35). Current reward after update: -1902.84, Optimal reward -1814.33
Iteration 44 took 2.33 seconds (mean sampled reward: -5095.38). Current reward after update: -2224.40, Optimal reward -1814.33
Iteration 45 took 2.31 seconds (mean sampled reward: -4838.99). Current reward after update: -2026.13, Optimal reward -1814.33
Iteration 46 took 2.30 seconds (mean sampled reward: -5079.33). Current reward after update: -2182.07, Optimal reward -1814.33
Iteration 47 took 2.25 seconds (mean sampled reward: -4709.74). Current reward after update: -2272.15, Optimal reward -1814.33
Iteration 48 took 2.35 seconds (mean sampled reward: -4289.55). Current reward after update: -2119.63, Optimal reward -1814.33
Iteration 49 took 2.16 seconds (mean sampled reward: -4941.71). Current reward after update: -1992.72, Optimal reward -1814.33
Iteration 50 took 2.25 seconds (mean sampled reward: -4520.31). Current reward after update: -1932.38, Optimal reward -1814.33
Iteration 51 took 2.13 seconds (mean sampled reward: -4349.88). Current reward after update: -1799.79, Optimal reward -1799.79
Iteration 52 took 2.17 seconds (mean sampled reward: -4358.09). Current reward after update: -1836.40, Optimal reward -1799.79
Iteration 53 took 2.41 seconds (mean sampled reward: -5274.59). Current reward after update: -3002.43, Optimal reward -1799.79
Iteration 54 took 2.24 seconds (mean sampled reward: -4434.99). Current reward after update: -1934.21, Optimal reward -1799.79
Iteration 55 took 2.13 seconds (mean sampled reward: -4094.73). Current reward after update: -2089.32, Optimal reward -1799.79
Iteration 56 took 2.33 seconds (mean sampled reward: -3873.63). Current reward after update: -1609.91, Optimal reward -1609.91
Iteration 57 took 2.35 seconds (mean sampled reward: -4167.90). Current reward after update: -1921.25, Optimal reward -1609.91
Iteration 58 took 2.17 seconds (mean sampled reward: -4054.44). Current reward after update: -1806.24, Optimal reward -1609.91
Iteration 59 took 2.13 seconds (mean sampled reward: -3805.58). Current reward after update: -1865.19, Optimal reward -1609.91
Iteration 60 took 2.20 seconds (mean sampled reward: -4310.27). Current reward after update: -1891.68, Optimal reward -1609.91
Iteration 61 took 2.16 seconds (mean sampled reward: -5324.38). Current reward after update: -1834.01, Optimal reward -1609.91
Iteration 62 took 2.25 seconds (mean sampled reward: -5344.30). Current reward after update: -1854.88, Optimal reward -1609.91
Iteration 63 took 2.19 seconds (mean sampled reward: -5041.14). Current reward after update: -1740.46, Optimal reward -1609.91
Iteration 64 took 2.14 seconds (mean sampled reward: -5260.48). Current reward after update: -1820.23, Optimal reward -1609.91
Iteration 65 took 2.16 seconds (mean sampled reward: -4920.00). Current reward after update: -6312.16, Optimal reward -1609.91
Iteration 66 took 2.31 seconds (mean sampled reward: -5485.01). Current reward after update: -1780.55, Optimal reward -1609.91
Iteration 67 took 2.18 seconds (mean sampled reward: -5401.16). Current reward after update: -1974.00, Optimal reward -1609.91
Iteration 68 took 2.29 seconds (mean sampled reward: -4572.95). Current reward after update: -1792.18, Optimal reward -1609.91
Iteration 69 took 2.18 seconds (mean sampled reward: -5240.51). Current reward after update: -1743.09, Optimal reward -1609.91
Iteration 70 took 2.26 seconds (mean sampled reward: -5497.08). Current reward after update: -2094.42, Optimal reward -1609.91
Iteration 71 took 2.18 seconds (mean sampled reward: -5013.68). Current reward after update: -1771.60, Optimal reward -1609.91
Iteration 72 took 2.24 seconds (mean sampled reward: -5187.89). Current reward after update: -1528.86, Optimal reward -1528.86
Iteration 73 took 2.24 seconds (mean sampled reward: -5610.13). Current reward after update: -6395.84, Optimal reward -1528.86
Iteration 74 took 2.27 seconds (mean sampled reward: -5123.82). Current reward after update: -1670.72, Optimal reward -1528.86
Iteration 75 took 2.27 seconds (mean sampled reward: -3937.36). Current reward after update: -1533.12, Optimal reward -1528.86
Iteration 76 took 2.24 seconds (mean sampled reward: -4920.89). Current reward after update: -1593.64, Optimal reward -1528.86
Iteration 77 took 2.28 seconds (mean sampled reward: -5048.40). Current reward after update: -1526.76, Optimal reward -1526.76
Iteration 78 took 2.24 seconds (mean sampled reward: -5337.41). Current reward after update: -1551.47, Optimal reward -1526.76
Iteration 79 took 2.21 seconds (mean sampled reward: -4819.38). Current reward after update: -1634.19, Optimal reward -1526.76
Iteration 80 took 2.23 seconds (mean sampled reward: -4418.41). Current reward after update: -1667.52, Optimal reward -1526.76
Iteration 81 took 2.25 seconds (mean sampled reward: -4844.91). Current reward after update: -1623.06, Optimal reward -1526.76
Iteration 82 took 2.18 seconds (mean sampled reward: -4154.06). Current reward after update: -1501.25, Optimal reward -1501.25
Iteration 83 took 2.16 seconds (mean sampled reward: -3884.25). Current reward after update: -1624.07, Optimal reward -1501.25
Iteration 84 took 2.28 seconds (mean sampled reward: -4417.88). Current reward after update: -1877.18, Optimal reward -1501.25
Iteration 85 took 2.18 seconds (mean sampled reward: -4353.45). Current reward after update: -1563.83, Optimal reward -1501.25
Iteration 86 took 2.17 seconds (mean sampled reward: -4620.43). Current reward after update: -1767.35, Optimal reward -1501.25
Iteration 87 took 2.24 seconds (mean sampled reward: -4028.23). Current reward after update: -1675.70, Optimal reward -1501.25
Iteration 88 took 2.27 seconds (mean sampled reward: -4668.16). Current reward after update: -1576.87, Optimal reward -1501.25
Iteration 89 took 2.27 seconds (mean sampled reward: -4229.10). Current reward after update: -1441.64, Optimal reward -1441.64
Iteration 90 took 2.22 seconds (mean sampled reward: -4788.21). Current reward after update: -1757.98, Optimal reward -1441.64
Iteration 91 took 2.14 seconds (mean sampled reward: -5165.28). Current reward after update: -2145.14, Optimal reward -1441.64
Iteration 92 took 2.16 seconds (mean sampled reward: -4501.41). Current reward after update: -1968.16, Optimal reward -1441.64
Iteration 93 took 2.12 seconds (mean sampled reward: -3776.55). Current reward after update: -3192.92, Optimal reward -1441.64
Iteration 94 took 2.17 seconds (mean sampled reward: -4424.59). Current reward after update: -1878.14, Optimal reward -1441.64
Iteration 95 took 2.20 seconds (mean sampled reward: -4129.03). Current reward after update: -1848.11, Optimal reward -1441.64
Iteration 96 took 2.30 seconds (mean sampled reward: -4509.49). Current reward after update: -1419.03, Optimal reward -1419.03
Iteration 97 took 2.27 seconds (mean sampled reward: -4097.31). Current reward after update: -3130.76, Optimal reward -1419.03
Iteration 98 took 2.27 seconds (mean sampled reward: -3704.50). Current reward after update: -3232.52, Optimal reward -1419.03
Iteration 99 took 2.18 seconds (mean sampled reward: -4200.31). Current reward after update: -1853.25, Optimal reward -1419.03
Iteration 100 took 2.22 seconds (mean sampled reward: -4245.98). Current reward after update: -1787.51, Optimal reward -1419.03
Iteration 101 took 2.10 seconds (mean sampled reward: -4094.89). Current reward after update: -1676.49, Optimal reward -1419.03
Iteration 102 took 2.12 seconds (mean sampled reward: -3754.13). Current reward after update: -1756.57, Optimal reward -1419.03
Iteration 103 took 2.10 seconds (mean sampled reward: -3977.89). Current reward after update: -1704.21, Optimal reward -1419.03
Iteration 104 took 2.12 seconds (mean sampled reward: -4054.11). Current reward after update: -2948.26, Optimal reward -1419.03
Iteration 105 took 2.11 seconds (mean sampled reward: -4326.46). Current reward after update: -2884.50, Optimal reward -1419.03
Iteration 106 took 2.16 seconds (mean sampled reward: -3888.89). Current reward after update: -1622.19, Optimal reward -1419.03
Iteration 107 took 2.16 seconds (mean sampled reward: -3890.19). Current reward after update: -3376.40, Optimal reward -1419.03
Iteration 108 took 2.10 seconds (mean sampled reward: -4325.52). Current reward after update: -1580.82, Optimal reward -1419.03
Iteration 109 took 2.25 seconds (mean sampled reward: -3911.31). Current reward after update: -2101.35, Optimal reward -1419.03
Iteration 110 took 2.20 seconds (mean sampled reward: -4237.87). Current reward after update: -1645.70, Optimal reward -1419.03
Iteration 111 took 2.33 seconds (mean sampled reward: -4937.50). Current reward after update: -1671.61, Optimal reward -1419.03
Iteration 112 took 2.39 seconds (mean sampled reward: -4384.18). Current reward after update: -1622.45, Optimal reward -1419.03
Iteration 113 took 2.12 seconds (mean sampled reward: -4228.08). Current reward after update: -1673.56, Optimal reward -1419.03
Iteration 114 took 2.24 seconds (mean sampled reward: -4267.35). Current reward after update: -1717.19, Optimal reward -1419.03
Iteration 115 took 2.11 seconds (mean sampled reward: -4051.29). Current reward after update: -1703.18, Optimal reward -1419.03
Iteration 116 took 2.19 seconds (mean sampled reward: -5367.79). Current reward after update: -1703.56, Optimal reward -1419.03
Iteration 117 took 2.15 seconds (mean sampled reward: -5122.38). Current reward after update: -2008.93, Optimal reward -1419.03
Iteration 118 took 2.12 seconds (mean sampled reward: -4821.92). Current reward after update: -1620.99, Optimal reward -1419.03
Iteration 119 took 2.12 seconds (mean sampled reward: -5153.93). Current reward after update: -1709.24, Optimal reward -1419.03
Iteration 120 took 2.22 seconds (mean sampled reward: -5946.30). Current reward after update: -1805.51, Optimal reward -1419.03
Iteration 121 took 2.12 seconds (mean sampled reward: -4712.97). Current reward after update: -1628.86, Optimal reward -1419.03
Iteration 122 took 2.20 seconds (mean sampled reward: -5022.46). Current reward after update: -3368.16, Optimal reward -1419.03
Iteration 123 took 2.17 seconds (mean sampled reward: -5561.50). Current reward after update: -2108.62, Optimal reward -1419.03
Iteration 124 took 2.18 seconds (mean sampled reward: -5043.84). Current reward after update: -1772.50, Optimal reward -1419.03
Iteration 125 took 2.13 seconds (mean sampled reward: -5081.03). Current reward after update: -1996.16, Optimal reward -1419.03
Iteration 126 took 2.12 seconds (mean sampled reward: -4894.86). Current reward after update: -1737.16, Optimal reward -1419.03
Iteration 127 took 2.13 seconds (mean sampled reward: -4979.99). Current reward after update: -1467.25, Optimal reward -1419.03
Iteration 128 took 2.16 seconds (mean sampled reward: -3599.89). Current reward after update: -1959.23, Optimal reward -1419.03
Iteration 129 took 2.14 seconds (mean sampled reward: -4089.49). Current reward after update: -1453.29, Optimal reward -1419.03
Iteration 130 took 2.14 seconds (mean sampled reward: -4934.49). Current reward after update: -2874.56, Optimal reward -1419.03
Iteration 131 took 2.19 seconds (mean sampled reward: -5300.99). Current reward after update: -1557.81, Optimal reward -1419.03
Iteration 132 took 2.14 seconds (mean sampled reward: -5624.34). Current reward after update: -1523.45, Optimal reward -1419.03
Iteration 133 took 2.17 seconds (mean sampled reward: -5257.15). Current reward after update: -1478.13, Optimal reward -1419.03
Iteration 134 took 2.14 seconds (mean sampled reward: -4766.38). Current reward after update: -1802.81, Optimal reward -1419.03
Iteration 135 took 2.12 seconds (mean sampled reward: -4430.75). Current reward after update: -1385.25, Optimal reward -1385.25
Iteration 136 took 2.10 seconds (mean sampled reward: -5104.10). Current reward after update: -1443.61, Optimal reward -1385.25
Iteration 137 took 2.08 seconds (mean sampled reward: -5153.70). Current reward after update: -1627.81, Optimal reward -1385.25
Iteration 138 took 2.12 seconds (mean sampled reward: -5408.69). Current reward after update: -1406.02, Optimal reward -1385.25
Iteration 139 took 2.15 seconds (mean sampled reward: -4381.12). Current reward after update: -1706.97, Optimal reward -1385.25
Iteration 140 took 2.13 seconds (mean sampled reward: -4298.74). Current reward after update: -2007.10, Optimal reward -1385.25
Iteration 141 took 2.12 seconds (mean sampled reward: -4518.14). Current reward after update: -1372.33, Optimal reward -1372.33
Iteration 142 took 2.12 seconds (mean sampled reward: -4446.91). Current reward after update: -1288.83, Optimal reward -1288.83
Iteration 143 took 2.16 seconds (mean sampled reward: -5800.53). Current reward after update: -1310.68, Optimal reward -1288.83
Iteration 144 took 2.15 seconds (mean sampled reward: -6061.47). Current reward after update: -1322.17, Optimal reward -1288.83
Iteration 145 took 2.14 seconds (mean sampled reward: -5186.76). Current reward after update: -1217.92, Optimal reward -1217.92
Iteration 146 took 2.16 seconds (mean sampled reward: -5242.96). Current reward after update: -1392.46, Optimal reward -1217.92
Iteration 147 took 2.17 seconds (mean sampled reward: -5412.60). Current reward after update: -1216.31, Optimal reward -1216.31
Iteration 148 took 2.18 seconds (mean sampled reward: -5759.66). Current reward after update: -1632.39, Optimal reward -1216.31
Iteration 149 took 2.15 seconds (mean sampled reward: -5569.94). Current reward after update: -1169.62, Optimal reward -1169.62
Iteration 150 took 2.13 seconds (mean sampled reward: -5196.91). Current reward after update: -1219.49, Optimal reward -1169.62
Iteration 151 took 2.16 seconds (mean sampled reward: -5483.17). Current reward after update: -1296.97, Optimal reward -1169.62
Iteration 152 took 2.18 seconds (mean sampled reward: -5124.72). Current reward after update: -2003.65, Optimal reward -1169.62
Iteration 153 took 2.20 seconds (mean sampled reward: -4865.74). Current reward after update: -1238.07, Optimal reward -1169.62
Iteration 154 took 2.19 seconds (mean sampled reward: -5967.76). Current reward after update: -1381.40, Optimal reward -1169.62
Iteration 155 took 2.21 seconds (mean sampled reward: -5557.36). Current reward after update: -1314.34, Optimal reward -1169.62
Iteration 156 took 2.27 seconds (mean sampled reward: -5560.63). Current reward after update: -1355.91, Optimal reward -1169.62
Iteration 157 took 2.25 seconds (mean sampled reward: -5964.67). Current reward after update: -1312.55, Optimal reward -1169.62
Iteration 158 took 2.23 seconds (mean sampled reward: -5242.76). Current reward after update: -1221.30, Optimal reward -1169.62
Iteration 159 took 2.28 seconds (mean sampled reward: -5696.44). Current reward after update: -1276.38, Optimal reward -1169.62
Iteration 160 took 2.21 seconds (mean sampled reward: -5298.18). Current reward after update: -1270.38, Optimal reward -1169.62
Iteration 161 took 2.23 seconds (mean sampled reward: -4705.08). Current reward after update: -1211.73, Optimal reward -1169.62
Iteration 162 took 2.27 seconds (mean sampled reward: -4820.31). Current reward after update: -1480.01, Optimal reward -1169.62
Iteration 163 took 2.28 seconds (mean sampled reward: -5622.94). Current reward after update: -1261.12, Optimal reward -1169.62
Iteration 164 took 2.24 seconds (mean sampled reward: -4445.59). Current reward after update: -1253.43, Optimal reward -1169.62
Iteration 165 took 2.18 seconds (mean sampled reward: -4899.26). Current reward after update: -1768.74, Optimal reward -1169.62
Iteration 166 took 2.26 seconds (mean sampled reward: -4911.46). Current reward after update: -6622.26, Optimal reward -1169.62
Iteration 167 took 2.25 seconds (mean sampled reward: -5420.76). Current reward after update: -1165.42, Optimal reward -1165.42
Iteration 168 took 2.22 seconds (mean sampled reward: -4381.38). Current reward after update: -1024.63, Optimal reward -1024.63
Iteration 169 took 2.24 seconds (mean sampled reward: -4017.16). Current reward after update: -1148.43, Optimal reward -1024.63
Iteration 170 took 2.24 seconds (mean sampled reward: -3834.28). Current reward after update: -1040.17, Optimal reward -1024.63
Iteration 171 took 2.26 seconds (mean sampled reward: -5553.17). Current reward after update: -1250.36, Optimal reward -1024.63
Iteration 172 took 2.29 seconds (mean sampled reward: -5392.14). Current reward after update: -1200.23, Optimal reward -1024.63
Iteration 173 took 2.27 seconds (mean sampled reward: -5298.53). Current reward after update: -1152.32, Optimal reward -1024.63
Iteration 174 took 2.27 seconds (mean sampled reward: -4886.80). Current reward after update: -1181.69, Optimal reward -1024.63
Iteration 175 took 2.31 seconds (mean sampled reward: -4753.96). Current reward after update: -1039.77, Optimal reward -1024.63
Iteration 176 took 2.25 seconds (mean sampled reward: -5477.86). Current reward after update: -1158.58, Optimal reward -1024.63
Iteration 177 took 2.23 seconds (mean sampled reward: -5489.15). Current reward after update: -1143.40, Optimal reward -1024.63
Iteration 178 took 2.26 seconds (mean sampled reward: -5715.96). Current reward after update: -1212.35, Optimal reward -1024.63
Iteration 179 took 2.18 seconds (mean sampled reward: -4563.06). Current reward after update: -969.37, Optimal reward -969.37
Iteration 180 took 2.22 seconds (mean sampled reward: -5079.58). Current reward after update: -1152.98, Optimal reward -969.37
Iteration 181 took 2.24 seconds (mean sampled reward: -4330.86). Current reward after update: -1457.78, Optimal reward -969.37
Iteration 182 took 2.18 seconds (mean sampled reward: -5658.68). Current reward after update: -1099.24, Optimal reward -969.37
Iteration 183 took 2.26 seconds (mean sampled reward: -5521.10). Current reward after update: -1099.78, Optimal reward -969.37
Iteration 184 took 2.22 seconds (mean sampled reward: -5056.14). Current reward after update: -1390.36, Optimal reward -969.37
Iteration 185 took 2.21 seconds (mean sampled reward: -4346.86). Current reward after update: -1081.45, Optimal reward -969.37
Iteration 186 took 2.19 seconds (mean sampled reward: -4244.26). Current reward after update: -1175.83, Optimal reward -969.37
Iteration 187 took 2.23 seconds (mean sampled reward: -4479.04). Current reward after update: -1111.70, Optimal reward -969.37
Iteration 188 took 2.17 seconds (mean sampled reward: -4898.26). Current reward after update: -1802.13, Optimal reward -969.37
Iteration 189 took 2.20 seconds (mean sampled reward: -5092.12). Current reward after update: -1296.98, Optimal reward -969.37
Iteration 190 took 2.22 seconds (mean sampled reward: -5448.02). Current reward after update: -1351.16, Optimal reward -969.37
Iteration 191 took 2.16 seconds (mean sampled reward: -4916.03). Current reward after update: -1726.18, Optimal reward -969.37
Iteration 192 took 2.20 seconds (mean sampled reward: -4346.33). Current reward after update: -1004.57, Optimal reward -969.37
Iteration 193 took 2.24 seconds (mean sampled reward: -4396.69). Current reward after update: -1248.00, Optimal reward -969.37
Iteration 194 took 2.17 seconds (mean sampled reward: -3714.46). Current reward after update: -946.86, Optimal reward -946.86
Iteration 195 took 2.19 seconds (mean sampled reward: -4149.96). Current reward after update: -1210.83, Optimal reward -946.86
Iteration 196 took 2.22 seconds (mean sampled reward: -4422.49). Current reward after update: -1091.70, Optimal reward -946.86
Iteration 197 took 2.21 seconds (mean sampled reward: -4351.66). Current reward after update: -1181.30, Optimal reward -946.86
Iteration 198 took 2.22 seconds (mean sampled reward: -4038.15). Current reward after update: -972.20, Optimal reward -946.86
Iteration 199 took 2.20 seconds (mean sampled reward: -5134.31). Current reward after update: -1054.62, Optimal reward -946.86
Iteration 200 took 2.22 seconds (mean sampled reward: -4350.64). Current reward after update: -1016.48, Optimal reward -946.86
Max force: 50 Sigma: 0.2 mean rewards: -727.9943223547605, best rewards:-597.2500984546716

Iteration 1 took 2.45 seconds (mean sampled reward: -7621.91). Current reward after update: -6105.14, Optimal reward -6105.14
Iteration 2 took 2.28 seconds (mean sampled reward: -7220.60). Current reward after update: -3485.27, Optimal reward -3485.27
Iteration 3 took 2.27 seconds (mean sampled reward: -6639.00). Current reward after update: -3053.27, Optimal reward -3053.27
Iteration 4 took 2.41 seconds (mean sampled reward: -5502.41). Current reward after update: -2209.26, Optimal reward -2209.26
Iteration 5 took 2.36 seconds (mean sampled reward: -6248.39). Current reward after update: -1926.29, Optimal reward -1926.29
Iteration 6 took 2.47 seconds (mean sampled reward: -4550.20). Current reward after update: -1415.55, Optimal reward -1415.55
Iteration 7 took 2.23 seconds (mean sampled reward: -5540.13). Current reward after update: -967.26, Optimal reward -967.26
Iteration 8 took 2.27 seconds (mean sampled reward: -6073.46). Current reward after update: -602.02, Optimal reward -602.02
Iteration 9 took 2.22 seconds (mean sampled reward: -5946.04). Current reward after update: -913.80, Optimal reward -602.02
Iteration 10 took 2.28 seconds (mean sampled reward: -6576.31). Current reward after update: -851.85, Optimal reward -602.02
Iteration 11 took 2.37 seconds (mean sampled reward: -4604.41). Current reward after update: -697.63, Optimal reward -602.02
Iteration 12 took 2.43 seconds (mean sampled reward: -5028.16). Current reward after update: -746.71, Optimal reward -602.02
Iteration 13 took 2.34 seconds (mean sampled reward: -4844.48). Current reward after update: -967.08, Optimal reward -602.02
Iteration 14 took 2.35 seconds (mean sampled reward: -4864.05). Current reward after update: -1113.78, Optimal reward -602.02
Iteration 15 took 2.49 seconds (mean sampled reward: -4912.71). Current reward after update: -933.80, Optimal reward -602.02
Iteration 16 took 2.26 seconds (mean sampled reward: -4733.14). Current reward after update: -666.59, Optimal reward -602.02
Iteration 17 took 2.32 seconds (mean sampled reward: -5878.93). Current reward after update: -1527.94, Optimal reward -602.02
Iteration 18 took 2.40 seconds (mean sampled reward: -5502.36). Current reward after update: -784.75, Optimal reward -602.02
Iteration 19 took 2.42 seconds (mean sampled reward: -4202.24). Current reward after update: -739.18, Optimal reward -602.02
Iteration 20 took 2.30 seconds (mean sampled reward: -4838.99). Current reward after update: -513.54, Optimal reward -513.54
Iteration 21 took 2.41 seconds (mean sampled reward: -4491.78). Current reward after update: -579.61, Optimal reward -513.54
Iteration 22 took 2.32 seconds (mean sampled reward: -5109.38). Current reward after update: -572.26, Optimal reward -513.54
Iteration 23 took 2.35 seconds (mean sampled reward: -5408.87). Current reward after update: -557.97, Optimal reward -513.54
Iteration 24 took 2.26 seconds (mean sampled reward: -5258.61). Current reward after update: -607.72, Optimal reward -513.54
Iteration 25 took 2.38 seconds (mean sampled reward: -6207.24). Current reward after update: -483.19, Optimal reward -483.19
Iteration 26 took 2.31 seconds (mean sampled reward: -5429.82). Current reward after update: -518.06, Optimal reward -483.19
Iteration 27 took 2.35 seconds (mean sampled reward: -5833.61). Current reward after update: -560.35, Optimal reward -483.19
Iteration 28 took 2.49 seconds (mean sampled reward: -4806.43). Current reward after update: -575.17, Optimal reward -483.19
Iteration 29 took 2.39 seconds (mean sampled reward: -4385.89). Current reward after update: -452.47, Optimal reward -452.47
Iteration 30 took 2.42 seconds (mean sampled reward: -4722.46). Current reward after update: -542.45, Optimal reward -452.47
Iteration 31 took 2.52 seconds (mean sampled reward: -5362.61). Current reward after update: -864.59, Optimal reward -452.47
Iteration 32 took 2.46 seconds (mean sampled reward: -5083.86). Current reward after update: -671.63, Optimal reward -452.47
Iteration 33 took 2.43 seconds (mean sampled reward: -5413.57). Current reward after update: -855.76, Optimal reward -452.47
Iteration 34 took 2.41 seconds (mean sampled reward: -5748.17). Current reward after update: -795.80, Optimal reward -452.47
Iteration 35 took 2.64 seconds (mean sampled reward: -5783.82). Current reward after update: -727.88, Optimal reward -452.47
Iteration 36 took 2.47 seconds (mean sampled reward: -5773.63). Current reward after update: -971.91, Optimal reward -452.47
Iteration 37 took 2.37 seconds (mean sampled reward: -4501.89). Current reward after update: -400.45, Optimal reward -400.45
Iteration 38 took 2.33 seconds (mean sampled reward: -4855.63). Current reward after update: -251.11, Optimal reward -251.11
Iteration 39 took 2.27 seconds (mean sampled reward: -4987.46). Current reward after update: -418.81, Optimal reward -251.11
Iteration 40 took 2.34 seconds (mean sampled reward: -5899.46). Current reward after update: -383.67, Optimal reward -251.11
Iteration 41 took 2.40 seconds (mean sampled reward: -5287.54). Current reward after update: -476.46, Optimal reward -251.11
Iteration 42 took 2.43 seconds (mean sampled reward: -6316.98). Current reward after update: -605.43, Optimal reward -251.11
Iteration 43 took 2.33 seconds (mean sampled reward: -5103.46). Current reward after update: -1158.75, Optimal reward -251.11
Iteration 44 took 2.42 seconds (mean sampled reward: -5731.37). Current reward after update: -230.25, Optimal reward -230.25
Iteration 45 took 2.30 seconds (mean sampled reward: -6115.19). Current reward after update: -247.01, Optimal reward -230.25
Iteration 46 took 2.28 seconds (mean sampled reward: -5844.48). Current reward after update: -337.29, Optimal reward -230.25
Iteration 47 took 2.27 seconds (mean sampled reward: -5716.80). Current reward after update: -2153.68, Optimal reward -230.25
Iteration 48 took 2.32 seconds (mean sampled reward: -5836.96). Current reward after update: -591.71, Optimal reward -230.25
Iteration 49 took 2.28 seconds (mean sampled reward: -5368.59). Current reward after update: -773.60, Optimal reward -230.25
Iteration 50 took 2.25 seconds (mean sampled reward: -4869.32). Current reward after update: -483.45, Optimal reward -230.25
Iteration 51 took 2.33 seconds (mean sampled reward: -5208.57). Current reward after update: -392.05, Optimal reward -230.25
Iteration 52 took 2.34 seconds (mean sampled reward: -6451.40). Current reward after update: -517.34, Optimal reward -230.25
Iteration 53 took 2.48 seconds (mean sampled reward: -6932.98). Current reward after update: -1450.97, Optimal reward -230.25
Iteration 54 took 2.64 seconds (mean sampled reward: -6363.07). Current reward after update: -543.89, Optimal reward -230.25
Iteration 55 took 2.30 seconds (mean sampled reward: -5758.85). Current reward after update: -577.07, Optimal reward -230.25
Iteration 56 took 2.38 seconds (mean sampled reward: -5928.62). Current reward after update: -1009.47, Optimal reward -230.25
Iteration 57 took 2.39 seconds (mean sampled reward: -5720.00). Current reward after update: -818.61, Optimal reward -230.25
Iteration 58 took 2.39 seconds (mean sampled reward: -5010.85). Current reward after update: -935.75, Optimal reward -230.25
Iteration 59 took 2.51 seconds (mean sampled reward: -4088.84). Current reward after update: -834.67, Optimal reward -230.25
Iteration 60 took 2.22 seconds (mean sampled reward: -4857.21). Current reward after update: -888.21, Optimal reward -230.25
Iteration 61 took 2.26 seconds (mean sampled reward: -4572.62). Current reward after update: -757.72, Optimal reward -230.25
Iteration 62 took 2.22 seconds (mean sampled reward: -4349.41). Current reward after update: -752.78, Optimal reward -230.25
Iteration 63 took 2.26 seconds (mean sampled reward: -5294.69). Current reward after update: -770.34, Optimal reward -230.25
Iteration 64 took 2.44 seconds (mean sampled reward: -3907.79). Current reward after update: -854.37, Optimal reward -230.25
Iteration 65 took 2.28 seconds (mean sampled reward: -4597.83). Current reward after update: -842.49, Optimal reward -230.25
Iteration 66 took 2.45 seconds (mean sampled reward: -4435.24). Current reward after update: -1036.53, Optimal reward -230.25
Iteration 67 took 2.41 seconds (mean sampled reward: -4596.45). Current reward after update: -1002.42, Optimal reward -230.25
Iteration 68 took 2.32 seconds (mean sampled reward: -4219.50). Current reward after update: -857.71, Optimal reward -230.25
Iteration 69 took 2.38 seconds (mean sampled reward: -4300.93). Current reward after update: -629.08, Optimal reward -230.25
Iteration 70 took 2.30 seconds (mean sampled reward: -3491.41). Current reward after update: -701.05, Optimal reward -230.25
Iteration 71 took 2.40 seconds (mean sampled reward: -5048.89). Current reward after update: -716.63, Optimal reward -230.25
Iteration 72 took 2.27 seconds (mean sampled reward: -5344.97). Current reward after update: -577.15, Optimal reward -230.25
Iteration 73 took 2.29 seconds (mean sampled reward: -4676.33). Current reward after update: -535.16, Optimal reward -230.25
Iteration 74 took 2.34 seconds (mean sampled reward: -4641.04). Current reward after update: -583.99, Optimal reward -230.25
Iteration 75 took 2.28 seconds (mean sampled reward: -3858.08). Current reward after update: -653.55, Optimal reward -230.25
Iteration 76 took 2.36 seconds (mean sampled reward: -4647.37). Current reward after update: -712.55, Optimal reward -230.25
Iteration 77 took 2.35 seconds (mean sampled reward: -4201.82). Current reward after update: -710.70, Optimal reward -230.25
Iteration 78 took 2.40 seconds (mean sampled reward: -4468.94). Current reward after update: -620.82, Optimal reward -230.25
Iteration 79 took 2.42 seconds (mean sampled reward: -5417.92). Current reward after update: -625.43, Optimal reward -230.25
Iteration 80 took 2.31 seconds (mean sampled reward: -4806.63). Current reward after update: -654.93, Optimal reward -230.25
Iteration 81 took 2.24 seconds (mean sampled reward: -4039.72). Current reward after update: -762.16, Optimal reward -230.25
Iteration 82 took 2.35 seconds (mean sampled reward: -5380.40). Current reward after update: -813.81, Optimal reward -230.25
Iteration 83 took 2.37 seconds (mean sampled reward: -5427.30). Current reward after update: -750.97, Optimal reward -230.25
Iteration 84 took 2.38 seconds (mean sampled reward: -4730.96). Current reward after update: -667.43, Optimal reward -230.25
Iteration 85 took 2.43 seconds (mean sampled reward: -4138.09). Current reward after update: -942.63, Optimal reward -230.25
Iteration 86 took 2.38 seconds (mean sampled reward: -4909.23). Current reward after update: -669.82, Optimal reward -230.25
Iteration 87 took 2.36 seconds (mean sampled reward: -4916.42). Current reward after update: -761.31, Optimal reward -230.25
Iteration 88 took 2.40 seconds (mean sampled reward: -4920.25). Current reward after update: -788.11, Optimal reward -230.25
Iteration 89 took 2.35 seconds (mean sampled reward: -4547.37). Current reward after update: -647.75, Optimal reward -230.25
Iteration 90 took 2.39 seconds (mean sampled reward: -3951.17). Current reward after update: -590.26, Optimal reward -230.25
Iteration 91 took 2.39 seconds (mean sampled reward: -4053.62). Current reward after update: -650.62, Optimal reward -230.25
Iteration 92 took 2.37 seconds (mean sampled reward: -4747.66). Current reward after update: -870.85, Optimal reward -230.25
Iteration 93 took 2.42 seconds (mean sampled reward: -4430.42). Current reward after update: -724.92, Optimal reward -230.25
Iteration 94 took 2.35 seconds (mean sampled reward: -4875.13). Current reward after update: -632.45, Optimal reward -230.25
Iteration 95 took 2.41 seconds (mean sampled reward: -4461.38). Current reward after update: -816.98, Optimal reward -230.25
Iteration 96 took 2.37 seconds (mean sampled reward: -4203.16). Current reward after update: -926.77, Optimal reward -230.25
Iteration 97 took 2.49 seconds (mean sampled reward: -5465.24). Current reward after update: -872.58, Optimal reward -230.25
Iteration 98 took 2.43 seconds (mean sampled reward: -4663.59). Current reward after update: -962.13, Optimal reward -230.25
Iteration 99 took 2.45 seconds (mean sampled reward: -6187.28). Current reward after update: -1320.51, Optimal reward -230.25
Iteration 100 took 2.60 seconds (mean sampled reward: -5959.54). Current reward after update: -1370.15, Optimal reward -230.25
Iteration 101 took 2.40 seconds (mean sampled reward: -4092.55). Current reward after update: -965.65, Optimal reward -230.25
Iteration 102 took 2.42 seconds (mean sampled reward: -4911.84). Current reward after update: -846.03, Optimal reward -230.25
Iteration 103 took 2.44 seconds (mean sampled reward: -4941.98). Current reward after update: -917.74, Optimal reward -230.25
Iteration 104 took 2.42 seconds (mean sampled reward: -5507.35). Current reward after update: -741.79, Optimal reward -230.25
Iteration 105 took 2.38 seconds (mean sampled reward: -5325.01). Current reward after update: -721.12, Optimal reward -230.25
Iteration 106 took 2.55 seconds (mean sampled reward: -6281.58). Current reward after update: -1023.64, Optimal reward -230.25
Iteration 107 took 2.64 seconds (mean sampled reward: -6072.99). Current reward after update: -982.21, Optimal reward -230.25
Iteration 108 took 2.43 seconds (mean sampled reward: -5326.94). Current reward after update: -1354.99, Optimal reward -230.25
Iteration 109 took 2.46 seconds (mean sampled reward: -5505.30). Current reward after update: -1049.79, Optimal reward -230.25
Iteration 110 took 2.67 seconds (mean sampled reward: -5061.90). Current reward after update: -1112.17, Optimal reward -230.25
Iteration 111 took 2.35 seconds (mean sampled reward: -4549.02). Current reward after update: -981.81, Optimal reward -230.25
Iteration 112 took 2.41 seconds (mean sampled reward: -4427.00). Current reward after update: -1067.22, Optimal reward -230.25
Iteration 113 took 2.39 seconds (mean sampled reward: -4786.67). Current reward after update: -1069.37, Optimal reward -230.25
Iteration 114 took 2.50 seconds (mean sampled reward: -5905.43). Current reward after update: -1208.85, Optimal reward -230.25
Iteration 115 took 2.38 seconds (mean sampled reward: -4643.85). Current reward after update: -1227.39, Optimal reward -230.25
Iteration 116 took 2.60 seconds (mean sampled reward: -6279.27). Current reward after update: -1539.96, Optimal reward -230.25
Iteration 117 took 2.51 seconds (mean sampled reward: -6613.56). Current reward after update: -1408.17, Optimal reward -230.25
Iteration 118 took 2.52 seconds (mean sampled reward: -6560.90). Current reward after update: -1529.52, Optimal reward -230.25
Iteration 119 took 2.42 seconds (mean sampled reward: -5873.84). Current reward after update: -1357.29, Optimal reward -230.25
Iteration 120 took 2.49 seconds (mean sampled reward: -5843.07). Current reward after update: -2401.58, Optimal reward -230.25
Iteration 121 took 2.49 seconds (mean sampled reward: -5817.21). Current reward after update: -1594.60, Optimal reward -230.25
Iteration 122 took 2.43 seconds (mean sampled reward: -5410.71). Current reward after update: -1299.52, Optimal reward -230.25
Iteration 123 took 2.40 seconds (mean sampled reward: -5285.52). Current reward after update: -1533.00, Optimal reward -230.25
Iteration 124 took 2.41 seconds (mean sampled reward: -5095.65). Current reward after update: -1313.83, Optimal reward -230.25
Iteration 125 took 2.39 seconds (mean sampled reward: -5003.20). Current reward after update: -1255.63, Optimal reward -230.25
Iteration 126 took 2.47 seconds (mean sampled reward: -5037.90). Current reward after update: -1501.30, Optimal reward -230.25
Iteration 127 took 2.38 seconds (mean sampled reward: -4925.75). Current reward after update: -1185.82, Optimal reward -230.25
Iteration 128 took 2.37 seconds (mean sampled reward: -5704.61). Current reward after update: -1785.54, Optimal reward -230.25
Iteration 129 took 2.33 seconds (mean sampled reward: -5338.54). Current reward after update: -1123.26, Optimal reward -230.25
Iteration 130 took 2.27 seconds (mean sampled reward: -4764.01). Current reward after update: -1368.63, Optimal reward -230.25
Iteration 131 took 2.25 seconds (mean sampled reward: -5057.86). Current reward after update: -1227.07, Optimal reward -230.25
Iteration 132 took 2.30 seconds (mean sampled reward: -4935.92). Current reward after update: -1051.33, Optimal reward -230.25
Iteration 133 took 2.29 seconds (mean sampled reward: -5735.32). Current reward after update: -1056.29, Optimal reward -230.25
Iteration 134 took 2.23 seconds (mean sampled reward: -5035.15). Current reward after update: -959.37, Optimal reward -230.25
Iteration 135 took 2.29 seconds (mean sampled reward: -5785.81). Current reward after update: -976.14, Optimal reward -230.25
Iteration 136 took 2.27 seconds (mean sampled reward: -5780.77). Current reward after update: -909.11, Optimal reward -230.25
Iteration 137 took 2.28 seconds (mean sampled reward: -5582.06). Current reward after update: -830.22, Optimal reward -230.25
Iteration 138 took 2.47 seconds (mean sampled reward: -5878.66). Current reward after update: -921.46, Optimal reward -230.25
Iteration 139 took 2.27 seconds (mean sampled reward: -5726.66). Current reward after update: -951.75, Optimal reward -230.25
Iteration 140 took 2.28 seconds (mean sampled reward: -6161.08). Current reward after update: -1217.71, Optimal reward -230.25
Iteration 141 took 2.30 seconds (mean sampled reward: -6329.49). Current reward after update: -971.48, Optimal reward -230.25
Iteration 142 took 2.34 seconds (mean sampled reward: -6401.16). Current reward after update: -944.00, Optimal reward -230.25
Iteration 143 took 2.29 seconds (mean sampled reward: -6807.29). Current reward after update: -937.53, Optimal reward -230.25
Iteration 144 took 2.30 seconds (mean sampled reward: -6537.91). Current reward after update: -1293.57, Optimal reward -230.25
Iteration 145 took 2.26 seconds (mean sampled reward: -6077.37). Current reward after update: -802.61, Optimal reward -230.25
Iteration 146 took 2.41 seconds (mean sampled reward: -6095.92). Current reward after update: -951.97, Optimal reward -230.25
Iteration 147 took 2.34 seconds (mean sampled reward: -6345.66). Current reward after update: -894.06, Optimal reward -230.25
Iteration 148 took 2.35 seconds (mean sampled reward: -6355.89). Current reward after update: -1033.38, Optimal reward -230.25
Iteration 149 took 2.29 seconds (mean sampled reward: -6323.72). Current reward after update: -933.00, Optimal reward -230.25
Iteration 150 took 2.26 seconds (mean sampled reward: -6229.56). Current reward after update: -1025.39, Optimal reward -230.25
Iteration 151 took 2.29 seconds (mean sampled reward: -6154.49). Current reward after update: -1101.06, Optimal reward -230.25
Iteration 152 took 2.30 seconds (mean sampled reward: -5997.60). Current reward after update: -1308.50, Optimal reward -230.25
Iteration 153 took 2.30 seconds (mean sampled reward: -5293.47). Current reward after update: -982.59, Optimal reward -230.25
Iteration 154 took 2.25 seconds (mean sampled reward: -5506.36). Current reward after update: -832.08, Optimal reward -230.25
Iteration 155 took 2.37 seconds (mean sampled reward: -5711.61). Current reward after update: -712.63, Optimal reward -230.25
Iteration 156 took 2.37 seconds (mean sampled reward: -5685.20). Current reward after update: -633.92, Optimal reward -230.25
Iteration 157 took 2.42 seconds (mean sampled reward: -5867.87). Current reward after update: -718.41, Optimal reward -230.25
Iteration 158 took 2.41 seconds (mean sampled reward: -5691.12). Current reward after update: -660.78, Optimal reward -230.25
Iteration 159 took 2.37 seconds (mean sampled reward: -6215.52). Current reward after update: -909.95, Optimal reward -230.25
Iteration 160 took 2.49 seconds (mean sampled reward: -6128.99). Current reward after update: -785.29, Optimal reward -230.25
Iteration 161 took 2.31 seconds (mean sampled reward: -5846.44). Current reward after update: -736.79, Optimal reward -230.25
Iteration 162 took 2.39 seconds (mean sampled reward: -5634.13). Current reward after update: -932.80, Optimal reward -230.25
Iteration 163 took 2.50 seconds (mean sampled reward: -6102.26). Current reward after update: -2099.46, Optimal reward -230.25
Iteration 164 took 2.50 seconds (mean sampled reward: -5818.81). Current reward after update: -982.89, Optimal reward -230.25
Iteration 165 took 2.55 seconds (mean sampled reward: -6034.25). Current reward after update: -1093.73, Optimal reward -230.25
Iteration 166 took 2.42 seconds (mean sampled reward: -6223.15). Current reward after update: -1036.05, Optimal reward -230.25
Iteration 167 took 2.49 seconds (mean sampled reward: -6082.51). Current reward after update: -1303.60, Optimal reward -230.25
Iteration 168 took 2.54 seconds (mean sampled reward: -6422.43). Current reward after update: -1378.22, Optimal reward -230.25
Iteration 169 took 2.45 seconds (mean sampled reward: -5743.15). Current reward after update: -1418.39, Optimal reward -230.25
Iteration 170 took 2.38 seconds (mean sampled reward: -5968.85). Current reward after update: -1208.01, Optimal reward -230.25
Iteration 171 took 2.44 seconds (mean sampled reward: -5611.79). Current reward after update: -2625.04, Optimal reward -230.25
Iteration 172 took 2.43 seconds (mean sampled reward: -5349.50). Current reward after update: -933.44, Optimal reward -230.25
Iteration 173 took 2.48 seconds (mean sampled reward: -5716.81). Current reward after update: -884.20, Optimal reward -230.25
Iteration 174 took 2.41 seconds (mean sampled reward: -5878.95). Current reward after update: -695.77, Optimal reward -230.25
Iteration 175 took 2.46 seconds (mean sampled reward: -5931.69). Current reward after update: -876.87, Optimal reward -230.25
Iteration 176 took 2.38 seconds (mean sampled reward: -6406.52). Current reward after update: -703.99, Optimal reward -230.25
Iteration 177 took 2.37 seconds (mean sampled reward: -5616.81). Current reward after update: -656.11, Optimal reward -230.25
Iteration 178 took 2.40 seconds (mean sampled reward: -5347.27). Current reward after update: -750.38, Optimal reward -230.25
Iteration 179 took 2.36 seconds (mean sampled reward: -5419.23). Current reward after update: -904.61, Optimal reward -230.25
Iteration 180 took 2.31 seconds (mean sampled reward: -5345.94). Current reward after update: -640.36, Optimal reward -230.25
Iteration 181 took 2.32 seconds (mean sampled reward: -5123.48). Current reward after update: -779.89, Optimal reward -230.25
Iteration 182 took 2.35 seconds (mean sampled reward: -4959.44). Current reward after update: -739.52, Optimal reward -230.25
Iteration 183 took 2.41 seconds (mean sampled reward: -5397.30). Current reward after update: -1098.82, Optimal reward -230.25
Iteration 184 took 2.41 seconds (mean sampled reward: -5651.59). Current reward after update: -850.37, Optimal reward -230.25
Iteration 185 took 2.39 seconds (mean sampled reward: -5175.54). Current reward after update: -783.02, Optimal reward -230.25
Iteration 186 took 2.37 seconds (mean sampled reward: -5133.67). Current reward after update: -602.56, Optimal reward -230.25
Iteration 187 took 2.33 seconds (mean sampled reward: -4102.98). Current reward after update: -643.51, Optimal reward -230.25
Iteration 188 took 2.30 seconds (mean sampled reward: -4414.43). Current reward after update: -574.71, Optimal reward -230.25
Iteration 189 took 2.38 seconds (mean sampled reward: -5281.47). Current reward after update: -828.64, Optimal reward -230.25
Iteration 190 took 2.39 seconds (mean sampled reward: -5540.95). Current reward after update: -713.79, Optimal reward -230.25
Iteration 191 took 2.29 seconds (mean sampled reward: -4574.58). Current reward after update: -597.18, Optimal reward -230.25
Iteration 192 took 2.35 seconds (mean sampled reward: -4799.13). Current reward after update: -589.36, Optimal reward -230.25
Iteration 193 took 2.32 seconds (mean sampled reward: -4704.47). Current reward after update: -585.89, Optimal reward -230.25
Iteration 194 took 2.31 seconds (mean sampled reward: -4794.43). Current reward after update: -576.84, Optimal reward -230.25
Iteration 195 took 2.24 seconds (mean sampled reward: -4697.76). Current reward after update: -986.58, Optimal reward -230.25
Iteration 196 took 2.24 seconds (mean sampled reward: -4557.16). Current reward after update: -650.01, Optimal reward -230.25
Iteration 197 took 2.36 seconds (mean sampled reward: -5148.92). Current reward after update: -922.16, Optimal reward -230.25
Iteration 198 took 2.37 seconds (mean sampled reward: -4628.45). Current reward after update: -675.68, Optimal reward -230.25
Iteration 199 took 2.42 seconds (mean sampled reward: -6187.03). Current reward after update: -713.15, Optimal reward -230.25
Iteration 200 took 2.41 seconds (mean sampled reward: -5046.02). Current reward after update: -905.67, Optimal reward -230.25
Iteration 1 took 2.42 seconds (mean sampled reward: -7627.79). Current reward after update: -6270.26, Optimal reward -6270.26
Iteration 2 took 2.28 seconds (mean sampled reward: -7388.09). Current reward after update: -2953.98, Optimal reward -2953.98
Iteration 3 took 2.29 seconds (mean sampled reward: -7097.97). Current reward after update: -2048.69, Optimal reward -2048.69
Iteration 4 took 2.20 seconds (mean sampled reward: -6759.77). Current reward after update: -1760.97, Optimal reward -1760.97
Iteration 5 took 2.40 seconds (mean sampled reward: -6986.10). Current reward after update: -1672.79, Optimal reward -1672.79
Iteration 6 took 2.36 seconds (mean sampled reward: -5998.32). Current reward after update: -1864.10, Optimal reward -1672.79
Iteration 7 took 2.27 seconds (mean sampled reward: -5668.50). Current reward after update: -1477.15, Optimal reward -1477.15
Iteration 8 took 2.63 seconds (mean sampled reward: -5661.03). Current reward after update: -1205.14, Optimal reward -1205.14
Iteration 9 took 2.22 seconds (mean sampled reward: -4855.36). Current reward after update: -1458.11, Optimal reward -1205.14
Iteration 10 took 2.22 seconds (mean sampled reward: -4348.90). Current reward after update: -1616.59, Optimal reward -1205.14
Iteration 11 took 2.30 seconds (mean sampled reward: -5157.80). Current reward after update: -1451.33, Optimal reward -1205.14
Iteration 12 took 2.18 seconds (mean sampled reward: -6095.97). Current reward after update: -1738.01, Optimal reward -1205.14
Iteration 13 took 2.14 seconds (mean sampled reward: -5536.38). Current reward after update: -1341.69, Optimal reward -1205.14
Iteration 14 took 2.10 seconds (mean sampled reward: -5348.12). Current reward after update: -1196.40, Optimal reward -1196.40
Iteration 15 took 2.21 seconds (mean sampled reward: -5603.18). Current reward after update: -1410.60, Optimal reward -1196.40
Iteration 16 took 2.24 seconds (mean sampled reward: -5550.38). Current reward after update: -1487.73, Optimal reward -1196.40
Iteration 17 took 2.30 seconds (mean sampled reward: -5130.42). Current reward after update: -1196.47, Optimal reward -1196.40
Iteration 18 took 2.07 seconds (mean sampled reward: -5202.07). Current reward after update: -1245.76, Optimal reward -1196.40
Iteration 19 took 2.48 seconds (mean sampled reward: -5440.65). Current reward after update: -2034.49, Optimal reward -1196.40
Iteration 20 took 2.12 seconds (mean sampled reward: -5174.60). Current reward after update: -3523.36, Optimal reward -1196.40
Iteration 21 took 2.04 seconds (mean sampled reward: -5612.70). Current reward after update: -4591.36, Optimal reward -1196.40
Iteration 22 took 2.08 seconds (mean sampled reward: -6424.41). Current reward after update: -1542.79, Optimal reward -1196.40
Iteration 23 took 2.22 seconds (mean sampled reward: -5781.49). Current reward after update: -1178.76, Optimal reward -1178.76
Iteration 24 took 2.18 seconds (mean sampled reward: -5891.23). Current reward after update: -1128.78, Optimal reward -1128.78
Iteration 25 took 2.16 seconds (mean sampled reward: -6280.89). Current reward after update: -844.63, Optimal reward -844.63
Iteration 26 took 2.16 seconds (mean sampled reward: -6944.10). Current reward after update: -1636.79, Optimal reward -844.63
Iteration 27 took 2.18 seconds (mean sampled reward: -6271.52). Current reward after update: -1444.85, Optimal reward -844.63
Iteration 28 took 2.17 seconds (mean sampled reward: -6393.01). Current reward after update: -1212.01, Optimal reward -844.63
Iteration 29 took 2.29 seconds (mean sampled reward: -6132.99). Current reward after update: -1077.87, Optimal reward -844.63
Iteration 30 took 2.39 seconds (mean sampled reward: -5228.39). Current reward after update: -1000.11, Optimal reward -844.63
Iteration 31 took 2.15 seconds (mean sampled reward: -4600.58). Current reward after update: -896.89, Optimal reward -844.63
Iteration 32 took 2.15 seconds (mean sampled reward: -4769.06). Current reward after update: -1352.25, Optimal reward -844.63
Iteration 33 took 2.17 seconds (mean sampled reward: -4699.49). Current reward after update: -852.27, Optimal reward -844.63
Iteration 34 took 2.24 seconds (mean sampled reward: -4715.35). Current reward after update: -975.61, Optimal reward -844.63
Iteration 35 took 2.17 seconds (mean sampled reward: -5628.21). Current reward after update: -867.67, Optimal reward -844.63
Iteration 36 took 2.32 seconds (mean sampled reward: -5356.95). Current reward after update: -979.44, Optimal reward -844.63
Iteration 37 took 2.18 seconds (mean sampled reward: -4914.87). Current reward after update: -965.65, Optimal reward -844.63
Iteration 38 took 2.23 seconds (mean sampled reward: -4744.60). Current reward after update: -837.81, Optimal reward -837.81
Iteration 39 took 2.22 seconds (mean sampled reward: -5624.42). Current reward after update: -842.00, Optimal reward -837.81
Iteration 40 took 2.25 seconds (mean sampled reward: -5659.96). Current reward after update: -1050.15, Optimal reward -837.81
Iteration 41 took 2.34 seconds (mean sampled reward: -5018.83). Current reward after update: -902.06, Optimal reward -837.81
Iteration 42 took 2.21 seconds (mean sampled reward: -5602.77). Current reward after update: -1003.74, Optimal reward -837.81
Iteration 43 took 2.25 seconds (mean sampled reward: -5550.74). Current reward after update: -1072.85, Optimal reward -837.81
Iteration 44 took 2.25 seconds (mean sampled reward: -5899.10). Current reward after update: -960.80, Optimal reward -837.81
Iteration 45 took 2.20 seconds (mean sampled reward: -5534.36). Current reward after update: -1054.52, Optimal reward -837.81
Iteration 46 took 2.21 seconds (mean sampled reward: -6002.15). Current reward after update: -870.87, Optimal reward -837.81
Iteration 47 took 2.27 seconds (mean sampled reward: -5673.41). Current reward after update: -1034.63, Optimal reward -837.81
Iteration 48 took 2.15 seconds (mean sampled reward: -5413.41). Current reward after update: -2743.97, Optimal reward -837.81
Iteration 49 took 2.16 seconds (mean sampled reward: -4964.77). Current reward after update: -1867.46, Optimal reward -837.81
Iteration 50 took 2.17 seconds (mean sampled reward: -6193.20). Current reward after update: -1079.63, Optimal reward -837.81
Iteration 51 took 2.10 seconds (mean sampled reward: -6750.28). Current reward after update: -1250.85, Optimal reward -837.81
Iteration 52 took 2.10 seconds (mean sampled reward: -5587.06). Current reward after update: -951.75, Optimal reward -837.81
Iteration 53 took 2.34 seconds (mean sampled reward: -4824.44). Current reward after update: -1040.45, Optimal reward -837.81
Iteration 54 took 2.33 seconds (mean sampled reward: -5724.39). Current reward after update: -1386.95, Optimal reward -837.81
Iteration 55 took 2.61 seconds (mean sampled reward: -4100.55). Current reward after update: -764.77, Optimal reward -764.77
Iteration 56 took 2.35 seconds (mean sampled reward: -3702.02). Current reward after update: -1054.26, Optimal reward -764.77
Iteration 57 took 2.14 seconds (mean sampled reward: -5227.15). Current reward after update: -983.28, Optimal reward -764.77
Iteration 58 took 2.25 seconds (mean sampled reward: -4535.74). Current reward after update: -962.31, Optimal reward -764.77
Iteration 59 took 2.25 seconds (mean sampled reward: -5170.32). Current reward after update: -1147.59, Optimal reward -764.77
Iteration 60 took 2.53 seconds (mean sampled reward: -3816.18). Current reward after update: -968.38, Optimal reward -764.77
Iteration 61 took 2.26 seconds (mean sampled reward: -5506.87). Current reward after update: -1063.62, Optimal reward -764.77
Iteration 62 took 2.28 seconds (mean sampled reward: -4949.71). Current reward after update: -894.67, Optimal reward -764.77
Iteration 63 took 2.25 seconds (mean sampled reward: -6306.30). Current reward after update: -1476.06, Optimal reward -764.77
Iteration 64 took 2.15 seconds (mean sampled reward: -6717.31). Current reward after update: -1176.37, Optimal reward -764.77
Iteration 65 took 2.07 seconds (mean sampled reward: -5606.54). Current reward after update: -987.96, Optimal reward -764.77
Iteration 66 took 2.10 seconds (mean sampled reward: -6016.94). Current reward after update: -915.77, Optimal reward -764.77
Iteration 67 took 2.08 seconds (mean sampled reward: -4573.81). Current reward after update: -955.93, Optimal reward -764.77
Iteration 68 took 2.13 seconds (mean sampled reward: -4237.78). Current reward after update: -924.95, Optimal reward -764.77
Iteration 69 took 2.11 seconds (mean sampled reward: -5306.72). Current reward after update: -7156.07, Optimal reward -764.77
Iteration 70 took 2.08 seconds (mean sampled reward: -5961.40). Current reward after update: -1188.03, Optimal reward -764.77
Iteration 71 took 2.08 seconds (mean sampled reward: -5151.63). Current reward after update: -1360.58, Optimal reward -764.77
Iteration 72 took 2.09 seconds (mean sampled reward: -4087.27). Current reward after update: -866.62, Optimal reward -764.77
Iteration 73 took 2.08 seconds (mean sampled reward: -4872.55). Current reward after update: -864.53, Optimal reward -764.77
Iteration 74 took 2.07 seconds (mean sampled reward: -5685.71). Current reward after update: -1047.31, Optimal reward -764.77
Iteration 75 took 2.15 seconds (mean sampled reward: -5929.54). Current reward after update: -880.41, Optimal reward -764.77
Iteration 76 took 2.16 seconds (mean sampled reward: -5707.25). Current reward after update: -1007.72, Optimal reward -764.77
Iteration 77 took 2.13 seconds (mean sampled reward: -5115.95). Current reward after update: -929.45, Optimal reward -764.77
Iteration 78 took 2.14 seconds (mean sampled reward: -6105.12). Current reward after update: -750.72, Optimal reward -750.72
Iteration 79 took 2.30 seconds (mean sampled reward: -5035.46). Current reward after update: -1090.52, Optimal reward -750.72
Iteration 80 took 2.14 seconds (mean sampled reward: -5000.20). Current reward after update: -1036.25, Optimal reward -750.72
Iteration 81 took 2.12 seconds (mean sampled reward: -4965.91). Current reward after update: -2471.76, Optimal reward -750.72
Iteration 82 took 2.09 seconds (mean sampled reward: -5130.12). Current reward after update: -1004.51, Optimal reward -750.72
Iteration 83 took 2.06 seconds (mean sampled reward: -4902.96). Current reward after update: -1097.91, Optimal reward -750.72
Iteration 84 took 2.17 seconds (mean sampled reward: -3696.15). Current reward after update: -853.14, Optimal reward -750.72
Iteration 85 took 2.13 seconds (mean sampled reward: -4735.95). Current reward after update: -1039.36, Optimal reward -750.72
Iteration 86 took 2.15 seconds (mean sampled reward: -3748.33). Current reward after update: -906.70, Optimal reward -750.72
Iteration 87 took 2.16 seconds (mean sampled reward: -4283.64). Current reward after update: -992.30, Optimal reward -750.72
Iteration 88 took 2.17 seconds (mean sampled reward: -4301.40). Current reward after update: -1949.72, Optimal reward -750.72
Iteration 89 took 2.08 seconds (mean sampled reward: -5686.13). Current reward after update: -958.50, Optimal reward -750.72
Iteration 90 took 2.13 seconds (mean sampled reward: -4380.89). Current reward after update: -772.75, Optimal reward -750.72
Iteration 91 took 2.14 seconds (mean sampled reward: -4018.76). Current reward after update: -829.07, Optimal reward -750.72
Iteration 92 took 2.14 seconds (mean sampled reward: -4137.78). Current reward after update: -7277.18, Optimal reward -750.72
Iteration 93 took 2.26 seconds (mean sampled reward: -4649.51). Current reward after update: -1010.73, Optimal reward -750.72
Iteration 94 took 2.23 seconds (mean sampled reward: -3713.71). Current reward after update: -852.60, Optimal reward -750.72
Iteration 95 took 2.23 seconds (mean sampled reward: -4668.01). Current reward after update: -694.75, Optimal reward -694.75
Iteration 96 took 2.17 seconds (mean sampled reward: -5195.98). Current reward after update: -848.32, Optimal reward -694.75
Iteration 97 took 2.24 seconds (mean sampled reward: -3864.71). Current reward after update: -825.97, Optimal reward -694.75
Iteration 98 took 2.14 seconds (mean sampled reward: -5524.51). Current reward after update: -928.32, Optimal reward -694.75
Iteration 99 took 2.25 seconds (mean sampled reward: -4168.52). Current reward after update: -755.03, Optimal reward -694.75
Iteration 100 took 2.16 seconds (mean sampled reward: -5377.33). Current reward after update: -772.67, Optimal reward -694.75
Iteration 101 took 2.10 seconds (mean sampled reward: -5154.46). Current reward after update: -2191.03, Optimal reward -694.75
Iteration 102 took 2.13 seconds (mean sampled reward: -4403.82). Current reward after update: -806.94, Optimal reward -694.75
Iteration 103 took 2.15 seconds (mean sampled reward: -3895.52). Current reward after update: -710.40, Optimal reward -694.75
Iteration 104 took 2.10 seconds (mean sampled reward: -5141.35). Current reward after update: -793.19, Optimal reward -694.75
Iteration 105 took 2.17 seconds (mean sampled reward: -3955.54). Current reward after update: -649.66, Optimal reward -649.66
Iteration 106 took 2.19 seconds (mean sampled reward: -4912.34). Current reward after update: -791.66, Optimal reward -649.66
Iteration 107 took 2.36 seconds (mean sampled reward: -4277.23). Current reward after update: -678.14, Optimal reward -649.66
Iteration 108 took 2.27 seconds (mean sampled reward: -3912.90). Current reward after update: -891.19, Optimal reward -649.66
Iteration 109 took 2.19 seconds (mean sampled reward: -3685.72). Current reward after update: -711.79, Optimal reward -649.66
Iteration 110 took 2.24 seconds (mean sampled reward: -4728.32). Current reward after update: -3621.56, Optimal reward -649.66
Iteration 111 took 2.14 seconds (mean sampled reward: -5576.11). Current reward after update: -760.59, Optimal reward -649.66
Iteration 112 took 2.12 seconds (mean sampled reward: -5115.47). Current reward after update: -806.48, Optimal reward -649.66
Iteration 113 took 2.19 seconds (mean sampled reward: -4834.47). Current reward after update: -701.93, Optimal reward -649.66
Iteration 114 took 2.32 seconds (mean sampled reward: -4101.31). Current reward after update: -755.79, Optimal reward -649.66
Iteration 115 took 2.32 seconds (mean sampled reward: -3707.40). Current reward after update: -688.27, Optimal reward -649.66
Iteration 116 took 2.34 seconds (mean sampled reward: -4615.60). Current reward after update: -1186.20, Optimal reward -649.66
Iteration 117 took 2.21 seconds (mean sampled reward: -4301.85). Current reward after update: -603.13, Optimal reward -603.13
Iteration 118 took 2.35 seconds (mean sampled reward: -5124.25). Current reward after update: -652.20, Optimal reward -603.13
Iteration 119 took 2.23 seconds (mean sampled reward: -4671.33). Current reward after update: -1094.63, Optimal reward -603.13
Iteration 120 took 2.29 seconds (mean sampled reward: -4759.03). Current reward after update: -641.19, Optimal reward -603.13
Iteration 121 took 2.21 seconds (mean sampled reward: -4604.62). Current reward after update: -681.34, Optimal reward -603.13
Iteration 122 took 2.11 seconds (mean sampled reward: -4518.81). Current reward after update: -674.58, Optimal reward -603.13
Iteration 123 took 2.07 seconds (mean sampled reward: -5310.14). Current reward after update: -589.93, Optimal reward -589.93
Iteration 124 took 2.08 seconds (mean sampled reward: -4196.02). Current reward after update: -769.43, Optimal reward -589.93
Iteration 125 took 2.22 seconds (mean sampled reward: -5798.68). Current reward after update: -693.89, Optimal reward -589.93
Iteration 126 took 2.17 seconds (mean sampled reward: -4526.46). Current reward after update: -7150.88, Optimal reward -589.93
Iteration 127 took 2.15 seconds (mean sampled reward: -4336.55). Current reward after update: -541.29, Optimal reward -541.29
Iteration 128 took 2.17 seconds (mean sampled reward: -4028.64). Current reward after update: -617.39, Optimal reward -541.29
Iteration 129 took 2.16 seconds (mean sampled reward: -3475.60). Current reward after update: -607.38, Optimal reward -541.29
Iteration 130 took 2.16 seconds (mean sampled reward: -3399.81). Current reward after update: -574.46, Optimal reward -541.29
Iteration 131 took 2.17 seconds (mean sampled reward: -3502.89). Current reward after update: -682.60, Optimal reward -541.29
Iteration 132 took 2.15 seconds (mean sampled reward: -5154.17). Current reward after update: -728.04, Optimal reward -541.29
Iteration 133 took 2.22 seconds (mean sampled reward: -4526.11). Current reward after update: -576.94, Optimal reward -541.29
Iteration 134 took 2.20 seconds (mean sampled reward: -4110.85). Current reward after update: -7051.45, Optimal reward -541.29
Iteration 135 took 2.18 seconds (mean sampled reward: -5941.55). Current reward after update: -752.35, Optimal reward -541.29
Iteration 136 took 2.20 seconds (mean sampled reward: -5765.68). Current reward after update: -704.97, Optimal reward -541.29
Iteration 137 took 2.15 seconds (mean sampled reward: -3925.21). Current reward after update: -624.34, Optimal reward -541.29
Iteration 138 took 2.16 seconds (mean sampled reward: -2910.05). Current reward after update: -458.55, Optimal reward -458.55
Iteration 139 took 2.18 seconds (mean sampled reward: -4969.16). Current reward after update: -417.53, Optimal reward -417.53
Iteration 140 took 2.14 seconds (mean sampled reward: -5991.05). Current reward after update: -713.68, Optimal reward -417.53
Iteration 141 took 2.19 seconds (mean sampled reward: -4476.81). Current reward after update: -571.64, Optimal reward -417.53
Iteration 142 took 2.17 seconds (mean sampled reward: -5575.94). Current reward after update: -605.92, Optimal reward -417.53
Iteration 143 took 2.20 seconds (mean sampled reward: -3803.33). Current reward after update: -582.06, Optimal reward -417.53
Iteration 144 took 2.09 seconds (mean sampled reward: -2984.24). Current reward after update: -575.84, Optimal reward -417.53
Iteration 145 took 2.13 seconds (mean sampled reward: -3058.99). Current reward after update: -597.18, Optimal reward -417.53
Iteration 146 took 2.15 seconds (mean sampled reward: -3259.14). Current reward after update: -546.89, Optimal reward -417.53
Iteration 147 took 2.17 seconds (mean sampled reward: -3514.13). Current reward after update: -698.89, Optimal reward -417.53
Iteration 148 took 2.17 seconds (mean sampled reward: -3462.94). Current reward after update: -830.24, Optimal reward -417.53
Iteration 149 took 2.14 seconds (mean sampled reward: -3702.43). Current reward after update: -630.66, Optimal reward -417.53
Iteration 150 took 2.15 seconds (mean sampled reward: -4221.65). Current reward after update: -7235.24, Optimal reward -417.53
Iteration 151 took 2.10 seconds (mean sampled reward: -4057.95). Current reward after update: -664.47, Optimal reward -417.53
Iteration 152 took 2.08 seconds (mean sampled reward: -3739.23). Current reward after update: -598.54, Optimal reward -417.53
Iteration 153 took 2.11 seconds (mean sampled reward: -3431.78). Current reward after update: -755.56, Optimal reward -417.53
Iteration 154 took 2.18 seconds (mean sampled reward: -2938.61). Current reward after update: -655.34, Optimal reward -417.53
Iteration 155 took 2.09 seconds (mean sampled reward: -3748.03). Current reward after update: -486.43, Optimal reward -417.53
Iteration 156 took 2.16 seconds (mean sampled reward: -3685.63). Current reward after update: -747.82, Optimal reward -417.53
Iteration 157 took 2.10 seconds (mean sampled reward: -3960.75). Current reward after update: -926.94, Optimal reward -417.53
Iteration 158 took 2.02 seconds (mean sampled reward: -3400.06). Current reward after update: -570.57, Optimal reward -417.53
Iteration 159 took 2.12 seconds (mean sampled reward: -2981.28). Current reward after update: -7105.69, Optimal reward -417.53
Iteration 160 took 2.06 seconds (mean sampled reward: -4376.09). Current reward after update: -638.05, Optimal reward -417.53
Iteration 161 took 2.08 seconds (mean sampled reward: -4076.06). Current reward after update: -541.74, Optimal reward -417.53
Iteration 162 took 2.09 seconds (mean sampled reward: -4040.00). Current reward after update: -714.96, Optimal reward -417.53
Iteration 163 took 2.16 seconds (mean sampled reward: -4175.09). Current reward after update: -636.52, Optimal reward -417.53
Iteration 164 took 2.12 seconds (mean sampled reward: -3669.63). Current reward after update: -730.20, Optimal reward -417.53
Iteration 165 took 2.05 seconds (mean sampled reward: -3594.27). Current reward after update: -855.97, Optimal reward -417.53
Iteration 166 took 2.19 seconds (mean sampled reward: -4872.24). Current reward after update: -950.80, Optimal reward -417.53
Iteration 167 took 2.16 seconds (mean sampled reward: -4931.33). Current reward after update: -1013.02, Optimal reward -417.53
Iteration 168 took 2.12 seconds (mean sampled reward: -4705.00). Current reward after update: -849.90, Optimal reward -417.53
Iteration 169 took 2.15 seconds (mean sampled reward: -4118.79). Current reward after update: -694.59, Optimal reward -417.53
Iteration 170 took 2.12 seconds (mean sampled reward: -3911.48). Current reward after update: -788.12, Optimal reward -417.53
Iteration 171 took 2.11 seconds (mean sampled reward: -3657.70). Current reward after update: -707.25, Optimal reward -417.53
Iteration 172 took 2.12 seconds (mean sampled reward: -4515.93). Current reward after update: -605.11, Optimal reward -417.53
Iteration 173 took 2.08 seconds (mean sampled reward: -3812.36). Current reward after update: -639.42, Optimal reward -417.53
Iteration 174 took 2.12 seconds (mean sampled reward: -4330.73). Current reward after update: -725.35, Optimal reward -417.53
Iteration 175 took 2.07 seconds (mean sampled reward: -4361.37). Current reward after update: -518.57, Optimal reward -417.53
Iteration 176 took 2.10 seconds (mean sampled reward: -4838.38). Current reward after update: -764.65, Optimal reward -417.53
Iteration 177 took 2.08 seconds (mean sampled reward: -4688.13). Current reward after update: -706.84, Optimal reward -417.53
Iteration 178 took 2.03 seconds (mean sampled reward: -5242.45). Current reward after update: -682.09, Optimal reward -417.53
Iteration 179 took 2.14 seconds (mean sampled reward: -5308.01). Current reward after update: -895.80, Optimal reward -417.53
Iteration 180 took 2.12 seconds (mean sampled reward: -5306.31). Current reward after update: -547.02, Optimal reward -417.53
Iteration 181 took 2.15 seconds (mean sampled reward: -6355.55). Current reward after update: -911.60, Optimal reward -417.53
Iteration 182 took 2.09 seconds (mean sampled reward: -6075.14). Current reward after update: -595.82, Optimal reward -417.53
Iteration 183 took 2.17 seconds (mean sampled reward: -5814.89). Current reward after update: -630.86, Optimal reward -417.53
Iteration 184 took 2.04 seconds (mean sampled reward: -5383.24). Current reward after update: -601.44, Optimal reward -417.53
Iteration 185 took 2.19 seconds (mean sampled reward: -5638.58). Current reward after update: -629.55, Optimal reward -417.53
Iteration 186 took 2.26 seconds (mean sampled reward: -5119.21). Current reward after update: -608.92, Optimal reward -417.53
Iteration 187 took 2.19 seconds (mean sampled reward: -5902.73). Current reward after update: -783.37, Optimal reward -417.53
Iteration 188 took 2.17 seconds (mean sampled reward: -6443.57). Current reward after update: -1322.69, Optimal reward -417.53
Iteration 189 took 2.12 seconds (mean sampled reward: -6558.03). Current reward after update: -1179.34, Optimal reward -417.53
Iteration 190 took 2.14 seconds (mean sampled reward: -6179.75). Current reward after update: -1218.43, Optimal reward -417.53
Iteration 191 took 2.09 seconds (mean sampled reward: -5775.20). Current reward after update: -939.48, Optimal reward -417.53
Iteration 192 took 2.11 seconds (mean sampled reward: -5745.96). Current reward after update: -1073.75, Optimal reward -417.53
Iteration 193 took 2.08 seconds (mean sampled reward: -5854.82). Current reward after update: -1109.02, Optimal reward -417.53
Iteration 194 took 2.10 seconds (mean sampled reward: -5355.25). Current reward after update: -964.03, Optimal reward -417.53
Iteration 195 took 2.11 seconds (mean sampled reward: -4523.18). Current reward after update: -926.50, Optimal reward -417.53
Iteration 196 took 2.10 seconds (mean sampled reward: -4055.68). Current reward after update: -1175.47, Optimal reward -417.53
Iteration 197 took 2.18 seconds (mean sampled reward: -4438.96). Current reward after update: -4345.29, Optimal reward -417.53
Iteration 198 took 2.08 seconds (mean sampled reward: -4203.65). Current reward after update: -776.08, Optimal reward -417.53
Iteration 199 took 2.13 seconds (mean sampled reward: -4494.49). Current reward after update: -951.03, Optimal reward -417.53
Iteration 200 took 2.11 seconds (mean sampled reward: -4787.09). Current reward after update: -811.81, Optimal reward -417.53
Iteration 1 took 2.35 seconds (mean sampled reward: -7635.14). Current reward after update: -6253.04, Optimal reward -6253.04
Iteration 2 took 2.29 seconds (mean sampled reward: -7463.75). Current reward after update: -4173.90, Optimal reward -4173.90
Iteration 3 took 2.33 seconds (mean sampled reward: -6588.19). Current reward after update: -2919.67, Optimal reward -2919.67
Iteration 4 took 2.33 seconds (mean sampled reward: -6188.78). Current reward after update: -2032.74, Optimal reward -2032.74
Iteration 5 took 2.67 seconds (mean sampled reward: -5263.47). Current reward after update: -1403.15, Optimal reward -1403.15
Iteration 6 took 2.61 seconds (mean sampled reward: -5671.15). Current reward after update: -1451.95, Optimal reward -1403.15
Iteration 7 took 2.56 seconds (mean sampled reward: -6388.63). Current reward after update: -1483.29, Optimal reward -1403.15
Iteration 8 took 2.48 seconds (mean sampled reward: -5672.98). Current reward after update: -1427.03, Optimal reward -1403.15
Iteration 9 took 2.57 seconds (mean sampled reward: -6673.62). Current reward after update: -1601.24, Optimal reward -1403.15
Iteration 10 took 2.42 seconds (mean sampled reward: -5391.33). Current reward after update: -1365.28, Optimal reward -1365.28
Iteration 11 took 2.24 seconds (mean sampled reward: -4480.75). Current reward after update: -1231.44, Optimal reward -1231.44
Iteration 12 took 2.48 seconds (mean sampled reward: -5762.63). Current reward after update: -1171.70, Optimal reward -1171.70
Iteration 13 took 2.29 seconds (mean sampled reward: -5342.84). Current reward after update: -1687.08, Optimal reward -1171.70
Iteration 14 took 2.23 seconds (mean sampled reward: -3426.53). Current reward after update: -1112.16, Optimal reward -1112.16
Iteration 15 took 2.37 seconds (mean sampled reward: -3375.48). Current reward after update: -1123.16, Optimal reward -1112.16
Iteration 16 took 2.33 seconds (mean sampled reward: -4847.85). Current reward after update: -984.46, Optimal reward -984.46
Iteration 17 took 2.34 seconds (mean sampled reward: -5321.40). Current reward after update: -1060.70, Optimal reward -984.46
Iteration 18 took 2.23 seconds (mean sampled reward: -4279.43). Current reward after update: -817.63, Optimal reward -817.63
Iteration 19 took 2.28 seconds (mean sampled reward: -4587.21). Current reward after update: -790.24, Optimal reward -790.24
Iteration 20 took 2.18 seconds (mean sampled reward: -5679.46). Current reward after update: -879.83, Optimal reward -790.24
Iteration 21 took 2.34 seconds (mean sampled reward: -5838.71). Current reward after update: -7184.92, Optimal reward -790.24
Iteration 22 took 2.34 seconds (mean sampled reward: -6434.84). Current reward after update: -1056.41, Optimal reward -790.24
Iteration 23 took 2.27 seconds (mean sampled reward: -6750.85). Current reward after update: -1318.28, Optimal reward -790.24
Iteration 24 took 2.31 seconds (mean sampled reward: -5602.94). Current reward after update: -885.59, Optimal reward -790.24
Iteration 25 took 2.21 seconds (mean sampled reward: -5984.53). Current reward after update: -1685.50, Optimal reward -790.24
Iteration 26 took 2.22 seconds (mean sampled reward: -5387.79). Current reward after update: -1013.12, Optimal reward -790.24
Iteration 27 took 2.25 seconds (mean sampled reward: -5506.95). Current reward after update: -1065.70, Optimal reward -790.24
Iteration 28 took 2.42 seconds (mean sampled reward: -5171.59). Current reward after update: -1228.82, Optimal reward -790.24
Iteration 29 took 2.34 seconds (mean sampled reward: -5632.18). Current reward after update: -1106.71, Optimal reward -790.24
Iteration 30 took 2.42 seconds (mean sampled reward: -6947.70). Current reward after update: -809.70, Optimal reward -790.24
Iteration 31 took 2.22 seconds (mean sampled reward: -6574.91). Current reward after update: -1019.70, Optimal reward -790.24
Iteration 32 took 2.41 seconds (mean sampled reward: -6929.24). Current reward after update: -967.26, Optimal reward -790.24
Iteration 33 took 2.24 seconds (mean sampled reward: -6232.75). Current reward after update: -827.76, Optimal reward -790.24
Iteration 34 took 2.24 seconds (mean sampled reward: -5427.09). Current reward after update: -1022.12, Optimal reward -790.24
Iteration 35 took 2.25 seconds (mean sampled reward: -6835.28). Current reward after update: -1233.26, Optimal reward -790.24
Iteration 36 took 2.28 seconds (mean sampled reward: -6400.78). Current reward after update: -991.05, Optimal reward -790.24
Iteration 37 took 2.34 seconds (mean sampled reward: -5984.62). Current reward after update: -996.03, Optimal reward -790.24
Iteration 38 took 2.21 seconds (mean sampled reward: -4913.32). Current reward after update: -860.95, Optimal reward -790.24
Iteration 39 took 2.21 seconds (mean sampled reward: -5333.42). Current reward after update: -948.27, Optimal reward -790.24
Iteration 40 took 2.27 seconds (mean sampled reward: -6129.19). Current reward after update: -1125.60, Optimal reward -790.24
Iteration 41 took 2.17 seconds (mean sampled reward: -6059.28). Current reward after update: -1286.50, Optimal reward -790.24
Iteration 42 took 2.22 seconds (mean sampled reward: -5609.19). Current reward after update: -994.00, Optimal reward -790.24
Iteration 43 took 2.24 seconds (mean sampled reward: -5757.42). Current reward after update: -1133.85, Optimal reward -790.24
Iteration 44 took 2.22 seconds (mean sampled reward: -5382.14). Current reward after update: -920.78, Optimal reward -790.24
Iteration 45 took 2.21 seconds (mean sampled reward: -6164.50). Current reward after update: -1145.34, Optimal reward -790.24
Iteration 46 took 2.10 seconds (mean sampled reward: -4786.84). Current reward after update: -979.55, Optimal reward -790.24
Iteration 47 took 2.19 seconds (mean sampled reward: -4221.57). Current reward after update: -838.96, Optimal reward -790.24
Iteration 48 took 2.20 seconds (mean sampled reward: -5042.40). Current reward after update: -825.90, Optimal reward -790.24
Iteration 49 took 2.16 seconds (mean sampled reward: -4827.65). Current reward after update: -962.45, Optimal reward -790.24
Iteration 50 took 2.12 seconds (mean sampled reward: -4970.78). Current reward after update: -968.44, Optimal reward -790.24
Iteration 51 took 2.08 seconds (mean sampled reward: -4553.42). Current reward after update: -4148.84, Optimal reward -790.24
Iteration 52 took 2.13 seconds (mean sampled reward: -4385.98). Current reward after update: -1088.90, Optimal reward -790.24
Iteration 53 took 2.30 seconds (mean sampled reward: -4585.29). Current reward after update: -1067.91, Optimal reward -790.24
Iteration 54 took 2.17 seconds (mean sampled reward: -5213.45). Current reward after update: -816.83, Optimal reward -790.24
Iteration 55 took 2.25 seconds (mean sampled reward: -5117.05). Current reward after update: -864.48, Optimal reward -790.24
Iteration 56 took 2.34 seconds (mean sampled reward: -5156.70). Current reward after update: -866.25, Optimal reward -790.24
Iteration 57 took 2.17 seconds (mean sampled reward: -5251.71). Current reward after update: -775.18, Optimal reward -775.18
Iteration 58 took 2.22 seconds (mean sampled reward: -5008.97). Current reward after update: -928.91, Optimal reward -775.18
Iteration 59 took 2.31 seconds (mean sampled reward: -4773.03). Current reward after update: -1480.05, Optimal reward -775.18
Iteration 60 took 2.48 seconds (mean sampled reward: -5841.12). Current reward after update: -842.46, Optimal reward -775.18
Iteration 61 took 2.40 seconds (mean sampled reward: -5759.03). Current reward after update: -1239.81, Optimal reward -775.18
Iteration 62 took 2.32 seconds (mean sampled reward: -5753.75). Current reward after update: -1728.64, Optimal reward -775.18
Iteration 63 took 2.31 seconds (mean sampled reward: -4890.93). Current reward after update: -887.30, Optimal reward -775.18
Iteration 64 took 2.28 seconds (mean sampled reward: -5140.67). Current reward after update: -680.33, Optimal reward -680.33
Iteration 65 took 2.44 seconds (mean sampled reward: -4894.04). Current reward after update: -814.04, Optimal reward -680.33
Iteration 66 took 2.35 seconds (mean sampled reward: -5995.94). Current reward after update: -827.03, Optimal reward -680.33
Iteration 67 took 2.24 seconds (mean sampled reward: -4514.80). Current reward after update: -736.83, Optimal reward -680.33
Iteration 68 took 2.27 seconds (mean sampled reward: -5470.80). Current reward after update: -932.55, Optimal reward -680.33
Iteration 69 took 2.30 seconds (mean sampled reward: -5637.81). Current reward after update: -813.24, Optimal reward -680.33
Iteration 70 took 2.28 seconds (mean sampled reward: -5801.74). Current reward after update: -795.76, Optimal reward -680.33
Iteration 71 took 2.27 seconds (mean sampled reward: -5698.34). Current reward after update: -915.19, Optimal reward -680.33
Iteration 72 took 2.33 seconds (mean sampled reward: -5071.08). Current reward after update: -1021.52, Optimal reward -680.33
Iteration 73 took 2.27 seconds (mean sampled reward: -5462.30). Current reward after update: -1316.29, Optimal reward -680.33
Iteration 74 took 2.36 seconds (mean sampled reward: -4486.18). Current reward after update: -5408.11, Optimal reward -680.33
Iteration 75 took 2.26 seconds (mean sampled reward: -4869.06). Current reward after update: -957.39, Optimal reward -680.33
Iteration 76 took 2.24 seconds (mean sampled reward: -5073.62). Current reward after update: -935.76, Optimal reward -680.33
Iteration 77 took 2.33 seconds (mean sampled reward: -5065.44). Current reward after update: -787.09, Optimal reward -680.33
Iteration 78 took 2.33 seconds (mean sampled reward: -4847.08). Current reward after update: -894.48, Optimal reward -680.33
Iteration 79 took 2.35 seconds (mean sampled reward: -5495.62). Current reward after update: -882.90, Optimal reward -680.33
Iteration 80 took 2.30 seconds (mean sampled reward: -5078.25). Current reward after update: -1029.63, Optimal reward -680.33
Iteration 81 took 2.36 seconds (mean sampled reward: -5584.80). Current reward after update: -1015.22, Optimal reward -680.33
Iteration 82 took 2.30 seconds (mean sampled reward: -4785.91). Current reward after update: -888.39, Optimal reward -680.33
Iteration 83 took 2.30 seconds (mean sampled reward: -3820.25). Current reward after update: -772.52, Optimal reward -680.33
Iteration 84 took 2.31 seconds (mean sampled reward: -5006.28). Current reward after update: -998.70, Optimal reward -680.33
Iteration 85 took 2.31 seconds (mean sampled reward: -4635.14). Current reward after update: -900.55, Optimal reward -680.33
Iteration 86 took 2.27 seconds (mean sampled reward: -4397.24). Current reward after update: -872.02, Optimal reward -680.33
Iteration 87 took 2.35 seconds (mean sampled reward: -5899.86). Current reward after update: -1033.75, Optimal reward -680.33
Iteration 88 took 2.26 seconds (mean sampled reward: -5105.34). Current reward after update: -848.95, Optimal reward -680.33
Iteration 89 took 2.22 seconds (mean sampled reward: -5445.48). Current reward after update: -781.31, Optimal reward -680.33
Iteration 90 took 2.26 seconds (mean sampled reward: -6171.31). Current reward after update: -863.73, Optimal reward -680.33
Iteration 91 took 2.31 seconds (mean sampled reward: -6096.05). Current reward after update: -751.85, Optimal reward -680.33
Iteration 92 took 2.24 seconds (mean sampled reward: -6336.70). Current reward after update: -645.37, Optimal reward -645.37
Iteration 93 took 2.43 seconds (mean sampled reward: -6042.79). Current reward after update: -857.08, Optimal reward -645.37
Iteration 94 took 2.34 seconds (mean sampled reward: -6028.61). Current reward after update: -1044.77, Optimal reward -645.37
Iteration 95 took 2.34 seconds (mean sampled reward: -6261.11). Current reward after update: -938.56, Optimal reward -645.37
Iteration 96 took 2.32 seconds (mean sampled reward: -6028.43). Current reward after update: -945.96, Optimal reward -645.37
Iteration 97 took 2.29 seconds (mean sampled reward: -6255.47). Current reward after update: -1091.90, Optimal reward -645.37
Iteration 98 took 2.35 seconds (mean sampled reward: -6467.51). Current reward after update: -857.22, Optimal reward -645.37
Iteration 99 took 2.30 seconds (mean sampled reward: -6212.05). Current reward after update: -765.16, Optimal reward -645.37
Iteration 100 took 2.33 seconds (mean sampled reward: -6426.18). Current reward after update: -724.37, Optimal reward -645.37
Iteration 101 took 2.34 seconds (mean sampled reward: -6615.75). Current reward after update: -907.74, Optimal reward -645.37
Iteration 102 took 2.39 seconds (mean sampled reward: -6173.72). Current reward after update: -857.83, Optimal reward -645.37
Iteration 103 took 2.47 seconds (mean sampled reward: -6026.65). Current reward after update: -860.13, Optimal reward -645.37
Iteration 104 took 2.30 seconds (mean sampled reward: -6417.74). Current reward after update: -1069.52, Optimal reward -645.37
Iteration 105 took 2.28 seconds (mean sampled reward: -5880.76). Current reward after update: -774.37, Optimal reward -645.37
Iteration 106 took 2.35 seconds (mean sampled reward: -5824.87). Current reward after update: -1342.12, Optimal reward -645.37
Iteration 107 took 2.33 seconds (mean sampled reward: -6176.33). Current reward after update: -822.08, Optimal reward -645.37
Iteration 108 took 2.45 seconds (mean sampled reward: -6514.51). Current reward after update: -906.91, Optimal reward -645.37
Iteration 109 took 2.38 seconds (mean sampled reward: -6136.38). Current reward after update: -6459.01, Optimal reward -645.37
Iteration 110 took 2.29 seconds (mean sampled reward: -5740.88). Current reward after update: -740.58, Optimal reward -645.37
Iteration 111 took 2.60 seconds (mean sampled reward: -5903.09). Current reward after update: -834.57, Optimal reward -645.37
Iteration 112 took 2.37 seconds (mean sampled reward: -5600.43). Current reward after update: -816.57, Optimal reward -645.37
Iteration 113 took 2.38 seconds (mean sampled reward: -5816.16). Current reward after update: -750.40, Optimal reward -645.37
Iteration 114 took 2.48 seconds (mean sampled reward: -5238.25). Current reward after update: -891.98, Optimal reward -645.37
Iteration 115 took 2.33 seconds (mean sampled reward: -5016.97). Current reward after update: -1677.70, Optimal reward -645.37
Iteration 116 took 2.28 seconds (mean sampled reward: -6369.91). Current reward after update: -906.92, Optimal reward -645.37
Iteration 117 took 2.33 seconds (mean sampled reward: -6512.02). Current reward after update: -1119.51, Optimal reward -645.37
Iteration 118 took 2.33 seconds (mean sampled reward: -5924.71). Current reward after update: -825.61, Optimal reward -645.37
Iteration 119 took 2.33 seconds (mean sampled reward: -5667.74). Current reward after update: -1014.08, Optimal reward -645.37
Iteration 120 took 2.31 seconds (mean sampled reward: -4351.51). Current reward after update: -939.26, Optimal reward -645.37
Iteration 121 took 2.39 seconds (mean sampled reward: -5017.59). Current reward after update: -1005.31, Optimal reward -645.37
Iteration 122 took 2.30 seconds (mean sampled reward: -5387.36). Current reward after update: -935.80, Optimal reward -645.37
Iteration 123 took 2.31 seconds (mean sampled reward: -5383.42). Current reward after update: -926.98, Optimal reward -645.37
Iteration 124 took 2.31 seconds (mean sampled reward: -5248.55). Current reward after update: -845.25, Optimal reward -645.37
Iteration 125 took 2.34 seconds (mean sampled reward: -6469.54). Current reward after update: -1034.50, Optimal reward -645.37
Iteration 126 took 2.35 seconds (mean sampled reward: -6590.08). Current reward after update: -1181.90, Optimal reward -645.37
Iteration 127 took 2.30 seconds (mean sampled reward: -5846.45). Current reward after update: -964.01, Optimal reward -645.37
Iteration 128 took 2.32 seconds (mean sampled reward: -6036.72). Current reward after update: -1042.30, Optimal reward -645.37
Iteration 129 took 2.32 seconds (mean sampled reward: -6223.71). Current reward after update: -1012.89, Optimal reward -645.37
Iteration 130 took 2.35 seconds (mean sampled reward: -6658.40). Current reward after update: -970.68, Optimal reward -645.37
Iteration 131 took 2.35 seconds (mean sampled reward: -6667.81). Current reward after update: -1038.14, Optimal reward -645.37
Iteration 132 took 2.36 seconds (mean sampled reward: -6396.99). Current reward after update: -1006.57, Optimal reward -645.37
Iteration 133 took 2.34 seconds (mean sampled reward: -6265.82). Current reward after update: -1107.11, Optimal reward -645.37
Iteration 134 took 2.36 seconds (mean sampled reward: -6405.86). Current reward after update: -1383.92, Optimal reward -645.37
Iteration 135 took 2.34 seconds (mean sampled reward: -6363.25). Current reward after update: -925.05, Optimal reward -645.37
Iteration 136 took 2.33 seconds (mean sampled reward: -6341.03). Current reward after update: -989.80, Optimal reward -645.37
Iteration 137 took 2.32 seconds (mean sampled reward: -6220.96). Current reward after update: -1044.22, Optimal reward -645.37
Iteration 138 took 2.31 seconds (mean sampled reward: -6772.50). Current reward after update: -1078.86, Optimal reward -645.37
Iteration 139 took 2.35 seconds (mean sampled reward: -6250.27). Current reward after update: -1427.02, Optimal reward -645.37
Iteration 140 took 2.34 seconds (mean sampled reward: -6734.81). Current reward after update: -1136.25, Optimal reward -645.37
Iteration 141 took 2.42 seconds (mean sampled reward: -6660.91). Current reward after update: -1142.39, Optimal reward -645.37
Iteration 142 took 2.32 seconds (mean sampled reward: -5803.20). Current reward after update: -1152.95, Optimal reward -645.37
Iteration 143 took 2.34 seconds (mean sampled reward: -6082.18). Current reward after update: -1081.26, Optimal reward -645.37
Iteration 144 took 2.35 seconds (mean sampled reward: -6492.34). Current reward after update: -2185.23, Optimal reward -645.37
Iteration 145 took 2.28 seconds (mean sampled reward: -5662.85). Current reward after update: -1179.70, Optimal reward -645.37
Iteration 146 took 2.30 seconds (mean sampled reward: -5321.54). Current reward after update: -1159.23, Optimal reward -645.37
Iteration 147 took 2.33 seconds (mean sampled reward: -5985.02). Current reward after update: -1075.41, Optimal reward -645.37
Iteration 148 took 2.36 seconds (mean sampled reward: -6445.37). Current reward after update: -1264.62, Optimal reward -645.37
Iteration 149 took 2.25 seconds (mean sampled reward: -6062.20). Current reward after update: -1001.57, Optimal reward -645.37
Iteration 150 took 2.33 seconds (mean sampled reward: -6500.49). Current reward after update: -1515.96, Optimal reward -645.37
Iteration 151 took 2.36 seconds (mean sampled reward: -6371.98). Current reward after update: -1110.32, Optimal reward -645.37
Iteration 152 took 2.34 seconds (mean sampled reward: -6602.85). Current reward after update: -1047.55, Optimal reward -645.37
Iteration 153 took 2.26 seconds (mean sampled reward: -5915.21). Current reward after update: -1234.17, Optimal reward -645.37
Iteration 154 took 2.27 seconds (mean sampled reward: -5446.70). Current reward after update: -983.49, Optimal reward -645.37
Iteration 155 took 2.32 seconds (mean sampled reward: -6003.17). Current reward after update: -987.46, Optimal reward -645.37
Iteration 156 took 2.38 seconds (mean sampled reward: -5860.67). Current reward after update: -952.23, Optimal reward -645.37
Iteration 157 took 2.26 seconds (mean sampled reward: -5390.70). Current reward after update: -1003.25, Optimal reward -645.37
Iteration 158 took 2.48 seconds (mean sampled reward: -6250.64). Current reward after update: -1314.61, Optimal reward -645.37
Iteration 159 took 2.37 seconds (mean sampled reward: -5183.46). Current reward after update: -925.60, Optimal reward -645.37
Iteration 160 took 2.31 seconds (mean sampled reward: -6056.64). Current reward after update: -977.54, Optimal reward -645.37
Iteration 161 took 2.37 seconds (mean sampled reward: -6403.64). Current reward after update: -1117.83, Optimal reward -645.37
Iteration 162 took 2.36 seconds (mean sampled reward: -6040.36). Current reward after update: -1047.53, Optimal reward -645.37
Iteration 163 took 2.38 seconds (mean sampled reward: -6374.89). Current reward after update: -1193.00, Optimal reward -645.37
Iteration 164 took 2.42 seconds (mean sampled reward: -6115.09). Current reward after update: -1100.04, Optimal reward -645.37
Iteration 165 took 2.36 seconds (mean sampled reward: -6229.90). Current reward after update: -1190.87, Optimal reward -645.37
Iteration 166 took 2.34 seconds (mean sampled reward: -6299.77). Current reward after update: -1184.12, Optimal reward -645.37
Iteration 167 took 2.39 seconds (mean sampled reward: -6796.95). Current reward after update: -1291.91, Optimal reward -645.37
Iteration 168 took 2.30 seconds (mean sampled reward: -6354.09). Current reward after update: -1173.58, Optimal reward -645.37
Iteration 169 took 2.36 seconds (mean sampled reward: -5916.22). Current reward after update: -1285.30, Optimal reward -645.37
Iteration 170 took 2.41 seconds (mean sampled reward: -5697.14). Current reward after update: -6607.65, Optimal reward -645.37
Iteration 171 took 2.33 seconds (mean sampled reward: -5801.72). Current reward after update: -1409.59, Optimal reward -645.37
Iteration 172 took 2.31 seconds (mean sampled reward: -5767.55). Current reward after update: -1388.79, Optimal reward -645.37
Iteration 173 took 2.34 seconds (mean sampled reward: -5925.99). Current reward after update: -1686.63, Optimal reward -645.37
Iteration 174 took 2.37 seconds (mean sampled reward: -6082.93). Current reward after update: -1377.99, Optimal reward -645.37
Iteration 175 took 2.42 seconds (mean sampled reward: -5922.43). Current reward after update: -1595.00, Optimal reward -645.37
Iteration 176 took 2.43 seconds (mean sampled reward: -6819.85). Current reward after update: -1383.77, Optimal reward -645.37
Iteration 177 took 2.38 seconds (mean sampled reward: -6546.96). Current reward after update: -1415.20, Optimal reward -645.37
Iteration 178 took 2.39 seconds (mean sampled reward: -6788.62). Current reward after update: -1474.32, Optimal reward -645.37
Iteration 179 took 2.44 seconds (mean sampled reward: -6752.06). Current reward after update: -1683.63, Optimal reward -645.37
Iteration 180 took 2.39 seconds (mean sampled reward: -6922.27). Current reward after update: -1929.66, Optimal reward -645.37
Iteration 181 took 2.44 seconds (mean sampled reward: -6585.10). Current reward after update: -1495.92, Optimal reward -645.37
Iteration 182 took 2.42 seconds (mean sampled reward: -6570.51). Current reward after update: -1475.05, Optimal reward -645.37
Iteration 183 took 2.48 seconds (mean sampled reward: -6298.82). Current reward after update: -1423.95, Optimal reward -645.37
Iteration 184 took 2.39 seconds (mean sampled reward: -6171.51). Current reward after update: -1273.62, Optimal reward -645.37
Iteration 185 took 2.44 seconds (mean sampled reward: -6789.12). Current reward after update: -1291.72, Optimal reward -645.37
Iteration 186 took 2.41 seconds (mean sampled reward: -6195.60). Current reward after update: -1401.64, Optimal reward -645.37
Iteration 187 took 2.31 seconds (mean sampled reward: -5695.33). Current reward after update: -1355.28, Optimal reward -645.37
Iteration 188 took 2.41 seconds (mean sampled reward: -5925.35). Current reward after update: -1134.45, Optimal reward -645.37
Iteration 189 took 2.47 seconds (mean sampled reward: -6179.18). Current reward after update: -6443.52, Optimal reward -645.37
Iteration 190 took 2.39 seconds (mean sampled reward: -5627.65). Current reward after update: -925.69, Optimal reward -645.37
Iteration 191 took 2.36 seconds (mean sampled reward: -5462.76). Current reward after update: -1054.77, Optimal reward -645.37
Iteration 192 took 2.33 seconds (mean sampled reward: -5736.37). Current reward after update: -1026.24, Optimal reward -645.37
Iteration 193 took 2.37 seconds (mean sampled reward: -5841.15). Current reward after update: -1153.04, Optimal reward -645.37
Iteration 194 took 2.46 seconds (mean sampled reward: -5944.51). Current reward after update: -912.93, Optimal reward -645.37
Iteration 195 took 2.50 seconds (mean sampled reward: -5905.27). Current reward after update: -1076.30, Optimal reward -645.37
Iteration 196 took 2.55 seconds (mean sampled reward: -5327.54). Current reward after update: -839.86, Optimal reward -645.37
Iteration 197 took 2.36 seconds (mean sampled reward: -5271.06). Current reward after update: -846.46, Optimal reward -645.37
Iteration 198 took 2.41 seconds (mean sampled reward: -5299.72). Current reward after update: -6440.65, Optimal reward -645.37
Iteration 199 took 2.49 seconds (mean sampled reward: -5860.18). Current reward after update: -1026.88, Optimal reward -645.37
Iteration 200 took 2.40 seconds (mean sampled reward: -5413.15). Current reward after update: -1002.03, Optimal reward -645.37
Max force: 50 Sigma: 0.4 mean rewards: -431.05078907151426, best rewards:-230.25102200114978

Iteration 1 took 2.40 seconds (mean sampled reward: -7448.63). Current reward after update: -5325.18, Optimal reward -5325.18
Iteration 2 took 2.42 seconds (mean sampled reward: -6942.28). Current reward after update: -2594.63, Optimal reward -2594.63
Iteration 3 took 2.63 seconds (mean sampled reward: -7169.17). Current reward after update: -2209.13, Optimal reward -2209.13
Iteration 4 took 2.58 seconds (mean sampled reward: -6522.38). Current reward after update: -2299.88, Optimal reward -2209.13
Iteration 5 took 2.59 seconds (mean sampled reward: -6395.31). Current reward after update: -1865.86, Optimal reward -1865.86
Iteration 6 took 2.52 seconds (mean sampled reward: -6372.87). Current reward after update: -1787.32, Optimal reward -1787.32
Iteration 7 took 2.36 seconds (mean sampled reward: -5546.10). Current reward after update: -1721.54, Optimal reward -1721.54
Iteration 8 took 2.46 seconds (mean sampled reward: -5722.18). Current reward after update: -2201.51, Optimal reward -1721.54
Iteration 9 took 2.24 seconds (mean sampled reward: -5429.43). Current reward after update: -1979.66, Optimal reward -1721.54
Iteration 10 took 2.37 seconds (mean sampled reward: -5551.14). Current reward after update: -1957.08, Optimal reward -1721.54
Iteration 11 took 2.74 seconds (mean sampled reward: -5113.46). Current reward after update: -4865.60, Optimal reward -1721.54
Iteration 12 took 2.40 seconds (mean sampled reward: -5285.40). Current reward after update: -1776.16, Optimal reward -1721.54
Iteration 13 took 2.52 seconds (mean sampled reward: -4785.36). Current reward after update: -1702.22, Optimal reward -1702.22
Iteration 14 took 2.65 seconds (mean sampled reward: -5359.36). Current reward after update: -1585.45, Optimal reward -1585.45
Iteration 15 took 2.46 seconds (mean sampled reward: -4958.49). Current reward after update: -1486.39, Optimal reward -1486.39
Iteration 16 took 2.88 seconds (mean sampled reward: -5632.04). Current reward after update: -1314.32, Optimal reward -1314.32
Iteration 17 took 2.58 seconds (mean sampled reward: -5982.79). Current reward after update: -1747.91, Optimal reward -1314.32
Iteration 18 took 2.69 seconds (mean sampled reward: -5944.05). Current reward after update: -1427.89, Optimal reward -1314.32
Iteration 19 took 2.68 seconds (mean sampled reward: -5956.35). Current reward after update: -1503.48, Optimal reward -1314.32
Iteration 20 took 2.49 seconds (mean sampled reward: -5375.94). Current reward after update: -1340.42, Optimal reward -1314.32
Iteration 21 took 2.54 seconds (mean sampled reward: -6484.70). Current reward after update: -1188.50, Optimal reward -1188.50
Iteration 22 took 2.54 seconds (mean sampled reward: -6247.46). Current reward after update: -1796.43, Optimal reward -1188.50
Iteration 23 took 2.53 seconds (mean sampled reward: -5741.13). Current reward after update: -1171.48, Optimal reward -1171.48
Iteration 24 took 2.50 seconds (mean sampled reward: -5251.30). Current reward after update: -1516.72, Optimal reward -1171.48
Iteration 25 took 2.43 seconds (mean sampled reward: -5034.85). Current reward after update: -1479.29, Optimal reward -1171.48
Iteration 26 took 2.40 seconds (mean sampled reward: -4704.36). Current reward after update: -1850.12, Optimal reward -1171.48
Iteration 27 took 2.58 seconds (mean sampled reward: -5023.65). Current reward after update: -1323.20, Optimal reward -1171.48
Iteration 28 took 2.41 seconds (mean sampled reward: -6002.18). Current reward after update: -1742.37, Optimal reward -1171.48
Iteration 29 took 2.45 seconds (mean sampled reward: -5740.86). Current reward after update: -1657.34, Optimal reward -1171.48
Iteration 30 took 2.47 seconds (mean sampled reward: -4950.83). Current reward after update: -1512.55, Optimal reward -1171.48
Iteration 31 took 2.51 seconds (mean sampled reward: -5559.28). Current reward after update: -1689.18, Optimal reward -1171.48
Iteration 32 took 2.60 seconds (mean sampled reward: -5742.15). Current reward after update: -3241.53, Optimal reward -1171.48
Iteration 33 took 2.50 seconds (mean sampled reward: -5372.06). Current reward after update: -1502.06, Optimal reward -1171.48
Iteration 34 took 2.71 seconds (mean sampled reward: -5601.02). Current reward after update: -1541.51, Optimal reward -1171.48
Iteration 35 took 2.36 seconds (mean sampled reward: -5113.48). Current reward after update: -2013.25, Optimal reward -1171.48
Iteration 36 took 2.43 seconds (mean sampled reward: -5443.31). Current reward after update: -1293.18, Optimal reward -1171.48
Iteration 37 took 2.41 seconds (mean sampled reward: -4527.33). Current reward after update: -1169.77, Optimal reward -1169.77
Iteration 38 took 2.39 seconds (mean sampled reward: -4602.77). Current reward after update: -1297.70, Optimal reward -1169.77
Iteration 39 took 2.56 seconds (mean sampled reward: -4775.84). Current reward after update: -1048.43, Optimal reward -1048.43
Iteration 40 took 2.54 seconds (mean sampled reward: -4710.32). Current reward after update: -5105.48, Optimal reward -1048.43
Iteration 41 took 2.46 seconds (mean sampled reward: -5273.08). Current reward after update: -1287.82, Optimal reward -1048.43
Iteration 42 took 2.54 seconds (mean sampled reward: -5385.62). Current reward after update: -1142.22, Optimal reward -1048.43
Iteration 43 took 2.41 seconds (mean sampled reward: -4915.22). Current reward after update: -1487.59, Optimal reward -1048.43
Iteration 44 took 2.54 seconds (mean sampled reward: -5099.02). Current reward after update: -1260.85, Optimal reward -1048.43
Iteration 45 took 2.43 seconds (mean sampled reward: -4547.98). Current reward after update: -1300.25, Optimal reward -1048.43
Iteration 46 took 2.52 seconds (mean sampled reward: -5281.40). Current reward after update: -1036.16, Optimal reward -1036.16
Iteration 47 took 2.41 seconds (mean sampled reward: -4967.82). Current reward after update: -1115.21, Optimal reward -1036.16
Iteration 48 took 2.48 seconds (mean sampled reward: -5218.65). Current reward after update: -1145.07, Optimal reward -1036.16
Iteration 49 took 2.53 seconds (mean sampled reward: -5025.72). Current reward after update: -1126.43, Optimal reward -1036.16
Iteration 50 took 2.59 seconds (mean sampled reward: -4667.67). Current reward after update: -1304.08, Optimal reward -1036.16
Iteration 51 took 2.47 seconds (mean sampled reward: -4517.54). Current reward after update: -1255.78, Optimal reward -1036.16
Iteration 52 took 2.39 seconds (mean sampled reward: -4398.77). Current reward after update: -1253.14, Optimal reward -1036.16
Iteration 53 took 2.39 seconds (mean sampled reward: -4278.37). Current reward after update: -1295.14, Optimal reward -1036.16
Iteration 54 took 2.57 seconds (mean sampled reward: -3681.53). Current reward after update: -1099.44, Optimal reward -1036.16
Iteration 55 took 2.71 seconds (mean sampled reward: -4622.67). Current reward after update: -1054.35, Optimal reward -1036.16
Iteration 56 took 2.53 seconds (mean sampled reward: -5176.04). Current reward after update: -1216.78, Optimal reward -1036.16
Iteration 57 took 2.38 seconds (mean sampled reward: -5189.67). Current reward after update: -1366.83, Optimal reward -1036.16
Iteration 58 took 2.69 seconds (mean sampled reward: -5150.03). Current reward after update: -1299.92, Optimal reward -1036.16
Iteration 59 took 2.49 seconds (mean sampled reward: -5178.45). Current reward after update: -6694.72, Optimal reward -1036.16
Iteration 60 took 2.64 seconds (mean sampled reward: -5543.91). Current reward after update: -6234.52, Optimal reward -1036.16
Iteration 61 took 2.48 seconds (mean sampled reward: -5677.86). Current reward after update: -1313.87, Optimal reward -1036.16
Iteration 62 took 2.45 seconds (mean sampled reward: -4802.09). Current reward after update: -1229.42, Optimal reward -1036.16
Iteration 63 took 2.57 seconds (mean sampled reward: -5360.58). Current reward after update: -1448.68, Optimal reward -1036.16
Iteration 64 took 2.50 seconds (mean sampled reward: -5323.52). Current reward after update: -1920.81, Optimal reward -1036.16
Iteration 65 took 2.44 seconds (mean sampled reward: -4736.13). Current reward after update: -1200.87, Optimal reward -1036.16
Iteration 66 took 2.53 seconds (mean sampled reward: -5827.49). Current reward after update: -2442.67, Optimal reward -1036.16
Iteration 67 took 2.54 seconds (mean sampled reward: -5057.62). Current reward after update: -1313.14, Optimal reward -1036.16
Iteration 68 took 2.49 seconds (mean sampled reward: -5727.82). Current reward after update: -1318.39, Optimal reward -1036.16
Iteration 69 took 2.48 seconds (mean sampled reward: -6119.29). Current reward after update: -1308.59, Optimal reward -1036.16
Iteration 70 took 2.57 seconds (mean sampled reward: -6032.74). Current reward after update: -1363.77, Optimal reward -1036.16
Iteration 71 took 2.67 seconds (mean sampled reward: -5999.67). Current reward after update: -1165.49, Optimal reward -1036.16
Iteration 72 took 2.56 seconds (mean sampled reward: -6489.92). Current reward after update: -1510.06, Optimal reward -1036.16
Iteration 73 took 2.44 seconds (mean sampled reward: -5668.04). Current reward after update: -1325.02, Optimal reward -1036.16
Iteration 74 took 2.50 seconds (mean sampled reward: -6120.62). Current reward after update: -1291.69, Optimal reward -1036.16
Iteration 75 took 2.48 seconds (mean sampled reward: -5501.21). Current reward after update: -1222.54, Optimal reward -1036.16
Iteration 76 took 2.52 seconds (mean sampled reward: -5931.21). Current reward after update: -1300.06, Optimal reward -1036.16
Iteration 77 took 2.61 seconds (mean sampled reward: -5950.01). Current reward after update: -1246.88, Optimal reward -1036.16
Iteration 78 took 2.53 seconds (mean sampled reward: -5745.92). Current reward after update: -1241.85, Optimal reward -1036.16
Iteration 79 took 2.54 seconds (mean sampled reward: -6401.30). Current reward after update: -1247.25, Optimal reward -1036.16
Iteration 80 took 2.52 seconds (mean sampled reward: -6100.27). Current reward after update: -1366.45, Optimal reward -1036.16
Iteration 81 took 2.47 seconds (mean sampled reward: -4912.74). Current reward after update: -1481.27, Optimal reward -1036.16
Iteration 82 took 2.45 seconds (mean sampled reward: -5166.20). Current reward after update: -1385.50, Optimal reward -1036.16
Iteration 83 took 2.51 seconds (mean sampled reward: -5508.67). Current reward after update: -2183.28, Optimal reward -1036.16
Iteration 84 took 2.54 seconds (mean sampled reward: -5483.24). Current reward after update: -1349.88, Optimal reward -1036.16
Iteration 85 took 2.44 seconds (mean sampled reward: -5755.16). Current reward after update: -7234.79, Optimal reward -1036.16
Iteration 86 took 2.46 seconds (mean sampled reward: -6060.78). Current reward after update: -6501.06, Optimal reward -1036.16
Iteration 87 took 2.52 seconds (mean sampled reward: -6081.62). Current reward after update: -1323.62, Optimal reward -1036.16
Iteration 88 took 2.37 seconds (mean sampled reward: -5400.33). Current reward after update: -1327.56, Optimal reward -1036.16
Iteration 89 took 2.45 seconds (mean sampled reward: -6178.61). Current reward after update: -1451.63, Optimal reward -1036.16
Iteration 90 took 2.35 seconds (mean sampled reward: -5996.89). Current reward after update: -1289.26, Optimal reward -1036.16
Iteration 91 took 2.50 seconds (mean sampled reward: -5998.82). Current reward after update: -2281.14, Optimal reward -1036.16
Iteration 92 took 2.31 seconds (mean sampled reward: -5285.56). Current reward after update: -1341.29, Optimal reward -1036.16
Iteration 93 took 2.34 seconds (mean sampled reward: -5328.57). Current reward after update: -1400.29, Optimal reward -1036.16
Iteration 94 took 2.33 seconds (mean sampled reward: -5266.31). Current reward after update: -1322.59, Optimal reward -1036.16
Iteration 95 took 2.32 seconds (mean sampled reward: -4615.49). Current reward after update: -1878.89, Optimal reward -1036.16
Iteration 96 took 2.34 seconds (mean sampled reward: -5009.03). Current reward after update: -1376.48, Optimal reward -1036.16
Iteration 97 took 2.38 seconds (mean sampled reward: -4583.87). Current reward after update: -1393.79, Optimal reward -1036.16
Iteration 98 took 2.48 seconds (mean sampled reward: -4459.07). Current reward after update: -6329.39, Optimal reward -1036.16
Iteration 99 took 2.47 seconds (mean sampled reward: -4801.22). Current reward after update: -1914.19, Optimal reward -1036.16
Iteration 100 took 2.49 seconds (mean sampled reward: -5569.59). Current reward after update: -1302.59, Optimal reward -1036.16
Iteration 101 took 2.48 seconds (mean sampled reward: -5462.01). Current reward after update: -1566.09, Optimal reward -1036.16
Iteration 102 took 2.47 seconds (mean sampled reward: -4993.74). Current reward after update: -1297.65, Optimal reward -1036.16
Iteration 103 took 2.45 seconds (mean sampled reward: -5511.60). Current reward after update: -1391.93, Optimal reward -1036.16
Iteration 104 took 2.44 seconds (mean sampled reward: -4397.18). Current reward after update: -1267.92, Optimal reward -1036.16
Iteration 105 took 2.44 seconds (mean sampled reward: -3832.98). Current reward after update: -1231.21, Optimal reward -1036.16
Iteration 106 took 2.43 seconds (mean sampled reward: -3428.18). Current reward after update: -1151.99, Optimal reward -1036.16
Iteration 107 took 2.69 seconds (mean sampled reward: -3991.15). Current reward after update: -1153.51, Optimal reward -1036.16
Iteration 108 took 2.48 seconds (mean sampled reward: -5026.19). Current reward after update: -1204.73, Optimal reward -1036.16
Iteration 109 took 2.55 seconds (mean sampled reward: -4332.82). Current reward after update: -1057.94, Optimal reward -1036.16
Iteration 110 took 2.47 seconds (mean sampled reward: -3127.03). Current reward after update: -1183.06, Optimal reward -1036.16
Iteration 111 took 2.33 seconds (mean sampled reward: -3398.66). Current reward after update: -1015.72, Optimal reward -1015.72
Iteration 112 took 2.38 seconds (mean sampled reward: -3659.56). Current reward after update: -1140.90, Optimal reward -1015.72
Iteration 113 took 2.44 seconds (mean sampled reward: -3860.70). Current reward after update: -982.18, Optimal reward -982.18
Iteration 114 took 2.48 seconds (mean sampled reward: -4490.90). Current reward after update: -4745.75, Optimal reward -982.18
Iteration 115 took 2.42 seconds (mean sampled reward: -5814.43). Current reward after update: -1023.69, Optimal reward -982.18
Iteration 116 took 2.56 seconds (mean sampled reward: -6215.29). Current reward after update: -1357.71, Optimal reward -982.18
Iteration 117 took 2.40 seconds (mean sampled reward: -5176.87). Current reward after update: -2425.80, Optimal reward -982.18
Iteration 118 took 2.42 seconds (mean sampled reward: -6399.24). Current reward after update: -1165.80, Optimal reward -982.18
Iteration 119 took 2.39 seconds (mean sampled reward: -6089.24). Current reward after update: -1229.94, Optimal reward -982.18
Iteration 120 took 2.37 seconds (mean sampled reward: -5667.29). Current reward after update: -1122.84, Optimal reward -982.18
Iteration 121 took 2.41 seconds (mean sampled reward: -5461.67). Current reward after update: -1082.07, Optimal reward -982.18
Iteration 122 took 2.41 seconds (mean sampled reward: -5012.18). Current reward after update: -993.88, Optimal reward -982.18
Iteration 123 took 2.46 seconds (mean sampled reward: -5192.62). Current reward after update: -1044.85, Optimal reward -982.18
Iteration 124 took 2.45 seconds (mean sampled reward: -5877.25). Current reward after update: -968.01, Optimal reward -968.01
Iteration 125 took 2.38 seconds (mean sampled reward: -4455.48). Current reward after update: -1037.01, Optimal reward -968.01
Iteration 126 took 2.41 seconds (mean sampled reward: -5215.88). Current reward after update: -1200.57, Optimal reward -968.01
Iteration 127 took 2.57 seconds (mean sampled reward: -6257.08). Current reward after update: -1599.79, Optimal reward -968.01
Iteration 128 took 2.61 seconds (mean sampled reward: -7004.48). Current reward after update: -1356.32, Optimal reward -968.01
Iteration 129 took 2.46 seconds (mean sampled reward: -6776.69). Current reward after update: -1463.08, Optimal reward -968.01
Iteration 130 took 2.39 seconds (mean sampled reward: -5251.83). Current reward after update: -1374.62, Optimal reward -968.01
Iteration 131 took 2.30 seconds (mean sampled reward: -3828.56). Current reward after update: -1097.97, Optimal reward -968.01
Iteration 132 took 2.43 seconds (mean sampled reward: -5056.84). Current reward after update: -1265.99, Optimal reward -968.01
Iteration 133 took 2.55 seconds (mean sampled reward: -6147.10). Current reward after update: -1212.70, Optimal reward -968.01
Iteration 134 took 2.38 seconds (mean sampled reward: -4247.45). Current reward after update: -1676.67, Optimal reward -968.01
Iteration 135 took 2.43 seconds (mean sampled reward: -5463.51). Current reward after update: -1722.19, Optimal reward -968.01
Iteration 136 took 2.43 seconds (mean sampled reward: -5028.80). Current reward after update: -1741.97, Optimal reward -968.01
Iteration 137 took 2.42 seconds (mean sampled reward: -4873.49). Current reward after update: -1253.94, Optimal reward -968.01
Iteration 138 took 2.45 seconds (mean sampled reward: -4006.75). Current reward after update: -837.81, Optimal reward -837.81
Iteration 139 took 2.41 seconds (mean sampled reward: -3614.68). Current reward after update: -778.68, Optimal reward -778.68
Iteration 140 took 2.29 seconds (mean sampled reward: -3110.40). Current reward after update: -858.23, Optimal reward -778.68
Iteration 141 took 2.29 seconds (mean sampled reward: -3210.74). Current reward after update: -762.44, Optimal reward -762.44
Iteration 142 took 2.25 seconds (mean sampled reward: -2880.21). Current reward after update: -1260.52, Optimal reward -762.44
Iteration 143 took 2.34 seconds (mean sampled reward: -3500.86). Current reward after update: -876.45, Optimal reward -762.44
Iteration 144 took 2.31 seconds (mean sampled reward: -3464.07). Current reward after update: -814.02, Optimal reward -762.44
Iteration 145 took 2.34 seconds (mean sampled reward: -3764.76). Current reward after update: -1382.74, Optimal reward -762.44
Iteration 146 took 2.39 seconds (mean sampled reward: -4424.93). Current reward after update: -900.14, Optimal reward -762.44
Iteration 147 took 2.32 seconds (mean sampled reward: -2948.85). Current reward after update: -844.02, Optimal reward -762.44
Iteration 148 took 2.35 seconds (mean sampled reward: -3305.80). Current reward after update: -804.57, Optimal reward -762.44
Iteration 149 took 2.24 seconds (mean sampled reward: -3468.84). Current reward after update: -912.64, Optimal reward -762.44
Iteration 150 took 2.25 seconds (mean sampled reward: -3497.37). Current reward after update: -831.85, Optimal reward -762.44
Iteration 151 took 2.31 seconds (mean sampled reward: -3745.89). Current reward after update: -908.74, Optimal reward -762.44
Iteration 152 took 2.36 seconds (mean sampled reward: -4138.79). Current reward after update: -783.31, Optimal reward -762.44
Iteration 153 took 2.32 seconds (mean sampled reward: -3491.91). Current reward after update: -808.93, Optimal reward -762.44
Iteration 154 took 2.20 seconds (mean sampled reward: -2710.10). Current reward after update: -733.82, Optimal reward -733.82
Iteration 155 took 2.24 seconds (mean sampled reward: -2800.21). Current reward after update: -786.43, Optimal reward -733.82
Iteration 156 took 2.41 seconds (mean sampled reward: -4881.65). Current reward after update: -633.78, Optimal reward -633.78
Iteration 157 took 2.32 seconds (mean sampled reward: -3443.67). Current reward after update: -751.15, Optimal reward -633.78
Iteration 158 took 2.36 seconds (mean sampled reward: -3873.83). Current reward after update: -780.73, Optimal reward -633.78
Iteration 159 took 2.25 seconds (mean sampled reward: -3093.30). Current reward after update: -810.05, Optimal reward -633.78
Iteration 160 took 2.33 seconds (mean sampled reward: -3482.08). Current reward after update: -681.20, Optimal reward -633.78
Iteration 161 took 2.29 seconds (mean sampled reward: -2753.92). Current reward after update: -723.21, Optimal reward -633.78
Iteration 162 took 2.29 seconds (mean sampled reward: -3031.22). Current reward after update: -1058.18, Optimal reward -633.78
Iteration 163 took 2.36 seconds (mean sampled reward: -2543.86). Current reward after update: -787.14, Optimal reward -633.78
Iteration 164 took 2.35 seconds (mean sampled reward: -2919.99). Current reward after update: -661.53, Optimal reward -633.78
Iteration 165 took 2.28 seconds (mean sampled reward: -2886.67). Current reward after update: -716.73, Optimal reward -633.78
Iteration 166 took 2.31 seconds (mean sampled reward: -2858.08). Current reward after update: -731.41, Optimal reward -633.78
Iteration 167 took 2.28 seconds (mean sampled reward: -3138.36). Current reward after update: -1713.62, Optimal reward -633.78
Iteration 168 took 2.27 seconds (mean sampled reward: -3331.24). Current reward after update: -782.46, Optimal reward -633.78
Iteration 169 took 2.23 seconds (mean sampled reward: -2899.09). Current reward after update: -760.67, Optimal reward -633.78
Iteration 170 took 2.27 seconds (mean sampled reward: -3057.69). Current reward after update: -728.37, Optimal reward -633.78
Iteration 171 took 2.37 seconds (mean sampled reward: -3435.05). Current reward after update: -679.09, Optimal reward -633.78
Iteration 172 took 2.37 seconds (mean sampled reward: -3844.26). Current reward after update: -911.87, Optimal reward -633.78
Iteration 173 took 2.36 seconds (mean sampled reward: -4319.53). Current reward after update: -768.64, Optimal reward -633.78
Iteration 174 took 2.34 seconds (mean sampled reward: -3726.60). Current reward after update: -1024.03, Optimal reward -633.78
Iteration 175 took 2.44 seconds (mean sampled reward: -4416.63). Current reward after update: -5575.47, Optimal reward -633.78
Iteration 176 took 2.46 seconds (mean sampled reward: -4416.01). Current reward after update: -858.45, Optimal reward -633.78
Iteration 177 took 2.44 seconds (mean sampled reward: -4928.91). Current reward after update: -1115.64, Optimal reward -633.78
Iteration 178 took 2.38 seconds (mean sampled reward: -4221.26). Current reward after update: -719.81, Optimal reward -633.78
Iteration 179 took 2.45 seconds (mean sampled reward: -4472.69). Current reward after update: -693.91, Optimal reward -633.78
Iteration 180 took 2.53 seconds (mean sampled reward: -4751.99). Current reward after update: -752.13, Optimal reward -633.78
Iteration 181 took 2.40 seconds (mean sampled reward: -4398.81). Current reward after update: -903.11, Optimal reward -633.78
Iteration 182 took 2.44 seconds (mean sampled reward: -4573.50). Current reward after update: -5460.11, Optimal reward -633.78
Iteration 183 took 2.47 seconds (mean sampled reward: -4408.06). Current reward after update: -745.00, Optimal reward -633.78
Iteration 184 took 2.44 seconds (mean sampled reward: -3765.21). Current reward after update: -895.22, Optimal reward -633.78
Iteration 185 took 2.35 seconds (mean sampled reward: -3528.69). Current reward after update: -810.47, Optimal reward -633.78
Iteration 186 took 2.25 seconds (mean sampled reward: -3279.36). Current reward after update: -773.34, Optimal reward -633.78
Iteration 187 took 2.30 seconds (mean sampled reward: -3462.49). Current reward after update: -961.61, Optimal reward -633.78
Iteration 188 took 2.32 seconds (mean sampled reward: -3302.28). Current reward after update: -986.03, Optimal reward -633.78
Iteration 189 took 2.29 seconds (mean sampled reward: -3433.31). Current reward after update: -942.67, Optimal reward -633.78
Iteration 190 took 2.38 seconds (mean sampled reward: -3512.50). Current reward after update: -1432.82, Optimal reward -633.78
Iteration 191 took 2.32 seconds (mean sampled reward: -3263.83). Current reward after update: -899.64, Optimal reward -633.78
Iteration 192 took 2.23 seconds (mean sampled reward: -2553.49). Current reward after update: -932.40, Optimal reward -633.78
Iteration 193 took 2.32 seconds (mean sampled reward: -3225.60). Current reward after update: -801.21, Optimal reward -633.78
Iteration 194 took 2.43 seconds (mean sampled reward: -3481.57). Current reward after update: -902.23, Optimal reward -633.78
Iteration 195 took 2.35 seconds (mean sampled reward: -3665.29). Current reward after update: -926.24, Optimal reward -633.78
Iteration 196 took 2.41 seconds (mean sampled reward: -4467.30). Current reward after update: -994.41, Optimal reward -633.78
Iteration 197 took 2.27 seconds (mean sampled reward: -4148.59). Current reward after update: -1036.95, Optimal reward -633.78
Iteration 198 took 2.32 seconds (mean sampled reward: -3939.16). Current reward after update: -1037.93, Optimal reward -633.78
Iteration 199 took 2.31 seconds (mean sampled reward: -3711.22). Current reward after update: -1102.00, Optimal reward -633.78
Iteration 200 took 2.41 seconds (mean sampled reward: -4326.22). Current reward after update: -724.29, Optimal reward -633.78
Iteration 1 took 2.45 seconds (mean sampled reward: -7472.66). Current reward after update: -3677.06, Optimal reward -3677.06
Iteration 2 took 2.28 seconds (mean sampled reward: -6876.10). Current reward after update: -3350.66, Optimal reward -3350.66
Iteration 3 took 2.35 seconds (mean sampled reward: -6710.32). Current reward after update: -3360.75, Optimal reward -3350.66
Iteration 4 took 2.26 seconds (mean sampled reward: -6110.59). Current reward after update: -3037.37, Optimal reward -3037.37
Iteration 5 took 2.47 seconds (mean sampled reward: -6010.90). Current reward after update: -2775.33, Optimal reward -2775.33
Iteration 6 took 2.39 seconds (mean sampled reward: -5582.68). Current reward after update: -2327.48, Optimal reward -2327.48
Iteration 7 took 2.38 seconds (mean sampled reward: -5649.68). Current reward after update: -2242.13, Optimal reward -2242.13
Iteration 8 took 2.48 seconds (mean sampled reward: -6018.65). Current reward after update: -1646.41, Optimal reward -1646.41
Iteration 9 took 2.38 seconds (mean sampled reward: -6135.87). Current reward after update: -1527.29, Optimal reward -1527.29
Iteration 10 took 2.70 seconds (mean sampled reward: -5640.59). Current reward after update: -1660.82, Optimal reward -1527.29
Iteration 11 took 2.70 seconds (mean sampled reward: -6457.67). Current reward after update: -1676.44, Optimal reward -1527.29
Iteration 12 took 2.42 seconds (mean sampled reward: -6036.95). Current reward after update: -1589.09, Optimal reward -1527.29
Iteration 13 took 2.40 seconds (mean sampled reward: -5793.98). Current reward after update: -1002.25, Optimal reward -1002.25
Iteration 14 took 2.38 seconds (mean sampled reward: -6371.59). Current reward after update: -1213.03, Optimal reward -1002.25
Iteration 15 took 2.46 seconds (mean sampled reward: -5376.97). Current reward after update: -1252.53, Optimal reward -1002.25
Iteration 16 took 2.27 seconds (mean sampled reward: -5955.04). Current reward after update: -1250.99, Optimal reward -1002.25
Iteration 17 took 2.33 seconds (mean sampled reward: -5513.35). Current reward after update: -1184.69, Optimal reward -1002.25
Iteration 18 took 2.32 seconds (mean sampled reward: -5638.63). Current reward after update: -1340.05, Optimal reward -1002.25
Iteration 19 took 2.32 seconds (mean sampled reward: -5340.85). Current reward after update: -1176.28, Optimal reward -1002.25
Iteration 20 took 2.22 seconds (mean sampled reward: -3440.83). Current reward after update: -1122.46, Optimal reward -1002.25
Iteration 21 took 2.30 seconds (mean sampled reward: -4601.11). Current reward after update: -1271.49, Optimal reward -1002.25
Iteration 22 took 2.45 seconds (mean sampled reward: -4946.30). Current reward after update: -1238.96, Optimal reward -1002.25
Iteration 23 took 2.44 seconds (mean sampled reward: -5549.26). Current reward after update: -1493.08, Optimal reward -1002.25
Iteration 24 took 2.43 seconds (mean sampled reward: -5192.75). Current reward after update: -1169.45, Optimal reward -1002.25
Iteration 25 took 2.25 seconds (mean sampled reward: -4085.77). Current reward after update: -1149.71, Optimal reward -1002.25
Iteration 26 took 2.29 seconds (mean sampled reward: -4316.03). Current reward after update: -1078.16, Optimal reward -1002.25
Iteration 27 took 2.25 seconds (mean sampled reward: -3710.57). Current reward after update: -1125.88, Optimal reward -1002.25
Iteration 28 took 2.43 seconds (mean sampled reward: -5897.80). Current reward after update: -2108.70, Optimal reward -1002.25
Iteration 29 took 2.22 seconds (mean sampled reward: -5139.55). Current reward after update: -2024.66, Optimal reward -1002.25
Iteration 30 took 2.13 seconds (mean sampled reward: -4305.52). Current reward after update: -1557.79, Optimal reward -1002.25
Iteration 31 took 2.30 seconds (mean sampled reward: -4440.71). Current reward after update: -1317.15, Optimal reward -1002.25
Iteration 32 took 2.21 seconds (mean sampled reward: -4084.21). Current reward after update: -1096.29, Optimal reward -1002.25
Iteration 33 took 2.32 seconds (mean sampled reward: -5049.14). Current reward after update: -1022.57, Optimal reward -1002.25
Iteration 34 took 2.24 seconds (mean sampled reward: -4323.17). Current reward after update: -1174.38, Optimal reward -1002.25
Iteration 35 took 2.45 seconds (mean sampled reward: -5012.53). Current reward after update: -1131.84, Optimal reward -1002.25
Iteration 36 took 2.23 seconds (mean sampled reward: -4014.19). Current reward after update: -1518.13, Optimal reward -1002.25
Iteration 37 took 2.22 seconds (mean sampled reward: -3445.42). Current reward after update: -2998.57, Optimal reward -1002.25
Iteration 38 took 2.24 seconds (mean sampled reward: -3654.13). Current reward after update: -858.57, Optimal reward -858.57
Iteration 39 took 2.35 seconds (mean sampled reward: -3549.62). Current reward after update: -1392.16, Optimal reward -858.57
Iteration 40 took 2.26 seconds (mean sampled reward: -2859.02). Current reward after update: -2562.86, Optimal reward -858.57
Iteration 41 took 2.36 seconds (mean sampled reward: -3379.71). Current reward after update: -807.24, Optimal reward -807.24
Iteration 42 took 2.34 seconds (mean sampled reward: -3624.82). Current reward after update: -798.97, Optimal reward -798.97
Iteration 43 took 2.26 seconds (mean sampled reward: -4137.78). Current reward after update: -538.63, Optimal reward -538.63
Iteration 44 took 2.28 seconds (mean sampled reward: -5302.09). Current reward after update: -807.98, Optimal reward -538.63
Iteration 45 took 2.30 seconds (mean sampled reward: -3713.69). Current reward after update: -713.89, Optimal reward -538.63
Iteration 46 took 2.27 seconds (mean sampled reward: -4081.69). Current reward after update: -611.86, Optimal reward -538.63
Iteration 47 took 2.29 seconds (mean sampled reward: -4938.18). Current reward after update: -527.16, Optimal reward -527.16
Iteration 48 took 2.35 seconds (mean sampled reward: -4854.93). Current reward after update: -565.14, Optimal reward -527.16
Iteration 49 took 2.24 seconds (mean sampled reward: -3502.01). Current reward after update: -470.62, Optimal reward -470.62
Iteration 50 took 2.24 seconds (mean sampled reward: -4935.56). Current reward after update: -514.30, Optimal reward -470.62
Iteration 51 took 2.32 seconds (mean sampled reward: -4769.29). Current reward after update: -579.38, Optimal reward -470.62
Iteration 52 took 2.26 seconds (mean sampled reward: -4016.12). Current reward after update: -622.53, Optimal reward -470.62
Iteration 53 took 2.59 seconds (mean sampled reward: -5641.14). Current reward after update: -593.93, Optimal reward -470.62
Iteration 54 took 2.61 seconds (mean sampled reward: -6534.84). Current reward after update: -644.39, Optimal reward -470.62
Iteration 55 took 2.42 seconds (mean sampled reward: -6194.13). Current reward after update: -677.32, Optimal reward -470.62
Iteration 56 took 2.43 seconds (mean sampled reward: -6154.11). Current reward after update: -7075.82, Optimal reward -470.62
Iteration 57 took 2.49 seconds (mean sampled reward: -6733.47). Current reward after update: -911.81, Optimal reward -470.62
Iteration 58 took 2.45 seconds (mean sampled reward: -6407.82). Current reward after update: -862.16, Optimal reward -470.62
Iteration 59 took 2.30 seconds (mean sampled reward: -4768.34). Current reward after update: -817.82, Optimal reward -470.62
Iteration 60 took 2.27 seconds (mean sampled reward: -3121.17). Current reward after update: -630.95, Optimal reward -470.62
Iteration 61 took 2.22 seconds (mean sampled reward: -3161.35). Current reward after update: -652.22, Optimal reward -470.62
Iteration 62 took 2.28 seconds (mean sampled reward: -4495.76). Current reward after update: -623.68, Optimal reward -470.62
Iteration 63 took 2.33 seconds (mean sampled reward: -5677.64). Current reward after update: -506.15, Optimal reward -470.62
Iteration 64 took 2.58 seconds (mean sampled reward: -5901.13). Current reward after update: -670.37, Optimal reward -470.62
Iteration 65 took 2.30 seconds (mean sampled reward: -5394.92). Current reward after update: -891.96, Optimal reward -470.62
Iteration 66 took 2.14 seconds (mean sampled reward: -3396.00). Current reward after update: -706.93, Optimal reward -470.62
Iteration 67 took 2.21 seconds (mean sampled reward: -4443.46). Current reward after update: -737.51, Optimal reward -470.62
Iteration 68 took 2.31 seconds (mean sampled reward: -4932.37). Current reward after update: -727.63, Optimal reward -470.62
Iteration 69 took 2.28 seconds (mean sampled reward: -5159.36). Current reward after update: -1562.86, Optimal reward -470.62
Iteration 70 took 2.28 seconds (mean sampled reward: -5197.15). Current reward after update: -729.39, Optimal reward -470.62
Iteration 71 took 2.25 seconds (mean sampled reward: -5512.70). Current reward after update: -1414.13, Optimal reward -470.62
Iteration 72 took 2.24 seconds (mean sampled reward: -5148.42). Current reward after update: -602.07, Optimal reward -470.62
Iteration 73 took 2.19 seconds (mean sampled reward: -4026.90). Current reward after update: -605.78, Optimal reward -470.62
Iteration 74 took 2.15 seconds (mean sampled reward: -3710.22). Current reward after update: -547.31, Optimal reward -470.62
Iteration 75 took 2.19 seconds (mean sampled reward: -4919.13). Current reward after update: -610.80, Optimal reward -470.62
Iteration 76 took 2.22 seconds (mean sampled reward: -5600.69). Current reward after update: -1607.42, Optimal reward -470.62
Iteration 77 took 2.21 seconds (mean sampled reward: -3723.91). Current reward after update: -583.45, Optimal reward -470.62
Iteration 78 took 2.21 seconds (mean sampled reward: -3435.72). Current reward after update: -628.79, Optimal reward -470.62
Iteration 79 took 2.15 seconds (mean sampled reward: -3525.05). Current reward after update: -523.50, Optimal reward -470.62
Iteration 80 took 2.17 seconds (mean sampled reward: -4981.92). Current reward after update: -482.45, Optimal reward -470.62
Iteration 81 took 2.20 seconds (mean sampled reward: -4234.49). Current reward after update: -586.63, Optimal reward -470.62
Iteration 82 took 2.24 seconds (mean sampled reward: -2263.85). Current reward after update: -603.23, Optimal reward -470.62
Iteration 83 took 2.15 seconds (mean sampled reward: -2752.91). Current reward after update: -472.24, Optimal reward -470.62
Iteration 84 took 2.17 seconds (mean sampled reward: -2677.31). Current reward after update: -496.04, Optimal reward -470.62
Iteration 85 took 2.17 seconds (mean sampled reward: -4476.03). Current reward after update: -557.93, Optimal reward -470.62
Iteration 86 took 2.23 seconds (mean sampled reward: -4570.39). Current reward after update: -853.52, Optimal reward -470.62
Iteration 87 took 2.15 seconds (mean sampled reward: -5462.34). Current reward after update: -672.74, Optimal reward -470.62
Iteration 88 took 2.12 seconds (mean sampled reward: -4722.01). Current reward after update: -748.80, Optimal reward -470.62
Iteration 89 took 2.14 seconds (mean sampled reward: -4023.64). Current reward after update: -2147.48, Optimal reward -470.62
Iteration 90 took 2.14 seconds (mean sampled reward: -4475.12). Current reward after update: -575.01, Optimal reward -470.62
Iteration 91 took 2.14 seconds (mean sampled reward: -5559.75). Current reward after update: -462.79, Optimal reward -462.79
Iteration 92 took 2.15 seconds (mean sampled reward: -5026.85). Current reward after update: -799.36, Optimal reward -462.79
Iteration 93 took 2.16 seconds (mean sampled reward: -4072.96). Current reward after update: -580.79, Optimal reward -462.79
Iteration 94 took 2.19 seconds (mean sampled reward: -3548.34). Current reward after update: -690.57, Optimal reward -462.79
Iteration 95 took 2.20 seconds (mean sampled reward: -5421.27). Current reward after update: -1073.58, Optimal reward -462.79
Iteration 96 took 2.26 seconds (mean sampled reward: -5888.41). Current reward after update: -1003.36, Optimal reward -462.79
Iteration 97 took 2.19 seconds (mean sampled reward: -4905.10). Current reward after update: -951.87, Optimal reward -462.79
Iteration 98 took 2.24 seconds (mean sampled reward: -4627.13). Current reward after update: -764.97, Optimal reward -462.79
Iteration 99 took 2.23 seconds (mean sampled reward: -4062.58). Current reward after update: -685.69, Optimal reward -462.79
Iteration 100 took 2.26 seconds (mean sampled reward: -3948.58). Current reward after update: -4996.88, Optimal reward -462.79
Iteration 101 took 2.22 seconds (mean sampled reward: -3256.14). Current reward after update: -754.54, Optimal reward -462.79
Iteration 102 took 2.29 seconds (mean sampled reward: -3629.29). Current reward after update: -515.71, Optimal reward -462.79
Iteration 103 took 2.18 seconds (mean sampled reward: -3441.02). Current reward after update: -535.45, Optimal reward -462.79
Iteration 104 took 2.18 seconds (mean sampled reward: -3360.20). Current reward after update: -359.08, Optimal reward -359.08
Iteration 105 took 2.17 seconds (mean sampled reward: -3068.00). Current reward after update: -437.54, Optimal reward -359.08
Iteration 106 took 2.35 seconds (mean sampled reward: -2808.36). Current reward after update: -503.32, Optimal reward -359.08
Iteration 107 took 2.28 seconds (mean sampled reward: -2788.99). Current reward after update: -360.09, Optimal reward -359.08
Iteration 108 took 2.16 seconds (mean sampled reward: -2775.98). Current reward after update: -507.26, Optimal reward -359.08
Iteration 109 took 2.26 seconds (mean sampled reward: -2939.48). Current reward after update: -468.63, Optimal reward -359.08
Iteration 110 took 2.34 seconds (mean sampled reward: -2525.25). Current reward after update: -480.82, Optimal reward -359.08
Iteration 111 took 2.20 seconds (mean sampled reward: -2380.09). Current reward after update: -420.70, Optimal reward -359.08
Iteration 112 took 2.22 seconds (mean sampled reward: -3187.54). Current reward after update: -691.84, Optimal reward -359.08
Iteration 113 took 2.29 seconds (mean sampled reward: -2822.31). Current reward after update: -455.78, Optimal reward -359.08
Iteration 114 took 2.28 seconds (mean sampled reward: -3358.55). Current reward after update: -589.44, Optimal reward -359.08
Iteration 115 took 2.20 seconds (mean sampled reward: -3052.03). Current reward after update: -388.97, Optimal reward -359.08
Iteration 116 took 2.21 seconds (mean sampled reward: -2967.74). Current reward after update: -387.42, Optimal reward -359.08
Iteration 117 took 2.20 seconds (mean sampled reward: -2572.38). Current reward after update: -404.10, Optimal reward -359.08
Iteration 118 took 2.22 seconds (mean sampled reward: -3494.88). Current reward after update: -437.75, Optimal reward -359.08
Iteration 119 took 2.16 seconds (mean sampled reward: -4118.52). Current reward after update: -639.60, Optimal reward -359.08
Iteration 120 took 2.16 seconds (mean sampled reward: -3907.12). Current reward after update: -410.41, Optimal reward -359.08
Iteration 121 took 2.18 seconds (mean sampled reward: -3393.58). Current reward after update: -453.82, Optimal reward -359.08
Iteration 122 took 2.25 seconds (mean sampled reward: -3720.34). Current reward after update: -1741.76, Optimal reward -359.08
Iteration 123 took 2.22 seconds (mean sampled reward: -3515.36). Current reward after update: -418.66, Optimal reward -359.08
Iteration 124 took 2.16 seconds (mean sampled reward: -2679.98). Current reward after update: -408.11, Optimal reward -359.08
Iteration 125 took 2.16 seconds (mean sampled reward: -2557.93). Current reward after update: -531.97, Optimal reward -359.08
Iteration 126 took 2.17 seconds (mean sampled reward: -2543.52). Current reward after update: -2666.32, Optimal reward -359.08
Iteration 127 took 2.21 seconds (mean sampled reward: -2501.60). Current reward after update: -307.53, Optimal reward -307.53
Iteration 128 took 2.17 seconds (mean sampled reward: -2436.89). Current reward after update: -400.22, Optimal reward -307.53
Iteration 129 took 2.10 seconds (mean sampled reward: -2593.38). Current reward after update: -1258.54, Optimal reward -307.53
Iteration 130 took 2.11 seconds (mean sampled reward: -2331.88). Current reward after update: -323.10, Optimal reward -307.53
Iteration 131 took 2.20 seconds (mean sampled reward: -1847.19). Current reward after update: -361.14, Optimal reward -307.53
Iteration 132 took 2.17 seconds (mean sampled reward: -2326.30). Current reward after update: -385.45, Optimal reward -307.53
Iteration 133 took 2.17 seconds (mean sampled reward: -1736.03). Current reward after update: -304.92, Optimal reward -304.92
Iteration 134 took 2.19 seconds (mean sampled reward: -1724.43). Current reward after update: -288.55, Optimal reward -288.55
Iteration 135 took 2.21 seconds (mean sampled reward: -2187.09). Current reward after update: -334.67, Optimal reward -288.55
Iteration 136 took 2.18 seconds (mean sampled reward: -2027.66). Current reward after update: -393.03, Optimal reward -288.55
Iteration 137 took 2.24 seconds (mean sampled reward: -1886.68). Current reward after update: -280.25, Optimal reward -280.25
Iteration 138 took 2.21 seconds (mean sampled reward: -2048.25). Current reward after update: -456.05, Optimal reward -280.25
Iteration 139 took 2.22 seconds (mean sampled reward: -2181.71). Current reward after update: -326.48, Optimal reward -280.25
Iteration 140 took 2.17 seconds (mean sampled reward: -1892.32). Current reward after update: -373.03, Optimal reward -280.25
Iteration 141 took 2.19 seconds (mean sampled reward: -2392.98). Current reward after update: -1822.92, Optimal reward -280.25
Iteration 142 took 2.20 seconds (mean sampled reward: -2933.97). Current reward after update: -498.85, Optimal reward -280.25
Iteration 143 took 2.26 seconds (mean sampled reward: -2750.41). Current reward after update: -492.22, Optimal reward -280.25
Iteration 144 took 2.27 seconds (mean sampled reward: -3404.70). Current reward after update: -316.42, Optimal reward -280.25
Iteration 145 took 2.21 seconds (mean sampled reward: -2654.71). Current reward after update: -393.58, Optimal reward -280.25
Iteration 146 took 2.17 seconds (mean sampled reward: -2553.30). Current reward after update: -383.92, Optimal reward -280.25
Iteration 147 took 2.20 seconds (mean sampled reward: -2473.30). Current reward after update: -491.79, Optimal reward -280.25
Iteration 148 took 2.19 seconds (mean sampled reward: -2505.47). Current reward after update: -341.52, Optimal reward -280.25
Iteration 149 took 2.19 seconds (mean sampled reward: -2301.08). Current reward after update: -494.16, Optimal reward -280.25
Iteration 150 took 2.16 seconds (mean sampled reward: -2411.50). Current reward after update: -355.15, Optimal reward -280.25
Iteration 151 took 2.18 seconds (mean sampled reward: -2831.30). Current reward after update: -445.32, Optimal reward -280.25
Iteration 152 took 2.16 seconds (mean sampled reward: -2563.56). Current reward after update: -294.15, Optimal reward -280.25
Iteration 153 took 2.12 seconds (mean sampled reward: -2871.36). Current reward after update: -458.16, Optimal reward -280.25
Iteration 154 took 2.12 seconds (mean sampled reward: -2606.45). Current reward after update: -365.19, Optimal reward -280.25
Iteration 155 took 2.15 seconds (mean sampled reward: -2718.34). Current reward after update: -336.48, Optimal reward -280.25
Iteration 156 took 2.18 seconds (mean sampled reward: -1974.41). Current reward after update: -342.53, Optimal reward -280.25
Iteration 157 took 2.20 seconds (mean sampled reward: -2274.60). Current reward after update: -945.66, Optimal reward -280.25
Iteration 158 took 2.13 seconds (mean sampled reward: -2150.79). Current reward after update: -369.23, Optimal reward -280.25
Iteration 159 took 2.15 seconds (mean sampled reward: -1979.23). Current reward after update: -364.64, Optimal reward -280.25
Iteration 160 took 2.17 seconds (mean sampled reward: -2247.59). Current reward after update: -395.27, Optimal reward -280.25
Iteration 161 took 2.19 seconds (mean sampled reward: -2162.38). Current reward after update: -1735.06, Optimal reward -280.25
Iteration 162 took 2.26 seconds (mean sampled reward: -2365.20). Current reward after update: -445.62, Optimal reward -280.25
Iteration 163 took 2.16 seconds (mean sampled reward: -2501.90). Current reward after update: -511.72, Optimal reward -280.25
Iteration 164 took 2.14 seconds (mean sampled reward: -2794.27). Current reward after update: -503.80, Optimal reward -280.25
Iteration 165 took 2.17 seconds (mean sampled reward: -2618.80). Current reward after update: -571.29, Optimal reward -280.25
Iteration 166 took 2.21 seconds (mean sampled reward: -2512.70). Current reward after update: -502.73, Optimal reward -280.25
Iteration 167 took 2.29 seconds (mean sampled reward: -2171.14). Current reward after update: -495.64, Optimal reward -280.25
Iteration 168 took 2.23 seconds (mean sampled reward: -2289.97). Current reward after update: -711.49, Optimal reward -280.25
Iteration 169 took 2.19 seconds (mean sampled reward: -2860.33). Current reward after update: -342.41, Optimal reward -280.25
Iteration 170 took 2.18 seconds (mean sampled reward: -2404.74). Current reward after update: -331.59, Optimal reward -280.25
Iteration 171 took 2.28 seconds (mean sampled reward: -2198.79). Current reward after update: -299.15, Optimal reward -280.25
Iteration 172 took 2.19 seconds (mean sampled reward: -2303.62). Current reward after update: -1514.24, Optimal reward -280.25
Iteration 173 took 2.26 seconds (mean sampled reward: -1884.99). Current reward after update: -1877.84, Optimal reward -280.25
Iteration 174 took 2.23 seconds (mean sampled reward: -1933.70). Current reward after update: -405.11, Optimal reward -280.25
Iteration 175 took 2.26 seconds (mean sampled reward: -2126.44). Current reward after update: -461.55, Optimal reward -280.25
Iteration 176 took 2.22 seconds (mean sampled reward: -2613.65). Current reward after update: -1211.93, Optimal reward -280.25
Iteration 177 took 2.19 seconds (mean sampled reward: -2755.40). Current reward after update: -458.27, Optimal reward -280.25
Iteration 178 took 2.18 seconds (mean sampled reward: -2411.48). Current reward after update: -449.62, Optimal reward -280.25
Iteration 179 took 2.16 seconds (mean sampled reward: -2990.05). Current reward after update: -688.89, Optimal reward -280.25
Iteration 180 took 2.30 seconds (mean sampled reward: -2317.18). Current reward after update: -561.59, Optimal reward -280.25
Iteration 181 took 2.21 seconds (mean sampled reward: -2149.05). Current reward after update: -412.32, Optimal reward -280.25
Iteration 182 took 2.23 seconds (mean sampled reward: -2209.22). Current reward after update: -2403.41, Optimal reward -280.25
Iteration 183 took 2.26 seconds (mean sampled reward: -2012.50). Current reward after update: -407.54, Optimal reward -280.25
Iteration 184 took 2.23 seconds (mean sampled reward: -2089.03). Current reward after update: -1840.63, Optimal reward -280.25
Iteration 185 took 2.23 seconds (mean sampled reward: -2131.11). Current reward after update: -414.87, Optimal reward -280.25
Iteration 186 took 2.21 seconds (mean sampled reward: -2151.33). Current reward after update: -354.92, Optimal reward -280.25
Iteration 187 took 2.22 seconds (mean sampled reward: -2421.58). Current reward after update: -1718.83, Optimal reward -280.25
Iteration 188 took 2.24 seconds (mean sampled reward: -2551.07). Current reward after update: -446.92, Optimal reward -280.25
Iteration 189 took 2.25 seconds (mean sampled reward: -2720.70). Current reward after update: -422.39, Optimal reward -280.25
Iteration 190 took 2.25 seconds (mean sampled reward: -2556.33). Current reward after update: -1383.90, Optimal reward -280.25
Iteration 191 took 2.23 seconds (mean sampled reward: -3297.33). Current reward after update: -410.31, Optimal reward -280.25
Iteration 192 took 2.17 seconds (mean sampled reward: -3043.82). Current reward after update: -475.73, Optimal reward -280.25
Iteration 193 took 2.20 seconds (mean sampled reward: -3105.08). Current reward after update: -395.10, Optimal reward -280.25
Iteration 194 took 2.22 seconds (mean sampled reward: -3305.90). Current reward after update: -385.94, Optimal reward -280.25
Iteration 195 took 2.33 seconds (mean sampled reward: -4107.26). Current reward after update: -373.02, Optimal reward -280.25
Iteration 196 took 2.30 seconds (mean sampled reward: -2978.98). Current reward after update: -1721.05, Optimal reward -280.25
Iteration 197 took 2.27 seconds (mean sampled reward: -3825.55). Current reward after update: -439.34, Optimal reward -280.25
Iteration 198 took 2.14 seconds (mean sampled reward: -3522.29). Current reward after update: -501.94, Optimal reward -280.25
Iteration 199 took 2.25 seconds (mean sampled reward: -4115.83). Current reward after update: -847.20, Optimal reward -280.25
Iteration 200 took 2.35 seconds (mean sampled reward: -3653.12). Current reward after update: -623.85, Optimal reward -280.25
Iteration 1 took 2.41 seconds (mean sampled reward: -7424.18). Current reward after update: -3775.36, Optimal reward -3775.36
Iteration 2 took 2.29 seconds (mean sampled reward: -7220.49). Current reward after update: -2988.65, Optimal reward -2988.65
Iteration 3 took 2.36 seconds (mean sampled reward: -6987.89). Current reward after update: -3133.50, Optimal reward -2988.65
Iteration 4 took 2.30 seconds (mean sampled reward: -6556.33). Current reward after update: -2817.78, Optimal reward -2817.78
Iteration 5 took 2.45 seconds (mean sampled reward: -6409.03). Current reward after update: -2597.19, Optimal reward -2597.19
Iteration 6 took 2.32 seconds (mean sampled reward: -6493.26). Current reward after update: -2810.66, Optimal reward -2597.19
Iteration 7 took 2.27 seconds (mean sampled reward: -5516.17). Current reward after update: -2378.33, Optimal reward -2378.33
Iteration 8 took 2.23 seconds (mean sampled reward: -5284.08). Current reward after update: -2494.28, Optimal reward -2378.33
Iteration 9 took 2.23 seconds (mean sampled reward: -6299.84). Current reward after update: -2401.28, Optimal reward -2378.33
Iteration 10 took 2.31 seconds (mean sampled reward: -5515.14). Current reward after update: -2225.02, Optimal reward -2225.02
Iteration 11 took 2.23 seconds (mean sampled reward: -5030.49). Current reward after update: -2307.75, Optimal reward -2225.02
Iteration 12 took 2.18 seconds (mean sampled reward: -4526.14). Current reward after update: -2279.86, Optimal reward -2225.02
Iteration 13 took 2.21 seconds (mean sampled reward: -4158.80). Current reward after update: -2245.54, Optimal reward -2225.02
Iteration 14 took 2.24 seconds (mean sampled reward: -4973.75). Current reward after update: -2205.95, Optimal reward -2205.95
Iteration 15 took 2.30 seconds (mean sampled reward: -5059.39). Current reward after update: -1868.02, Optimal reward -1868.02
Iteration 16 took 2.23 seconds (mean sampled reward: -5404.91). Current reward after update: -2095.57, Optimal reward -1868.02
Iteration 17 took 2.20 seconds (mean sampled reward: -5004.22). Current reward after update: -2057.31, Optimal reward -1868.02
Iteration 18 took 2.29 seconds (mean sampled reward: -6210.65). Current reward after update: -2020.17, Optimal reward -1868.02
Iteration 19 took 2.28 seconds (mean sampled reward: -6463.05). Current reward after update: -1648.60, Optimal reward -1648.60
Iteration 20 took 2.29 seconds (mean sampled reward: -6677.53). Current reward after update: -2130.47, Optimal reward -1648.60
Iteration 21 took 2.38 seconds (mean sampled reward: -5970.13). Current reward after update: -1911.88, Optimal reward -1648.60
Iteration 22 took 2.29 seconds (mean sampled reward: -5927.63). Current reward after update: -2030.99, Optimal reward -1648.60
Iteration 23 took 2.22 seconds (mean sampled reward: -6259.96). Current reward after update: -2054.33, Optimal reward -1648.60
Iteration 24 took 2.20 seconds (mean sampled reward: -4569.87). Current reward after update: -2039.42, Optimal reward -1648.60
Iteration 25 took 2.17 seconds (mean sampled reward: -5051.33). Current reward after update: -1827.46, Optimal reward -1648.60
Iteration 26 took 2.18 seconds (mean sampled reward: -5344.26). Current reward after update: -1959.35, Optimal reward -1648.60
Iteration 27 took 2.33 seconds (mean sampled reward: -5490.36). Current reward after update: -1997.07, Optimal reward -1648.60
Iteration 28 took 2.25 seconds (mean sampled reward: -6251.54). Current reward after update: -1777.87, Optimal reward -1648.60
Iteration 29 took 2.26 seconds (mean sampled reward: -5310.40). Current reward after update: -2640.55, Optimal reward -1648.60
Iteration 30 took 2.25 seconds (mean sampled reward: -5482.68). Current reward after update: -1879.99, Optimal reward -1648.60
Iteration 31 took 2.43 seconds (mean sampled reward: -5809.77). Current reward after update: -1885.62, Optimal reward -1648.60
Iteration 32 took 2.23 seconds (mean sampled reward: -5179.04). Current reward after update: -1771.78, Optimal reward -1648.60
Iteration 33 took 2.26 seconds (mean sampled reward: -5613.15). Current reward after update: -1986.22, Optimal reward -1648.60
Iteration 34 took 2.25 seconds (mean sampled reward: -4448.90). Current reward after update: -1826.32, Optimal reward -1648.60
Iteration 35 took 2.29 seconds (mean sampled reward: -4663.12). Current reward after update: -1875.47, Optimal reward -1648.60
Iteration 36 took 2.27 seconds (mean sampled reward: -4141.63). Current reward after update: -1885.35, Optimal reward -1648.60
Iteration 37 took 2.27 seconds (mean sampled reward: -4328.01). Current reward after update: -1877.59, Optimal reward -1648.60
Iteration 38 took 2.30 seconds (mean sampled reward: -5436.80). Current reward after update: -1792.84, Optimal reward -1648.60
Iteration 39 took 2.25 seconds (mean sampled reward: -5879.61). Current reward after update: -2016.39, Optimal reward -1648.60
Iteration 40 took 2.23 seconds (mean sampled reward: -5282.36). Current reward after update: -1880.47, Optimal reward -1648.60
Iteration 41 took 2.22 seconds (mean sampled reward: -5191.49). Current reward after update: -1981.95, Optimal reward -1648.60
Iteration 42 took 2.16 seconds (mean sampled reward: -5899.76). Current reward after update: -1924.58, Optimal reward -1648.60
Iteration 43 took 2.27 seconds (mean sampled reward: -4940.30). Current reward after update: -1953.21, Optimal reward -1648.60
Iteration 44 took 2.25 seconds (mean sampled reward: -4426.88). Current reward after update: -2051.81, Optimal reward -1648.60
Iteration 45 took 2.32 seconds (mean sampled reward: -4046.38). Current reward after update: -1779.82, Optimal reward -1648.60
Iteration 46 took 2.36 seconds (mean sampled reward: -4267.01). Current reward after update: -1803.07, Optimal reward -1648.60
Iteration 47 took 2.25 seconds (mean sampled reward: -4676.20). Current reward after update: -1862.39, Optimal reward -1648.60
Iteration 48 took 2.38 seconds (mean sampled reward: -4082.42). Current reward after update: -2008.58, Optimal reward -1648.60
Iteration 49 took 2.40 seconds (mean sampled reward: -3732.54). Current reward after update: -1760.37, Optimal reward -1648.60
Iteration 50 took 2.33 seconds (mean sampled reward: -4238.12). Current reward after update: -1712.53, Optimal reward -1648.60
Iteration 51 took 2.17 seconds (mean sampled reward: -3298.25). Current reward after update: -1741.71, Optimal reward -1648.60
Iteration 52 took 2.26 seconds (mean sampled reward: -4868.96). Current reward after update: -1826.18, Optimal reward -1648.60
Iteration 53 took 2.27 seconds (mean sampled reward: -5374.25). Current reward after update: -1792.83, Optimal reward -1648.60
Iteration 54 took 2.50 seconds (mean sampled reward: -4744.53). Current reward after update: -1755.79, Optimal reward -1648.60
Iteration 55 took 2.46 seconds (mean sampled reward: -5591.24). Current reward after update: -1787.16, Optimal reward -1648.60
Iteration 56 took 2.42 seconds (mean sampled reward: -4799.37). Current reward after update: -1689.20, Optimal reward -1648.60
Iteration 57 took 2.45 seconds (mean sampled reward: -5085.27). Current reward after update: -1714.44, Optimal reward -1648.60
Iteration 58 took 2.30 seconds (mean sampled reward: -5169.08). Current reward after update: -1725.48, Optimal reward -1648.60
Iteration 59 took 2.37 seconds (mean sampled reward: -5571.50). Current reward after update: -1779.35, Optimal reward -1648.60
Iteration 60 took 2.32 seconds (mean sampled reward: -4972.55). Current reward after update: -1761.47, Optimal reward -1648.60
Iteration 61 took 2.27 seconds (mean sampled reward: -5006.36). Current reward after update: -1777.71, Optimal reward -1648.60
Iteration 62 took 2.26 seconds (mean sampled reward: -5322.36). Current reward after update: -1732.87, Optimal reward -1648.60
Iteration 63 took 2.33 seconds (mean sampled reward: -4680.83). Current reward after update: -2278.02, Optimal reward -1648.60
Iteration 64 took 2.24 seconds (mean sampled reward: -3675.21). Current reward after update: -2101.60, Optimal reward -1648.60
Iteration 65 took 2.31 seconds (mean sampled reward: -4552.46). Current reward after update: -1623.92, Optimal reward -1623.92
Iteration 66 took 2.23 seconds (mean sampled reward: -5287.79). Current reward after update: -1727.77, Optimal reward -1623.92
Iteration 67 took 2.37 seconds (mean sampled reward: -5621.34). Current reward after update: -1715.08, Optimal reward -1623.92
Iteration 68 took 2.30 seconds (mean sampled reward: -4326.21). Current reward after update: -1627.86, Optimal reward -1623.92
Iteration 69 took 2.26 seconds (mean sampled reward: -4382.91). Current reward after update: -1656.08, Optimal reward -1623.92
Iteration 70 took 2.21 seconds (mean sampled reward: -3833.68). Current reward after update: -1656.16, Optimal reward -1623.92
Iteration 71 took 2.20 seconds (mean sampled reward: -3037.47). Current reward after update: -3282.29, Optimal reward -1623.92
Iteration 72 took 2.25 seconds (mean sampled reward: -3672.76). Current reward after update: -1581.14, Optimal reward -1581.14
Iteration 73 took 2.27 seconds (mean sampled reward: -3655.93). Current reward after update: -1657.63, Optimal reward -1581.14
Iteration 74 took 2.26 seconds (mean sampled reward: -3260.71). Current reward after update: -1658.69, Optimal reward -1581.14
Iteration 75 took 2.22 seconds (mean sampled reward: -3477.74). Current reward after update: -1640.84, Optimal reward -1581.14
Iteration 76 took 2.26 seconds (mean sampled reward: -3241.05). Current reward after update: -1703.11, Optimal reward -1581.14
Iteration 77 took 2.37 seconds (mean sampled reward: -3284.17). Current reward after update: -1607.13, Optimal reward -1581.14
Iteration 78 took 2.34 seconds (mean sampled reward: -3605.45). Current reward after update: -1609.49, Optimal reward -1581.14
Iteration 79 took 2.50 seconds (mean sampled reward: -3883.52). Current reward after update: -1565.45, Optimal reward -1565.45
Iteration 80 took 2.26 seconds (mean sampled reward: -4495.96). Current reward after update: -1589.54, Optimal reward -1565.45
Iteration 81 took 2.27 seconds (mean sampled reward: -4524.55). Current reward after update: -1647.08, Optimal reward -1565.45
Iteration 82 took 2.30 seconds (mean sampled reward: -5292.96). Current reward after update: -1706.78, Optimal reward -1565.45
Iteration 83 took 2.25 seconds (mean sampled reward: -5359.04). Current reward after update: -1627.59, Optimal reward -1565.45
Iteration 84 took 2.28 seconds (mean sampled reward: -5019.21). Current reward after update: -1616.78, Optimal reward -1565.45
Iteration 85 took 2.24 seconds (mean sampled reward: -4571.27). Current reward after update: -1552.71, Optimal reward -1552.71
Iteration 86 took 2.29 seconds (mean sampled reward: -5154.27). Current reward after update: -1559.22, Optimal reward -1552.71
Iteration 87 took 2.31 seconds (mean sampled reward: -5521.69). Current reward after update: -1553.37, Optimal reward -1552.71
Iteration 88 took 2.31 seconds (mean sampled reward: -5767.63). Current reward after update: -2255.02, Optimal reward -1552.71
Iteration 89 took 2.30 seconds (mean sampled reward: -5297.98). Current reward after update: -1874.30, Optimal reward -1552.71
Iteration 90 took 2.28 seconds (mean sampled reward: -5345.77). Current reward after update: -1904.12, Optimal reward -1552.71
Iteration 91 took 2.22 seconds (mean sampled reward: -5311.81). Current reward after update: -1807.92, Optimal reward -1552.71
Iteration 92 took 2.26 seconds (mean sampled reward: -4576.45). Current reward after update: -1695.51, Optimal reward -1552.71
Iteration 93 took 2.28 seconds (mean sampled reward: -5622.16). Current reward after update: -1880.71, Optimal reward -1552.71
Iteration 94 took 2.22 seconds (mean sampled reward: -5200.41). Current reward after update: -1692.28, Optimal reward -1552.71
Iteration 95 took 2.30 seconds (mean sampled reward: -4560.45). Current reward after update: -1712.62, Optimal reward -1552.71
Iteration 96 took 2.23 seconds (mean sampled reward: -4965.89). Current reward after update: -1859.33, Optimal reward -1552.71
Iteration 97 took 2.27 seconds (mean sampled reward: -4869.99). Current reward after update: -1613.31, Optimal reward -1552.71
Iteration 98 took 2.21 seconds (mean sampled reward: -5178.49). Current reward after update: -1529.87, Optimal reward -1529.87
Iteration 99 took 2.28 seconds (mean sampled reward: -4963.71). Current reward after update: -1623.75, Optimal reward -1529.87
Iteration 100 took 2.21 seconds (mean sampled reward: -4924.37). Current reward after update: -1669.14, Optimal reward -1529.87
Iteration 101 took 2.26 seconds (mean sampled reward: -4605.02). Current reward after update: -6607.78, Optimal reward -1529.87
Iteration 102 took 2.27 seconds (mean sampled reward: -4668.60). Current reward after update: -1657.33, Optimal reward -1529.87
Iteration 103 took 2.23 seconds (mean sampled reward: -5613.17). Current reward after update: -1718.12, Optimal reward -1529.87
Iteration 104 took 2.17 seconds (mean sampled reward: -5622.08). Current reward after update: -2095.60, Optimal reward -1529.87
Iteration 105 took 2.19 seconds (mean sampled reward: -5505.19). Current reward after update: -2175.49, Optimal reward -1529.87
Iteration 106 took 2.29 seconds (mean sampled reward: -4604.33). Current reward after update: -1771.10, Optimal reward -1529.87
Iteration 107 took 2.40 seconds (mean sampled reward: -4055.03). Current reward after update: -1694.85, Optimal reward -1529.87
Iteration 108 took 2.20 seconds (mean sampled reward: -4072.92). Current reward after update: -1745.14, Optimal reward -1529.87
Iteration 109 took 2.38 seconds (mean sampled reward: -4999.82). Current reward after update: -1712.67, Optimal reward -1529.87
Iteration 110 took 2.25 seconds (mean sampled reward: -5572.82). Current reward after update: -1798.99, Optimal reward -1529.87
Iteration 111 took 2.38 seconds (mean sampled reward: -5002.43). Current reward after update: -1692.84, Optimal reward -1529.87
Iteration 112 took 2.30 seconds (mean sampled reward: -5019.00). Current reward after update: -2200.58, Optimal reward -1529.87
Iteration 113 took 2.31 seconds (mean sampled reward: -4811.24). Current reward after update: -1702.31, Optimal reward -1529.87
Iteration 114 took 2.28 seconds (mean sampled reward: -4585.39). Current reward after update: -1816.21, Optimal reward -1529.87
Iteration 115 took 2.21 seconds (mean sampled reward: -4473.70). Current reward after update: -1660.78, Optimal reward -1529.87
Iteration 116 took 2.20 seconds (mean sampled reward: -4786.25). Current reward after update: -1643.09, Optimal reward -1529.87
Iteration 117 took 2.24 seconds (mean sampled reward: -4615.79). Current reward after update: -1774.52, Optimal reward -1529.87
Iteration 118 took 2.23 seconds (mean sampled reward: -4486.57). Current reward after update: -1681.31, Optimal reward -1529.87
Iteration 119 took 2.21 seconds (mean sampled reward: -4442.52). Current reward after update: -1772.58, Optimal reward -1529.87
Iteration 120 took 2.25 seconds (mean sampled reward: -4818.33). Current reward after update: -2708.84, Optimal reward -1529.87
Iteration 121 took 2.24 seconds (mean sampled reward: -5424.47). Current reward after update: -1653.58, Optimal reward -1529.87
Iteration 122 took 2.22 seconds (mean sampled reward: -4016.91). Current reward after update: -2277.30, Optimal reward -1529.87
Iteration 123 took 2.22 seconds (mean sampled reward: -4264.06). Current reward after update: -1755.64, Optimal reward -1529.87
Iteration 124 took 2.20 seconds (mean sampled reward: -4107.62). Current reward after update: -1713.06, Optimal reward -1529.87
Iteration 125 took 2.21 seconds (mean sampled reward: -3699.32). Current reward after update: -2650.77, Optimal reward -1529.87
Iteration 126 took 2.23 seconds (mean sampled reward: -4446.29). Current reward after update: -1739.03, Optimal reward -1529.87
Iteration 127 took 2.25 seconds (mean sampled reward: -3898.31). Current reward after update: -1623.81, Optimal reward -1529.87
Iteration 128 took 2.25 seconds (mean sampled reward: -4113.19). Current reward after update: -1699.17, Optimal reward -1529.87
Iteration 129 took 2.27 seconds (mean sampled reward: -3912.08). Current reward after update: -1610.36, Optimal reward -1529.87
Iteration 130 took 2.28 seconds (mean sampled reward: -4849.83). Current reward after update: -2419.85, Optimal reward -1529.87
Iteration 131 took 2.28 seconds (mean sampled reward: -4244.32). Current reward after update: -1623.54, Optimal reward -1529.87
Iteration 132 took 2.30 seconds (mean sampled reward: -5520.60). Current reward after update: -1711.46, Optimal reward -1529.87
Iteration 133 took 2.24 seconds (mean sampled reward: -4994.45). Current reward after update: -1712.46, Optimal reward -1529.87
Iteration 134 took 2.34 seconds (mean sampled reward: -5336.38). Current reward after update: -1663.93, Optimal reward -1529.87
Iteration 135 took 2.28 seconds (mean sampled reward: -5649.90). Current reward after update: -1739.60, Optimal reward -1529.87
Iteration 136 took 2.20 seconds (mean sampled reward: -5145.08). Current reward after update: -1799.77, Optimal reward -1529.87
Iteration 137 took 2.31 seconds (mean sampled reward: -4668.04). Current reward after update: -1729.76, Optimal reward -1529.87
Iteration 138 took 2.29 seconds (mean sampled reward: -4753.11). Current reward after update: -1784.95, Optimal reward -1529.87
Iteration 139 took 2.28 seconds (mean sampled reward: -5013.21). Current reward after update: -1824.70, Optimal reward -1529.87
Iteration 140 took 2.26 seconds (mean sampled reward: -5764.23). Current reward after update: -1882.70, Optimal reward -1529.87
Iteration 141 took 2.32 seconds (mean sampled reward: -4770.77). Current reward after update: -1744.12, Optimal reward -1529.87
Iteration 142 took 2.30 seconds (mean sampled reward: -4963.97). Current reward after update: -1676.46, Optimal reward -1529.87
Iteration 143 took 2.30 seconds (mean sampled reward: -5151.27). Current reward after update: -1773.72, Optimal reward -1529.87
Iteration 144 took 2.31 seconds (mean sampled reward: -5432.10). Current reward after update: -1807.07, Optimal reward -1529.87
Iteration 145 took 2.33 seconds (mean sampled reward: -4746.35). Current reward after update: -1722.91, Optimal reward -1529.87
Iteration 146 took 2.21 seconds (mean sampled reward: -4785.30). Current reward after update: -1857.37, Optimal reward -1529.87
Iteration 147 took 2.21 seconds (mean sampled reward: -5370.55). Current reward after update: -1997.47, Optimal reward -1529.87
Iteration 148 took 2.17 seconds (mean sampled reward: -5143.08). Current reward after update: -1817.11, Optimal reward -1529.87
Iteration 149 took 2.22 seconds (mean sampled reward: -5040.19). Current reward after update: -1824.84, Optimal reward -1529.87
Iteration 150 took 2.22 seconds (mean sampled reward: -4622.83). Current reward after update: -1739.63, Optimal reward -1529.87
Iteration 151 took 2.20 seconds (mean sampled reward: -4480.98). Current reward after update: -1698.04, Optimal reward -1529.87
Iteration 152 took 2.23 seconds (mean sampled reward: -4272.19). Current reward after update: -1666.72, Optimal reward -1529.87
Iteration 153 took 2.17 seconds (mean sampled reward: -5003.36). Current reward after update: -1740.15, Optimal reward -1529.87
Iteration 154 took 2.19 seconds (mean sampled reward: -5051.91). Current reward after update: -2877.18, Optimal reward -1529.87
Iteration 155 took 2.20 seconds (mean sampled reward: -4279.01). Current reward after update: -1699.08, Optimal reward -1529.87
Iteration 156 took 2.24 seconds (mean sampled reward: -4960.04). Current reward after update: -1723.15, Optimal reward -1529.87
Iteration 157 took 2.20 seconds (mean sampled reward: -5158.27). Current reward after update: -1726.60, Optimal reward -1529.87
Iteration 158 took 2.26 seconds (mean sampled reward: -5630.97). Current reward after update: -2375.29, Optimal reward -1529.87
Iteration 159 took 2.24 seconds (mean sampled reward: -5385.39). Current reward after update: -1803.07, Optimal reward -1529.87
Iteration 160 took 2.23 seconds (mean sampled reward: -5358.52). Current reward after update: -1684.36, Optimal reward -1529.87
Iteration 161 took 2.27 seconds (mean sampled reward: -5077.06). Current reward after update: -1632.42, Optimal reward -1529.87
Iteration 162 took 2.22 seconds (mean sampled reward: -5824.11). Current reward after update: -1564.27, Optimal reward -1529.87
Iteration 163 took 2.15 seconds (mean sampled reward: -5707.87). Current reward after update: -1655.06, Optimal reward -1529.87
Iteration 164 took 2.24 seconds (mean sampled reward: -6118.02). Current reward after update: -1483.09, Optimal reward -1483.09
Iteration 165 took 2.29 seconds (mean sampled reward: -5862.22). Current reward after update: -1676.45, Optimal reward -1483.09
Iteration 166 took 2.29 seconds (mean sampled reward: -5394.93). Current reward after update: -1635.06, Optimal reward -1483.09
Iteration 167 took 2.24 seconds (mean sampled reward: -5381.58). Current reward after update: -1587.11, Optimal reward -1483.09
Iteration 168 took 2.22 seconds (mean sampled reward: -5498.90). Current reward after update: -1596.23, Optimal reward -1483.09
Iteration 169 took 2.26 seconds (mean sampled reward: -5205.50). Current reward after update: -1532.55, Optimal reward -1483.09
Iteration 170 took 2.26 seconds (mean sampled reward: -5423.14). Current reward after update: -2105.89, Optimal reward -1483.09
Iteration 171 took 2.25 seconds (mean sampled reward: -5924.29). Current reward after update: -1694.15, Optimal reward -1483.09
Iteration 172 took 2.29 seconds (mean sampled reward: -4951.38). Current reward after update: -1564.75, Optimal reward -1483.09
Iteration 173 took 2.27 seconds (mean sampled reward: -5297.83). Current reward after update: -1555.09, Optimal reward -1483.09
Iteration 174 took 2.29 seconds (mean sampled reward: -5863.99). Current reward after update: -1597.35, Optimal reward -1483.09
Iteration 175 took 2.27 seconds (mean sampled reward: -6090.31). Current reward after update: -1737.94, Optimal reward -1483.09
Iteration 176 took 2.24 seconds (mean sampled reward: -5299.06). Current reward after update: -1660.66, Optimal reward -1483.09
Iteration 177 took 2.24 seconds (mean sampled reward: -4693.24). Current reward after update: -1636.52, Optimal reward -1483.09
Iteration 178 took 2.31 seconds (mean sampled reward: -4994.52). Current reward after update: -1723.77, Optimal reward -1483.09
Iteration 179 took 2.29 seconds (mean sampled reward: -4662.98). Current reward after update: -1676.88, Optimal reward -1483.09
Iteration 180 took 2.25 seconds (mean sampled reward: -4789.83). Current reward after update: -1668.53, Optimal reward -1483.09
Iteration 181 took 2.30 seconds (mean sampled reward: -5273.14). Current reward after update: -2228.77, Optimal reward -1483.09
Iteration 182 took 2.31 seconds (mean sampled reward: -4611.08). Current reward after update: -1991.54, Optimal reward -1483.09
Iteration 183 took 2.28 seconds (mean sampled reward: -4690.02). Current reward after update: -1659.07, Optimal reward -1483.09
Iteration 184 took 2.24 seconds (mean sampled reward: -4584.83). Current reward after update: -1689.27, Optimal reward -1483.09
Iteration 185 took 2.24 seconds (mean sampled reward: -4076.77). Current reward after update: -1666.13, Optimal reward -1483.09
Iteration 186 took 2.27 seconds (mean sampled reward: -4258.03). Current reward after update: -1687.20, Optimal reward -1483.09
Iteration 187 took 2.26 seconds (mean sampled reward: -4612.17). Current reward after update: -1558.32, Optimal reward -1483.09
Iteration 188 took 2.19 seconds (mean sampled reward: -6111.91). Current reward after update: -1500.89, Optimal reward -1483.09
Iteration 189 took 2.26 seconds (mean sampled reward: -5911.50). Current reward after update: -1703.06, Optimal reward -1483.09
Iteration 190 took 2.18 seconds (mean sampled reward: -5143.82). Current reward after update: -1683.87, Optimal reward -1483.09
Iteration 191 took 2.28 seconds (mean sampled reward: -5278.98). Current reward after update: -1638.38, Optimal reward -1483.09
Iteration 192 took 2.27 seconds (mean sampled reward: -5240.20). Current reward after update: -1406.70, Optimal reward -1406.70
Iteration 193 took 2.23 seconds (mean sampled reward: -5759.73). Current reward after update: -1414.84, Optimal reward -1406.70
Iteration 194 took 2.24 seconds (mean sampled reward: -5769.92). Current reward after update: -1320.76, Optimal reward -1320.76
Iteration 195 took 2.26 seconds (mean sampled reward: -5698.94). Current reward after update: -1379.95, Optimal reward -1320.76
Iteration 196 took 2.24 seconds (mean sampled reward: -5258.43). Current reward after update: -1286.25, Optimal reward -1286.25
Iteration 197 took 2.21 seconds (mean sampled reward: -5610.54). Current reward after update: -3194.60, Optimal reward -1286.25
Iteration 198 took 2.22 seconds (mean sampled reward: -5979.62). Current reward after update: -1390.20, Optimal reward -1286.25
Iteration 199 took 2.24 seconds (mean sampled reward: -5666.57). Current reward after update: -1491.59, Optimal reward -1286.25
Iteration 200 took 2.26 seconds (mean sampled reward: -5754.14). Current reward after update: -1244.13, Optimal reward -1244.13
Max force: 50 Sigma: 0.8 mean rewards: -719.3856886880133, best rewards:-280.24612114424076

Iteration 1 took 2.43 seconds (mean sampled reward: -7595.46). Current reward after update: -7292.23, Optimal reward -7292.23
Iteration 2 took 2.21 seconds (mean sampled reward: -7591.18). Current reward after update: -7297.06, Optimal reward -7292.23
Iteration 3 took 2.23 seconds (mean sampled reward: -7571.74). Current reward after update: -7209.81, Optimal reward -7209.81
Iteration 4 took 2.20 seconds (mean sampled reward: -7529.13). Current reward after update: -7078.92, Optimal reward -7078.92
Iteration 5 took 2.21 seconds (mean sampled reward: -7552.02). Current reward after update: -7068.62, Optimal reward -7068.62
Iteration 6 took 2.33 seconds (mean sampled reward: -7472.72). Current reward after update: -7067.83, Optimal reward -7067.83
Iteration 7 took 2.44 seconds (mean sampled reward: -7472.79). Current reward after update: -6995.00, Optimal reward -6995.00
Iteration 8 took 2.55 seconds (mean sampled reward: -7488.07). Current reward after update: -6952.30, Optimal reward -6952.30
Iteration 9 took 2.56 seconds (mean sampled reward: -7478.93). Current reward after update: -7058.29, Optimal reward -6952.30
Iteration 10 took 2.50 seconds (mean sampled reward: -7463.32). Current reward after update: -6960.75, Optimal reward -6952.30
Iteration 11 took 2.26 seconds (mean sampled reward: -7504.18). Current reward after update: -7010.69, Optimal reward -6952.30
Iteration 12 took 2.39 seconds (mean sampled reward: -7511.80). Current reward after update: -7185.76, Optimal reward -6952.30
Iteration 13 took 2.17 seconds (mean sampled reward: -7486.96). Current reward after update: -6946.27, Optimal reward -6946.27
Iteration 14 took 2.26 seconds (mean sampled reward: -7468.83). Current reward after update: -7330.16, Optimal reward -6946.27
Iteration 15 took 2.26 seconds (mean sampled reward: -7435.45). Current reward after update: -6858.74, Optimal reward -6858.74
Iteration 16 took 2.30 seconds (mean sampled reward: -7415.77). Current reward after update: -6741.91, Optimal reward -6741.91
Iteration 17 took 2.29 seconds (mean sampled reward: -7342.37). Current reward after update: -6523.08, Optimal reward -6523.08
Iteration 18 took 2.53 seconds (mean sampled reward: -7021.61). Current reward after update: -6309.42, Optimal reward -6309.42
Iteration 19 took 2.35 seconds (mean sampled reward: -6809.68). Current reward after update: -6087.32, Optimal reward -6087.32
Iteration 20 took 2.41 seconds (mean sampled reward: -6956.10). Current reward after update: -6175.45, Optimal reward -6087.32
Iteration 21 took 2.40 seconds (mean sampled reward: -7066.15). Current reward after update: -6066.16, Optimal reward -6066.16
Iteration 22 took 2.39 seconds (mean sampled reward: -6677.43). Current reward after update: -6136.79, Optimal reward -6066.16
Iteration 23 took 2.39 seconds (mean sampled reward: -6532.92). Current reward after update: -3891.33, Optimal reward -3891.33
Iteration 24 took 2.41 seconds (mean sampled reward: -6143.25). Current reward after update: -3850.57, Optimal reward -3850.57
Iteration 25 took 2.32 seconds (mean sampled reward: -5882.52). Current reward after update: -3707.08, Optimal reward -3707.08
Iteration 26 took 2.29 seconds (mean sampled reward: -6010.91). Current reward after update: -3759.58, Optimal reward -3707.08
Iteration 27 took 2.34 seconds (mean sampled reward: -6105.93). Current reward after update: -3573.84, Optimal reward -3573.84
Iteration 28 took 2.33 seconds (mean sampled reward: -5645.33). Current reward after update: -3529.51, Optimal reward -3529.51
Iteration 29 took 2.46 seconds (mean sampled reward: -5259.78). Current reward after update: -3357.69, Optimal reward -3357.69
Iteration 30 took 2.34 seconds (mean sampled reward: -5772.35). Current reward after update: -3165.62, Optimal reward -3165.62
Iteration 31 took 2.34 seconds (mean sampled reward: -6179.95). Current reward after update: -3445.67, Optimal reward -3165.62
Iteration 32 took 2.35 seconds (mean sampled reward: -5823.53). Current reward after update: -3426.94, Optimal reward -3165.62
Iteration 33 took 2.35 seconds (mean sampled reward: -5917.10). Current reward after update: -3520.71, Optimal reward -3165.62
Iteration 34 took 2.35 seconds (mean sampled reward: -5899.91). Current reward after update: -3111.97, Optimal reward -3111.97
Iteration 35 took 2.40 seconds (mean sampled reward: -6326.04). Current reward after update: -3278.27, Optimal reward -3111.97
Iteration 36 took 2.30 seconds (mean sampled reward: -5249.17). Current reward after update: -3571.11, Optimal reward -3111.97
Iteration 37 took 2.29 seconds (mean sampled reward: -5479.09). Current reward after update: -2985.99, Optimal reward -2985.99
Iteration 38 took 2.27 seconds (mean sampled reward: -6502.62). Current reward after update: -3256.88, Optimal reward -2985.99
Iteration 39 took 2.28 seconds (mean sampled reward: -5461.92). Current reward after update: -3110.50, Optimal reward -2985.99
Iteration 40 took 2.33 seconds (mean sampled reward: -5978.86). Current reward after update: -3086.34, Optimal reward -2985.99
Iteration 41 took 2.35 seconds (mean sampled reward: -5863.43). Current reward after update: -3476.79, Optimal reward -2985.99
Iteration 42 took 2.36 seconds (mean sampled reward: -5514.97). Current reward after update: -3575.42, Optimal reward -2985.99
Iteration 43 took 2.36 seconds (mean sampled reward: -5558.12). Current reward after update: -3116.09, Optimal reward -2985.99
Iteration 44 took 2.26 seconds (mean sampled reward: -5290.99). Current reward after update: -3087.51, Optimal reward -2985.99
Iteration 45 took 2.27 seconds (mean sampled reward: -5519.14). Current reward after update: -2804.57, Optimal reward -2804.57
Iteration 46 took 2.25 seconds (mean sampled reward: -5037.24). Current reward after update: -2871.13, Optimal reward -2804.57
Iteration 47 took 2.38 seconds (mean sampled reward: -4987.70). Current reward after update: -3503.58, Optimal reward -2804.57
Iteration 48 took 2.35 seconds (mean sampled reward: -4903.29). Current reward after update: -2847.77, Optimal reward -2804.57
Iteration 49 took 2.27 seconds (mean sampled reward: -4791.26). Current reward after update: -3042.50, Optimal reward -2804.57
Iteration 50 took 2.16 seconds (mean sampled reward: -4696.02). Current reward after update: -2824.84, Optimal reward -2804.57
Iteration 51 took 2.31 seconds (mean sampled reward: -4895.27). Current reward after update: -2633.51, Optimal reward -2633.51
Iteration 52 took 2.26 seconds (mean sampled reward: -5389.74). Current reward after update: -3212.81, Optimal reward -2633.51
Iteration 53 took 2.32 seconds (mean sampled reward: -5626.31). Current reward after update: -2624.48, Optimal reward -2624.48
Iteration 54 took 2.30 seconds (mean sampled reward: -5566.91). Current reward after update: -2797.59, Optimal reward -2624.48
Iteration 55 took 2.43 seconds (mean sampled reward: -5167.81). Current reward after update: -2832.02, Optimal reward -2624.48
Iteration 56 took 2.22 seconds (mean sampled reward: -4229.61). Current reward after update: -1994.00, Optimal reward -1994.00
Iteration 57 took 2.21 seconds (mean sampled reward: -4381.28). Current reward after update: -1958.57, Optimal reward -1958.57
Iteration 58 took 2.36 seconds (mean sampled reward: -4648.29). Current reward after update: -1877.89, Optimal reward -1877.89
Iteration 59 took 2.42 seconds (mean sampled reward: -5168.76). Current reward after update: -2002.72, Optimal reward -1877.89
Iteration 60 took 2.36 seconds (mean sampled reward: -5030.59). Current reward after update: -1886.92, Optimal reward -1877.89
Iteration 61 took 2.40 seconds (mean sampled reward: -4643.06). Current reward after update: -1927.84, Optimal reward -1877.89
Iteration 62 took 2.33 seconds (mean sampled reward: -4355.35). Current reward after update: -2941.93, Optimal reward -1877.89
Iteration 63 took 2.43 seconds (mean sampled reward: -4698.57). Current reward after update: -2117.72, Optimal reward -1877.89
Iteration 64 took 2.31 seconds (mean sampled reward: -4995.87). Current reward after update: -1938.01, Optimal reward -1877.89
Iteration 65 took 2.38 seconds (mean sampled reward: -5142.71). Current reward after update: -1936.70, Optimal reward -1877.89
Iteration 66 took 2.30 seconds (mean sampled reward: -4557.46). Current reward after update: -1800.69, Optimal reward -1800.69
Iteration 67 took 2.30 seconds (mean sampled reward: -5263.67). Current reward after update: -2425.75, Optimal reward -1800.69
Iteration 68 took 2.35 seconds (mean sampled reward: -5953.24). Current reward after update: -2438.02, Optimal reward -1800.69
Iteration 69 took 2.36 seconds (mean sampled reward: -5679.33). Current reward after update: -1824.26, Optimal reward -1800.69
Iteration 70 took 2.46 seconds (mean sampled reward: -5788.13). Current reward after update: -2041.52, Optimal reward -1800.69
Iteration 71 took 2.48 seconds (mean sampled reward: -5673.34). Current reward after update: -1848.55, Optimal reward -1800.69
Iteration 72 took 2.32 seconds (mean sampled reward: -5622.30). Current reward after update: -1822.79, Optimal reward -1800.69
Iteration 73 took 2.34 seconds (mean sampled reward: -5484.41). Current reward after update: -2055.33, Optimal reward -1800.69
Iteration 74 took 2.30 seconds (mean sampled reward: -5728.48). Current reward after update: -1988.20, Optimal reward -1800.69
Iteration 75 took 2.35 seconds (mean sampled reward: -5663.16). Current reward after update: -1757.37, Optimal reward -1757.37
Iteration 76 took 2.35 seconds (mean sampled reward: -5403.08). Current reward after update: -1860.28, Optimal reward -1757.37
Iteration 77 took 2.28 seconds (mean sampled reward: -6270.25). Current reward after update: -1959.15, Optimal reward -1757.37
Iteration 78 took 2.40 seconds (mean sampled reward: -4922.74). Current reward after update: -1770.58, Optimal reward -1757.37
Iteration 79 took 2.28 seconds (mean sampled reward: -4038.30). Current reward after update: -1895.68, Optimal reward -1757.37
Iteration 80 took 2.30 seconds (mean sampled reward: -4131.27). Current reward after update: -1911.48, Optimal reward -1757.37
Iteration 81 took 2.45 seconds (mean sampled reward: -4755.34). Current reward after update: -2424.96, Optimal reward -1757.37
Iteration 82 took 2.33 seconds (mean sampled reward: -5759.25). Current reward after update: -1960.47, Optimal reward -1757.37
Iteration 83 took 2.29 seconds (mean sampled reward: -4749.81). Current reward after update: -2105.24, Optimal reward -1757.37
Iteration 84 took 2.31 seconds (mean sampled reward: -4703.64). Current reward after update: -1903.14, Optimal reward -1757.37
Iteration 85 took 2.33 seconds (mean sampled reward: -4298.10). Current reward after update: -5915.99, Optimal reward -1757.37
Iteration 86 took 2.31 seconds (mean sampled reward: -4879.90). Current reward after update: -1711.04, Optimal reward -1711.04
Iteration 87 took 2.32 seconds (mean sampled reward: -5145.11). Current reward after update: -1946.35, Optimal reward -1711.04
Iteration 88 took 2.31 seconds (mean sampled reward: -5441.77). Current reward after update: -1890.67, Optimal reward -1711.04
Iteration 89 took 2.27 seconds (mean sampled reward: -5479.05). Current reward after update: -1846.18, Optimal reward -1711.04
Iteration 90 took 2.33 seconds (mean sampled reward: -4325.58). Current reward after update: -1897.94, Optimal reward -1711.04
Iteration 91 took 2.35 seconds (mean sampled reward: -3685.77). Current reward after update: -1762.85, Optimal reward -1711.04
Iteration 92 took 2.25 seconds (mean sampled reward: -3384.05). Current reward after update: -1648.55, Optimal reward -1648.55
Iteration 93 took 2.25 seconds (mean sampled reward: -3032.49). Current reward after update: -1961.00, Optimal reward -1648.55
Iteration 94 took 2.31 seconds (mean sampled reward: -3302.07). Current reward after update: -1741.30, Optimal reward -1648.55
Iteration 95 took 2.28 seconds (mean sampled reward: -4067.70). Current reward after update: -1817.02, Optimal reward -1648.55
Iteration 96 took 2.29 seconds (mean sampled reward: -3799.79). Current reward after update: -1798.83, Optimal reward -1648.55
Iteration 97 took 2.30 seconds (mean sampled reward: -3773.97). Current reward after update: -2281.25, Optimal reward -1648.55
Iteration 98 took 2.35 seconds (mean sampled reward: -3355.55). Current reward after update: -1736.03, Optimal reward -1648.55
Iteration 99 took 2.30 seconds (mean sampled reward: -3113.03). Current reward after update: -2486.61, Optimal reward -1648.55
Iteration 100 took 2.33 seconds (mean sampled reward: -3273.81). Current reward after update: -2066.41, Optimal reward -1648.55
Iteration 101 took 2.25 seconds (mean sampled reward: -3537.72). Current reward after update: -1926.58, Optimal reward -1648.55
Iteration 102 took 2.36 seconds (mean sampled reward: -4025.36). Current reward after update: -1714.80, Optimal reward -1648.55
Iteration 103 took 2.25 seconds (mean sampled reward: -4413.85). Current reward after update: -1708.18, Optimal reward -1648.55
Iteration 104 took 2.23 seconds (mean sampled reward: -4224.87). Current reward after update: -1793.49, Optimal reward -1648.55
Iteration 105 took 2.29 seconds (mean sampled reward: -4944.02). Current reward after update: -1668.90, Optimal reward -1648.55
Iteration 106 took 2.37 seconds (mean sampled reward: -5118.81). Current reward after update: -1616.10, Optimal reward -1616.10
Iteration 107 took 2.37 seconds (mean sampled reward: -5460.95). Current reward after update: -1720.66, Optimal reward -1616.10
Iteration 108 took 2.57 seconds (mean sampled reward: -5480.42). Current reward after update: -2753.37, Optimal reward -1616.10
Iteration 109 took 2.34 seconds (mean sampled reward: -5543.98). Current reward after update: -1664.06, Optimal reward -1616.10
Iteration 110 took 2.27 seconds (mean sampled reward: -4696.19). Current reward after update: -1603.17, Optimal reward -1603.17
Iteration 111 took 2.28 seconds (mean sampled reward: -4361.56). Current reward after update: -1586.72, Optimal reward -1586.72
Iteration 112 took 2.57 seconds (mean sampled reward: -4732.21). Current reward after update: -1682.34, Optimal reward -1586.72
Iteration 113 took 2.50 seconds (mean sampled reward: -4514.10). Current reward after update: -1677.46, Optimal reward -1586.72
Iteration 114 took 2.34 seconds (mean sampled reward: -4448.49). Current reward after update: -1677.59, Optimal reward -1586.72
Iteration 115 took 2.33 seconds (mean sampled reward: -4460.80). Current reward after update: -1674.61, Optimal reward -1586.72
Iteration 116 took 2.30 seconds (mean sampled reward: -4260.44). Current reward after update: -1607.06, Optimal reward -1586.72
Iteration 117 took 2.26 seconds (mean sampled reward: -4330.40). Current reward after update: -1556.08, Optimal reward -1556.08
Iteration 118 took 2.27 seconds (mean sampled reward: -4911.28). Current reward after update: -1567.10, Optimal reward -1556.08
Iteration 119 took 2.25 seconds (mean sampled reward: -4402.87). Current reward after update: -1642.95, Optimal reward -1556.08
Iteration 120 took 2.30 seconds (mean sampled reward: -4716.50). Current reward after update: -1645.86, Optimal reward -1556.08
Iteration 121 took 2.37 seconds (mean sampled reward: -4532.42). Current reward after update: -1783.07, Optimal reward -1556.08
Iteration 122 took 2.24 seconds (mean sampled reward: -3493.62). Current reward after update: -2727.95, Optimal reward -1556.08
Iteration 123 took 2.23 seconds (mean sampled reward: -3214.34). Current reward after update: -1540.64, Optimal reward -1540.64
Iteration 124 took 2.23 seconds (mean sampled reward: -3340.81). Current reward after update: -1810.09, Optimal reward -1540.64
Iteration 125 took 2.20 seconds (mean sampled reward: -3391.39). Current reward after update: -2443.10, Optimal reward -1540.64
Iteration 126 took 2.29 seconds (mean sampled reward: -3462.15). Current reward after update: -1460.20, Optimal reward -1460.20
Iteration 127 took 2.29 seconds (mean sampled reward: -3621.34). Current reward after update: -1471.10, Optimal reward -1460.20
Iteration 128 took 2.24 seconds (mean sampled reward: -3273.84). Current reward after update: -1487.37, Optimal reward -1460.20
Iteration 129 took 2.21 seconds (mean sampled reward: -3455.90). Current reward after update: -1518.94, Optimal reward -1460.20
Iteration 130 took 2.27 seconds (mean sampled reward: -3728.79). Current reward after update: -1556.11, Optimal reward -1460.20
Iteration 131 took 2.31 seconds (mean sampled reward: -5137.39). Current reward after update: -1726.11, Optimal reward -1460.20
Iteration 132 took 2.24 seconds (mean sampled reward: -4578.16). Current reward after update: -1493.11, Optimal reward -1460.20
Iteration 133 took 2.28 seconds (mean sampled reward: -4886.41). Current reward after update: -1579.42, Optimal reward -1460.20
Iteration 134 took 2.36 seconds (mean sampled reward: -4460.53). Current reward after update: -1547.73, Optimal reward -1460.20
Iteration 135 took 2.35 seconds (mean sampled reward: -4520.63). Current reward after update: -1601.53, Optimal reward -1460.20
Iteration 136 took 2.27 seconds (mean sampled reward: -4577.58). Current reward after update: -1523.71, Optimal reward -1460.20
Iteration 137 took 2.27 seconds (mean sampled reward: -4548.66). Current reward after update: -1489.20, Optimal reward -1460.20
Iteration 138 took 2.34 seconds (mean sampled reward: -6194.97). Current reward after update: -1509.12, Optimal reward -1460.20
Iteration 139 took 2.28 seconds (mean sampled reward: -5791.40). Current reward after update: -1487.35, Optimal reward -1460.20
Iteration 140 took 2.27 seconds (mean sampled reward: -5397.11). Current reward after update: -1543.30, Optimal reward -1460.20
Iteration 141 took 2.32 seconds (mean sampled reward: -4826.18). Current reward after update: -1544.14, Optimal reward -1460.20
Iteration 142 took 2.41 seconds (mean sampled reward: -5864.05). Current reward after update: -1561.23, Optimal reward -1460.20
Iteration 143 took 2.36 seconds (mean sampled reward: -5468.85). Current reward after update: -1611.17, Optimal reward -1460.20
Iteration 144 took 2.28 seconds (mean sampled reward: -5085.35). Current reward after update: -1552.23, Optimal reward -1460.20
Iteration 145 took 2.29 seconds (mean sampled reward: -5329.01). Current reward after update: -1599.82, Optimal reward -1460.20
Iteration 146 took 2.33 seconds (mean sampled reward: -5531.22). Current reward after update: -1580.69, Optimal reward -1460.20
Iteration 147 took 2.40 seconds (mean sampled reward: -4401.95). Current reward after update: -1470.09, Optimal reward -1460.20
Iteration 148 took 2.28 seconds (mean sampled reward: -4803.62). Current reward after update: -1440.01, Optimal reward -1440.01
Iteration 149 took 2.25 seconds (mean sampled reward: -4264.96). Current reward after update: -1423.01, Optimal reward -1423.01
Iteration 150 took 2.25 seconds (mean sampled reward: -3865.44). Current reward after update: -1445.03, Optimal reward -1423.01
Iteration 151 took 2.24 seconds (mean sampled reward: -4439.99). Current reward after update: -1430.92, Optimal reward -1423.01
Iteration 152 took 2.33 seconds (mean sampled reward: -4222.86). Current reward after update: -1488.58, Optimal reward -1423.01
Iteration 153 took 2.30 seconds (mean sampled reward: -5266.50). Current reward after update: -1481.16, Optimal reward -1423.01
Iteration 154 took 2.28 seconds (mean sampled reward: -5563.26). Current reward after update: -1455.71, Optimal reward -1423.01
Iteration 155 took 2.33 seconds (mean sampled reward: -5423.30). Current reward after update: -1426.75, Optimal reward -1423.01
Iteration 156 took 2.32 seconds (mean sampled reward: -4833.57). Current reward after update: -1456.62, Optimal reward -1423.01
Iteration 157 took 2.35 seconds (mean sampled reward: -4454.06). Current reward after update: -1450.34, Optimal reward -1423.01
Iteration 158 took 2.32 seconds (mean sampled reward: -4629.61). Current reward after update: -2228.05, Optimal reward -1423.01
Iteration 159 took 2.32 seconds (mean sampled reward: -4103.82). Current reward after update: -1767.68, Optimal reward -1423.01
Iteration 160 took 2.36 seconds (mean sampled reward: -4734.14). Current reward after update: -1511.87, Optimal reward -1423.01
Iteration 161 took 2.32 seconds (mean sampled reward: -5002.92). Current reward after update: -1572.11, Optimal reward -1423.01
Iteration 162 took 2.43 seconds (mean sampled reward: -4997.62). Current reward after update: -1604.98, Optimal reward -1423.01
Iteration 163 took 2.31 seconds (mean sampled reward: -4404.43). Current reward after update: -1568.62, Optimal reward -1423.01
Iteration 164 took 2.39 seconds (mean sampled reward: -4936.83). Current reward after update: -1766.95, Optimal reward -1423.01
Iteration 165 took 2.30 seconds (mean sampled reward: -4105.72). Current reward after update: -1586.51, Optimal reward -1423.01
Iteration 166 took 2.19 seconds (mean sampled reward: -3556.80). Current reward after update: -1484.82, Optimal reward -1423.01
Iteration 167 took 2.32 seconds (mean sampled reward: -5017.06). Current reward after update: -1523.28, Optimal reward -1423.01
Iteration 168 took 2.32 seconds (mean sampled reward: -4347.07). Current reward after update: -1825.56, Optimal reward -1423.01
Iteration 169 took 2.22 seconds (mean sampled reward: -3601.83). Current reward after update: -1584.32, Optimal reward -1423.01
Iteration 170 took 2.35 seconds (mean sampled reward: -4173.00). Current reward after update: -1567.00, Optimal reward -1423.01
Iteration 171 took 2.26 seconds (mean sampled reward: -5082.45). Current reward after update: -1614.45, Optimal reward -1423.01
Iteration 172 took 2.26 seconds (mean sampled reward: -5187.46). Current reward after update: -1545.22, Optimal reward -1423.01
Iteration 173 took 2.28 seconds (mean sampled reward: -4543.48). Current reward after update: -1507.67, Optimal reward -1423.01
Iteration 174 took 2.26 seconds (mean sampled reward: -5080.93). Current reward after update: -1505.45, Optimal reward -1423.01
Iteration 175 took 2.26 seconds (mean sampled reward: -4481.57). Current reward after update: -1998.19, Optimal reward -1423.01
Iteration 176 took 2.23 seconds (mean sampled reward: -4034.05). Current reward after update: -1572.18, Optimal reward -1423.01
Iteration 177 took 2.36 seconds (mean sampled reward: -4308.61). Current reward after update: -1908.01, Optimal reward -1423.01
Iteration 178 took 2.29 seconds (mean sampled reward: -3993.78). Current reward after update: -1535.80, Optimal reward -1423.01
Iteration 179 took 2.31 seconds (mean sampled reward: -3888.10). Current reward after update: -1563.00, Optimal reward -1423.01
Iteration 180 took 2.30 seconds (mean sampled reward: -3813.99). Current reward after update: -1593.09, Optimal reward -1423.01
Iteration 181 took 2.25 seconds (mean sampled reward: -4560.05). Current reward after update: -1583.80, Optimal reward -1423.01
Iteration 182 took 2.21 seconds (mean sampled reward: -3993.46). Current reward after update: -1594.20, Optimal reward -1423.01
Iteration 183 took 2.29 seconds (mean sampled reward: -5285.22). Current reward after update: -1641.38, Optimal reward -1423.01
Iteration 184 took 2.22 seconds (mean sampled reward: -4992.95). Current reward after update: -1592.34, Optimal reward -1423.01
Iteration 185 took 2.21 seconds (mean sampled reward: -3984.23). Current reward after update: -1624.07, Optimal reward -1423.01
Iteration 186 took 2.33 seconds (mean sampled reward: -4143.09). Current reward after update: -1616.49, Optimal reward -1423.01
Iteration 187 took 2.31 seconds (mean sampled reward: -4654.76). Current reward after update: -1725.51, Optimal reward -1423.01
Iteration 188 took 2.24 seconds (mean sampled reward: -5234.23). Current reward after update: -1750.03, Optimal reward -1423.01
Iteration 189 took 2.27 seconds (mean sampled reward: -4538.44). Current reward after update: -1745.24, Optimal reward -1423.01
Iteration 190 took 2.23 seconds (mean sampled reward: -3762.22). Current reward after update: -1693.68, Optimal reward -1423.01
Iteration 191 took 2.24 seconds (mean sampled reward: -4655.66). Current reward after update: -1701.53, Optimal reward -1423.01
Iteration 192 took 2.26 seconds (mean sampled reward: -4711.93). Current reward after update: -1720.47, Optimal reward -1423.01
Iteration 193 took 2.23 seconds (mean sampled reward: -5099.63). Current reward after update: -1646.72, Optimal reward -1423.01
Iteration 194 took 2.23 seconds (mean sampled reward: -4417.63). Current reward after update: -1629.16, Optimal reward -1423.01
Iteration 195 took 2.35 seconds (mean sampled reward: -4476.49). Current reward after update: -1857.56, Optimal reward -1423.01
Iteration 196 took 2.21 seconds (mean sampled reward: -3942.53). Current reward after update: -1669.74, Optimal reward -1423.01
Iteration 197 took 2.23 seconds (mean sampled reward: -4828.30). Current reward after update: -2864.96, Optimal reward -1423.01
Iteration 198 took 2.28 seconds (mean sampled reward: -4740.85). Current reward after update: -2119.48, Optimal reward -1423.01
Iteration 199 took 2.24 seconds (mean sampled reward: -4746.01). Current reward after update: -1744.06, Optimal reward -1423.01
Iteration 200 took 2.25 seconds (mean sampled reward: -4825.18). Current reward after update: -1727.10, Optimal reward -1423.01
Iteration 1 took 2.40 seconds (mean sampled reward: -7592.72). Current reward after update: -7197.38, Optimal reward -7197.38
Iteration 2 took 2.28 seconds (mean sampled reward: -7537.73). Current reward after update: -7141.65, Optimal reward -7141.65
Iteration 3 took 2.23 seconds (mean sampled reward: -7485.26). Current reward after update: -7299.86, Optimal reward -7141.65
Iteration 4 took 2.25 seconds (mean sampled reward: -7470.36). Current reward after update: -7092.81, Optimal reward -7092.81
Iteration 5 took 2.35 seconds (mean sampled reward: -7471.15). Current reward after update: -7010.31, Optimal reward -7010.31
Iteration 6 took 2.49 seconds (mean sampled reward: -7469.65). Current reward after update: -7051.87, Optimal reward -7010.31
Iteration 7 took 2.35 seconds (mean sampled reward: -7472.10). Current reward after update: -7009.51, Optimal reward -7009.51
Iteration 8 took 2.37 seconds (mean sampled reward: -7469.16). Current reward after update: -6900.69, Optimal reward -6900.69
Iteration 9 took 2.39 seconds (mean sampled reward: -7415.68). Current reward after update: -6549.37, Optimal reward -6549.37
Iteration 10 took 2.52 seconds (mean sampled reward: -7363.86). Current reward after update: -6256.98, Optimal reward -6256.98
Iteration 11 took 2.53 seconds (mean sampled reward: -7113.32). Current reward after update: -5881.07, Optimal reward -5881.07
Iteration 12 took 2.53 seconds (mean sampled reward: -6700.84). Current reward after update: -5389.72, Optimal reward -5389.72
Iteration 13 took 2.42 seconds (mean sampled reward: -6792.93). Current reward after update: -5716.25, Optimal reward -5389.72
Iteration 14 took 2.40 seconds (mean sampled reward: -6483.77). Current reward after update: -4929.10, Optimal reward -4929.10
Iteration 15 took 2.33 seconds (mean sampled reward: -6669.69). Current reward after update: -4512.42, Optimal reward -4512.42
Iteration 16 took 2.35 seconds (mean sampled reward: -6722.11). Current reward after update: -4228.96, Optimal reward -4228.96
Iteration 17 took 2.35 seconds (mean sampled reward: -5604.01). Current reward after update: -3867.18, Optimal reward -3867.18
Iteration 18 took 2.36 seconds (mean sampled reward: -5128.35). Current reward after update: -3692.25, Optimal reward -3692.25
Iteration 19 took 2.26 seconds (mean sampled reward: -5029.57). Current reward after update: -3616.05, Optimal reward -3616.05
Iteration 20 took 2.39 seconds (mean sampled reward: -4715.36). Current reward after update: -3550.17, Optimal reward -3550.17
Iteration 21 took 2.26 seconds (mean sampled reward: -4614.91). Current reward after update: -3559.96, Optimal reward -3550.17
Iteration 22 took 2.29 seconds (mean sampled reward: -4026.31). Current reward after update: -3504.02, Optimal reward -3504.02
Iteration 23 took 2.41 seconds (mean sampled reward: -4511.29). Current reward after update: -3503.37, Optimal reward -3503.37
Iteration 24 took 2.35 seconds (mean sampled reward: -4465.20). Current reward after update: -3273.38, Optimal reward -3273.38
Iteration 25 took 2.33 seconds (mean sampled reward: -4120.43). Current reward after update: -3941.66, Optimal reward -3273.38
Iteration 26 took 2.29 seconds (mean sampled reward: -4174.68). Current reward after update: -3573.08, Optimal reward -3273.38
Iteration 27 took 2.27 seconds (mean sampled reward: -4247.98). Current reward after update: -3182.12, Optimal reward -3182.12
Iteration 28 took 2.40 seconds (mean sampled reward: -4337.34). Current reward after update: -3225.07, Optimal reward -3182.12
Iteration 29 took 2.35 seconds (mean sampled reward: -4758.52). Current reward after update: -3245.20, Optimal reward -3182.12
Iteration 30 took 2.49 seconds (mean sampled reward: -4818.71). Current reward after update: -3199.17, Optimal reward -3182.12
Iteration 31 took 2.27 seconds (mean sampled reward: -4550.84). Current reward after update: -5061.91, Optimal reward -3182.12
Iteration 32 took 2.59 seconds (mean sampled reward: -4402.56). Current reward after update: -3415.57, Optimal reward -3182.12
Iteration 33 took 2.27 seconds (mean sampled reward: -4491.37). Current reward after update: -3010.86, Optimal reward -3010.86
Iteration 34 took 2.34 seconds (mean sampled reward: -4512.12). Current reward after update: -2991.55, Optimal reward -2991.55
Iteration 35 took 2.33 seconds (mean sampled reward: -5086.06). Current reward after update: -2986.36, Optimal reward -2986.36
Iteration 36 took 2.37 seconds (mean sampled reward: -5108.14). Current reward after update: -2903.31, Optimal reward -2903.31
Iteration 37 took 2.39 seconds (mean sampled reward: -4708.21). Current reward after update: -2944.70, Optimal reward -2903.31
Iteration 38 took 2.27 seconds (mean sampled reward: -4569.39). Current reward after update: -2672.06, Optimal reward -2672.06
Iteration 39 took 2.31 seconds (mean sampled reward: -4733.12). Current reward after update: -2697.39, Optimal reward -2672.06
Iteration 40 took 2.39 seconds (mean sampled reward: -4338.25). Current reward after update: -3804.34, Optimal reward -2672.06
Iteration 41 took 2.45 seconds (mean sampled reward: -4326.51). Current reward after update: -2604.10, Optimal reward -2604.10
Iteration 42 took 2.43 seconds (mean sampled reward: -4440.40). Current reward after update: -3864.77, Optimal reward -2604.10
Iteration 43 took 2.35 seconds (mean sampled reward: -5759.28). Current reward after update: -2661.21, Optimal reward -2604.10
Iteration 44 took 2.30 seconds (mean sampled reward: -5870.30). Current reward after update: -2586.55, Optimal reward -2586.55
Iteration 45 took 2.39 seconds (mean sampled reward: -5292.34). Current reward after update: -2490.76, Optimal reward -2490.76
Iteration 46 took 2.35 seconds (mean sampled reward: -5960.27). Current reward after update: -4472.88, Optimal reward -2490.76
Iteration 47 took 2.40 seconds (mean sampled reward: -5085.44). Current reward after update: -2749.33, Optimal reward -2490.76
Iteration 48 took 2.26 seconds (mean sampled reward: -4041.78). Current reward after update: -2467.46, Optimal reward -2467.46
Iteration 49 took 2.29 seconds (mean sampled reward: -4910.30). Current reward after update: -2529.96, Optimal reward -2467.46
Iteration 50 took 2.38 seconds (mean sampled reward: -4840.67). Current reward after update: -2501.72, Optimal reward -2467.46
Iteration 51 took 2.43 seconds (mean sampled reward: -5041.67). Current reward after update: -2697.81, Optimal reward -2467.46
Iteration 52 took 2.33 seconds (mean sampled reward: -4305.84). Current reward after update: -4356.36, Optimal reward -2467.46
Iteration 53 took 2.38 seconds (mean sampled reward: -4606.70). Current reward after update: -2566.42, Optimal reward -2467.46
Iteration 54 took 2.41 seconds (mean sampled reward: -3858.27). Current reward after update: -2477.39, Optimal reward -2467.46
Iteration 55 took 2.35 seconds (mean sampled reward: -4131.47). Current reward after update: -2901.69, Optimal reward -2467.46
Iteration 56 took 2.43 seconds (mean sampled reward: -4305.72). Current reward after update: -2474.96, Optimal reward -2467.46
Iteration 57 took 2.45 seconds (mean sampled reward: -5270.56). Current reward after update: -2495.26, Optimal reward -2467.46
Iteration 58 took 2.34 seconds (mean sampled reward: -5354.91). Current reward after update: -2585.50, Optimal reward -2467.46
Iteration 59 took 2.37 seconds (mean sampled reward: -5362.14). Current reward after update: -2765.49, Optimal reward -2467.46
Iteration 60 took 2.41 seconds (mean sampled reward: -5078.96). Current reward after update: -3018.40, Optimal reward -2467.46
Iteration 61 took 2.34 seconds (mean sampled reward: -5290.21). Current reward after update: -2494.76, Optimal reward -2467.46
Iteration 62 took 2.28 seconds (mean sampled reward: -4827.59). Current reward after update: -2426.76, Optimal reward -2426.76
Iteration 63 took 2.27 seconds (mean sampled reward: -5140.77). Current reward after update: -2844.85, Optimal reward -2426.76
Iteration 64 took 2.28 seconds (mean sampled reward: -4529.86). Current reward after update: -2454.10, Optimal reward -2426.76
Iteration 65 took 2.29 seconds (mean sampled reward: -4320.86). Current reward after update: -2447.14, Optimal reward -2426.76
Iteration 66 took 2.32 seconds (mean sampled reward: -4210.60). Current reward after update: -2496.56, Optimal reward -2426.76
Iteration 67 took 2.29 seconds (mean sampled reward: -5134.91). Current reward after update: -2479.05, Optimal reward -2426.76
Iteration 68 took 2.25 seconds (mean sampled reward: -5554.02). Current reward after update: -2655.49, Optimal reward -2426.76
Iteration 69 took 2.28 seconds (mean sampled reward: -4690.01). Current reward after update: -2846.51, Optimal reward -2426.76
Iteration 70 took 2.37 seconds (mean sampled reward: -4971.06). Current reward after update: -3998.75, Optimal reward -2426.76
Iteration 71 took 2.29 seconds (mean sampled reward: -4815.59). Current reward after update: -2355.42, Optimal reward -2355.42
Iteration 72 took 2.34 seconds (mean sampled reward: -5978.90). Current reward after update: -2405.40, Optimal reward -2355.42
Iteration 73 took 2.36 seconds (mean sampled reward: -6185.02). Current reward after update: -2318.78, Optimal reward -2318.78
Iteration 74 took 2.36 seconds (mean sampled reward: -5777.67). Current reward after update: -2376.55, Optimal reward -2318.78
Iteration 75 took 2.24 seconds (mean sampled reward: -5816.06). Current reward after update: -2415.96, Optimal reward -2318.78
Iteration 76 took 2.41 seconds (mean sampled reward: -5240.21). Current reward after update: -2555.51, Optimal reward -2318.78
Iteration 77 took 2.26 seconds (mean sampled reward: -5353.18). Current reward after update: -2418.14, Optimal reward -2318.78
Iteration 78 took 2.27 seconds (mean sampled reward: -5249.60). Current reward after update: -2515.91, Optimal reward -2318.78
Iteration 79 took 2.46 seconds (mean sampled reward: -5367.63). Current reward after update: -2518.61, Optimal reward -2318.78
Iteration 80 took 2.30 seconds (mean sampled reward: -4282.01). Current reward after update: -2911.55, Optimal reward -2318.78
Iteration 81 took 2.30 seconds (mean sampled reward: -5307.47). Current reward after update: -2492.28, Optimal reward -2318.78
Iteration 82 took 2.28 seconds (mean sampled reward: -4774.75). Current reward after update: -2274.91, Optimal reward -2274.91
Iteration 83 took 2.42 seconds (mean sampled reward: -6625.37). Current reward after update: -2586.85, Optimal reward -2274.91
Iteration 84 took 2.34 seconds (mean sampled reward: -5927.08). Current reward after update: -2503.37, Optimal reward -2274.91
Iteration 85 took 2.24 seconds (mean sampled reward: -4845.84). Current reward after update: -2406.21, Optimal reward -2274.91
Iteration 86 took 2.36 seconds (mean sampled reward: -3969.38). Current reward after update: -2496.13, Optimal reward -2274.91
Iteration 87 took 2.32 seconds (mean sampled reward: -4354.03). Current reward after update: -2467.36, Optimal reward -2274.91
Iteration 88 took 2.33 seconds (mean sampled reward: -3904.30). Current reward after update: -5416.32, Optimal reward -2274.91
Iteration 89 took 2.33 seconds (mean sampled reward: -5599.13). Current reward after update: -2129.27, Optimal reward -2129.27
Iteration 90 took 2.34 seconds (mean sampled reward: -4486.22). Current reward after update: -2002.89, Optimal reward -2002.89
Iteration 91 took 2.32 seconds (mean sampled reward: -4386.83). Current reward after update: -1960.46, Optimal reward -1960.46
Iteration 92 took 2.29 seconds (mean sampled reward: -4065.08). Current reward after update: -1754.87, Optimal reward -1754.87
Iteration 93 took 2.25 seconds (mean sampled reward: -4039.95). Current reward after update: -1598.72, Optimal reward -1598.72
Iteration 94 took 2.30 seconds (mean sampled reward: -4138.93). Current reward after update: -1546.59, Optimal reward -1546.59
Iteration 95 took 2.30 seconds (mean sampled reward: -4410.38). Current reward after update: -1779.02, Optimal reward -1546.59
Iteration 96 took 2.38 seconds (mean sampled reward: -4849.12). Current reward after update: -1817.29, Optimal reward -1546.59
Iteration 97 took 2.36 seconds (mean sampled reward: -4494.81). Current reward after update: -1487.80, Optimal reward -1487.80
Iteration 98 took 2.28 seconds (mean sampled reward: -4705.55). Current reward after update: -1740.11, Optimal reward -1487.80
Iteration 99 took 2.29 seconds (mean sampled reward: -4549.96). Current reward after update: -1663.39, Optimal reward -1487.80
Iteration 100 took 2.36 seconds (mean sampled reward: -4047.34). Current reward after update: -1556.22, Optimal reward -1487.80
Iteration 101 took 2.28 seconds (mean sampled reward: -4230.41). Current reward after update: -1712.88, Optimal reward -1487.80
Iteration 102 took 2.29 seconds (mean sampled reward: -4284.40). Current reward after update: -1882.98, Optimal reward -1487.80
Iteration 103 took 2.30 seconds (mean sampled reward: -4189.06). Current reward after update: -1596.12, Optimal reward -1487.80
Iteration 104 took 2.30 seconds (mean sampled reward: -4268.11). Current reward after update: -1609.58, Optimal reward -1487.80
Iteration 105 took 2.33 seconds (mean sampled reward: -3935.28). Current reward after update: -1484.08, Optimal reward -1484.08
Iteration 106 took 2.33 seconds (mean sampled reward: -4169.50). Current reward after update: -1540.38, Optimal reward -1484.08
Iteration 107 took 2.35 seconds (mean sampled reward: -4461.12). Current reward after update: -1764.68, Optimal reward -1484.08
Iteration 108 took 2.52 seconds (mean sampled reward: -4186.38). Current reward after update: -1800.45, Optimal reward -1484.08
Iteration 109 took 2.42 seconds (mean sampled reward: -4366.60). Current reward after update: -1892.81, Optimal reward -1484.08
Iteration 110 took 2.23 seconds (mean sampled reward: -3935.30). Current reward after update: -1789.06, Optimal reward -1484.08
Iteration 111 took 2.28 seconds (mean sampled reward: -4143.48). Current reward after update: -1785.75, Optimal reward -1484.08
Iteration 112 took 2.24 seconds (mean sampled reward: -3980.72). Current reward after update: -1858.98, Optimal reward -1484.08
Iteration 113 took 2.26 seconds (mean sampled reward: -4254.89). Current reward after update: -1833.48, Optimal reward -1484.08
Iteration 114 took 2.38 seconds (mean sampled reward: -4161.09). Current reward after update: -1789.38, Optimal reward -1484.08
Iteration 115 took 2.58 seconds (mean sampled reward: -4001.84). Current reward after update: -1749.21, Optimal reward -1484.08
Iteration 116 took 2.35 seconds (mean sampled reward: -4083.51). Current reward after update: -1796.73, Optimal reward -1484.08
Iteration 117 took 2.35 seconds (mean sampled reward: -3829.62). Current reward after update: -1776.15, Optimal reward -1484.08
Iteration 118 took 2.38 seconds (mean sampled reward: -3702.51). Current reward after update: -1821.00, Optimal reward -1484.08
Iteration 119 took 2.38 seconds (mean sampled reward: -4219.80). Current reward after update: -1817.60, Optimal reward -1484.08
Iteration 120 took 2.37 seconds (mean sampled reward: -4086.17). Current reward after update: -3248.78, Optimal reward -1484.08
Iteration 121 took 2.41 seconds (mean sampled reward: -4090.69). Current reward after update: -1817.23, Optimal reward -1484.08
Iteration 122 took 2.30 seconds (mean sampled reward: -4082.48). Current reward after update: -1870.61, Optimal reward -1484.08
Iteration 123 took 2.29 seconds (mean sampled reward: -4062.70). Current reward after update: -1795.03, Optimal reward -1484.08
Iteration 124 took 2.31 seconds (mean sampled reward: -3680.10). Current reward after update: -1864.80, Optimal reward -1484.08
Iteration 125 took 2.28 seconds (mean sampled reward: -3824.68). Current reward after update: -1835.08, Optimal reward -1484.08
Iteration 126 took 2.28 seconds (mean sampled reward: -3569.26). Current reward after update: -1818.66, Optimal reward -1484.08
Iteration 127 took 2.26 seconds (mean sampled reward: -3753.12). Current reward after update: -2495.97, Optimal reward -1484.08
Iteration 128 took 2.46 seconds (mean sampled reward: -3698.96). Current reward after update: -1793.36, Optimal reward -1484.08
Iteration 129 took 2.30 seconds (mean sampled reward: -4096.68). Current reward after update: -1784.29, Optimal reward -1484.08
Iteration 130 took 2.25 seconds (mean sampled reward: -4003.46). Current reward after update: -1819.05, Optimal reward -1484.08
Iteration 131 took 2.28 seconds (mean sampled reward: -3844.32). Current reward after update: -4308.37, Optimal reward -1484.08
Iteration 132 took 2.32 seconds (mean sampled reward: -4866.74). Current reward after update: -1837.06, Optimal reward -1484.08
Iteration 133 took 2.26 seconds (mean sampled reward: -4839.86). Current reward after update: -1816.20, Optimal reward -1484.08
Iteration 134 took 2.33 seconds (mean sampled reward: -4429.03). Current reward after update: -1735.73, Optimal reward -1484.08
Iteration 135 took 2.30 seconds (mean sampled reward: -4738.70). Current reward after update: -1838.95, Optimal reward -1484.08
Iteration 136 took 2.30 seconds (mean sampled reward: -3621.50). Current reward after update: -1765.10, Optimal reward -1484.08
Iteration 137 took 2.31 seconds (mean sampled reward: -4409.51). Current reward after update: -1653.98, Optimal reward -1484.08
Iteration 138 took 2.33 seconds (mean sampled reward: -4650.62). Current reward after update: -1804.77, Optimal reward -1484.08
Iteration 139 took 2.27 seconds (mean sampled reward: -5251.99). Current reward after update: -1872.76, Optimal reward -1484.08
Iteration 140 took 2.31 seconds (mean sampled reward: -5615.60). Current reward after update: -2464.45, Optimal reward -1484.08
Iteration 141 took 2.26 seconds (mean sampled reward: -5422.59). Current reward after update: -1839.64, Optimal reward -1484.08
Iteration 142 took 2.36 seconds (mean sampled reward: -5503.59). Current reward after update: -1761.26, Optimal reward -1484.08
Iteration 143 took 2.27 seconds (mean sampled reward: -5401.05). Current reward after update: -2860.25, Optimal reward -1484.08
Iteration 144 took 2.37 seconds (mean sampled reward: -4936.30). Current reward after update: -1817.20, Optimal reward -1484.08
Iteration 145 took 2.29 seconds (mean sampled reward: -4553.80). Current reward after update: -1829.36, Optimal reward -1484.08
Iteration 146 took 2.30 seconds (mean sampled reward: -3697.15). Current reward after update: -1722.94, Optimal reward -1484.08
Iteration 147 took 2.26 seconds (mean sampled reward: -3788.10). Current reward after update: -1731.29, Optimal reward -1484.08
Iteration 148 took 2.29 seconds (mean sampled reward: -3892.98). Current reward after update: -1778.45, Optimal reward -1484.08
Iteration 149 took 2.27 seconds (mean sampled reward: -4075.49). Current reward after update: -1678.26, Optimal reward -1484.08
Iteration 150 took 2.36 seconds (mean sampled reward: -4064.32). Current reward after update: -1705.17, Optimal reward -1484.08
Iteration 151 took 2.30 seconds (mean sampled reward: -4159.71). Current reward after update: -2796.59, Optimal reward -1484.08
Iteration 152 took 2.31 seconds (mean sampled reward: -3638.00). Current reward after update: -2606.54, Optimal reward -1484.08
Iteration 153 took 2.30 seconds (mean sampled reward: -3703.60). Current reward after update: -1742.20, Optimal reward -1484.08
Iteration 154 took 2.33 seconds (mean sampled reward: -3607.46). Current reward after update: -1752.31, Optimal reward -1484.08
Iteration 155 took 2.39 seconds (mean sampled reward: -3551.32). Current reward after update: -3966.48, Optimal reward -1484.08
Iteration 156 took 2.46 seconds (mean sampled reward: -3460.96). Current reward after update: -1810.03, Optimal reward -1484.08
Iteration 157 took 2.42 seconds (mean sampled reward: -3801.70). Current reward after update: -1683.49, Optimal reward -1484.08
Iteration 158 took 2.40 seconds (mean sampled reward: -3505.44). Current reward after update: -1751.03, Optimal reward -1484.08
Iteration 159 took 2.54 seconds (mean sampled reward: -3532.17). Current reward after update: -3413.29, Optimal reward -1484.08
Iteration 160 took 2.36 seconds (mean sampled reward: -3535.52). Current reward after update: -2083.13, Optimal reward -1484.08
Iteration 161 took 2.34 seconds (mean sampled reward: -3458.81). Current reward after update: -1793.72, Optimal reward -1484.08
Iteration 162 took 2.41 seconds (mean sampled reward: -3333.42). Current reward after update: -1737.10, Optimal reward -1484.08
Iteration 163 took 2.35 seconds (mean sampled reward: -3876.35). Current reward after update: -3275.37, Optimal reward -1484.08
Iteration 164 took 2.31 seconds (mean sampled reward: -4099.64). Current reward after update: -4029.43, Optimal reward -1484.08
Iteration 165 took 2.34 seconds (mean sampled reward: -4202.42). Current reward after update: -1800.91, Optimal reward -1484.08
Iteration 166 took 2.37 seconds (mean sampled reward: -3807.45). Current reward after update: -1759.26, Optimal reward -1484.08
Iteration 167 took 2.41 seconds (mean sampled reward: -3480.83). Current reward after update: -1785.57, Optimal reward -1484.08
Iteration 168 took 2.41 seconds (mean sampled reward: -3261.61). Current reward after update: -1788.58, Optimal reward -1484.08
Iteration 169 took 2.35 seconds (mean sampled reward: -3615.37). Current reward after update: -1777.65, Optimal reward -1484.08
Iteration 170 took 2.33 seconds (mean sampled reward: -3641.06). Current reward after update: -1838.85, Optimal reward -1484.08
Iteration 171 took 2.34 seconds (mean sampled reward: -3452.53). Current reward after update: -1798.13, Optimal reward -1484.08
Iteration 172 took 2.34 seconds (mean sampled reward: -3809.69). Current reward after update: -1937.06, Optimal reward -1484.08
Iteration 173 took 2.47 seconds (mean sampled reward: -3725.53). Current reward after update: -2637.60, Optimal reward -1484.08
Iteration 174 took 2.33 seconds (mean sampled reward: -3628.33). Current reward after update: -2249.03, Optimal reward -1484.08
Iteration 175 took 2.43 seconds (mean sampled reward: -4146.38). Current reward after update: -1930.27, Optimal reward -1484.08
Iteration 176 took 2.36 seconds (mean sampled reward: -4748.75). Current reward after update: -1805.16, Optimal reward -1484.08
Iteration 177 took 2.36 seconds (mean sampled reward: -4765.79). Current reward after update: -1860.73, Optimal reward -1484.08
Iteration 178 took 2.37 seconds (mean sampled reward: -4067.09). Current reward after update: -2304.81, Optimal reward -1484.08
Iteration 179 took 2.48 seconds (mean sampled reward: -4183.17). Current reward after update: -1915.11, Optimal reward -1484.08
Iteration 180 took 2.42 seconds (mean sampled reward: -4362.09). Current reward after update: -1810.10, Optimal reward -1484.08
Iteration 181 took 2.38 seconds (mean sampled reward: -4073.58). Current reward after update: -1882.50, Optimal reward -1484.08
Iteration 182 took 2.35 seconds (mean sampled reward: -3684.89). Current reward after update: -3484.67, Optimal reward -1484.08
Iteration 183 took 2.38 seconds (mean sampled reward: -3624.05). Current reward after update: -1793.08, Optimal reward -1484.08
Iteration 184 took 2.33 seconds (mean sampled reward: -3534.27). Current reward after update: -1794.33, Optimal reward -1484.08
Iteration 185 took 2.35 seconds (mean sampled reward: -3543.75). Current reward after update: -2405.11, Optimal reward -1484.08
Iteration 186 took 2.35 seconds (mean sampled reward: -3580.30). Current reward after update: -2749.74, Optimal reward -1484.08
Iteration 187 took 2.33 seconds (mean sampled reward: -3884.60). Current reward after update: -1742.00, Optimal reward -1484.08
Iteration 188 took 2.40 seconds (mean sampled reward: -3610.02). Current reward after update: -1677.05, Optimal reward -1484.08
Iteration 189 took 2.31 seconds (mean sampled reward: -3532.63). Current reward after update: -1799.02, Optimal reward -1484.08
Iteration 190 took 2.33 seconds (mean sampled reward: -3514.43). Current reward after update: -2812.10, Optimal reward -1484.08
Iteration 191 took 2.36 seconds (mean sampled reward: -3431.43). Current reward after update: -2688.14, Optimal reward -1484.08
Iteration 192 took 2.34 seconds (mean sampled reward: -4396.65). Current reward after update: -1791.64, Optimal reward -1484.08
Iteration 193 took 2.29 seconds (mean sampled reward: -4036.71). Current reward after update: -1741.57, Optimal reward -1484.08
Iteration 194 took 2.36 seconds (mean sampled reward: -3805.80). Current reward after update: -1765.45, Optimal reward -1484.08
Iteration 195 took 2.32 seconds (mean sampled reward: -3198.16). Current reward after update: -1680.15, Optimal reward -1484.08
Iteration 196 took 2.37 seconds (mean sampled reward: -3481.93). Current reward after update: -1699.57, Optimal reward -1484.08
Iteration 197 took 2.37 seconds (mean sampled reward: -3454.30). Current reward after update: -1773.46, Optimal reward -1484.08
Iteration 198 took 2.41 seconds (mean sampled reward: -3311.19). Current reward after update: -1841.89, Optimal reward -1484.08
Iteration 199 took 2.45 seconds (mean sampled reward: -3679.93). Current reward after update: -1696.74, Optimal reward -1484.08
Iteration 200 took 2.44 seconds (mean sampled reward: -3741.61). Current reward after update: -1671.08, Optimal reward -1484.08
Iteration 1 took 2.36 seconds (mean sampled reward: -7597.71). Current reward after update: -7288.07, Optimal reward -7288.07
Iteration 2 took 2.41 seconds (mean sampled reward: -7540.44). Current reward after update: -7506.97, Optimal reward -7288.07
Iteration 3 took 2.26 seconds (mean sampled reward: -7520.49). Current reward after update: -7547.11, Optimal reward -7288.07
Iteration 4 took 2.25 seconds (mean sampled reward: -7496.13). Current reward after update: -7225.77, Optimal reward -7225.77
Iteration 5 took 2.25 seconds (mean sampled reward: -7525.12). Current reward after update: -7102.24, Optimal reward -7102.24
Iteration 6 took 2.50 seconds (mean sampled reward: -7513.19). Current reward after update: -7143.11, Optimal reward -7102.24
Iteration 7 took 2.33 seconds (mean sampled reward: -7564.80). Current reward after update: -7238.43, Optimal reward -7102.24
Iteration 8 took 2.40 seconds (mean sampled reward: -7512.05). Current reward after update: -7027.09, Optimal reward -7027.09
Iteration 9 took 2.61 seconds (mean sampled reward: -7541.24). Current reward after update: -7299.50, Optimal reward -7027.09
Iteration 10 took 2.53 seconds (mean sampled reward: -7521.41). Current reward after update: -7013.43, Optimal reward -7013.43
Iteration 11 took 2.52 seconds (mean sampled reward: -7522.14). Current reward after update: -6999.99, Optimal reward -6999.99
Iteration 12 took 2.37 seconds (mean sampled reward: -7488.89). Current reward after update: -6948.55, Optimal reward -6948.55
Iteration 13 took 2.41 seconds (mean sampled reward: -7470.65). Current reward after update: -7143.28, Optimal reward -6948.55
Iteration 14 took 2.38 seconds (mean sampled reward: -7488.13). Current reward after update: -6990.96, Optimal reward -6948.55
Iteration 15 took 2.23 seconds (mean sampled reward: -7512.92). Current reward after update: -6993.04, Optimal reward -6948.55
Iteration 16 took 2.25 seconds (mean sampled reward: -7517.43). Current reward after update: -6964.06, Optimal reward -6948.55
Iteration 17 took 2.23 seconds (mean sampled reward: -7468.27). Current reward after update: -6602.60, Optimal reward -6602.60
Iteration 18 took 2.36 seconds (mean sampled reward: -7235.46). Current reward after update: -5922.89, Optimal reward -5922.89
Iteration 19 took 2.26 seconds (mean sampled reward: -7028.37). Current reward after update: -5533.14, Optimal reward -5533.14
Iteration 20 took 2.19 seconds (mean sampled reward: -6938.93). Current reward after update: -5411.46, Optimal reward -5411.46
Iteration 21 took 2.26 seconds (mean sampled reward: -6719.64). Current reward after update: -5231.95, Optimal reward -5231.95
Iteration 22 took 2.18 seconds (mean sampled reward: -6248.43). Current reward after update: -4078.05, Optimal reward -4078.05
Iteration 23 took 2.09 seconds (mean sampled reward: -6218.00). Current reward after update: -3966.79, Optimal reward -3966.79
Iteration 24 took 2.17 seconds (mean sampled reward: -6205.30). Current reward after update: -3900.88, Optimal reward -3900.88
Iteration 25 took 2.17 seconds (mean sampled reward: -5833.44). Current reward after update: -3557.19, Optimal reward -3557.19
Iteration 26 took 2.21 seconds (mean sampled reward: -5749.36). Current reward after update: -2805.78, Optimal reward -2805.78
Iteration 27 took 2.21 seconds (mean sampled reward: -5045.26). Current reward after update: -2588.97, Optimal reward -2588.97
Iteration 28 took 2.30 seconds (mean sampled reward: -5036.21). Current reward after update: -2409.86, Optimal reward -2409.86
Iteration 29 took 2.30 seconds (mean sampled reward: -4712.40). Current reward after update: -2254.58, Optimal reward -2254.58
Iteration 30 took 2.17 seconds (mean sampled reward: -4683.81). Current reward after update: -2252.79, Optimal reward -2252.79
Iteration 31 took 2.21 seconds (mean sampled reward: -5301.90). Current reward after update: -2213.63, Optimal reward -2213.63
Iteration 32 took 2.17 seconds (mean sampled reward: -6155.19). Current reward after update: -2225.17, Optimal reward -2213.63
Iteration 33 took 2.27 seconds (mean sampled reward: -5094.08). Current reward after update: -2376.88, Optimal reward -2213.63
Iteration 34 took 2.20 seconds (mean sampled reward: -5638.90). Current reward after update: -3207.47, Optimal reward -2213.63
Iteration 35 took 2.21 seconds (mean sampled reward: -6208.51). Current reward after update: -2323.83, Optimal reward -2213.63
Iteration 36 took 2.40 seconds (mean sampled reward: -5745.95). Current reward after update: -3076.58, Optimal reward -2213.63
Iteration 37 took 2.19 seconds (mean sampled reward: -4464.70). Current reward after update: -2355.24, Optimal reward -2213.63
Iteration 38 took 2.18 seconds (mean sampled reward: -4802.08). Current reward after update: -2368.09, Optimal reward -2213.63
Iteration 39 took 2.26 seconds (mean sampled reward: -5084.19). Current reward after update: -2055.32, Optimal reward -2055.32
Iteration 40 took 2.21 seconds (mean sampled reward: -5896.31). Current reward after update: -2639.43, Optimal reward -2055.32
Iteration 41 took 2.25 seconds (mean sampled reward: -6376.97). Current reward after update: -2426.38, Optimal reward -2055.32
Iteration 42 took 2.19 seconds (mean sampled reward: -5396.12). Current reward after update: -2211.03, Optimal reward -2055.32
Iteration 43 took 2.15 seconds (mean sampled reward: -5285.91). Current reward after update: -2236.39, Optimal reward -2055.32
Iteration 44 took 2.15 seconds (mean sampled reward: -5044.16). Current reward after update: -1888.50, Optimal reward -1888.50
Iteration 45 took 2.18 seconds (mean sampled reward: -5111.48). Current reward after update: -1883.52, Optimal reward -1883.52
Iteration 46 took 2.26 seconds (mean sampled reward: -5482.66). Current reward after update: -1991.70, Optimal reward -1883.52
Iteration 47 took 2.24 seconds (mean sampled reward: -5465.77). Current reward after update: -2209.10, Optimal reward -1883.52
Iteration 48 took 2.27 seconds (mean sampled reward: -4826.25). Current reward after update: -1953.08, Optimal reward -1883.52
Iteration 49 took 2.23 seconds (mean sampled reward: -4835.46). Current reward after update: -1893.75, Optimal reward -1883.52
Iteration 50 took 2.25 seconds (mean sampled reward: -5087.02). Current reward after update: -1995.71, Optimal reward -1883.52
Iteration 51 took 2.21 seconds (mean sampled reward: -5231.79). Current reward after update: -2091.98, Optimal reward -1883.52
Iteration 52 took 2.27 seconds (mean sampled reward: -6155.86). Current reward after update: -1963.61, Optimal reward -1883.52
Iteration 53 took 2.33 seconds (mean sampled reward: -5940.35). Current reward after update: -1980.19, Optimal reward -1883.52
Iteration 54 took 2.32 seconds (mean sampled reward: -6155.46). Current reward after update: -1956.89, Optimal reward -1883.52
Iteration 55 took 2.26 seconds (mean sampled reward: -5578.43). Current reward after update: -1820.89, Optimal reward -1820.89
Iteration 56 took 2.28 seconds (mean sampled reward: -4957.36). Current reward after update: -1896.41, Optimal reward -1820.89
Iteration 57 took 2.28 seconds (mean sampled reward: -5293.52). Current reward after update: -1576.59, Optimal reward -1576.59
Iteration 58 took 2.14 seconds (mean sampled reward: -5755.92). Current reward after update: -1797.27, Optimal reward -1576.59
Iteration 59 took 2.17 seconds (mean sampled reward: -4776.58). Current reward after update: -1537.76, Optimal reward -1537.76
Iteration 60 took 2.34 seconds (mean sampled reward: -5811.05). Current reward after update: -1344.08, Optimal reward -1344.08
Iteration 61 took 2.25 seconds (mean sampled reward: -5269.72). Current reward after update: -1309.63, Optimal reward -1309.63
Iteration 62 took 2.51 seconds (mean sampled reward: -5136.36). Current reward after update: -1348.77, Optimal reward -1309.63
Iteration 63 took 2.29 seconds (mean sampled reward: -4567.26). Current reward after update: -1352.41, Optimal reward -1309.63
Iteration 64 took 2.28 seconds (mean sampled reward: -4736.82). Current reward after update: -1195.10, Optimal reward -1195.10
Iteration 65 took 2.41 seconds (mean sampled reward: -6356.37). Current reward after update: -1254.47, Optimal reward -1195.10
Iteration 66 took 2.39 seconds (mean sampled reward: -5102.09). Current reward after update: -1385.47, Optimal reward -1195.10
Iteration 67 took 2.39 seconds (mean sampled reward: -4192.72). Current reward after update: -1544.16, Optimal reward -1195.10
Iteration 68 took 2.51 seconds (mean sampled reward: -5377.37). Current reward after update: -1098.05, Optimal reward -1098.05
Iteration 69 took 2.38 seconds (mean sampled reward: -2630.66). Current reward after update: -1480.03, Optimal reward -1098.05
Iteration 70 took 2.17 seconds (mean sampled reward: -4665.59). Current reward after update: -1094.86, Optimal reward -1094.86
Iteration 71 took 2.20 seconds (mean sampled reward: -4767.62). Current reward after update: -1233.43, Optimal reward -1094.86
Iteration 72 took 2.23 seconds (mean sampled reward: -3927.92). Current reward after update: -1276.67, Optimal reward -1094.86
Iteration 73 took 2.19 seconds (mean sampled reward: -4358.70). Current reward after update: -1095.55, Optimal reward -1094.86
Iteration 74 took 2.22 seconds (mean sampled reward: -4300.03). Current reward after update: -1127.84, Optimal reward -1094.86
Iteration 75 took 2.18 seconds (mean sampled reward: -3815.28). Current reward after update: -1037.58, Optimal reward -1037.58
Iteration 76 took 2.25 seconds (mean sampled reward: -5049.26). Current reward after update: -1066.60, Optimal reward -1037.58
Iteration 77 took 2.24 seconds (mean sampled reward: -3935.40). Current reward after update: -988.98, Optimal reward -988.98
Iteration 78 took 2.17 seconds (mean sampled reward: -2590.74). Current reward after update: -951.34, Optimal reward -951.34
Iteration 79 took 2.38 seconds (mean sampled reward: -3786.34). Current reward after update: -1536.05, Optimal reward -951.34
Iteration 80 took 2.22 seconds (mean sampled reward: -3929.12). Current reward after update: -1087.32, Optimal reward -951.34
Iteration 81 took 2.38 seconds (mean sampled reward: -3854.16). Current reward after update: -2025.52, Optimal reward -951.34
Iteration 82 took 2.35 seconds (mean sampled reward: -5182.12). Current reward after update: -1025.47, Optimal reward -951.34
Iteration 83 took 2.33 seconds (mean sampled reward: -3694.80). Current reward after update: -1073.37, Optimal reward -951.34
Iteration 84 took 2.39 seconds (mean sampled reward: -4218.34). Current reward after update: -979.88, Optimal reward -951.34
Iteration 85 took 2.30 seconds (mean sampled reward: -3165.72). Current reward after update: -1195.36, Optimal reward -951.34
Iteration 86 took 2.20 seconds (mean sampled reward: -3413.15). Current reward after update: -1748.98, Optimal reward -951.34
Iteration 87 took 2.18 seconds (mean sampled reward: -2895.26). Current reward after update: -1059.83, Optimal reward -951.34
Iteration 88 took 2.22 seconds (mean sampled reward: -3564.73). Current reward after update: -2565.56, Optimal reward -951.34
Iteration 89 took 2.29 seconds (mean sampled reward: -2846.91). Current reward after update: -1126.34, Optimal reward -951.34
Iteration 90 took 2.18 seconds (mean sampled reward: -2400.57). Current reward after update: -1546.07, Optimal reward -951.34
Iteration 91 took 2.23 seconds (mean sampled reward: -2479.93). Current reward after update: -1037.17, Optimal reward -951.34
Iteration 92 took 2.31 seconds (mean sampled reward: -5580.27). Current reward after update: -1195.17, Optimal reward -951.34
Iteration 93 took 2.25 seconds (mean sampled reward: -5478.52). Current reward after update: -1067.64, Optimal reward -951.34
Iteration 94 took 2.26 seconds (mean sampled reward: -3980.88). Current reward after update: -1097.33, Optimal reward -951.34
Iteration 95 took 2.32 seconds (mean sampled reward: -2943.88). Current reward after update: -1076.97, Optimal reward -951.34
Iteration 96 took 2.22 seconds (mean sampled reward: -4119.79). Current reward after update: -1213.37, Optimal reward -951.34
Iteration 97 took 2.21 seconds (mean sampled reward: -5065.84). Current reward after update: -1125.61, Optimal reward -951.34
Iteration 98 took 2.19 seconds (mean sampled reward: -3779.33). Current reward after update: -1138.10, Optimal reward -951.34
Iteration 99 took 2.29 seconds (mean sampled reward: -5269.42). Current reward after update: -2127.68, Optimal reward -951.34
Iteration 100 took 2.21 seconds (mean sampled reward: -4180.29). Current reward after update: -1115.45, Optimal reward -951.34
Iteration 101 took 2.13 seconds (mean sampled reward: -3730.09). Current reward after update: -1521.70, Optimal reward -951.34
Iteration 102 took 2.23 seconds (mean sampled reward: -4078.50). Current reward after update: -1203.07, Optimal reward -951.34
Iteration 103 took 2.27 seconds (mean sampled reward: -4622.44). Current reward after update: -1105.53, Optimal reward -951.34
Iteration 104 took 2.24 seconds (mean sampled reward: -4647.62). Current reward after update: -1050.88, Optimal reward -951.34
Iteration 105 took 2.32 seconds (mean sampled reward: -5352.60). Current reward after update: -1079.98, Optimal reward -951.34
Iteration 106 took 2.36 seconds (mean sampled reward: -4565.62). Current reward after update: -1082.81, Optimal reward -951.34
Iteration 107 took 2.41 seconds (mean sampled reward: -5009.07). Current reward after update: -7058.69, Optimal reward -951.34
Iteration 108 took 2.31 seconds (mean sampled reward: -4199.34). Current reward after update: -1053.81, Optimal reward -951.34
Iteration 109 took 2.24 seconds (mean sampled reward: -4482.68). Current reward after update: -1104.54, Optimal reward -951.34
Iteration 110 took 2.22 seconds (mean sampled reward: -3331.12). Current reward after update: -1582.41, Optimal reward -951.34
Iteration 111 took 2.38 seconds (mean sampled reward: -4046.54). Current reward after update: -1006.24, Optimal reward -951.34
Iteration 112 took 2.32 seconds (mean sampled reward: -3871.89). Current reward after update: -1716.72, Optimal reward -951.34
Iteration 113 took 2.34 seconds (mean sampled reward: -4023.94). Current reward after update: -1180.08, Optimal reward -951.34
Iteration 114 took 2.35 seconds (mean sampled reward: -4732.90). Current reward after update: -1102.23, Optimal reward -951.34
Iteration 115 took 2.21 seconds (mean sampled reward: -4885.30). Current reward after update: -1393.02, Optimal reward -951.34
Iteration 116 took 2.25 seconds (mean sampled reward: -4116.70). Current reward after update: -1049.56, Optimal reward -951.34
Iteration 117 took 2.16 seconds (mean sampled reward: -3763.10). Current reward after update: -1090.94, Optimal reward -951.34
Iteration 118 took 2.16 seconds (mean sampled reward: -3172.28). Current reward after update: -1227.86, Optimal reward -951.34
Iteration 119 took 2.20 seconds (mean sampled reward: -3675.72). Current reward after update: -1087.65, Optimal reward -951.34
Iteration 120 took 2.20 seconds (mean sampled reward: -3656.90). Current reward after update: -1091.59, Optimal reward -951.34
Iteration 121 took 2.19 seconds (mean sampled reward: -4561.30). Current reward after update: -1023.08, Optimal reward -951.34
Iteration 122 took 2.18 seconds (mean sampled reward: -3170.17). Current reward after update: -1808.72, Optimal reward -951.34
Iteration 123 took 2.16 seconds (mean sampled reward: -3741.71). Current reward after update: -6989.81, Optimal reward -951.34
Iteration 124 took 2.17 seconds (mean sampled reward: -4802.11). Current reward after update: -1056.32, Optimal reward -951.34
Iteration 125 took 2.18 seconds (mean sampled reward: -4734.98). Current reward after update: -919.94, Optimal reward -919.94
Iteration 126 took 2.21 seconds (mean sampled reward: -5614.02). Current reward after update: -951.36, Optimal reward -919.94
Iteration 127 took 2.18 seconds (mean sampled reward: -5621.38). Current reward after update: -907.30, Optimal reward -907.30
Iteration 128 took 2.20 seconds (mean sampled reward: -4470.70). Current reward after update: -893.35, Optimal reward -893.35
Iteration 129 took 2.18 seconds (mean sampled reward: -4483.19). Current reward after update: -850.04, Optimal reward -850.04
Iteration 130 took 2.14 seconds (mean sampled reward: -3761.01). Current reward after update: -909.43, Optimal reward -850.04
Iteration 131 took 2.21 seconds (mean sampled reward: -3934.59). Current reward after update: -1160.23, Optimal reward -850.04
Iteration 132 took 2.20 seconds (mean sampled reward: -4521.52). Current reward after update: -1052.82, Optimal reward -850.04
Iteration 133 took 2.25 seconds (mean sampled reward: -4347.25). Current reward after update: -994.65, Optimal reward -850.04
Iteration 134 took 2.22 seconds (mean sampled reward: -4165.83). Current reward after update: -830.25, Optimal reward -830.25
Iteration 135 took 2.20 seconds (mean sampled reward: -4163.93). Current reward after update: -871.02, Optimal reward -830.25
Iteration 136 took 2.20 seconds (mean sampled reward: -3205.62). Current reward after update: -940.57, Optimal reward -830.25
Iteration 137 took 2.18 seconds (mean sampled reward: -4698.65). Current reward after update: -847.87, Optimal reward -830.25
Iteration 138 took 2.23 seconds (mean sampled reward: -3968.14). Current reward after update: -1004.10, Optimal reward -830.25
Iteration 139 took 2.27 seconds (mean sampled reward: -3323.57). Current reward after update: -781.10, Optimal reward -781.10
Iteration 140 took 2.23 seconds (mean sampled reward: -3922.36). Current reward after update: -829.98, Optimal reward -781.10
Iteration 141 took 2.22 seconds (mean sampled reward: -3977.82). Current reward after update: -783.25, Optimal reward -781.10
Iteration 142 took 2.17 seconds (mean sampled reward: -4004.16). Current reward after update: -748.54, Optimal reward -748.54
Iteration 143 took 2.21 seconds (mean sampled reward: -3314.47). Current reward after update: -925.89, Optimal reward -748.54
Iteration 144 took 2.21 seconds (mean sampled reward: -3020.72). Current reward after update: -7342.98, Optimal reward -748.54
Iteration 145 took 2.21 seconds (mean sampled reward: -2574.62). Current reward after update: -663.88, Optimal reward -663.88
Iteration 146 took 2.18 seconds (mean sampled reward: -2833.61). Current reward after update: -766.18, Optimal reward -663.88
Iteration 147 took 2.28 seconds (mean sampled reward: -1965.73). Current reward after update: -711.02, Optimal reward -663.88
Iteration 148 took 2.20 seconds (mean sampled reward: -3194.89). Current reward after update: -481.82, Optimal reward -481.82
Iteration 149 took 2.23 seconds (mean sampled reward: -2203.36). Current reward after update: -477.47, Optimal reward -477.47
Iteration 150 took 2.23 seconds (mean sampled reward: -2512.31). Current reward after update: -1051.37, Optimal reward -477.47
Iteration 151 took 2.17 seconds (mean sampled reward: -2135.83). Current reward after update: -587.17, Optimal reward -477.47
Iteration 152 took 2.17 seconds (mean sampled reward: -2226.67). Current reward after update: -541.06, Optimal reward -477.47
Iteration 153 took 2.19 seconds (mean sampled reward: -2318.33). Current reward after update: -513.30, Optimal reward -477.47
Iteration 154 took 2.20 seconds (mean sampled reward: -2471.22). Current reward after update: -511.56, Optimal reward -477.47
Iteration 155 took 2.30 seconds (mean sampled reward: -2407.63). Current reward after update: -515.44, Optimal reward -477.47
Iteration 156 took 2.25 seconds (mean sampled reward: -2976.17). Current reward after update: -512.71, Optimal reward -477.47
Iteration 157 took 2.19 seconds (mean sampled reward: -2248.86). Current reward after update: -478.31, Optimal reward -477.47
Iteration 158 took 2.19 seconds (mean sampled reward: -3196.13). Current reward after update: -1664.91, Optimal reward -477.47
Iteration 159 took 2.33 seconds (mean sampled reward: -2647.27). Current reward after update: -501.90, Optimal reward -477.47
Iteration 160 took 2.28 seconds (mean sampled reward: -2348.45). Current reward after update: -570.64, Optimal reward -477.47
Iteration 161 took 2.21 seconds (mean sampled reward: -1999.53). Current reward after update: -452.25, Optimal reward -452.25
Iteration 162 took 2.25 seconds (mean sampled reward: -2024.70). Current reward after update: -473.19, Optimal reward -452.25
Iteration 163 took 2.22 seconds (mean sampled reward: -2573.89). Current reward after update: -551.53, Optimal reward -452.25
Iteration 164 took 2.28 seconds (mean sampled reward: -3011.02). Current reward after update: -696.28, Optimal reward -452.25
Iteration 165 took 2.27 seconds (mean sampled reward: -3150.11). Current reward after update: -686.46, Optimal reward -452.25
Iteration 166 took 2.23 seconds (mean sampled reward: -3330.45). Current reward after update: -678.62, Optimal reward -452.25
Iteration 167 took 2.22 seconds (mean sampled reward: -3361.56). Current reward after update: -560.56, Optimal reward -452.25
Iteration 168 took 2.23 seconds (mean sampled reward: -2409.94). Current reward after update: -675.84, Optimal reward -452.25
Iteration 169 took 2.21 seconds (mean sampled reward: -2598.98). Current reward after update: -709.95, Optimal reward -452.25
Iteration 170 took 2.20 seconds (mean sampled reward: -3040.98). Current reward after update: -487.15, Optimal reward -452.25
Iteration 171 took 2.25 seconds (mean sampled reward: -3774.61). Current reward after update: -620.45, Optimal reward -452.25
Iteration 172 took 2.22 seconds (mean sampled reward: -4382.10). Current reward after update: -727.65, Optimal reward -452.25
Iteration 173 took 2.27 seconds (mean sampled reward: -5307.90). Current reward after update: -834.81, Optimal reward -452.25
Iteration 174 took 2.23 seconds (mean sampled reward: -3285.86). Current reward after update: -806.18, Optimal reward -452.25
Iteration 175 took 2.24 seconds (mean sampled reward: -3549.11). Current reward after update: -821.45, Optimal reward -452.25
Iteration 176 took 2.25 seconds (mean sampled reward: -4556.31). Current reward after update: -1251.93, Optimal reward -452.25
Iteration 177 took 2.33 seconds (mean sampled reward: -4398.52). Current reward after update: -772.79, Optimal reward -452.25
Iteration 178 took 2.21 seconds (mean sampled reward: -4211.60). Current reward after update: -755.07, Optimal reward -452.25
Iteration 179 took 2.25 seconds (mean sampled reward: -5122.20). Current reward after update: -1659.12, Optimal reward -452.25
Iteration 180 took 2.21 seconds (mean sampled reward: -3777.65). Current reward after update: -774.81, Optimal reward -452.25
Iteration 181 took 2.36 seconds (mean sampled reward: -4096.63). Current reward after update: -756.59, Optimal reward -452.25
Iteration 182 took 2.22 seconds (mean sampled reward: -4546.86). Current reward after update: -917.15, Optimal reward -452.25
Iteration 183 took 2.26 seconds (mean sampled reward: -4170.39). Current reward after update: -630.74, Optimal reward -452.25
Iteration 184 took 2.25 seconds (mean sampled reward: -3985.29). Current reward after update: -657.87, Optimal reward -452.25
Iteration 185 took 2.24 seconds (mean sampled reward: -4783.75). Current reward after update: -482.15, Optimal reward -452.25
Iteration 186 took 2.25 seconds (mean sampled reward: -5185.78). Current reward after update: -601.91, Optimal reward -452.25
Iteration 187 took 2.28 seconds (mean sampled reward: -4377.49). Current reward after update: -503.29, Optimal reward -452.25
Iteration 188 took 2.24 seconds (mean sampled reward: -3946.81). Current reward after update: -502.41, Optimal reward -452.25
Iteration 189 took 2.24 seconds (mean sampled reward: -4132.19). Current reward after update: -521.29, Optimal reward -452.25
Iteration 190 took 2.21 seconds (mean sampled reward: -3309.75). Current reward after update: -557.51, Optimal reward -452.25
Iteration 191 took 2.19 seconds (mean sampled reward: -2630.96). Current reward after update: -372.49, Optimal reward -372.49
Iteration 192 took 2.19 seconds (mean sampled reward: -2657.43). Current reward after update: -475.75, Optimal reward -372.49
Iteration 193 took 2.16 seconds (mean sampled reward: -2201.56). Current reward after update: -425.29, Optimal reward -372.49
Iteration 194 took 2.23 seconds (mean sampled reward: -3401.34). Current reward after update: -413.02, Optimal reward -372.49
Iteration 195 took 2.28 seconds (mean sampled reward: -3315.00). Current reward after update: -432.70, Optimal reward -372.49
Iteration 196 took 2.24 seconds (mean sampled reward: -3938.68). Current reward after update: -784.94, Optimal reward -372.49
Iteration 197 took 2.28 seconds (mean sampled reward: -4527.08). Current reward after update: -474.32, Optimal reward -372.49
Iteration 198 took 2.29 seconds (mean sampled reward: -4248.70). Current reward after update: -491.76, Optimal reward -372.49
Iteration 199 took 2.19 seconds (mean sampled reward: -2070.95). Current reward after update: -518.76, Optimal reward -372.49
Iteration 200 took 2.23 seconds (mean sampled reward: -1718.08). Current reward after update: -472.00, Optimal reward -372.49
Max force: 100 Sigma: 0.1 mean rewards: -1093.1921624138072, best rewards:-372.49002369336523

Iteration 1 took 2.41 seconds (mean sampled reward: -7603.29). Current reward after update: -7254.21, Optimal reward -7254.21
Iteration 2 took 2.28 seconds (mean sampled reward: -7594.05). Current reward after update: -7311.63, Optimal reward -7254.21
Iteration 3 took 2.36 seconds (mean sampled reward: -7619.12). Current reward after update: -6684.71, Optimal reward -6684.71
Iteration 4 took 2.38 seconds (mean sampled reward: -7414.68). Current reward after update: -6132.93, Optimal reward -6132.93
Iteration 5 took 2.41 seconds (mean sampled reward: -7444.42). Current reward after update: -6154.19, Optimal reward -6132.93
Iteration 6 took 2.52 seconds (mean sampled reward: -7270.56). Current reward after update: -6045.09, Optimal reward -6045.09
Iteration 7 took 2.52 seconds (mean sampled reward: -6973.94). Current reward after update: -5406.35, Optimal reward -5406.35
Iteration 8 took 2.49 seconds (mean sampled reward: -6722.64). Current reward after update: -3705.49, Optimal reward -3705.49
Iteration 9 took 2.44 seconds (mean sampled reward: -6600.78). Current reward after update: -2800.68, Optimal reward -2800.68
Iteration 10 took 2.42 seconds (mean sampled reward: -7236.22). Current reward after update: -2901.85, Optimal reward -2800.68
Iteration 11 took 2.34 seconds (mean sampled reward: -6857.09). Current reward after update: -2630.87, Optimal reward -2630.87
Iteration 12 took 2.40 seconds (mean sampled reward: -6222.19). Current reward after update: -1945.72, Optimal reward -1945.72
Iteration 13 took 2.23 seconds (mean sampled reward: -5427.10). Current reward after update: -1896.52, Optimal reward -1896.52
Iteration 14 took 2.26 seconds (mean sampled reward: -5509.86). Current reward after update: -2125.94, Optimal reward -1896.52
Iteration 15 took 2.27 seconds (mean sampled reward: -5081.99). Current reward after update: -1884.06, Optimal reward -1884.06
Iteration 16 took 2.55 seconds (mean sampled reward: -6507.54). Current reward after update: -2159.93, Optimal reward -1884.06
Iteration 17 took 2.31 seconds (mean sampled reward: -6789.62). Current reward after update: -2584.74, Optimal reward -1884.06
Iteration 18 took 2.27 seconds (mean sampled reward: -6885.85). Current reward after update: -2200.84, Optimal reward -1884.06
Iteration 19 took 2.40 seconds (mean sampled reward: -7044.42). Current reward after update: -7072.40, Optimal reward -1884.06
Iteration 20 took 2.41 seconds (mean sampled reward: -7052.53). Current reward after update: -2147.83, Optimal reward -1884.06
Iteration 21 took 2.34 seconds (mean sampled reward: -7114.75). Current reward after update: -2016.48, Optimal reward -1884.06
Iteration 22 took 2.35 seconds (mean sampled reward: -6932.46). Current reward after update: -2108.51, Optimal reward -1884.06
Iteration 23 took 2.20 seconds (mean sampled reward: -6923.75). Current reward after update: -2155.42, Optimal reward -1884.06
Iteration 24 took 2.21 seconds (mean sampled reward: -6750.44). Current reward after update: -2003.68, Optimal reward -1884.06
Iteration 25 took 2.20 seconds (mean sampled reward: -6827.84). Current reward after update: -1754.04, Optimal reward -1754.04
Iteration 26 took 2.16 seconds (mean sampled reward: -7055.30). Current reward after update: -2230.83, Optimal reward -1754.04
Iteration 27 took 2.33 seconds (mean sampled reward: -6885.10). Current reward after update: -2092.56, Optimal reward -1754.04
Iteration 28 took 2.22 seconds (mean sampled reward: -6717.00). Current reward after update: -2002.00, Optimal reward -1754.04
Iteration 29 took 2.29 seconds (mean sampled reward: -5813.96). Current reward after update: -2242.77, Optimal reward -1754.04
Iteration 30 took 2.16 seconds (mean sampled reward: -5503.10). Current reward after update: -1764.38, Optimal reward -1754.04
Iteration 31 took 2.22 seconds (mean sampled reward: -6317.84). Current reward after update: -1763.73, Optimal reward -1754.04
Iteration 32 took 2.26 seconds (mean sampled reward: -5331.75). Current reward after update: -2153.16, Optimal reward -1754.04
Iteration 33 took 2.28 seconds (mean sampled reward: -5000.24). Current reward after update: -1568.69, Optimal reward -1568.69
Iteration 34 took 2.23 seconds (mean sampled reward: -5848.18). Current reward after update: -2665.46, Optimal reward -1568.69
Iteration 35 took 2.43 seconds (mean sampled reward: -5914.09). Current reward after update: -1761.75, Optimal reward -1568.69
Iteration 36 took 2.23 seconds (mean sampled reward: -5819.23). Current reward after update: -1674.67, Optimal reward -1568.69
Iteration 37 took 2.21 seconds (mean sampled reward: -6239.24). Current reward after update: -1565.99, Optimal reward -1565.99
Iteration 38 took 2.24 seconds (mean sampled reward: -5815.31). Current reward after update: -1463.38, Optimal reward -1463.38
Iteration 39 took 2.20 seconds (mean sampled reward: -5557.42). Current reward after update: -1397.66, Optimal reward -1397.66
Iteration 40 took 2.20 seconds (mean sampled reward: -5236.06). Current reward after update: -1261.17, Optimal reward -1261.17
Iteration 41 took 2.22 seconds (mean sampled reward: -5329.44). Current reward after update: -1529.47, Optimal reward -1261.17
Iteration 42 took 2.18 seconds (mean sampled reward: -5955.38). Current reward after update: -1879.67, Optimal reward -1261.17
Iteration 43 took 2.24 seconds (mean sampled reward: -5311.82). Current reward after update: -1321.58, Optimal reward -1261.17
Iteration 44 took 2.13 seconds (mean sampled reward: -5696.28). Current reward after update: -1226.82, Optimal reward -1226.82
Iteration 45 took 2.12 seconds (mean sampled reward: -5032.15). Current reward after update: -1036.51, Optimal reward -1036.51
Iteration 46 took 2.16 seconds (mean sampled reward: -4007.97). Current reward after update: -1103.00, Optimal reward -1036.51
Iteration 47 took 2.09 seconds (mean sampled reward: -4705.41). Current reward after update: -1107.95, Optimal reward -1036.51
Iteration 48 took 2.10 seconds (mean sampled reward: -4007.22). Current reward after update: -2268.47, Optimal reward -1036.51
Iteration 49 took 2.16 seconds (mean sampled reward: -4242.27). Current reward after update: -1076.32, Optimal reward -1036.51
Iteration 50 took 2.14 seconds (mean sampled reward: -3784.41). Current reward after update: -1014.50, Optimal reward -1014.50
Iteration 51 took 2.11 seconds (mean sampled reward: -3981.44). Current reward after update: -1068.54, Optimal reward -1014.50
Iteration 52 took 2.15 seconds (mean sampled reward: -2782.51). Current reward after update: -1063.01, Optimal reward -1014.50
Iteration 53 took 2.11 seconds (mean sampled reward: -3582.74). Current reward after update: -1152.57, Optimal reward -1014.50
Iteration 54 took 2.21 seconds (mean sampled reward: -3638.49). Current reward after update: -860.85, Optimal reward -860.85
Iteration 55 took 2.15 seconds (mean sampled reward: -3222.32). Current reward after update: -1464.78, Optimal reward -860.85
Iteration 56 took 2.19 seconds (mean sampled reward: -2626.25). Current reward after update: -761.50, Optimal reward -761.50
Iteration 57 took 2.20 seconds (mean sampled reward: -2026.57). Current reward after update: -641.86, Optimal reward -641.86
Iteration 58 took 2.21 seconds (mean sampled reward: -2498.45). Current reward after update: -689.25, Optimal reward -641.86
Iteration 59 took 2.22 seconds (mean sampled reward: -3013.65). Current reward after update: -665.94, Optimal reward -641.86
Iteration 60 took 2.15 seconds (mean sampled reward: -2717.46). Current reward after update: -700.60, Optimal reward -641.86
Iteration 61 took 2.23 seconds (mean sampled reward: -2894.10). Current reward after update: -688.34, Optimal reward -641.86
Iteration 62 took 2.16 seconds (mean sampled reward: -2168.61). Current reward after update: -745.66, Optimal reward -641.86
Iteration 63 took 2.15 seconds (mean sampled reward: -2959.06). Current reward after update: -673.30, Optimal reward -641.86
Iteration 64 took 2.23 seconds (mean sampled reward: -3684.13). Current reward after update: -716.79, Optimal reward -641.86
Iteration 65 took 2.30 seconds (mean sampled reward: -4024.73). Current reward after update: -630.77, Optimal reward -630.77
Iteration 66 took 2.25 seconds (mean sampled reward: -4777.48). Current reward after update: -1543.22, Optimal reward -630.77
Iteration 67 took 2.18 seconds (mean sampled reward: -4277.08). Current reward after update: -598.53, Optimal reward -598.53
Iteration 68 took 2.23 seconds (mean sampled reward: -2124.48). Current reward after update: -651.17, Optimal reward -598.53
Iteration 69 took 2.26 seconds (mean sampled reward: -3117.40). Current reward after update: -665.20, Optimal reward -598.53
Iteration 70 took 2.16 seconds (mean sampled reward: -2610.61). Current reward after update: -651.60, Optimal reward -598.53
Iteration 71 took 2.15 seconds (mean sampled reward: -2228.52). Current reward after update: -655.62, Optimal reward -598.53
Iteration 72 took 2.16 seconds (mean sampled reward: -2908.36). Current reward after update: -607.96, Optimal reward -598.53
Iteration 73 took 2.12 seconds (mean sampled reward: -2745.49). Current reward after update: -705.52, Optimal reward -598.53
Iteration 74 took 2.12 seconds (mean sampled reward: -1806.80). Current reward after update: -641.13, Optimal reward -598.53
Iteration 75 took 2.13 seconds (mean sampled reward: -2445.73). Current reward after update: -1492.06, Optimal reward -598.53
Iteration 76 took 2.13 seconds (mean sampled reward: -2074.88). Current reward after update: -819.95, Optimal reward -598.53
Iteration 77 took 2.12 seconds (mean sampled reward: -2635.26). Current reward after update: -628.50, Optimal reward -598.53
Iteration 78 took 2.13 seconds (mean sampled reward: -3820.26). Current reward after update: -789.53, Optimal reward -598.53
Iteration 79 took 2.19 seconds (mean sampled reward: -3301.37). Current reward after update: -659.81, Optimal reward -598.53
Iteration 80 took 2.18 seconds (mean sampled reward: -3350.38). Current reward after update: -717.91, Optimal reward -598.53
Iteration 81 took 2.15 seconds (mean sampled reward: -2501.40). Current reward after update: -685.03, Optimal reward -598.53
Iteration 82 took 2.26 seconds (mean sampled reward: -3873.80). Current reward after update: -700.76, Optimal reward -598.53
Iteration 83 took 2.22 seconds (mean sampled reward: -3248.85). Current reward after update: -703.17, Optimal reward -598.53
Iteration 84 took 2.17 seconds (mean sampled reward: -2716.49). Current reward after update: -569.02, Optimal reward -569.02
Iteration 85 took 2.18 seconds (mean sampled reward: -4548.12). Current reward after update: -619.29, Optimal reward -569.02
Iteration 86 took 2.16 seconds (mean sampled reward: -4971.56). Current reward after update: -762.99, Optimal reward -569.02
Iteration 87 took 2.18 seconds (mean sampled reward: -4315.09). Current reward after update: -630.79, Optimal reward -569.02
Iteration 88 took 2.18 seconds (mean sampled reward: -4690.94). Current reward after update: -987.02, Optimal reward -569.02
Iteration 89 took 2.22 seconds (mean sampled reward: -4240.17). Current reward after update: -653.89, Optimal reward -569.02
Iteration 90 took 2.14 seconds (mean sampled reward: -3732.02). Current reward after update: -681.67, Optimal reward -569.02
Iteration 91 took 2.16 seconds (mean sampled reward: -3009.53). Current reward after update: -1020.39, Optimal reward -569.02
Iteration 92 took 2.17 seconds (mean sampled reward: -4045.03). Current reward after update: -728.56, Optimal reward -569.02
Iteration 93 took 2.18 seconds (mean sampled reward: -2773.71). Current reward after update: -709.43, Optimal reward -569.02
Iteration 94 took 2.14 seconds (mean sampled reward: -2745.15). Current reward after update: -710.44, Optimal reward -569.02
Iteration 95 took 2.22 seconds (mean sampled reward: -4592.06). Current reward after update: -807.76, Optimal reward -569.02
Iteration 96 took 2.23 seconds (mean sampled reward: -4498.46). Current reward after update: -930.91, Optimal reward -569.02
Iteration 97 took 2.20 seconds (mean sampled reward: -4793.38). Current reward after update: -829.58, Optimal reward -569.02
Iteration 98 took 2.21 seconds (mean sampled reward: -4239.74). Current reward after update: -1329.35, Optimal reward -569.02
Iteration 99 took 2.14 seconds (mean sampled reward: -4506.55). Current reward after update: -776.03, Optimal reward -569.02
Iteration 100 took 2.12 seconds (mean sampled reward: -3838.07). Current reward after update: -6564.37, Optimal reward -569.02
Iteration 101 took 2.12 seconds (mean sampled reward: -4193.23). Current reward after update: -750.23, Optimal reward -569.02
Iteration 102 took 2.13 seconds (mean sampled reward: -3016.31). Current reward after update: -1349.64, Optimal reward -569.02
Iteration 103 took 2.12 seconds (mean sampled reward: -2102.41). Current reward after update: -733.24, Optimal reward -569.02
Iteration 104 took 2.11 seconds (mean sampled reward: -2305.62). Current reward after update: -1539.89, Optimal reward -569.02
Iteration 105 took 2.17 seconds (mean sampled reward: -3385.58). Current reward after update: -717.65, Optimal reward -569.02
Iteration 106 took 2.22 seconds (mean sampled reward: -3330.28). Current reward after update: -763.69, Optimal reward -569.02
Iteration 107 took 2.38 seconds (mean sampled reward: -3700.73). Current reward after update: -800.70, Optimal reward -569.02
Iteration 108 took 2.25 seconds (mean sampled reward: -4987.13). Current reward after update: -898.10, Optimal reward -569.02
Iteration 109 took 2.40 seconds (mean sampled reward: -2926.26). Current reward after update: -1969.08, Optimal reward -569.02
Iteration 110 took 2.13 seconds (mean sampled reward: -3058.41). Current reward after update: -979.05, Optimal reward -569.02
Iteration 111 took 2.12 seconds (mean sampled reward: -2582.28). Current reward after update: -1092.17, Optimal reward -569.02
Iteration 112 took 2.17 seconds (mean sampled reward: -2329.37). Current reward after update: -827.27, Optimal reward -569.02
Iteration 113 took 2.46 seconds (mean sampled reward: -2524.75). Current reward after update: -720.11, Optimal reward -569.02
Iteration 114 took 2.24 seconds (mean sampled reward: -2819.10). Current reward after update: -1371.66, Optimal reward -569.02
Iteration 115 took 2.13 seconds (mean sampled reward: -2893.74). Current reward after update: -812.16, Optimal reward -569.02
Iteration 116 took 2.40 seconds (mean sampled reward: -2871.00). Current reward after update: -862.46, Optimal reward -569.02
Iteration 117 took 2.46 seconds (mean sampled reward: -3302.24). Current reward after update: -947.13, Optimal reward -569.02
Iteration 118 took 2.27 seconds (mean sampled reward: -2349.07). Current reward after update: -913.77, Optimal reward -569.02
Iteration 119 took 2.25 seconds (mean sampled reward: -3863.01). Current reward after update: -854.06, Optimal reward -569.02
Iteration 120 took 2.20 seconds (mean sampled reward: -2943.63). Current reward after update: -948.74, Optimal reward -569.02
Iteration 121 took 2.15 seconds (mean sampled reward: -3547.23). Current reward after update: -806.87, Optimal reward -569.02
Iteration 122 took 2.17 seconds (mean sampled reward: -4615.57). Current reward after update: -705.83, Optimal reward -569.02
Iteration 123 took 2.16 seconds (mean sampled reward: -3879.87). Current reward after update: -846.57, Optimal reward -569.02
Iteration 124 took 2.12 seconds (mean sampled reward: -3535.32). Current reward after update: -899.78, Optimal reward -569.02
Iteration 125 took 2.10 seconds (mean sampled reward: -3377.14). Current reward after update: -974.76, Optimal reward -569.02
Iteration 126 took 2.10 seconds (mean sampled reward: -4010.14). Current reward after update: -749.15, Optimal reward -569.02
Iteration 127 took 2.13 seconds (mean sampled reward: -3930.47). Current reward after update: -735.13, Optimal reward -569.02
Iteration 128 took 2.13 seconds (mean sampled reward: -4812.12). Current reward after update: -681.85, Optimal reward -569.02
Iteration 129 took 2.14 seconds (mean sampled reward: -4260.54). Current reward after update: -639.89, Optimal reward -569.02
Iteration 130 took 2.12 seconds (mean sampled reward: -3822.16). Current reward after update: -746.55, Optimal reward -569.02
Iteration 131 took 2.27 seconds (mean sampled reward: -3080.31). Current reward after update: -673.58, Optimal reward -569.02
Iteration 132 took 2.11 seconds (mean sampled reward: -2762.73). Current reward after update: -573.32, Optimal reward -569.02
Iteration 133 took 2.13 seconds (mean sampled reward: -3179.28). Current reward after update: -757.46, Optimal reward -569.02
Iteration 134 took 2.13 seconds (mean sampled reward: -3074.68). Current reward after update: -781.67, Optimal reward -569.02
Iteration 135 took 2.11 seconds (mean sampled reward: -2514.07). Current reward after update: -734.65, Optimal reward -569.02
Iteration 136 took 2.13 seconds (mean sampled reward: -2343.98). Current reward after update: -784.75, Optimal reward -569.02
Iteration 137 took 2.10 seconds (mean sampled reward: -2614.55). Current reward after update: -918.90, Optimal reward -569.02
Iteration 138 took 2.13 seconds (mean sampled reward: -3441.65). Current reward after update: -791.32, Optimal reward -569.02
Iteration 139 took 2.22 seconds (mean sampled reward: -3971.22). Current reward after update: -686.10, Optimal reward -569.02
Iteration 140 took 2.14 seconds (mean sampled reward: -4000.46). Current reward after update: -743.94, Optimal reward -569.02
Iteration 141 took 2.13 seconds (mean sampled reward: -2807.31). Current reward after update: -778.83, Optimal reward -569.02
Iteration 142 took 2.17 seconds (mean sampled reward: -3055.14). Current reward after update: -749.63, Optimal reward -569.02
Iteration 143 took 2.17 seconds (mean sampled reward: -3023.78). Current reward after update: -1121.26, Optimal reward -569.02
Iteration 144 took 2.17 seconds (mean sampled reward: -3305.88). Current reward after update: -996.26, Optimal reward -569.02
Iteration 145 took 2.18 seconds (mean sampled reward: -3991.59). Current reward after update: -1064.33, Optimal reward -569.02
Iteration 146 took 2.16 seconds (mean sampled reward: -4930.31). Current reward after update: -1029.14, Optimal reward -569.02
Iteration 147 took 2.15 seconds (mean sampled reward: -4101.57). Current reward after update: -1120.23, Optimal reward -569.02
Iteration 148 took 2.17 seconds (mean sampled reward: -3470.61). Current reward after update: -2975.44, Optimal reward -569.02
Iteration 149 took 2.15 seconds (mean sampled reward: -2935.60). Current reward after update: -753.78, Optimal reward -569.02
Iteration 150 took 2.12 seconds (mean sampled reward: -3372.49). Current reward after update: -1411.89, Optimal reward -569.02
Iteration 151 took 2.18 seconds (mean sampled reward: -3318.76). Current reward after update: -2042.29, Optimal reward -569.02
Iteration 152 took 2.15 seconds (mean sampled reward: -2903.20). Current reward after update: -921.15, Optimal reward -569.02
Iteration 153 took 2.12 seconds (mean sampled reward: -2669.03). Current reward after update: -802.05, Optimal reward -569.02
Iteration 154 took 2.12 seconds (mean sampled reward: -2399.57). Current reward after update: -723.14, Optimal reward -569.02
Iteration 155 took 2.15 seconds (mean sampled reward: -2451.51). Current reward after update: -875.42, Optimal reward -569.02
Iteration 156 took 2.19 seconds (mean sampled reward: -2799.98). Current reward after update: -935.45, Optimal reward -569.02
Iteration 157 took 2.17 seconds (mean sampled reward: -2518.92). Current reward after update: -1137.51, Optimal reward -569.02
Iteration 158 took 2.14 seconds (mean sampled reward: -2515.26). Current reward after update: -935.26, Optimal reward -569.02
Iteration 159 took 2.15 seconds (mean sampled reward: -2390.19). Current reward after update: -1237.00, Optimal reward -569.02
Iteration 160 took 2.10 seconds (mean sampled reward: -2384.08). Current reward after update: -2281.28, Optimal reward -569.02
Iteration 161 took 2.19 seconds (mean sampled reward: -2735.03). Current reward after update: -891.65, Optimal reward -569.02
Iteration 162 took 2.15 seconds (mean sampled reward: -2652.89). Current reward after update: -937.24, Optimal reward -569.02
Iteration 163 took 2.18 seconds (mean sampled reward: -2405.99). Current reward after update: -865.78, Optimal reward -569.02
Iteration 164 took 2.17 seconds (mean sampled reward: -2284.11). Current reward after update: -982.70, Optimal reward -569.02
Iteration 165 took 2.15 seconds (mean sampled reward: -2236.41). Current reward after update: -902.09, Optimal reward -569.02
Iteration 166 took 2.12 seconds (mean sampled reward: -2436.01). Current reward after update: -812.96, Optimal reward -569.02
Iteration 167 took 2.21 seconds (mean sampled reward: -2176.85). Current reward after update: -914.89, Optimal reward -569.02
Iteration 168 took 2.19 seconds (mean sampled reward: -2971.73). Current reward after update: -1022.14, Optimal reward -569.02
Iteration 169 took 2.13 seconds (mean sampled reward: -2305.18). Current reward after update: -905.04, Optimal reward -569.02
Iteration 170 took 2.15 seconds (mean sampled reward: -2657.11). Current reward after update: -1094.69, Optimal reward -569.02
Iteration 171 took 2.20 seconds (mean sampled reward: -2619.65). Current reward after update: -1084.99, Optimal reward -569.02
Iteration 172 took 2.17 seconds (mean sampled reward: -2812.88). Current reward after update: -1115.97, Optimal reward -569.02
Iteration 173 took 2.14 seconds (mean sampled reward: -3546.12). Current reward after update: -1042.15, Optimal reward -569.02
Iteration 174 took 2.20 seconds (mean sampled reward: -3926.37). Current reward after update: -1136.62, Optimal reward -569.02
Iteration 175 took 2.31 seconds (mean sampled reward: -4656.65). Current reward after update: -1919.54, Optimal reward -569.02
Iteration 176 took 2.18 seconds (mean sampled reward: -4533.18). Current reward after update: -1046.51, Optimal reward -569.02
Iteration 177 took 2.12 seconds (mean sampled reward: -4514.06). Current reward after update: -1050.17, Optimal reward -569.02
Iteration 178 took 2.14 seconds (mean sampled reward: -3192.59). Current reward after update: -1011.94, Optimal reward -569.02
Iteration 179 took 2.17 seconds (mean sampled reward: -2960.08). Current reward after update: -960.78, Optimal reward -569.02
Iteration 180 took 2.17 seconds (mean sampled reward: -4514.49). Current reward after update: -1042.12, Optimal reward -569.02
Iteration 181 took 2.21 seconds (mean sampled reward: -3288.76). Current reward after update: -970.55, Optimal reward -569.02
Iteration 182 took 2.18 seconds (mean sampled reward: -2987.64). Current reward after update: -917.61, Optimal reward -569.02
Iteration 183 took 2.18 seconds (mean sampled reward: -2555.54). Current reward after update: -2082.92, Optimal reward -569.02
Iteration 184 took 2.15 seconds (mean sampled reward: -2545.61). Current reward after update: -2271.81, Optimal reward -569.02
Iteration 185 took 2.15 seconds (mean sampled reward: -2344.47). Current reward after update: -874.74, Optimal reward -569.02
Iteration 186 took 2.12 seconds (mean sampled reward: -3076.05). Current reward after update: -846.50, Optimal reward -569.02
Iteration 187 took 2.15 seconds (mean sampled reward: -4195.74). Current reward after update: -866.26, Optimal reward -569.02
Iteration 188 took 2.13 seconds (mean sampled reward: -4715.77). Current reward after update: -2183.35, Optimal reward -569.02
Iteration 189 took 2.12 seconds (mean sampled reward: -3052.61). Current reward after update: -762.69, Optimal reward -569.02
Iteration 190 took 2.13 seconds (mean sampled reward: -3537.75). Current reward after update: -797.43, Optimal reward -569.02
Iteration 191 took 2.14 seconds (mean sampled reward: -2961.31). Current reward after update: -796.49, Optimal reward -569.02
Iteration 192 took 2.18 seconds (mean sampled reward: -4089.72). Current reward after update: -922.63, Optimal reward -569.02
Iteration 193 took 2.17 seconds (mean sampled reward: -2970.81). Current reward after update: -843.02, Optimal reward -569.02
Iteration 194 took 2.22 seconds (mean sampled reward: -4373.95). Current reward after update: -840.65, Optimal reward -569.02
Iteration 195 took 2.23 seconds (mean sampled reward: -4662.51). Current reward after update: -840.62, Optimal reward -569.02
Iteration 196 took 2.21 seconds (mean sampled reward: -4030.67). Current reward after update: -1100.81, Optimal reward -569.02
Iteration 197 took 2.21 seconds (mean sampled reward: -2400.14). Current reward after update: -1034.25, Optimal reward -569.02
Iteration 198 took 2.16 seconds (mean sampled reward: -2713.99). Current reward after update: -901.08, Optimal reward -569.02
Iteration 199 took 2.13 seconds (mean sampled reward: -3136.77). Current reward after update: -897.80, Optimal reward -569.02
Iteration 200 took 2.15 seconds (mean sampled reward: -3700.38). Current reward after update: -951.93, Optimal reward -569.02
Iteration 1 took 2.38 seconds (mean sampled reward: -7606.55). Current reward after update: -7302.50, Optimal reward -7302.50
Iteration 2 took 2.37 seconds (mean sampled reward: -7546.68). Current reward after update: -5740.51, Optimal reward -5740.51
Iteration 3 took 2.26 seconds (mean sampled reward: -7251.86). Current reward after update: -3516.69, Optimal reward -3516.69
Iteration 4 took 2.38 seconds (mean sampled reward: -6827.45). Current reward after update: -3550.76, Optimal reward -3516.69
Iteration 5 took 2.27 seconds (mean sampled reward: -5148.35). Current reward after update: -3491.66, Optimal reward -3491.66
Iteration 6 took 2.32 seconds (mean sampled reward: -5382.62). Current reward after update: -3322.83, Optimal reward -3322.83
Iteration 7 took 2.34 seconds (mean sampled reward: -5948.09). Current reward after update: -3027.50, Optimal reward -3027.50
Iteration 8 took 2.28 seconds (mean sampled reward: -6168.94). Current reward after update: -3399.21, Optimal reward -3027.50
Iteration 9 took 2.27 seconds (mean sampled reward: -5142.50). Current reward after update: -3120.35, Optimal reward -3027.50
Iteration 10 took 2.33 seconds (mean sampled reward: -5008.85). Current reward after update: -3012.26, Optimal reward -3012.26
Iteration 11 took 2.36 seconds (mean sampled reward: -5859.58). Current reward after update: -3165.41, Optimal reward -3012.26
Iteration 12 took 2.35 seconds (mean sampled reward: -6129.66). Current reward after update: -3216.78, Optimal reward -3012.26
Iteration 13 took 2.43 seconds (mean sampled reward: -5015.69). Current reward after update: -3217.01, Optimal reward -3012.26
Iteration 14 took 2.28 seconds (mean sampled reward: -5459.15). Current reward after update: -2923.21, Optimal reward -2923.21
Iteration 15 took 2.29 seconds (mean sampled reward: -5403.39). Current reward after update: -2961.22, Optimal reward -2923.21
Iteration 16 took 2.31 seconds (mean sampled reward: -5004.23). Current reward after update: -2872.40, Optimal reward -2872.40
Iteration 17 took 2.31 seconds (mean sampled reward: -5244.10). Current reward after update: -2649.36, Optimal reward -2649.36
Iteration 18 took 2.27 seconds (mean sampled reward: -6072.67). Current reward after update: -5054.52, Optimal reward -2649.36
Iteration 19 took 2.39 seconds (mean sampled reward: -5472.77). Current reward after update: -2604.70, Optimal reward -2604.70
Iteration 20 took 2.46 seconds (mean sampled reward: -4761.93). Current reward after update: -2685.09, Optimal reward -2604.70
Iteration 21 took 2.24 seconds (mean sampled reward: -5648.41). Current reward after update: -3084.33, Optimal reward -2604.70
Iteration 22 took 2.39 seconds (mean sampled reward: -5484.85). Current reward after update: -2623.51, Optimal reward -2604.70
Iteration 23 took 2.29 seconds (mean sampled reward: -5870.62). Current reward after update: -2848.53, Optimal reward -2604.70
Iteration 24 took 2.22 seconds (mean sampled reward: -5869.13). Current reward after update: -2484.07, Optimal reward -2484.07
Iteration 25 took 2.27 seconds (mean sampled reward: -5972.00). Current reward after update: -2527.02, Optimal reward -2484.07
Iteration 26 took 2.30 seconds (mean sampled reward: -5579.65). Current reward after update: -2623.22, Optimal reward -2484.07
Iteration 27 took 2.32 seconds (mean sampled reward: -5257.09). Current reward after update: -2639.78, Optimal reward -2484.07
Iteration 28 took 2.28 seconds (mean sampled reward: -4368.39). Current reward after update: -2613.29, Optimal reward -2484.07
Iteration 29 took 2.27 seconds (mean sampled reward: -3748.92). Current reward after update: -2569.43, Optimal reward -2484.07
Iteration 30 took 2.28 seconds (mean sampled reward: -3896.18). Current reward after update: -2561.49, Optimal reward -2484.07
Iteration 31 took 2.21 seconds (mean sampled reward: -3819.88). Current reward after update: -2499.95, Optimal reward -2484.07
Iteration 32 took 2.22 seconds (mean sampled reward: -3762.11). Current reward after update: -2120.75, Optimal reward -2120.75
Iteration 33 took 2.28 seconds (mean sampled reward: -4187.48). Current reward after update: -2399.75, Optimal reward -2120.75
Iteration 34 took 2.15 seconds (mean sampled reward: -4104.66). Current reward after update: -2209.78, Optimal reward -2120.75
Iteration 35 took 2.27 seconds (mean sampled reward: -4269.99). Current reward after update: -2243.63, Optimal reward -2120.75
Iteration 36 took 2.21 seconds (mean sampled reward: -4497.83). Current reward after update: -2813.93, Optimal reward -2120.75
Iteration 37 took 2.26 seconds (mean sampled reward: -4457.37). Current reward after update: -2103.08, Optimal reward -2103.08
Iteration 38 took 2.37 seconds (mean sampled reward: -4963.60). Current reward after update: -2197.96, Optimal reward -2103.08
Iteration 39 took 2.20 seconds (mean sampled reward: -5201.73). Current reward after update: -2277.00, Optimal reward -2103.08
Iteration 40 took 2.27 seconds (mean sampled reward: -5196.15). Current reward after update: -2402.55, Optimal reward -2103.08
Iteration 41 took 2.16 seconds (mean sampled reward: -5621.71). Current reward after update: -2262.74, Optimal reward -2103.08
Iteration 42 took 2.18 seconds (mean sampled reward: -6069.22). Current reward after update: -2464.18, Optimal reward -2103.08
Iteration 43 took 2.20 seconds (mean sampled reward: -5274.23). Current reward after update: -2519.05, Optimal reward -2103.08
Iteration 44 took 2.27 seconds (mean sampled reward: -5163.15). Current reward after update: -2223.26, Optimal reward -2103.08
Iteration 45 took 2.25 seconds (mean sampled reward: -5657.30). Current reward after update: -2548.52, Optimal reward -2103.08
Iteration 46 took 2.15 seconds (mean sampled reward: -4841.38). Current reward after update: -2796.71, Optimal reward -2103.08
Iteration 47 took 2.24 seconds (mean sampled reward: -4657.82). Current reward after update: -2090.91, Optimal reward -2090.91
Iteration 48 took 2.19 seconds (mean sampled reward: -4590.71). Current reward after update: -2051.64, Optimal reward -2051.64
Iteration 49 took 2.17 seconds (mean sampled reward: -5208.09). Current reward after update: -2030.97, Optimal reward -2030.97
Iteration 50 took 2.26 seconds (mean sampled reward: -5185.48). Current reward after update: -2236.81, Optimal reward -2030.97
Iteration 51 took 2.20 seconds (mean sampled reward: -5149.56). Current reward after update: -2448.70, Optimal reward -2030.97
Iteration 52 took 2.21 seconds (mean sampled reward: -5086.41). Current reward after update: -2211.94, Optimal reward -2030.97
Iteration 53 took 2.36 seconds (mean sampled reward: -5311.75). Current reward after update: -2068.23, Optimal reward -2030.97
Iteration 54 took 2.36 seconds (mean sampled reward: -5264.03). Current reward after update: -2040.99, Optimal reward -2030.97
Iteration 55 took 2.32 seconds (mean sampled reward: -4702.21). Current reward after update: -2019.01, Optimal reward -2019.01
Iteration 56 took 2.25 seconds (mean sampled reward: -4380.42). Current reward after update: -2063.20, Optimal reward -2019.01
Iteration 57 took 2.32 seconds (mean sampled reward: -4866.14). Current reward after update: -1958.33, Optimal reward -1958.33
Iteration 58 took 2.31 seconds (mean sampled reward: -4300.89). Current reward after update: -2676.94, Optimal reward -1958.33
Iteration 59 took 2.18 seconds (mean sampled reward: -4995.00). Current reward after update: -3574.70, Optimal reward -1958.33
Iteration 60 took 2.15 seconds (mean sampled reward: -4842.26). Current reward after update: -2166.18, Optimal reward -1958.33
Iteration 61 took 2.31 seconds (mean sampled reward: -5346.54). Current reward after update: -2976.34, Optimal reward -1958.33
Iteration 62 took 2.23 seconds (mean sampled reward: -5818.02). Current reward after update: -2114.76, Optimal reward -1958.33
Iteration 63 took 2.19 seconds (mean sampled reward: -6262.44). Current reward after update: -2081.50, Optimal reward -1958.33
Iteration 64 took 2.28 seconds (mean sampled reward: -5719.61). Current reward after update: -2215.36, Optimal reward -1958.33
Iteration 65 took 2.18 seconds (mean sampled reward: -6308.27). Current reward after update: -2187.86, Optimal reward -1958.33
Iteration 66 took 2.21 seconds (mean sampled reward: -6079.17). Current reward after update: -2293.48, Optimal reward -1958.33
Iteration 67 took 2.14 seconds (mean sampled reward: -5278.90). Current reward after update: -2096.91, Optimal reward -1958.33
Iteration 68 took 2.15 seconds (mean sampled reward: -5325.65). Current reward after update: -2017.93, Optimal reward -1958.33
Iteration 69 took 2.15 seconds (mean sampled reward: -5554.13). Current reward after update: -3753.89, Optimal reward -1958.33
Iteration 70 took 2.18 seconds (mean sampled reward: -4712.70). Current reward after update: -2118.84, Optimal reward -1958.33
Iteration 71 took 2.14 seconds (mean sampled reward: -4556.48). Current reward after update: -2134.16, Optimal reward -1958.33
Iteration 72 took 2.17 seconds (mean sampled reward: -4844.08). Current reward after update: -2067.41, Optimal reward -1958.33
Iteration 73 took 2.28 seconds (mean sampled reward: -4825.43). Current reward after update: -2127.49, Optimal reward -1958.33
Iteration 74 took 2.20 seconds (mean sampled reward: -4610.20). Current reward after update: -2824.89, Optimal reward -1958.33
Iteration 75 took 2.21 seconds (mean sampled reward: -4940.49). Current reward after update: -2366.87, Optimal reward -1958.33
Iteration 76 took 2.16 seconds (mean sampled reward: -4994.97). Current reward after update: -2092.86, Optimal reward -1958.33
Iteration 77 took 2.22 seconds (mean sampled reward: -4760.53). Current reward after update: -2059.12, Optimal reward -1958.33
Iteration 78 took 2.20 seconds (mean sampled reward: -4577.83). Current reward after update: -2126.89, Optimal reward -1958.33
Iteration 79 took 2.35 seconds (mean sampled reward: -5127.19). Current reward after update: -2190.27, Optimal reward -1958.33
Iteration 80 took 2.25 seconds (mean sampled reward: -4982.04). Current reward after update: -2275.33, Optimal reward -1958.33
Iteration 81 took 2.33 seconds (mean sampled reward: -4317.79). Current reward after update: -2075.49, Optimal reward -1958.33
Iteration 82 took 2.22 seconds (mean sampled reward: -4093.42). Current reward after update: -2407.39, Optimal reward -1958.33
Iteration 83 took 2.19 seconds (mean sampled reward: -3841.61). Current reward after update: -2452.83, Optimal reward -1958.33
Iteration 84 took 2.18 seconds (mean sampled reward: -4585.90). Current reward after update: -1991.47, Optimal reward -1958.33
Iteration 85 took 2.23 seconds (mean sampled reward: -4261.39). Current reward after update: -1931.93, Optimal reward -1931.93
Iteration 86 took 2.11 seconds (mean sampled reward: -3984.36). Current reward after update: -1973.93, Optimal reward -1931.93
Iteration 87 took 2.20 seconds (mean sampled reward: -4141.65). Current reward after update: -1936.48, Optimal reward -1931.93
Iteration 88 took 2.23 seconds (mean sampled reward: -4862.50). Current reward after update: -1965.28, Optimal reward -1931.93
Iteration 89 took 2.14 seconds (mean sampled reward: -4838.09). Current reward after update: -1936.80, Optimal reward -1931.93
Iteration 90 took 2.18 seconds (mean sampled reward: -5077.49). Current reward after update: -1956.28, Optimal reward -1931.93
Iteration 91 took 2.17 seconds (mean sampled reward: -4300.21). Current reward after update: -4311.44, Optimal reward -1931.93
Iteration 92 took 2.19 seconds (mean sampled reward: -4266.90). Current reward after update: -7225.82, Optimal reward -1931.93
Iteration 93 took 2.21 seconds (mean sampled reward: -5031.04). Current reward after update: -1929.38, Optimal reward -1929.38
Iteration 94 took 2.14 seconds (mean sampled reward: -5107.58). Current reward after update: -2806.71, Optimal reward -1929.38
Iteration 95 took 2.12 seconds (mean sampled reward: -4573.43). Current reward after update: -2071.76, Optimal reward -1929.38
Iteration 96 took 2.13 seconds (mean sampled reward: -4619.92). Current reward after update: -3716.54, Optimal reward -1929.38
Iteration 97 took 2.15 seconds (mean sampled reward: -4480.73). Current reward after update: -1953.37, Optimal reward -1929.38
Iteration 98 took 2.23 seconds (mean sampled reward: -4653.87). Current reward after update: -2020.10, Optimal reward -1929.38
Iteration 99 took 2.14 seconds (mean sampled reward: -4186.33). Current reward after update: -1908.71, Optimal reward -1908.71
Iteration 100 took 2.14 seconds (mean sampled reward: -4105.28). Current reward after update: -1873.85, Optimal reward -1873.85
Iteration 101 took 2.18 seconds (mean sampled reward: -4604.02). Current reward after update: -1888.65, Optimal reward -1873.85
Iteration 102 took 2.22 seconds (mean sampled reward: -4458.26). Current reward after update: -3208.46, Optimal reward -1873.85
Iteration 103 took 2.21 seconds (mean sampled reward: -4806.26). Current reward after update: -2216.39, Optimal reward -1873.85
Iteration 104 took 2.16 seconds (mean sampled reward: -4384.45). Current reward after update: -1884.45, Optimal reward -1873.85
Iteration 105 took 2.14 seconds (mean sampled reward: -4306.99). Current reward after update: -1892.50, Optimal reward -1873.85
Iteration 106 took 2.28 seconds (mean sampled reward: -4746.80). Current reward after update: -4038.76, Optimal reward -1873.85
Iteration 107 took 2.45 seconds (mean sampled reward: -4680.34). Current reward after update: -2065.60, Optimal reward -1873.85
Iteration 108 took 2.25 seconds (mean sampled reward: -4937.77). Current reward after update: -2018.33, Optimal reward -1873.85
Iteration 109 took 2.30 seconds (mean sampled reward: -4400.23). Current reward after update: -1932.29, Optimal reward -1873.85
Iteration 110 took 2.30 seconds (mean sampled reward: -4821.69). Current reward after update: -1997.14, Optimal reward -1873.85
Iteration 111 took 2.32 seconds (mean sampled reward: -3481.04). Current reward after update: -2009.38, Optimal reward -1873.85
Iteration 112 took 2.34 seconds (mean sampled reward: -3869.60). Current reward after update: -2034.07, Optimal reward -1873.85
Iteration 113 took 2.17 seconds (mean sampled reward: -4918.04). Current reward after update: -1961.04, Optimal reward -1873.85
Iteration 114 took 2.32 seconds (mean sampled reward: -4516.92). Current reward after update: -1934.14, Optimal reward -1873.85
Iteration 115 took 2.18 seconds (mean sampled reward: -5027.20). Current reward after update: -2027.77, Optimal reward -1873.85
Iteration 116 took 2.39 seconds (mean sampled reward: -4641.65). Current reward after update: -1936.77, Optimal reward -1873.85
Iteration 117 took 2.19 seconds (mean sampled reward: -4254.80). Current reward after update: -1906.00, Optimal reward -1873.85
Iteration 118 took 2.15 seconds (mean sampled reward: -4398.74). Current reward after update: -1853.43, Optimal reward -1853.43
Iteration 119 took 2.28 seconds (mean sampled reward: -4760.37). Current reward after update: -1820.64, Optimal reward -1820.64
Iteration 120 took 2.16 seconds (mean sampled reward: -4653.86). Current reward after update: -1892.69, Optimal reward -1820.64
Iteration 121 took 2.24 seconds (mean sampled reward: -4685.04). Current reward after update: -1949.82, Optimal reward -1820.64
Iteration 122 took 2.26 seconds (mean sampled reward: -4945.06). Current reward after update: -5311.95, Optimal reward -1820.64
Iteration 123 took 2.18 seconds (mean sampled reward: -6013.28). Current reward after update: -6191.07, Optimal reward -1820.64
Iteration 124 took 2.21 seconds (mean sampled reward: -5783.34). Current reward after update: -2136.39, Optimal reward -1820.64
Iteration 125 took 2.21 seconds (mean sampled reward: -6036.86). Current reward after update: -1976.31, Optimal reward -1820.64
Iteration 126 took 2.10 seconds (mean sampled reward: -4792.03). Current reward after update: -1953.73, Optimal reward -1820.64
Iteration 127 took 2.24 seconds (mean sampled reward: -4756.74). Current reward after update: -2040.61, Optimal reward -1820.64
Iteration 128 took 2.14 seconds (mean sampled reward: -4812.25). Current reward after update: -2145.44, Optimal reward -1820.64
Iteration 129 took 2.13 seconds (mean sampled reward: -4028.27). Current reward after update: -1990.61, Optimal reward -1820.64
Iteration 130 took 2.18 seconds (mean sampled reward: -4009.74). Current reward after update: -1983.47, Optimal reward -1820.64
Iteration 131 took 2.26 seconds (mean sampled reward: -4545.14). Current reward after update: -1859.48, Optimal reward -1820.64
Iteration 132 took 2.30 seconds (mean sampled reward: -5282.84). Current reward after update: -2110.71, Optimal reward -1820.64
Iteration 133 took 2.27 seconds (mean sampled reward: -4830.38). Current reward after update: -1947.98, Optimal reward -1820.64
Iteration 134 took 2.35 seconds (mean sampled reward: -4978.88). Current reward after update: -1775.59, Optimal reward -1775.59
Iteration 135 took 2.33 seconds (mean sampled reward: -6128.34). Current reward after update: -1969.55, Optimal reward -1775.59
Iteration 136 took 2.30 seconds (mean sampled reward: -6559.28). Current reward after update: -1777.38, Optimal reward -1775.59
Iteration 137 took 2.31 seconds (mean sampled reward: -5553.38). Current reward after update: -6022.32, Optimal reward -1775.59
Iteration 138 took 2.36 seconds (mean sampled reward: -5356.76). Current reward after update: -1651.75, Optimal reward -1651.75
Iteration 139 took 2.41 seconds (mean sampled reward: -4336.89). Current reward after update: -1729.79, Optimal reward -1651.75
Iteration 140 took 2.31 seconds (mean sampled reward: -4391.10). Current reward after update: -1824.46, Optimal reward -1651.75
Iteration 141 took 2.32 seconds (mean sampled reward: -4332.72). Current reward after update: -1633.81, Optimal reward -1633.81
Iteration 142 took 2.31 seconds (mean sampled reward: -4618.19). Current reward after update: -2103.91, Optimal reward -1633.81
Iteration 143 took 2.31 seconds (mean sampled reward: -5393.35). Current reward after update: -1739.45, Optimal reward -1633.81
Iteration 144 took 2.32 seconds (mean sampled reward: -4926.41). Current reward after update: -1689.53, Optimal reward -1633.81
Iteration 145 took 2.33 seconds (mean sampled reward: -5791.77). Current reward after update: -1756.63, Optimal reward -1633.81
Iteration 146 took 2.30 seconds (mean sampled reward: -5879.96). Current reward after update: -2307.65, Optimal reward -1633.81
Iteration 147 took 2.26 seconds (mean sampled reward: -5209.32). Current reward after update: -1841.42, Optimal reward -1633.81
Iteration 148 took 2.27 seconds (mean sampled reward: -4863.82). Current reward after update: -5341.27, Optimal reward -1633.81
Iteration 149 took 2.40 seconds (mean sampled reward: -4627.02). Current reward after update: -1643.41, Optimal reward -1633.81
Iteration 150 took 2.32 seconds (mean sampled reward: -4094.18). Current reward after update: -1686.67, Optimal reward -1633.81
Iteration 151 took 2.27 seconds (mean sampled reward: -4351.01). Current reward after update: -5183.12, Optimal reward -1633.81
Iteration 152 took 2.28 seconds (mean sampled reward: -4312.23). Current reward after update: -1678.72, Optimal reward -1633.81
Iteration 153 took 2.32 seconds (mean sampled reward: -3942.24). Current reward after update: -1526.19, Optimal reward -1526.19
Iteration 154 took 2.32 seconds (mean sampled reward: -4796.95). Current reward after update: -1586.68, Optimal reward -1526.19
Iteration 155 took 2.38 seconds (mean sampled reward: -4731.59). Current reward after update: -2139.64, Optimal reward -1526.19
Iteration 156 took 2.32 seconds (mean sampled reward: -4602.02). Current reward after update: -4838.44, Optimal reward -1526.19
Iteration 157 took 2.33 seconds (mean sampled reward: -4624.56). Current reward after update: -3248.58, Optimal reward -1526.19
Iteration 158 took 2.30 seconds (mean sampled reward: -4368.10). Current reward after update: -1634.05, Optimal reward -1526.19
Iteration 159 took 2.27 seconds (mean sampled reward: -3603.81). Current reward after update: -1551.25, Optimal reward -1526.19
Iteration 160 took 2.16 seconds (mean sampled reward: -3427.69). Current reward after update: -1664.98, Optimal reward -1526.19
Iteration 161 took 2.21 seconds (mean sampled reward: -4029.50). Current reward after update: -1662.57, Optimal reward -1526.19
Iteration 162 took 2.18 seconds (mean sampled reward: -3837.54). Current reward after update: -1475.76, Optimal reward -1475.76
Iteration 163 took 2.15 seconds (mean sampled reward: -3129.40). Current reward after update: -1583.33, Optimal reward -1475.76
Iteration 164 took 2.27 seconds (mean sampled reward: -3187.03). Current reward after update: -1157.68, Optimal reward -1157.68
Iteration 165 took 2.21 seconds (mean sampled reward: -3184.10). Current reward after update: -1091.56, Optimal reward -1091.56
Iteration 166 took 2.22 seconds (mean sampled reward: -2861.44). Current reward after update: -1040.50, Optimal reward -1040.50
Iteration 167 took 2.18 seconds (mean sampled reward: -3073.55). Current reward after update: -1101.43, Optimal reward -1040.50
Iteration 168 took 2.16 seconds (mean sampled reward: -3044.98). Current reward after update: -1127.91, Optimal reward -1040.50
Iteration 169 took 2.15 seconds (mean sampled reward: -3409.84). Current reward after update: -2179.95, Optimal reward -1040.50
Iteration 170 took 2.20 seconds (mean sampled reward: -3372.67). Current reward after update: -1109.76, Optimal reward -1040.50
Iteration 171 took 2.22 seconds (mean sampled reward: -3637.54). Current reward after update: -1091.25, Optimal reward -1040.50
Iteration 172 took 2.19 seconds (mean sampled reward: -3637.08). Current reward after update: -1132.83, Optimal reward -1040.50
Iteration 173 took 2.21 seconds (mean sampled reward: -3407.37). Current reward after update: -1036.69, Optimal reward -1036.69
Iteration 174 took 2.22 seconds (mean sampled reward: -3223.67). Current reward after update: -1175.40, Optimal reward -1036.69
Iteration 175 took 2.25 seconds (mean sampled reward: -3330.49). Current reward after update: -1200.90, Optimal reward -1036.69
Iteration 176 took 2.31 seconds (mean sampled reward: -3768.82). Current reward after update: -1130.29, Optimal reward -1036.69
Iteration 177 took 2.29 seconds (mean sampled reward: -3967.78). Current reward after update: -1017.49, Optimal reward -1017.49
Iteration 178 took 2.16 seconds (mean sampled reward: -3658.37). Current reward after update: -1003.58, Optimal reward -1003.58
Iteration 179 took 2.22 seconds (mean sampled reward: -4163.90). Current reward after update: -1099.39, Optimal reward -1003.58
Iteration 180 took 2.21 seconds (mean sampled reward: -4124.39). Current reward after update: -1802.23, Optimal reward -1003.58
Iteration 181 took 2.25 seconds (mean sampled reward: -3430.22). Current reward after update: -1753.91, Optimal reward -1003.58
Iteration 182 took 2.24 seconds (mean sampled reward: -4179.36). Current reward after update: -1186.09, Optimal reward -1003.58
Iteration 183 took 2.22 seconds (mean sampled reward: -4921.73). Current reward after update: -1135.52, Optimal reward -1003.58
Iteration 184 took 2.28 seconds (mean sampled reward: -6028.51). Current reward after update: -1093.47, Optimal reward -1003.58
Iteration 185 took 2.26 seconds (mean sampled reward: -5671.03). Current reward after update: -1208.54, Optimal reward -1003.58
Iteration 186 took 2.32 seconds (mean sampled reward: -5220.44). Current reward after update: -1074.82, Optimal reward -1003.58
Iteration 187 took 2.30 seconds (mean sampled reward: -4946.69). Current reward after update: -1187.26, Optimal reward -1003.58
Iteration 188 took 2.33 seconds (mean sampled reward: -5003.99). Current reward after update: -1112.30, Optimal reward -1003.58
Iteration 189 took 2.28 seconds (mean sampled reward: -4613.68). Current reward after update: -1191.73, Optimal reward -1003.58
Iteration 190 took 2.24 seconds (mean sampled reward: -4205.25). Current reward after update: -1295.30, Optimal reward -1003.58
Iteration 191 took 2.33 seconds (mean sampled reward: -4064.27). Current reward after update: -1217.76, Optimal reward -1003.58
Iteration 192 took 2.33 seconds (mean sampled reward: -5030.27). Current reward after update: -2862.05, Optimal reward -1003.58
Iteration 193 took 2.12 seconds (mean sampled reward: -4440.35). Current reward after update: -1336.22, Optimal reward -1003.58
Iteration 194 took 2.14 seconds (mean sampled reward: -3775.36). Current reward after update: -1124.61, Optimal reward -1003.58
Iteration 195 took 2.14 seconds (mean sampled reward: -3692.42). Current reward after update: -1102.66, Optimal reward -1003.58
Iteration 196 took 2.13 seconds (mean sampled reward: -3459.57). Current reward after update: -1890.09, Optimal reward -1003.58
Iteration 197 took 2.09 seconds (mean sampled reward: -3934.66). Current reward after update: -1249.45, Optimal reward -1003.58
Iteration 198 took 2.12 seconds (mean sampled reward: -4025.90). Current reward after update: -1139.35, Optimal reward -1003.58
Iteration 199 took 2.18 seconds (mean sampled reward: -3656.29). Current reward after update: -1085.79, Optimal reward -1003.58
Iteration 200 took 2.19 seconds (mean sampled reward: -3905.57). Current reward after update: -1158.56, Optimal reward -1003.58
Iteration 1 took 2.37 seconds (mean sampled reward: -7604.40). Current reward after update: -7227.71, Optimal reward -7227.71
Iteration 2 took 2.44 seconds (mean sampled reward: -7575.54). Current reward after update: -7025.91, Optimal reward -7025.91
Iteration 3 took 2.32 seconds (mean sampled reward: -7481.10). Current reward after update: -6333.10, Optimal reward -6333.10
Iteration 4 took 2.39 seconds (mean sampled reward: -7154.52). Current reward after update: -5030.87, Optimal reward -5030.87
Iteration 5 took 2.30 seconds (mean sampled reward: -6690.70). Current reward after update: -3221.43, Optimal reward -3221.43
Iteration 6 took 2.29 seconds (mean sampled reward: -6862.67). Current reward after update: -3331.61, Optimal reward -3221.43
Iteration 7 took 2.28 seconds (mean sampled reward: -5891.98). Current reward after update: -2861.41, Optimal reward -2861.41
Iteration 8 took 2.31 seconds (mean sampled reward: -5512.40). Current reward after update: -2905.68, Optimal reward -2861.41
Iteration 9 took 2.35 seconds (mean sampled reward: -5302.89). Current reward after update: -2841.65, Optimal reward -2841.65
Iteration 10 took 2.51 seconds (mean sampled reward: -5279.92). Current reward after update: -2721.51, Optimal reward -2721.51
Iteration 11 took 2.33 seconds (mean sampled reward: -4950.48). Current reward after update: -2519.63, Optimal reward -2519.63
Iteration 12 took 2.33 seconds (mean sampled reward: -5609.93). Current reward after update: -2405.21, Optimal reward -2405.21
Iteration 13 took 2.22 seconds (mean sampled reward: -4530.12). Current reward after update: -2348.61, Optimal reward -2348.61
Iteration 14 took 2.15 seconds (mean sampled reward: -4344.96). Current reward after update: -2444.67, Optimal reward -2348.61
Iteration 15 took 2.40 seconds (mean sampled reward: -4254.33). Current reward after update: -2254.29, Optimal reward -2254.29
Iteration 16 took 2.31 seconds (mean sampled reward: -4044.04). Current reward after update: -2221.95, Optimal reward -2221.95
Iteration 17 took 2.21 seconds (mean sampled reward: -4271.86). Current reward after update: -1908.97, Optimal reward -1908.97
Iteration 18 took 2.49 seconds (mean sampled reward: -4128.07). Current reward after update: -1643.66, Optimal reward -1643.66
Iteration 19 took 2.26 seconds (mean sampled reward: -3713.52). Current reward after update: -1476.74, Optimal reward -1476.74
Iteration 20 took 2.33 seconds (mean sampled reward: -4565.11). Current reward after update: -2009.09, Optimal reward -1476.74
Iteration 21 took 2.21 seconds (mean sampled reward: -4315.21). Current reward after update: -1704.94, Optimal reward -1476.74
Iteration 22 took 2.31 seconds (mean sampled reward: -4387.90). Current reward after update: -1534.74, Optimal reward -1476.74
Iteration 23 took 2.28 seconds (mean sampled reward: -4412.04). Current reward after update: -1751.65, Optimal reward -1476.74
Iteration 24 took 2.37 seconds (mean sampled reward: -4269.53). Current reward after update: -1707.47, Optimal reward -1476.74
Iteration 25 took 2.33 seconds (mean sampled reward: -4057.90). Current reward after update: -1581.22, Optimal reward -1476.74
Iteration 26 took 2.40 seconds (mean sampled reward: -4309.80). Current reward after update: -1507.51, Optimal reward -1476.74
Iteration 27 took 2.33 seconds (mean sampled reward: -4593.88). Current reward after update: -1372.73, Optimal reward -1372.73
Iteration 28 took 2.32 seconds (mean sampled reward: -3954.08). Current reward after update: -1341.49, Optimal reward -1341.49
Iteration 29 took 2.46 seconds (mean sampled reward: -3524.27). Current reward after update: -889.66, Optimal reward -889.66
Iteration 30 took 2.39 seconds (mean sampled reward: -3907.21). Current reward after update: -5527.10, Optimal reward -889.66
Iteration 31 took 2.39 seconds (mean sampled reward: -5157.01). Current reward after update: -1270.59, Optimal reward -889.66
Iteration 32 took 2.34 seconds (mean sampled reward: -3927.31). Current reward after update: -1082.03, Optimal reward -889.66
Iteration 33 took 2.45 seconds (mean sampled reward: -3302.45). Current reward after update: -889.34, Optimal reward -889.34
Iteration 34 took 2.28 seconds (mean sampled reward: -4074.36). Current reward after update: -799.28, Optimal reward -799.28
Iteration 35 took 2.41 seconds (mean sampled reward: -4555.71). Current reward after update: -4352.78, Optimal reward -799.28
Iteration 36 took 2.31 seconds (mean sampled reward: -3737.14). Current reward after update: -784.27, Optimal reward -784.27
Iteration 37 took 2.39 seconds (mean sampled reward: -4301.62). Current reward after update: -1091.10, Optimal reward -784.27
Iteration 38 took 2.31 seconds (mean sampled reward: -4298.67). Current reward after update: -1060.91, Optimal reward -784.27
Iteration 39 took 2.39 seconds (mean sampled reward: -4413.90). Current reward after update: -787.65, Optimal reward -784.27
Iteration 40 took 2.49 seconds (mean sampled reward: -4335.38). Current reward after update: -2301.99, Optimal reward -784.27
Iteration 41 took 2.40 seconds (mean sampled reward: -3579.14). Current reward after update: -727.34, Optimal reward -727.34
Iteration 42 took 2.44 seconds (mean sampled reward: -3547.05). Current reward after update: -4309.09, Optimal reward -727.34
Iteration 43 took 2.31 seconds (mean sampled reward: -3378.72). Current reward after update: -1655.64, Optimal reward -727.34
Iteration 44 took 2.36 seconds (mean sampled reward: -3642.38). Current reward after update: -1050.98, Optimal reward -727.34
Iteration 45 took 2.23 seconds (mean sampled reward: -3590.65). Current reward after update: -1012.33, Optimal reward -727.34
Iteration 46 took 2.22 seconds (mean sampled reward: -4036.57). Current reward after update: -625.60, Optimal reward -625.60
Iteration 47 took 2.26 seconds (mean sampled reward: -4521.95). Current reward after update: -802.02, Optimal reward -625.60
Iteration 48 took 2.19 seconds (mean sampled reward: -4675.68). Current reward after update: -850.13, Optimal reward -625.60
Iteration 49 took 2.29 seconds (mean sampled reward: -4546.57). Current reward after update: -1042.73, Optimal reward -625.60
Iteration 50 took 2.43 seconds (mean sampled reward: -4758.49). Current reward after update: -761.17, Optimal reward -625.60
Iteration 51 took 2.34 seconds (mean sampled reward: -5140.41). Current reward after update: -1102.73, Optimal reward -625.60
Iteration 52 took 2.33 seconds (mean sampled reward: -4634.32). Current reward after update: -1065.84, Optimal reward -625.60
Iteration 53 took 2.41 seconds (mean sampled reward: -4550.84). Current reward after update: -1066.04, Optimal reward -625.60
Iteration 54 took 2.54 seconds (mean sampled reward: -4431.82). Current reward after update: -1031.87, Optimal reward -625.60
Iteration 55 took 2.50 seconds (mean sampled reward: -4256.17). Current reward after update: -895.41, Optimal reward -625.60
Iteration 56 took 2.55 seconds (mean sampled reward: -4725.14). Current reward after update: -870.56, Optimal reward -625.60
Iteration 57 took 2.37 seconds (mean sampled reward: -4008.93). Current reward after update: -895.10, Optimal reward -625.60
Iteration 58 took 2.54 seconds (mean sampled reward: -3841.90). Current reward after update: -741.32, Optimal reward -625.60
Iteration 59 took 2.31 seconds (mean sampled reward: -4031.50). Current reward after update: -1025.51, Optimal reward -625.60
Iteration 60 took 2.49 seconds (mean sampled reward: -3973.12). Current reward after update: -866.24, Optimal reward -625.60
Iteration 61 took 2.30 seconds (mean sampled reward: -4572.66). Current reward after update: -1025.08, Optimal reward -625.60
Iteration 62 took 2.23 seconds (mean sampled reward: -5191.60). Current reward after update: -1121.74, Optimal reward -625.60
Iteration 63 took 2.65 seconds (mean sampled reward: -4195.32). Current reward after update: -913.35, Optimal reward -625.60
Iteration 64 took 2.46 seconds (mean sampled reward: -4201.56). Current reward after update: -1021.10, Optimal reward -625.60
Iteration 65 took 2.42 seconds (mean sampled reward: -3777.34). Current reward after update: -960.20, Optimal reward -625.60
Iteration 66 took 2.22 seconds (mean sampled reward: -4609.71). Current reward after update: -1095.60, Optimal reward -625.60
Iteration 67 took 2.26 seconds (mean sampled reward: -3786.30). Current reward after update: -959.77, Optimal reward -625.60
Iteration 68 took 2.42 seconds (mean sampled reward: -3727.25). Current reward after update: -1055.17, Optimal reward -625.60
Iteration 69 took 2.33 seconds (mean sampled reward: -3680.45). Current reward after update: -869.32, Optimal reward -625.60
Iteration 70 took 2.33 seconds (mean sampled reward: -3633.63). Current reward after update: -1013.50, Optimal reward -625.60
Iteration 71 took 2.32 seconds (mean sampled reward: -4307.24). Current reward after update: -992.08, Optimal reward -625.60
Iteration 72 took 2.26 seconds (mean sampled reward: -4646.88). Current reward after update: -953.16, Optimal reward -625.60
Iteration 73 took 2.33 seconds (mean sampled reward: -4209.76). Current reward after update: -1014.39, Optimal reward -625.60
Iteration 74 took 2.29 seconds (mean sampled reward: -4239.41). Current reward after update: -967.70, Optimal reward -625.60
Iteration 75 took 2.26 seconds (mean sampled reward: -4507.88). Current reward after update: -847.76, Optimal reward -625.60
Iteration 76 took 2.26 seconds (mean sampled reward: -4603.55). Current reward after update: -831.22, Optimal reward -625.60
Iteration 77 took 2.28 seconds (mean sampled reward: -4852.31). Current reward after update: -815.60, Optimal reward -625.60
Iteration 78 took 2.29 seconds (mean sampled reward: -4341.77). Current reward after update: -825.47, Optimal reward -625.60
Iteration 79 took 2.32 seconds (mean sampled reward: -4272.26). Current reward after update: -756.17, Optimal reward -625.60
Iteration 80 took 2.33 seconds (mean sampled reward: -4875.78). Current reward after update: -715.43, Optimal reward -625.60
Iteration 81 took 2.36 seconds (mean sampled reward: -4714.34). Current reward after update: -1829.35, Optimal reward -625.60
Iteration 82 took 2.34 seconds (mean sampled reward: -4616.10). Current reward after update: -5758.43, Optimal reward -625.60
Iteration 83 took 2.27 seconds (mean sampled reward: -5373.45). Current reward after update: -788.88, Optimal reward -625.60
Iteration 84 took 2.28 seconds (mean sampled reward: -4543.43). Current reward after update: -1279.60, Optimal reward -625.60
Iteration 85 took 2.23 seconds (mean sampled reward: -4931.46). Current reward after update: -986.72, Optimal reward -625.60
Iteration 86 took 2.33 seconds (mean sampled reward: -5160.64). Current reward after update: -825.09, Optimal reward -625.60
Iteration 87 took 2.34 seconds (mean sampled reward: -4702.97). Current reward after update: -761.10, Optimal reward -625.60
Iteration 88 took 2.31 seconds (mean sampled reward: -5160.37). Current reward after update: -764.98, Optimal reward -625.60
Iteration 89 took 2.41 seconds (mean sampled reward: -5370.80). Current reward after update: -5596.11, Optimal reward -625.60
Iteration 90 took 2.43 seconds (mean sampled reward: -5811.29). Current reward after update: -1241.66, Optimal reward -625.60
Iteration 91 took 2.29 seconds (mean sampled reward: -4520.79). Current reward after update: -802.54, Optimal reward -625.60
Iteration 92 took 2.25 seconds (mean sampled reward: -4687.42). Current reward after update: -804.98, Optimal reward -625.60
Iteration 93 took 2.34 seconds (mean sampled reward: -5117.21). Current reward after update: -719.75, Optimal reward -625.60
Iteration 94 took 2.30 seconds (mean sampled reward: -4359.00). Current reward after update: -711.81, Optimal reward -625.60
Iteration 95 took 2.37 seconds (mean sampled reward: -5156.27). Current reward after update: -697.03, Optimal reward -625.60
Iteration 96 took 2.33 seconds (mean sampled reward: -4280.70). Current reward after update: -622.16, Optimal reward -622.16
Iteration 97 took 2.33 seconds (mean sampled reward: -3546.55). Current reward after update: -571.76, Optimal reward -571.76
Iteration 98 took 2.36 seconds (mean sampled reward: -3332.26). Current reward after update: -610.62, Optimal reward -571.76
Iteration 99 took 2.31 seconds (mean sampled reward: -3542.86). Current reward after update: -671.86, Optimal reward -571.76
Iteration 100 took 2.39 seconds (mean sampled reward: -4314.25). Current reward after update: -680.53, Optimal reward -571.76
Iteration 101 took 2.33 seconds (mean sampled reward: -4273.79). Current reward after update: -755.53, Optimal reward -571.76
Iteration 102 took 2.31 seconds (mean sampled reward: -3591.44). Current reward after update: -1628.21, Optimal reward -571.76
Iteration 103 took 2.29 seconds (mean sampled reward: -2581.43). Current reward after update: -591.30, Optimal reward -571.76
Iteration 104 took 2.47 seconds (mean sampled reward: -2708.65). Current reward after update: -632.18, Optimal reward -571.76
Iteration 105 took 2.28 seconds (mean sampled reward: -3018.60). Current reward after update: -554.53, Optimal reward -554.53
Iteration 106 took 2.32 seconds (mean sampled reward: -3318.46). Current reward after update: -610.11, Optimal reward -554.53
Iteration 107 took 2.29 seconds (mean sampled reward: -2753.41). Current reward after update: -537.88, Optimal reward -537.88
Iteration 108 took 2.32 seconds (mean sampled reward: -3529.46). Current reward after update: -2246.38, Optimal reward -537.88
Iteration 109 took 2.38 seconds (mean sampled reward: -3697.35). Current reward after update: -520.67, Optimal reward -520.67
Iteration 110 took 2.21 seconds (mean sampled reward: -4050.64). Current reward after update: -591.88, Optimal reward -520.67
Iteration 111 took 2.19 seconds (mean sampled reward: -3290.30). Current reward after update: -580.05, Optimal reward -520.67
Iteration 112 took 2.25 seconds (mean sampled reward: -3922.38). Current reward after update: -673.67, Optimal reward -520.67
Iteration 113 took 2.10 seconds (mean sampled reward: -2891.97). Current reward after update: -609.29, Optimal reward -520.67
Iteration 114 took 2.24 seconds (mean sampled reward: -2924.75). Current reward after update: -1502.55, Optimal reward -520.67
Iteration 115 took 2.21 seconds (mean sampled reward: -3378.89). Current reward after update: -623.54, Optimal reward -520.67
Iteration 116 took 2.51 seconds (mean sampled reward: -3449.74). Current reward after update: -918.47, Optimal reward -520.67
Iteration 117 took 2.45 seconds (mean sampled reward: -4182.46). Current reward after update: -574.86, Optimal reward -520.67
Iteration 118 took 2.27 seconds (mean sampled reward: -3138.34). Current reward after update: -570.12, Optimal reward -520.67
Iteration 119 took 2.25 seconds (mean sampled reward: -2887.85). Current reward after update: -1648.40, Optimal reward -520.67
Iteration 120 took 2.34 seconds (mean sampled reward: -3703.62). Current reward after update: -573.59, Optimal reward -520.67
Iteration 121 took 2.30 seconds (mean sampled reward: -3861.92). Current reward after update: -621.60, Optimal reward -520.67
Iteration 122 took 2.32 seconds (mean sampled reward: -3617.72). Current reward after update: -606.00, Optimal reward -520.67
Iteration 123 took 2.37 seconds (mean sampled reward: -3761.15). Current reward after update: -1227.49, Optimal reward -520.67
Iteration 124 took 2.28 seconds (mean sampled reward: -4821.60). Current reward after update: -687.20, Optimal reward -520.67
Iteration 125 took 2.33 seconds (mean sampled reward: -4337.53). Current reward after update: -606.42, Optimal reward -520.67
Iteration 126 took 2.30 seconds (mean sampled reward: -4028.47). Current reward after update: -802.04, Optimal reward -520.67
Iteration 127 took 2.27 seconds (mean sampled reward: -3585.31). Current reward after update: -575.79, Optimal reward -520.67
Iteration 128 took 2.32 seconds (mean sampled reward: -5343.49). Current reward after update: -1440.88, Optimal reward -520.67
Iteration 129 took 2.27 seconds (mean sampled reward: -5045.00). Current reward after update: -567.61, Optimal reward -520.67
Iteration 130 took 2.30 seconds (mean sampled reward: -3251.63). Current reward after update: -830.10, Optimal reward -520.67
Iteration 131 took 2.31 seconds (mean sampled reward: -3977.44). Current reward after update: -591.12, Optimal reward -520.67
Iteration 132 took 2.40 seconds (mean sampled reward: -4814.25). Current reward after update: -546.22, Optimal reward -520.67
Iteration 133 took 2.30 seconds (mean sampled reward: -3567.89). Current reward after update: -566.77, Optimal reward -520.67
Iteration 134 took 2.31 seconds (mean sampled reward: -3567.98). Current reward after update: -528.04, Optimal reward -520.67
Iteration 135 took 2.37 seconds (mean sampled reward: -4602.46). Current reward after update: -610.69, Optimal reward -520.67
Iteration 136 took 2.32 seconds (mean sampled reward: -3839.80). Current reward after update: -549.97, Optimal reward -520.67
Iteration 137 took 2.30 seconds (mean sampled reward: -4200.80). Current reward after update: -658.14, Optimal reward -520.67
Iteration 138 took 2.34 seconds (mean sampled reward: -4125.28). Current reward after update: -1666.11, Optimal reward -520.67
Iteration 139 took 2.34 seconds (mean sampled reward: -4388.57). Current reward after update: -684.60, Optimal reward -520.67
Iteration 140 took 2.44 seconds (mean sampled reward: -4214.50). Current reward after update: -740.10, Optimal reward -520.67
Iteration 141 took 2.39 seconds (mean sampled reward: -4388.61). Current reward after update: -676.55, Optimal reward -520.67
Iteration 142 took 2.42 seconds (mean sampled reward: -4157.65). Current reward after update: -1647.57, Optimal reward -520.67
Iteration 143 took 2.36 seconds (mean sampled reward: -3683.31). Current reward after update: -809.96, Optimal reward -520.67
Iteration 144 took 2.50 seconds (mean sampled reward: -4636.49). Current reward after update: -702.96, Optimal reward -520.67
Iteration 145 took 2.40 seconds (mean sampled reward: -4460.66). Current reward after update: -814.65, Optimal reward -520.67
Iteration 146 took 2.40 seconds (mean sampled reward: -4505.91). Current reward after update: -1139.67, Optimal reward -520.67
Iteration 147 took 2.40 seconds (mean sampled reward: -4591.87). Current reward after update: -780.94, Optimal reward -520.67
Iteration 148 took 2.35 seconds (mean sampled reward: -3533.49). Current reward after update: -1572.86, Optimal reward -520.67
Iteration 149 took 2.39 seconds (mean sampled reward: -4079.90). Current reward after update: -5304.10, Optimal reward -520.67
Iteration 150 took 2.39 seconds (mean sampled reward: -4944.22). Current reward after update: -658.60, Optimal reward -520.67
Iteration 151 took 2.40 seconds (mean sampled reward: -4391.63). Current reward after update: -676.43, Optimal reward -520.67
Iteration 152 took 2.36 seconds (mean sampled reward: -3117.59). Current reward after update: -650.12, Optimal reward -520.67
Iteration 153 took 2.36 seconds (mean sampled reward: -3828.19). Current reward after update: -602.45, Optimal reward -520.67
Iteration 154 took 2.34 seconds (mean sampled reward: -3223.68). Current reward after update: -558.64, Optimal reward -520.67
Iteration 155 took 2.33 seconds (mean sampled reward: -3065.84). Current reward after update: -671.53, Optimal reward -520.67
Iteration 156 took 2.35 seconds (mean sampled reward: -3783.22). Current reward after update: -681.94, Optimal reward -520.67
Iteration 157 took 2.34 seconds (mean sampled reward: -3260.70). Current reward after update: -579.22, Optimal reward -520.67
Iteration 158 took 2.30 seconds (mean sampled reward: -2708.57). Current reward after update: -632.85, Optimal reward -520.67
Iteration 159 took 2.40 seconds (mean sampled reward: -3822.42). Current reward after update: -626.18, Optimal reward -520.67
Iteration 160 took 2.38 seconds (mean sampled reward: -4853.75). Current reward after update: -668.82, Optimal reward -520.67
Iteration 161 took 2.44 seconds (mean sampled reward: -3950.41). Current reward after update: -699.92, Optimal reward -520.67
Iteration 162 took 2.34 seconds (mean sampled reward: -4066.06). Current reward after update: -674.92, Optimal reward -520.67
Iteration 163 took 2.46 seconds (mean sampled reward: -4518.29). Current reward after update: -674.82, Optimal reward -520.67
Iteration 164 took 2.34 seconds (mean sampled reward: -3314.31). Current reward after update: -5193.83, Optimal reward -520.67
Iteration 165 took 2.40 seconds (mean sampled reward: -4397.82). Current reward after update: -652.99, Optimal reward -520.67
Iteration 166 took 2.45 seconds (mean sampled reward: -3478.55). Current reward after update: -665.29, Optimal reward -520.67
Iteration 167 took 2.35 seconds (mean sampled reward: -3247.98). Current reward after update: -747.25, Optimal reward -520.67
Iteration 168 took 2.37 seconds (mean sampled reward: -3094.38). Current reward after update: -623.58, Optimal reward -520.67
Iteration 169 took 2.34 seconds (mean sampled reward: -2484.44). Current reward after update: -656.18, Optimal reward -520.67
Iteration 170 took 2.34 seconds (mean sampled reward: -3407.27). Current reward after update: -623.86, Optimal reward -520.67
Iteration 171 took 2.36 seconds (mean sampled reward: -3104.49). Current reward after update: -1165.68, Optimal reward -520.67
Iteration 172 took 2.34 seconds (mean sampled reward: -2146.57). Current reward after update: -1081.44, Optimal reward -520.67
Iteration 173 took 2.36 seconds (mean sampled reward: -3055.72). Current reward after update: -590.31, Optimal reward -520.67
Iteration 174 took 2.46 seconds (mean sampled reward: -3160.93). Current reward after update: -570.19, Optimal reward -520.67
Iteration 175 took 2.37 seconds (mean sampled reward: -2780.39). Current reward after update: -509.14, Optimal reward -509.14
Iteration 176 took 2.41 seconds (mean sampled reward: -3547.32). Current reward after update: -586.67, Optimal reward -509.14
Iteration 177 took 2.44 seconds (mean sampled reward: -3914.72). Current reward after update: -599.23, Optimal reward -509.14
Iteration 178 took 2.39 seconds (mean sampled reward: -5180.83). Current reward after update: -646.46, Optimal reward -509.14
Iteration 179 took 2.36 seconds (mean sampled reward: -2775.53). Current reward after update: -646.88, Optimal reward -509.14
Iteration 180 took 2.35 seconds (mean sampled reward: -3065.41). Current reward after update: -582.03, Optimal reward -509.14
Iteration 181 took 2.30 seconds (mean sampled reward: -2171.47). Current reward after update: -610.62, Optimal reward -509.14
Iteration 182 took 2.41 seconds (mean sampled reward: -3052.74). Current reward after update: -703.22, Optimal reward -509.14
Iteration 183 took 2.40 seconds (mean sampled reward: -3837.34). Current reward after update: -660.96, Optimal reward -509.14
Iteration 184 took 2.39 seconds (mean sampled reward: -2883.14). Current reward after update: -641.43, Optimal reward -509.14
Iteration 185 took 2.37 seconds (mean sampled reward: -3091.90). Current reward after update: -1496.23, Optimal reward -509.14
Iteration 186 took 2.43 seconds (mean sampled reward: -3160.11). Current reward after update: -947.50, Optimal reward -509.14
Iteration 187 took 2.43 seconds (mean sampled reward: -3714.17). Current reward after update: -550.80, Optimal reward -509.14
Iteration 188 took 2.38 seconds (mean sampled reward: -3336.01). Current reward after update: -547.11, Optimal reward -509.14
Iteration 189 took 2.35 seconds (mean sampled reward: -2890.67). Current reward after update: -641.35, Optimal reward -509.14
Iteration 190 took 2.32 seconds (mean sampled reward: -2713.14). Current reward after update: -656.09, Optimal reward -509.14
Iteration 191 took 2.33 seconds (mean sampled reward: -2655.49). Current reward after update: -597.26, Optimal reward -509.14
Iteration 192 took 2.34 seconds (mean sampled reward: -3081.74). Current reward after update: -543.42, Optimal reward -509.14
Iteration 193 took 2.40 seconds (mean sampled reward: -3290.55). Current reward after update: -480.62, Optimal reward -480.62
Iteration 194 took 2.40 seconds (mean sampled reward: -2767.97). Current reward after update: -439.98, Optimal reward -439.98
Iteration 195 took 2.40 seconds (mean sampled reward: -4061.52). Current reward after update: -421.98, Optimal reward -421.98
Iteration 196 took 2.46 seconds (mean sampled reward: -4957.91). Current reward after update: -1147.57, Optimal reward -421.98
Iteration 197 took 2.41 seconds (mean sampled reward: -3922.04). Current reward after update: -432.63, Optimal reward -421.98
Iteration 198 took 2.51 seconds (mean sampled reward: -5256.98). Current reward after update: -555.82, Optimal reward -421.98
Iteration 199 took 2.45 seconds (mean sampled reward: -3968.24). Current reward after update: -889.27, Optimal reward -421.98
Iteration 200 took 2.40 seconds (mean sampled reward: -3883.83). Current reward after update: -483.87, Optimal reward -421.98
Max force: 100 Sigma: 0.2 mean rewards: -664.8619325051759, best rewards:-421.9831665539035

Iteration 1 took 2.43 seconds (mean sampled reward: -7591.49). Current reward after update: -5817.36, Optimal reward -5817.36
Iteration 2 took 2.28 seconds (mean sampled reward: -7165.93). Current reward after update: -4406.71, Optimal reward -4406.71
Iteration 3 took 2.26 seconds (mean sampled reward: -6914.25). Current reward after update: -4133.55, Optimal reward -4133.55
Iteration 4 took 2.42 seconds (mean sampled reward: -6082.03). Current reward after update: -3179.00, Optimal reward -3179.00
Iteration 5 took 2.37 seconds (mean sampled reward: -5820.19). Current reward after update: -2939.21, Optimal reward -2939.21
Iteration 6 took 2.49 seconds (mean sampled reward: -4960.43). Current reward after update: -2759.43, Optimal reward -2759.43
Iteration 7 took 2.43 seconds (mean sampled reward: -5634.15). Current reward after update: -2668.84, Optimal reward -2668.84
Iteration 8 took 2.49 seconds (mean sampled reward: -5740.99). Current reward after update: -2320.01, Optimal reward -2320.01
Iteration 9 took 2.67 seconds (mean sampled reward: -6454.32). Current reward after update: -2299.25, Optimal reward -2299.25
Iteration 10 took 2.52 seconds (mean sampled reward: -6368.07). Current reward after update: -6052.16, Optimal reward -2299.25
Iteration 11 took 2.36 seconds (mean sampled reward: -5700.64). Current reward after update: -1975.76, Optimal reward -1975.76
Iteration 12 took 2.36 seconds (mean sampled reward: -5616.29). Current reward after update: -1975.85, Optimal reward -1975.76
Iteration 13 took 2.35 seconds (mean sampled reward: -4814.93). Current reward after update: -6024.73, Optimal reward -1975.76
Iteration 14 took 2.46 seconds (mean sampled reward: -6042.28). Current reward after update: -1799.68, Optimal reward -1799.68
Iteration 15 took 2.43 seconds (mean sampled reward: -4448.15). Current reward after update: -1661.95, Optimal reward -1661.95
Iteration 16 took 2.15 seconds (mean sampled reward: -3702.66). Current reward after update: -1676.85, Optimal reward -1661.95
Iteration 17 took 2.33 seconds (mean sampled reward: -5010.05). Current reward after update: -1951.69, Optimal reward -1661.95
Iteration 18 took 2.28 seconds (mean sampled reward: -4144.49). Current reward after update: -1730.67, Optimal reward -1661.95
Iteration 19 took 2.16 seconds (mean sampled reward: -3454.10). Current reward after update: -1629.35, Optimal reward -1629.35
Iteration 20 took 2.16 seconds (mean sampled reward: -3884.04). Current reward after update: -1905.64, Optimal reward -1629.35
Iteration 21 took 2.17 seconds (mean sampled reward: -4243.07). Current reward after update: -6288.51, Optimal reward -1629.35
Iteration 22 took 2.18 seconds (mean sampled reward: -3587.12). Current reward after update: -1670.69, Optimal reward -1629.35
Iteration 23 took 2.22 seconds (mean sampled reward: -3532.01). Current reward after update: -1529.73, Optimal reward -1529.73
Iteration 24 took 2.27 seconds (mean sampled reward: -3576.13). Current reward after update: -2480.65, Optimal reward -1529.73
Iteration 25 took 2.22 seconds (mean sampled reward: -3814.02). Current reward after update: -1411.07, Optimal reward -1411.07
Iteration 26 took 2.72 seconds (mean sampled reward: -4062.08). Current reward after update: -1568.99, Optimal reward -1411.07
Iteration 27 took 2.40 seconds (mean sampled reward: -4364.49). Current reward after update: -1555.53, Optimal reward -1411.07
Iteration 28 took 2.68 seconds (mean sampled reward: -4422.70). Current reward after update: -1257.45, Optimal reward -1257.45
Iteration 29 took 2.55 seconds (mean sampled reward: -3780.66). Current reward after update: -1109.13, Optimal reward -1109.13
Iteration 30 took 2.46 seconds (mean sampled reward: -3792.77). Current reward after update: -1438.46, Optimal reward -1109.13
Iteration 31 took 2.46 seconds (mean sampled reward: -4744.34). Current reward after update: -1572.64, Optimal reward -1109.13
Iteration 32 took 2.40 seconds (mean sampled reward: -4823.05). Current reward after update: -1613.53, Optimal reward -1109.13
Iteration 33 took 2.35 seconds (mean sampled reward: -4630.79). Current reward after update: -1599.05, Optimal reward -1109.13
Iteration 34 took 2.28 seconds (mean sampled reward: -4740.23). Current reward after update: -1735.09, Optimal reward -1109.13
Iteration 35 took 2.24 seconds (mean sampled reward: -4639.93). Current reward after update: -1484.64, Optimal reward -1109.13
Iteration 36 took 2.14 seconds (mean sampled reward: -3896.89). Current reward after update: -1600.77, Optimal reward -1109.13
Iteration 37 took 2.16 seconds (mean sampled reward: -3959.80). Current reward after update: -1645.50, Optimal reward -1109.13
Iteration 38 took 2.18 seconds (mean sampled reward: -4665.04). Current reward after update: -1803.79, Optimal reward -1109.13
Iteration 39 took 2.31 seconds (mean sampled reward: -4298.05). Current reward after update: -1707.27, Optimal reward -1109.13
Iteration 40 took 2.20 seconds (mean sampled reward: -4991.22). Current reward after update: -1750.58, Optimal reward -1109.13
Iteration 41 took 2.39 seconds (mean sampled reward: -5655.10). Current reward after update: -1563.09, Optimal reward -1109.13
Iteration 42 took 2.30 seconds (mean sampled reward: -5957.00). Current reward after update: -1736.27, Optimal reward -1109.13
Iteration 43 took 2.17 seconds (mean sampled reward: -4407.10). Current reward after update: -1570.32, Optimal reward -1109.13
Iteration 44 took 2.20 seconds (mean sampled reward: -5028.79). Current reward after update: -1669.40, Optimal reward -1109.13
Iteration 45 took 2.26 seconds (mean sampled reward: -5756.89). Current reward after update: -1746.89, Optimal reward -1109.13
Iteration 46 took 2.22 seconds (mean sampled reward: -5053.89). Current reward after update: -1530.18, Optimal reward -1109.13
Iteration 47 took 2.17 seconds (mean sampled reward: -4560.52). Current reward after update: -1635.61, Optimal reward -1109.13
Iteration 48 took 2.28 seconds (mean sampled reward: -4155.27). Current reward after update: -1429.46, Optimal reward -1109.13
Iteration 49 took 2.12 seconds (mean sampled reward: -5553.42). Current reward after update: -1551.78, Optimal reward -1109.13
Iteration 50 took 2.27 seconds (mean sampled reward: -4843.36). Current reward after update: -1641.90, Optimal reward -1109.13
Iteration 51 took 2.31 seconds (mean sampled reward: -4206.56). Current reward after update: -1424.09, Optimal reward -1109.13
Iteration 52 took 2.35 seconds (mean sampled reward: -5176.79). Current reward after update: -1539.77, Optimal reward -1109.13
Iteration 53 took 2.34 seconds (mean sampled reward: -5884.07). Current reward after update: -1436.32, Optimal reward -1109.13
Iteration 54 took 2.40 seconds (mean sampled reward: -6211.56). Current reward after update: -1284.07, Optimal reward -1109.13
Iteration 55 took 2.38 seconds (mean sampled reward: -6730.76). Current reward after update: -1701.69, Optimal reward -1109.13
Iteration 56 took 2.28 seconds (mean sampled reward: -6033.30). Current reward after update: -1535.75, Optimal reward -1109.13
Iteration 57 took 2.25 seconds (mean sampled reward: -5189.23). Current reward after update: -1334.07, Optimal reward -1109.13
Iteration 58 took 2.53 seconds (mean sampled reward: -5285.72). Current reward after update: -1466.37, Optimal reward -1109.13
Iteration 59 took 2.25 seconds (mean sampled reward: -5287.02). Current reward after update: -1356.94, Optimal reward -1109.13
Iteration 60 took 2.34 seconds (mean sampled reward: -5563.14). Current reward after update: -1331.98, Optimal reward -1109.13
Iteration 61 took 2.24 seconds (mean sampled reward: -5616.50). Current reward after update: -1207.76, Optimal reward -1109.13
Iteration 62 took 2.33 seconds (mean sampled reward: -5550.69). Current reward after update: -2775.35, Optimal reward -1109.13
Iteration 63 took 2.28 seconds (mean sampled reward: -4636.57). Current reward after update: -2521.91, Optimal reward -1109.13
Iteration 64 took 2.34 seconds (mean sampled reward: -4570.29). Current reward after update: -4101.77, Optimal reward -1109.13
Iteration 65 took 2.29 seconds (mean sampled reward: -4263.63). Current reward after update: -1162.92, Optimal reward -1109.13
Iteration 66 took 2.24 seconds (mean sampled reward: -4558.37). Current reward after update: -1330.65, Optimal reward -1109.13
Iteration 67 took 2.25 seconds (mean sampled reward: -4792.13). Current reward after update: -1211.30, Optimal reward -1109.13
Iteration 68 took 2.32 seconds (mean sampled reward: -4491.12). Current reward after update: -1168.99, Optimal reward -1109.13
Iteration 69 took 2.15 seconds (mean sampled reward: -4355.14). Current reward after update: -1130.05, Optimal reward -1109.13
Iteration 70 took 2.15 seconds (mean sampled reward: -4365.65). Current reward after update: -1152.90, Optimal reward -1109.13
Iteration 71 took 2.10 seconds (mean sampled reward: -2694.70). Current reward after update: -1178.11, Optimal reward -1109.13
Iteration 72 took 2.13 seconds (mean sampled reward: -3142.30). Current reward after update: -897.52, Optimal reward -897.52
Iteration 73 took 2.19 seconds (mean sampled reward: -3140.55). Current reward after update: -1039.31, Optimal reward -897.52
Iteration 74 took 2.19 seconds (mean sampled reward: -3686.13). Current reward after update: -771.64, Optimal reward -771.64
Iteration 75 took 2.22 seconds (mean sampled reward: -4961.21). Current reward after update: -824.27, Optimal reward -771.64
Iteration 76 took 2.23 seconds (mean sampled reward: -4702.48). Current reward after update: -1044.32, Optimal reward -771.64
Iteration 77 took 2.19 seconds (mean sampled reward: -5376.47). Current reward after update: -1089.85, Optimal reward -771.64
Iteration 78 took 2.18 seconds (mean sampled reward: -3106.03). Current reward after update: -837.09, Optimal reward -771.64
Iteration 79 took 2.24 seconds (mean sampled reward: -3125.65). Current reward after update: -839.28, Optimal reward -771.64
Iteration 80 took 2.25 seconds (mean sampled reward: -2854.12). Current reward after update: -1280.61, Optimal reward -771.64
Iteration 81 took 2.14 seconds (mean sampled reward: -3999.48). Current reward after update: -1085.57, Optimal reward -771.64
Iteration 82 took 2.29 seconds (mean sampled reward: -4388.54). Current reward after update: -909.63, Optimal reward -771.64
Iteration 83 took 2.25 seconds (mean sampled reward: -4235.25). Current reward after update: -580.04, Optimal reward -580.04
Iteration 84 took 2.24 seconds (mean sampled reward: -4778.13). Current reward after update: -1035.50, Optimal reward -580.04
Iteration 85 took 2.16 seconds (mean sampled reward: -3105.47). Current reward after update: -2068.98, Optimal reward -580.04
Iteration 86 took 2.16 seconds (mean sampled reward: -2716.04). Current reward after update: -848.69, Optimal reward -580.04
Iteration 87 took 2.29 seconds (mean sampled reward: -4146.90). Current reward after update: -839.85, Optimal reward -580.04
Iteration 88 took 2.17 seconds (mean sampled reward: -3740.60). Current reward after update: -2337.54, Optimal reward -580.04
Iteration 89 took 2.13 seconds (mean sampled reward: -3004.95). Current reward after update: -1545.56, Optimal reward -580.04
Iteration 90 took 2.24 seconds (mean sampled reward: -4013.23). Current reward after update: -887.96, Optimal reward -580.04
Iteration 91 took 2.22 seconds (mean sampled reward: -2697.25). Current reward after update: -615.15, Optimal reward -580.04
Iteration 92 took 2.15 seconds (mean sampled reward: -2825.23). Current reward after update: -936.24, Optimal reward -580.04
Iteration 93 took 2.18 seconds (mean sampled reward: -3141.24). Current reward after update: -752.07, Optimal reward -580.04
Iteration 94 took 2.17 seconds (mean sampled reward: -3414.86). Current reward after update: -911.90, Optimal reward -580.04
Iteration 95 took 2.28 seconds (mean sampled reward: -4275.01). Current reward after update: -899.26, Optimal reward -580.04
Iteration 96 took 2.23 seconds (mean sampled reward: -3679.44). Current reward after update: -1041.55, Optimal reward -580.04
Iteration 97 took 2.27 seconds (mean sampled reward: -4008.60). Current reward after update: -860.12, Optimal reward -580.04
Iteration 98 took 2.30 seconds (mean sampled reward: -3860.28). Current reward after update: -1126.29, Optimal reward -580.04
Iteration 99 took 2.20 seconds (mean sampled reward: -4413.57). Current reward after update: -925.92, Optimal reward -580.04
Iteration 100 took 2.30 seconds (mean sampled reward: -4302.93). Current reward after update: -923.49, Optimal reward -580.04
Iteration 101 took 2.23 seconds (mean sampled reward: -3563.72). Current reward after update: -839.48, Optimal reward -580.04
Iteration 102 took 2.21 seconds (mean sampled reward: -4001.79). Current reward after update: -1020.13, Optimal reward -580.04
Iteration 103 took 2.25 seconds (mean sampled reward: -4055.26). Current reward after update: -984.17, Optimal reward -580.04
Iteration 104 took 2.30 seconds (mean sampled reward: -4425.93). Current reward after update: -1092.44, Optimal reward -580.04
Iteration 105 took 2.25 seconds (mean sampled reward: -4594.74). Current reward after update: -1007.97, Optimal reward -580.04
Iteration 106 took 2.29 seconds (mean sampled reward: -4843.31). Current reward after update: -1230.86, Optimal reward -580.04
Iteration 107 took 2.39 seconds (mean sampled reward: -4575.86). Current reward after update: -1175.61, Optimal reward -580.04
Iteration 108 took 2.41 seconds (mean sampled reward: -4749.56). Current reward after update: -1065.46, Optimal reward -580.04
Iteration 109 took 2.20 seconds (mean sampled reward: -5472.31). Current reward after update: -1312.01, Optimal reward -580.04
Iteration 110 took 2.22 seconds (mean sampled reward: -5721.02). Current reward after update: -1197.03, Optimal reward -580.04
Iteration 111 took 2.30 seconds (mean sampled reward: -4775.99). Current reward after update: -1208.54, Optimal reward -580.04
Iteration 112 took 2.28 seconds (mean sampled reward: -4907.61). Current reward after update: -704.38, Optimal reward -580.04
Iteration 113 took 2.21 seconds (mean sampled reward: -4638.28). Current reward after update: -1169.86, Optimal reward -580.04
Iteration 114 took 2.25 seconds (mean sampled reward: -4470.91). Current reward after update: -1316.93, Optimal reward -580.04
Iteration 115 took 2.23 seconds (mean sampled reward: -4364.02). Current reward after update: -1185.45, Optimal reward -580.04
Iteration 116 took 2.30 seconds (mean sampled reward: -3890.17). Current reward after update: -982.37, Optimal reward -580.04
Iteration 117 took 2.23 seconds (mean sampled reward: -3622.83). Current reward after update: -1068.30, Optimal reward -580.04
Iteration 118 took 2.17 seconds (mean sampled reward: -3588.04). Current reward after update: -892.03, Optimal reward -580.04
Iteration 119 took 2.18 seconds (mean sampled reward: -3901.67). Current reward after update: -1013.56, Optimal reward -580.04
Iteration 120 took 2.19 seconds (mean sampled reward: -3720.24). Current reward after update: -2040.70, Optimal reward -580.04
Iteration 121 took 2.33 seconds (mean sampled reward: -4058.17). Current reward after update: -1109.03, Optimal reward -580.04
Iteration 122 took 2.17 seconds (mean sampled reward: -4317.54). Current reward after update: -1108.08, Optimal reward -580.04
Iteration 123 took 2.17 seconds (mean sampled reward: -3585.22). Current reward after update: -1881.34, Optimal reward -580.04
Iteration 124 took 2.16 seconds (mean sampled reward: -3788.64). Current reward after update: -2232.22, Optimal reward -580.04
Iteration 125 took 2.16 seconds (mean sampled reward: -3790.14). Current reward after update: -946.66, Optimal reward -580.04
Iteration 126 took 2.19 seconds (mean sampled reward: -3998.41). Current reward after update: -856.23, Optimal reward -580.04
Iteration 127 took 2.19 seconds (mean sampled reward: -2649.75). Current reward after update: -807.35, Optimal reward -580.04
Iteration 128 took 2.11 seconds (mean sampled reward: -2693.38). Current reward after update: -894.61, Optimal reward -580.04
Iteration 129 took 2.24 seconds (mean sampled reward: -2812.49). Current reward after update: -773.12, Optimal reward -580.04
Iteration 130 took 2.25 seconds (mean sampled reward: -2669.18). Current reward after update: -753.35, Optimal reward -580.04
Iteration 131 took 2.18 seconds (mean sampled reward: -3072.60). Current reward after update: -730.48, Optimal reward -580.04
Iteration 132 took 2.17 seconds (mean sampled reward: -3007.72). Current reward after update: -800.96, Optimal reward -580.04
Iteration 133 took 2.21 seconds (mean sampled reward: -4152.81). Current reward after update: -1010.71, Optimal reward -580.04
Iteration 134 took 2.24 seconds (mean sampled reward: -3998.22). Current reward after update: -757.03, Optimal reward -580.04
Iteration 135 took 2.18 seconds (mean sampled reward: -3004.11). Current reward after update: -720.37, Optimal reward -580.04
Iteration 136 took 2.22 seconds (mean sampled reward: -3648.89). Current reward after update: -892.52, Optimal reward -580.04
Iteration 137 took 2.25 seconds (mean sampled reward: -3560.18). Current reward after update: -712.59, Optimal reward -580.04
Iteration 138 took 2.13 seconds (mean sampled reward: -3362.63). Current reward after update: -735.16, Optimal reward -580.04
Iteration 139 took 2.16 seconds (mean sampled reward: -3322.19). Current reward after update: -800.88, Optimal reward -580.04
Iteration 140 took 2.23 seconds (mean sampled reward: -3414.28). Current reward after update: -619.13, Optimal reward -580.04
Iteration 141 took 2.22 seconds (mean sampled reward: -3671.93). Current reward after update: -748.34, Optimal reward -580.04
Iteration 142 took 2.20 seconds (mean sampled reward: -2974.29). Current reward after update: -668.23, Optimal reward -580.04
Iteration 143 took 2.18 seconds (mean sampled reward: -3062.29). Current reward after update: -612.75, Optimal reward -580.04
Iteration 144 took 2.21 seconds (mean sampled reward: -3217.66). Current reward after update: -1233.60, Optimal reward -580.04
Iteration 145 took 2.14 seconds (mean sampled reward: -3744.91). Current reward after update: -655.66, Optimal reward -580.04
Iteration 146 took 2.17 seconds (mean sampled reward: -3696.78). Current reward after update: -783.76, Optimal reward -580.04
Iteration 147 took 2.16 seconds (mean sampled reward: -2630.55). Current reward after update: -792.75, Optimal reward -580.04
Iteration 148 took 2.16 seconds (mean sampled reward: -3165.28). Current reward after update: -1717.34, Optimal reward -580.04
Iteration 149 took 2.18 seconds (mean sampled reward: -3547.31). Current reward after update: -938.21, Optimal reward -580.04
Iteration 150 took 2.13 seconds (mean sampled reward: -3043.47). Current reward after update: -906.14, Optimal reward -580.04
Iteration 151 took 2.12 seconds (mean sampled reward: -2738.79). Current reward after update: -2874.62, Optimal reward -580.04
Iteration 152 took 2.12 seconds (mean sampled reward: -2563.49). Current reward after update: -880.74, Optimal reward -580.04
Iteration 153 took 2.10 seconds (mean sampled reward: -2746.67). Current reward after update: -1116.33, Optimal reward -580.04
Iteration 154 took 2.14 seconds (mean sampled reward: -3063.58). Current reward after update: -957.98, Optimal reward -580.04
Iteration 155 took 2.13 seconds (mean sampled reward: -2774.17). Current reward after update: -584.95, Optimal reward -580.04
Iteration 156 took 2.20 seconds (mean sampled reward: -3205.15). Current reward after update: -544.67, Optimal reward -544.67
Iteration 157 took 2.30 seconds (mean sampled reward: -2927.60). Current reward after update: -548.96, Optimal reward -544.67
Iteration 158 took 2.21 seconds (mean sampled reward: -2915.41). Current reward after update: -833.53, Optimal reward -544.67
Iteration 159 took 2.29 seconds (mean sampled reward: -2904.00). Current reward after update: -647.98, Optimal reward -544.67
Iteration 160 took 2.23 seconds (mean sampled reward: -3046.11). Current reward after update: -796.19, Optimal reward -544.67
Iteration 161 took 2.22 seconds (mean sampled reward: -2998.29). Current reward after update: -652.79, Optimal reward -544.67
Iteration 162 took 2.20 seconds (mean sampled reward: -3391.87). Current reward after update: -654.03, Optimal reward -544.67
Iteration 163 took 2.22 seconds (mean sampled reward: -2836.66). Current reward after update: -523.95, Optimal reward -523.95
Iteration 164 took 2.27 seconds (mean sampled reward: -2830.75). Current reward after update: -703.87, Optimal reward -523.95
Iteration 165 took 2.14 seconds (mean sampled reward: -2705.38). Current reward after update: -569.68, Optimal reward -523.95
Iteration 166 took 2.22 seconds (mean sampled reward: -3332.64). Current reward after update: -867.06, Optimal reward -523.95
Iteration 167 took 2.23 seconds (mean sampled reward: -2254.79). Current reward after update: -423.79, Optimal reward -423.79
Iteration 168 took 2.21 seconds (mean sampled reward: -2404.26). Current reward after update: -1282.72, Optimal reward -423.79
Iteration 169 took 2.27 seconds (mean sampled reward: -2555.20). Current reward after update: -668.75, Optimal reward -423.79
Iteration 170 took 2.19 seconds (mean sampled reward: -2368.51). Current reward after update: -635.02, Optimal reward -423.79
Iteration 171 took 2.22 seconds (mean sampled reward: -2764.96). Current reward after update: -642.88, Optimal reward -423.79
Iteration 172 took 2.26 seconds (mean sampled reward: -2809.92). Current reward after update: -975.00, Optimal reward -423.79
Iteration 173 took 2.31 seconds (mean sampled reward: -2926.79). Current reward after update: -977.29, Optimal reward -423.79
Iteration 174 took 2.30 seconds (mean sampled reward: -3015.93). Current reward after update: -774.76, Optimal reward -423.79
Iteration 175 took 2.23 seconds (mean sampled reward: -2771.88). Current reward after update: -916.94, Optimal reward -423.79
Iteration 176 took 2.13 seconds (mean sampled reward: -2669.71). Current reward after update: -835.91, Optimal reward -423.79
Iteration 177 took 2.20 seconds (mean sampled reward: -3365.96). Current reward after update: -783.46, Optimal reward -423.79
Iteration 178 took 2.21 seconds (mean sampled reward: -2799.73). Current reward after update: -619.59, Optimal reward -423.79
Iteration 179 took 2.24 seconds (mean sampled reward: -3916.29). Current reward after update: -847.33, Optimal reward -423.79
Iteration 180 took 2.17 seconds (mean sampled reward: -3400.42). Current reward after update: -798.84, Optimal reward -423.79
Iteration 181 took 2.21 seconds (mean sampled reward: -5394.48). Current reward after update: -1111.59, Optimal reward -423.79
Iteration 182 took 2.26 seconds (mean sampled reward: -5443.93). Current reward after update: -1032.13, Optimal reward -423.79
Iteration 183 took 2.24 seconds (mean sampled reward: -4737.89). Current reward after update: -983.67, Optimal reward -423.79
Iteration 184 took 2.24 seconds (mean sampled reward: -4128.73). Current reward after update: -1125.08, Optimal reward -423.79
Iteration 185 took 2.30 seconds (mean sampled reward: -4141.12). Current reward after update: -1150.48, Optimal reward -423.79
Iteration 186 took 2.26 seconds (mean sampled reward: -3720.42). Current reward after update: -980.87, Optimal reward -423.79
Iteration 187 took 2.28 seconds (mean sampled reward: -3598.88). Current reward after update: -928.12, Optimal reward -423.79
Iteration 188 took 2.19 seconds (mean sampled reward: -3494.29). Current reward after update: -821.12, Optimal reward -423.79
Iteration 189 took 2.20 seconds (mean sampled reward: -2961.30). Current reward after update: -620.24, Optimal reward -423.79
Iteration 190 took 2.19 seconds (mean sampled reward: -2691.82). Current reward after update: -937.96, Optimal reward -423.79
Iteration 191 took 2.24 seconds (mean sampled reward: -3583.69). Current reward after update: -825.06, Optimal reward -423.79
Iteration 192 took 2.18 seconds (mean sampled reward: -3571.33). Current reward after update: -801.78, Optimal reward -423.79
Iteration 193 took 2.18 seconds (mean sampled reward: -3916.71). Current reward after update: -745.86, Optimal reward -423.79
Iteration 194 took 2.29 seconds (mean sampled reward: -3786.91). Current reward after update: -916.19, Optimal reward -423.79
Iteration 195 took 2.16 seconds (mean sampled reward: -4270.08). Current reward after update: -1165.57, Optimal reward -423.79
Iteration 196 took 2.21 seconds (mean sampled reward: -3789.90). Current reward after update: -1307.62, Optimal reward -423.79
Iteration 197 took 2.20 seconds (mean sampled reward: -4454.78). Current reward after update: -1299.38, Optimal reward -423.79
Iteration 198 took 2.17 seconds (mean sampled reward: -4993.43). Current reward after update: -1416.57, Optimal reward -423.79
Iteration 199 took 2.28 seconds (mean sampled reward: -4780.70). Current reward after update: -776.13, Optimal reward -423.79
Iteration 200 took 2.24 seconds (mean sampled reward: -4187.27). Current reward after update: -972.92, Optimal reward -423.79
Iteration 1 took 2.40 seconds (mean sampled reward: -7618.68). Current reward after update: -6338.49, Optimal reward -6338.49
Iteration 2 took 2.35 seconds (mean sampled reward: -7401.00). Current reward after update: -4879.07, Optimal reward -4879.07
Iteration 3 took 2.26 seconds (mean sampled reward: -7203.26). Current reward after update: -4398.27, Optimal reward -4398.27
Iteration 4 took 2.32 seconds (mean sampled reward: -6448.01). Current reward after update: -4510.40, Optimal reward -4398.27
Iteration 5 took 2.33 seconds (mean sampled reward: -5498.39). Current reward after update: -3794.63, Optimal reward -3794.63
Iteration 6 took 2.45 seconds (mean sampled reward: -6309.17). Current reward after update: -2766.83, Optimal reward -2766.83
Iteration 7 took 2.43 seconds (mean sampled reward: -5517.39). Current reward after update: -3029.03, Optimal reward -2766.83
Iteration 8 took 2.48 seconds (mean sampled reward: -5753.30). Current reward after update: -2884.34, Optimal reward -2766.83
Iteration 9 took 2.71 seconds (mean sampled reward: -6162.00). Current reward after update: -2694.06, Optimal reward -2694.06
Iteration 10 took 2.42 seconds (mean sampled reward: -6229.47). Current reward after update: -2643.91, Optimal reward -2643.91
Iteration 11 took 2.34 seconds (mean sampled reward: -4887.80). Current reward after update: -2413.34, Optimal reward -2413.34
Iteration 12 took 2.50 seconds (mean sampled reward: -5246.36). Current reward after update: -2608.93, Optimal reward -2413.34
Iteration 13 took 2.35 seconds (mean sampled reward: -4139.27). Current reward after update: -2472.25, Optimal reward -2413.34
Iteration 14 took 2.39 seconds (mean sampled reward: -4900.45). Current reward after update: -2346.24, Optimal reward -2346.24
Iteration 15 took 2.51 seconds (mean sampled reward: -5331.95). Current reward after update: -2093.51, Optimal reward -2093.51
Iteration 16 took 2.68 seconds (mean sampled reward: -6001.12). Current reward after update: -2076.16, Optimal reward -2076.16
Iteration 17 took 2.63 seconds (mean sampled reward: -5484.10). Current reward after update: -2520.14, Optimal reward -2076.16
Iteration 18 took 2.34 seconds (mean sampled reward: -5122.04). Current reward after update: -2377.45, Optimal reward -2076.16
Iteration 19 took 2.53 seconds (mean sampled reward: -4575.72). Current reward after update: -2389.92, Optimal reward -2076.16
Iteration 20 took 2.35 seconds (mean sampled reward: -5141.51). Current reward after update: -2398.61, Optimal reward -2076.16
Iteration 21 took 2.22 seconds (mean sampled reward: -4831.69). Current reward after update: -2299.94, Optimal reward -2076.16
Iteration 22 took 2.13 seconds (mean sampled reward: -4456.65). Current reward after update: -2229.46, Optimal reward -2076.16
Iteration 23 took 2.18 seconds (mean sampled reward: -4902.18). Current reward after update: -2122.12, Optimal reward -2076.16
Iteration 24 took 2.13 seconds (mean sampled reward: -4298.42). Current reward after update: -2195.42, Optimal reward -2076.16
Iteration 25 took 2.15 seconds (mean sampled reward: -4803.69). Current reward after update: -2075.84, Optimal reward -2075.84
Iteration 26 took 2.13 seconds (mean sampled reward: -4265.69). Current reward after update: -2097.66, Optimal reward -2075.84
Iteration 27 took 2.14 seconds (mean sampled reward: -3829.83). Current reward after update: -1909.09, Optimal reward -1909.09
Iteration 28 took 2.45 seconds (mean sampled reward: -4607.18). Current reward after update: -2079.26, Optimal reward -1909.09
Iteration 29 took 2.29 seconds (mean sampled reward: -4309.82). Current reward after update: -1837.48, Optimal reward -1837.48
Iteration 30 took 2.31 seconds (mean sampled reward: -4114.52). Current reward after update: -1743.23, Optimal reward -1743.23
Iteration 31 took 2.18 seconds (mean sampled reward: -4177.30). Current reward after update: -1824.82, Optimal reward -1743.23
Iteration 32 took 2.26 seconds (mean sampled reward: -3577.44). Current reward after update: -1675.31, Optimal reward -1675.31
Iteration 33 took 2.40 seconds (mean sampled reward: -3343.74). Current reward after update: -1540.36, Optimal reward -1540.36
Iteration 34 took 2.32 seconds (mean sampled reward: -3246.48). Current reward after update: -1679.07, Optimal reward -1540.36
Iteration 35 took 2.31 seconds (mean sampled reward: -3490.14). Current reward after update: -1690.93, Optimal reward -1540.36
Iteration 36 took 2.22 seconds (mean sampled reward: -3628.09). Current reward after update: -1672.10, Optimal reward -1540.36
Iteration 37 took 2.23 seconds (mean sampled reward: -4015.97). Current reward after update: -1819.47, Optimal reward -1540.36
Iteration 38 took 2.29 seconds (mean sampled reward: -3546.63). Current reward after update: -1593.26, Optimal reward -1540.36
Iteration 39 took 2.15 seconds (mean sampled reward: -3618.41). Current reward after update: -1537.49, Optimal reward -1537.49
Iteration 40 took 2.22 seconds (mean sampled reward: -3891.67). Current reward after update: -1737.05, Optimal reward -1537.49
Iteration 41 took 2.28 seconds (mean sampled reward: -3480.82). Current reward after update: -1705.77, Optimal reward -1537.49
Iteration 42 took 2.26 seconds (mean sampled reward: -3772.12). Current reward after update: -1695.61, Optimal reward -1537.49
Iteration 43 took 2.18 seconds (mean sampled reward: -3918.27). Current reward after update: -1818.37, Optimal reward -1537.49
Iteration 44 took 2.26 seconds (mean sampled reward: -4002.33). Current reward after update: -1602.92, Optimal reward -1537.49
Iteration 45 took 2.28 seconds (mean sampled reward: -4143.78). Current reward after update: -1972.82, Optimal reward -1537.49
Iteration 46 took 2.24 seconds (mean sampled reward: -4290.24). Current reward after update: -1749.54, Optimal reward -1537.49
Iteration 47 took 2.42 seconds (mean sampled reward: -4784.20). Current reward after update: -1824.12, Optimal reward -1537.49
Iteration 48 took 2.27 seconds (mean sampled reward: -3914.55). Current reward after update: -3118.85, Optimal reward -1537.49
Iteration 49 took 2.32 seconds (mean sampled reward: -4056.48). Current reward after update: -1850.87, Optimal reward -1537.49
Iteration 50 took 2.30 seconds (mean sampled reward: -4115.66). Current reward after update: -1544.85, Optimal reward -1537.49
Iteration 51 took 2.40 seconds (mean sampled reward: -5227.86). Current reward after update: -1580.86, Optimal reward -1537.49
Iteration 52 took 2.37 seconds (mean sampled reward: -5055.27). Current reward after update: -1474.55, Optimal reward -1474.55
Iteration 53 took 2.33 seconds (mean sampled reward: -4423.93). Current reward after update: -1336.93, Optimal reward -1336.93
Iteration 54 took 2.41 seconds (mean sampled reward: -3898.20). Current reward after update: -1381.13, Optimal reward -1336.93
Iteration 55 took 2.53 seconds (mean sampled reward: -4314.78). Current reward after update: -1508.40, Optimal reward -1336.93
Iteration 56 took 2.32 seconds (mean sampled reward: -4860.12). Current reward after update: -1467.03, Optimal reward -1336.93
Iteration 57 took 2.32 seconds (mean sampled reward: -4357.57). Current reward after update: -1690.50, Optimal reward -1336.93
Iteration 58 took 2.23 seconds (mean sampled reward: -4497.42). Current reward after update: -1940.17, Optimal reward -1336.93
Iteration 59 took 2.28 seconds (mean sampled reward: -4421.21). Current reward after update: -1739.62, Optimal reward -1336.93
Iteration 60 took 2.27 seconds (mean sampled reward: -4170.14). Current reward after update: -1743.49, Optimal reward -1336.93
Iteration 61 took 2.13 seconds (mean sampled reward: -4110.37). Current reward after update: -1529.05, Optimal reward -1336.93
Iteration 62 took 2.17 seconds (mean sampled reward: -4376.15). Current reward after update: -1651.35, Optimal reward -1336.93
Iteration 63 took 2.35 seconds (mean sampled reward: -4281.57). Current reward after update: -1761.54, Optimal reward -1336.93
Iteration 64 took 2.19 seconds (mean sampled reward: -5240.56). Current reward after update: -1872.36, Optimal reward -1336.93
Iteration 65 took 2.51 seconds (mean sampled reward: -4445.04). Current reward after update: -2706.63, Optimal reward -1336.93
Iteration 66 took 2.24 seconds (mean sampled reward: -4451.47). Current reward after update: -1754.82, Optimal reward -1336.93
Iteration 67 took 2.28 seconds (mean sampled reward: -4070.24). Current reward after update: -1702.33, Optimal reward -1336.93
Iteration 68 took 2.14 seconds (mean sampled reward: -4427.48). Current reward after update: -1292.86, Optimal reward -1292.86
Iteration 69 took 2.22 seconds (mean sampled reward: -4415.35). Current reward after update: -1629.77, Optimal reward -1292.86
Iteration 70 took 2.25 seconds (mean sampled reward: -5180.64). Current reward after update: -1658.36, Optimal reward -1292.86
Iteration 71 took 2.22 seconds (mean sampled reward: -4212.29). Current reward after update: -1612.66, Optimal reward -1292.86
Iteration 72 took 2.17 seconds (mean sampled reward: -3559.36). Current reward after update: -7206.65, Optimal reward -1292.86
Iteration 73 took 2.30 seconds (mean sampled reward: -5095.49). Current reward after update: -1575.32, Optimal reward -1292.86
Iteration 74 took 2.14 seconds (mean sampled reward: -3955.68). Current reward after update: -6582.49, Optimal reward -1292.86
Iteration 75 took 2.12 seconds (mean sampled reward: -3250.71). Current reward after update: -1574.63, Optimal reward -1292.86
Iteration 76 took 2.13 seconds (mean sampled reward: -3264.28). Current reward after update: -1550.52, Optimal reward -1292.86
Iteration 77 took 2.20 seconds (mean sampled reward: -3952.50). Current reward after update: -1625.68, Optimal reward -1292.86
Iteration 78 took 2.17 seconds (mean sampled reward: -3777.18). Current reward after update: -1930.99, Optimal reward -1292.86
Iteration 79 took 2.21 seconds (mean sampled reward: -3759.04). Current reward after update: -1629.54, Optimal reward -1292.86
Iteration 80 took 2.18 seconds (mean sampled reward: -3865.64). Current reward after update: -6646.22, Optimal reward -1292.86
Iteration 81 took 2.18 seconds (mean sampled reward: -4590.60). Current reward after update: -1703.62, Optimal reward -1292.86
Iteration 82 took 2.15 seconds (mean sampled reward: -5216.05). Current reward after update: -1908.21, Optimal reward -1292.86
Iteration 83 took 2.13 seconds (mean sampled reward: -4349.96). Current reward after update: -1627.35, Optimal reward -1292.86
Iteration 84 took 2.18 seconds (mean sampled reward: -4270.27). Current reward after update: -1516.58, Optimal reward -1292.86
Iteration 85 took 2.21 seconds (mean sampled reward: -4838.48). Current reward after update: -1794.65, Optimal reward -1292.86
Iteration 86 took 2.13 seconds (mean sampled reward: -4408.49). Current reward after update: -2571.95, Optimal reward -1292.86
Iteration 87 took 2.19 seconds (mean sampled reward: -4300.09). Current reward after update: -1785.07, Optimal reward -1292.86
Iteration 88 took 2.29 seconds (mean sampled reward: -3818.63). Current reward after update: -1649.15, Optimal reward -1292.86
Iteration 89 took 2.30 seconds (mean sampled reward: -5020.98). Current reward after update: -2803.52, Optimal reward -1292.86
Iteration 90 took 2.37 seconds (mean sampled reward: -5336.66). Current reward after update: -1294.87, Optimal reward -1292.86
Iteration 91 took 2.42 seconds (mean sampled reward: -5209.76). Current reward after update: -1539.91, Optimal reward -1292.86
Iteration 92 took 2.33 seconds (mean sampled reward: -4955.08). Current reward after update: -4619.40, Optimal reward -1292.86
Iteration 93 took 2.27 seconds (mean sampled reward: -4309.75). Current reward after update: -1574.49, Optimal reward -1292.86
Iteration 94 took 2.27 seconds (mean sampled reward: -4245.73). Current reward after update: -1500.53, Optimal reward -1292.86
Iteration 95 took 2.31 seconds (mean sampled reward: -4521.44). Current reward after update: -1615.52, Optimal reward -1292.86
Iteration 96 took 2.18 seconds (mean sampled reward: -4307.11). Current reward after update: -1571.52, Optimal reward -1292.86
Iteration 97 took 2.18 seconds (mean sampled reward: -4422.38). Current reward after update: -1524.26, Optimal reward -1292.86
Iteration 98 took 2.12 seconds (mean sampled reward: -4306.28). Current reward after update: -1430.53, Optimal reward -1292.86
Iteration 99 took 2.16 seconds (mean sampled reward: -3795.00). Current reward after update: -1361.80, Optimal reward -1292.86
Iteration 100 took 2.27 seconds (mean sampled reward: -3314.13). Current reward after update: -2457.39, Optimal reward -1292.86
Iteration 101 took 2.25 seconds (mean sampled reward: -3396.90). Current reward after update: -1467.66, Optimal reward -1292.86
Iteration 102 took 2.14 seconds (mean sampled reward: -3305.96). Current reward after update: -1464.38, Optimal reward -1292.86
Iteration 103 took 2.20 seconds (mean sampled reward: -3555.94). Current reward after update: -1463.14, Optimal reward -1292.86
Iteration 104 took 2.13 seconds (mean sampled reward: -3441.21). Current reward after update: -1458.84, Optimal reward -1292.86
Iteration 105 took 2.18 seconds (mean sampled reward: -3722.09). Current reward after update: -1821.92, Optimal reward -1292.86
Iteration 106 took 2.23 seconds (mean sampled reward: -3293.75). Current reward after update: -1481.57, Optimal reward -1292.86
Iteration 107 took 2.16 seconds (mean sampled reward: -3631.13). Current reward after update: -1463.33, Optimal reward -1292.86
Iteration 108 took 2.28 seconds (mean sampled reward: -2729.05). Current reward after update: -1263.68, Optimal reward -1263.68
Iteration 109 took 2.13 seconds (mean sampled reward: -3516.86). Current reward after update: -1433.80, Optimal reward -1263.68
Iteration 110 took 2.21 seconds (mean sampled reward: -3810.45). Current reward after update: -1388.14, Optimal reward -1263.68
Iteration 111 took 2.16 seconds (mean sampled reward: -3905.79). Current reward after update: -1473.87, Optimal reward -1263.68
Iteration 112 took 2.27 seconds (mean sampled reward: -3602.05). Current reward after update: -2502.55, Optimal reward -1263.68
Iteration 113 took 2.20 seconds (mean sampled reward: -3797.36). Current reward after update: -1456.38, Optimal reward -1263.68
Iteration 114 took 2.22 seconds (mean sampled reward: -4753.91). Current reward after update: -1432.42, Optimal reward -1263.68
Iteration 115 took 2.24 seconds (mean sampled reward: -3768.78). Current reward after update: -1445.69, Optimal reward -1263.68
Iteration 116 took 2.25 seconds (mean sampled reward: -4039.46). Current reward after update: -1481.48, Optimal reward -1263.68
Iteration 117 took 2.25 seconds (mean sampled reward: -3773.50). Current reward after update: -1421.61, Optimal reward -1263.68
Iteration 118 took 2.11 seconds (mean sampled reward: -3544.44). Current reward after update: -1476.31, Optimal reward -1263.68
Iteration 119 took 2.11 seconds (mean sampled reward: -3394.84). Current reward after update: -2795.52, Optimal reward -1263.68
Iteration 120 took 2.20 seconds (mean sampled reward: -3403.05). Current reward after update: -1670.18, Optimal reward -1263.68
Iteration 121 took 2.21 seconds (mean sampled reward: -3210.45). Current reward after update: -1496.99, Optimal reward -1263.68
Iteration 122 took 2.15 seconds (mean sampled reward: -3540.94). Current reward after update: -2178.12, Optimal reward -1263.68
Iteration 123 took 2.14 seconds (mean sampled reward: -2892.16). Current reward after update: -1395.88, Optimal reward -1263.68
Iteration 124 took 2.13 seconds (mean sampled reward: -3073.08). Current reward after update: -1413.00, Optimal reward -1263.68
Iteration 125 took 2.08 seconds (mean sampled reward: -3344.48). Current reward after update: -2445.21, Optimal reward -1263.68
Iteration 126 took 2.09 seconds (mean sampled reward: -3025.01). Current reward after update: -1412.55, Optimal reward -1263.68
Iteration 127 took 2.11 seconds (mean sampled reward: -3830.33). Current reward after update: -6740.54, Optimal reward -1263.68
Iteration 128 took 2.10 seconds (mean sampled reward: -3762.53). Current reward after update: -1572.94, Optimal reward -1263.68
Iteration 129 took 2.09 seconds (mean sampled reward: -3603.88). Current reward after update: -3461.24, Optimal reward -1263.68
Iteration 130 took 2.03 seconds (mean sampled reward: -2964.14). Current reward after update: -1352.67, Optimal reward -1263.68
Iteration 131 took 2.14 seconds (mean sampled reward: -4105.23). Current reward after update: -3343.40, Optimal reward -1263.68
Iteration 132 took 2.11 seconds (mean sampled reward: -4467.22). Current reward after update: -1569.05, Optimal reward -1263.68
Iteration 133 took 2.15 seconds (mean sampled reward: -5902.34). Current reward after update: -1927.03, Optimal reward -1263.68
Iteration 134 took 2.25 seconds (mean sampled reward: -5863.81). Current reward after update: -1794.83, Optimal reward -1263.68
Iteration 135 took 2.12 seconds (mean sampled reward: -3213.40). Current reward after update: -1567.96, Optimal reward -1263.68
Iteration 136 took 2.15 seconds (mean sampled reward: -4821.90). Current reward after update: -1725.52, Optimal reward -1263.68
Iteration 137 took 2.13 seconds (mean sampled reward: -3798.46). Current reward after update: -1678.21, Optimal reward -1263.68
Iteration 138 took 2.13 seconds (mean sampled reward: -3684.79). Current reward after update: -1532.33, Optimal reward -1263.68
Iteration 139 took 2.11 seconds (mean sampled reward: -3548.28). Current reward after update: -1754.28, Optimal reward -1263.68
Iteration 140 took 2.08 seconds (mean sampled reward: -2622.22). Current reward after update: -3927.44, Optimal reward -1263.68
Iteration 141 took 2.11 seconds (mean sampled reward: -3370.62). Current reward after update: -1520.49, Optimal reward -1263.68
Iteration 142 took 2.11 seconds (mean sampled reward: -2875.83). Current reward after update: -1355.99, Optimal reward -1263.68
Iteration 143 took 2.11 seconds (mean sampled reward: -3215.90). Current reward after update: -1466.64, Optimal reward -1263.68
Iteration 144 took 2.13 seconds (mean sampled reward: -3034.14). Current reward after update: -1407.19, Optimal reward -1263.68
Iteration 145 took 2.13 seconds (mean sampled reward: -2688.04). Current reward after update: -1367.97, Optimal reward -1263.68
Iteration 146 took 2.04 seconds (mean sampled reward: -2803.69). Current reward after update: -1552.66, Optimal reward -1263.68
Iteration 147 took 2.11 seconds (mean sampled reward: -2690.06). Current reward after update: -1554.09, Optimal reward -1263.68
Iteration 148 took 2.10 seconds (mean sampled reward: -2987.14). Current reward after update: -1504.95, Optimal reward -1263.68
Iteration 149 took 2.18 seconds (mean sampled reward: -3466.04). Current reward after update: -1442.58, Optimal reward -1263.68
Iteration 150 took 2.18 seconds (mean sampled reward: -3926.29). Current reward after update: -1714.48, Optimal reward -1263.68
Iteration 151 took 2.14 seconds (mean sampled reward: -3780.76). Current reward after update: -6941.79, Optimal reward -1263.68
Iteration 152 took 2.18 seconds (mean sampled reward: -4733.89). Current reward after update: -1374.44, Optimal reward -1263.68
Iteration 153 took 2.15 seconds (mean sampled reward: -4908.01). Current reward after update: -1576.84, Optimal reward -1263.68
Iteration 154 took 2.15 seconds (mean sampled reward: -4185.67). Current reward after update: -1489.10, Optimal reward -1263.68
Iteration 155 took 2.14 seconds (mean sampled reward: -4933.76). Current reward after update: -1469.33, Optimal reward -1263.68
Iteration 156 took 2.14 seconds (mean sampled reward: -3977.82). Current reward after update: -1446.59, Optimal reward -1263.68
Iteration 157 took 2.14 seconds (mean sampled reward: -4268.59). Current reward after update: -1706.57, Optimal reward -1263.68
Iteration 158 took 2.19 seconds (mean sampled reward: -4289.52). Current reward after update: -1332.98, Optimal reward -1263.68
Iteration 159 took 2.20 seconds (mean sampled reward: -5135.50). Current reward after update: -6906.48, Optimal reward -1263.68
Iteration 160 took 2.18 seconds (mean sampled reward: -4284.53). Current reward after update: -1528.07, Optimal reward -1263.68
Iteration 161 took 2.16 seconds (mean sampled reward: -4644.60). Current reward after update: -1315.70, Optimal reward -1263.68
Iteration 162 took 2.16 seconds (mean sampled reward: -4429.35). Current reward after update: -1385.16, Optimal reward -1263.68
Iteration 163 took 2.20 seconds (mean sampled reward: -4653.39). Current reward after update: -1342.42, Optimal reward -1263.68
Iteration 164 took 2.17 seconds (mean sampled reward: -4795.48). Current reward after update: -1377.84, Optimal reward -1263.68
Iteration 165 took 2.15 seconds (mean sampled reward: -4439.99). Current reward after update: -1414.68, Optimal reward -1263.68
Iteration 166 took 2.23 seconds (mean sampled reward: -4611.21). Current reward after update: -1477.67, Optimal reward -1263.68
Iteration 167 took 2.20 seconds (mean sampled reward: -4855.56). Current reward after update: -1410.81, Optimal reward -1263.68
Iteration 168 took 2.19 seconds (mean sampled reward: -4831.33). Current reward after update: -1484.81, Optimal reward -1263.68
Iteration 169 took 2.20 seconds (mean sampled reward: -4881.37). Current reward after update: -1254.87, Optimal reward -1254.87
Iteration 170 took 2.17 seconds (mean sampled reward: -4551.79). Current reward after update: -1556.59, Optimal reward -1254.87
Iteration 171 took 2.16 seconds (mean sampled reward: -4725.41). Current reward after update: -1685.28, Optimal reward -1254.87
Iteration 172 took 2.20 seconds (mean sampled reward: -5542.47). Current reward after update: -1652.05, Optimal reward -1254.87
Iteration 173 took 2.24 seconds (mean sampled reward: -5563.16). Current reward after update: -1607.26, Optimal reward -1254.87
Iteration 174 took 2.31 seconds (mean sampled reward: -6304.32). Current reward after update: -1453.65, Optimal reward -1254.87
Iteration 175 took 2.24 seconds (mean sampled reward: -5664.90). Current reward after update: -1284.92, Optimal reward -1254.87
Iteration 176 took 2.23 seconds (mean sampled reward: -5299.59). Current reward after update: -1450.80, Optimal reward -1254.87
Iteration 177 took 2.26 seconds (mean sampled reward: -5467.61). Current reward after update: -1753.92, Optimal reward -1254.87
Iteration 178 took 2.33 seconds (mean sampled reward: -5482.13). Current reward after update: -6894.58, Optimal reward -1254.87
Iteration 179 took 2.20 seconds (mean sampled reward: -5344.26). Current reward after update: -6090.99, Optimal reward -1254.87
Iteration 180 took 2.15 seconds (mean sampled reward: -5407.89). Current reward after update: -1590.34, Optimal reward -1254.87
Iteration 181 took 2.22 seconds (mean sampled reward: -5576.02). Current reward after update: -1714.53, Optimal reward -1254.87
Iteration 182 took 2.19 seconds (mean sampled reward: -5413.90). Current reward after update: -1307.80, Optimal reward -1254.87
Iteration 183 took 2.20 seconds (mean sampled reward: -5001.55). Current reward after update: -2023.28, Optimal reward -1254.87
Iteration 184 took 2.15 seconds (mean sampled reward: -4685.15). Current reward after update: -1364.17, Optimal reward -1254.87
Iteration 185 took 2.21 seconds (mean sampled reward: -5088.92). Current reward after update: -1490.23, Optimal reward -1254.87
Iteration 186 took 2.25 seconds (mean sampled reward: -5810.69). Current reward after update: -1379.65, Optimal reward -1254.87
Iteration 187 took 2.19 seconds (mean sampled reward: -5254.21). Current reward after update: -1558.54, Optimal reward -1254.87
Iteration 188 took 2.19 seconds (mean sampled reward: -5053.01). Current reward after update: -1436.97, Optimal reward -1254.87
Iteration 189 took 2.31 seconds (mean sampled reward: -4753.28). Current reward after update: -3212.45, Optimal reward -1254.87
Iteration 190 took 2.20 seconds (mean sampled reward: -4642.04). Current reward after update: -1410.19, Optimal reward -1254.87
Iteration 191 took 2.21 seconds (mean sampled reward: -5580.48). Current reward after update: -1326.81, Optimal reward -1254.87
Iteration 192 took 2.25 seconds (mean sampled reward: -5848.20). Current reward after update: -1396.59, Optimal reward -1254.87
Iteration 193 took 2.27 seconds (mean sampled reward: -6588.74). Current reward after update: -1363.82, Optimal reward -1254.87
Iteration 194 took 2.28 seconds (mean sampled reward: -6404.25). Current reward after update: -1458.07, Optimal reward -1254.87
Iteration 195 took 2.35 seconds (mean sampled reward: -6538.58). Current reward after update: -1900.87, Optimal reward -1254.87
Iteration 196 took 2.29 seconds (mean sampled reward: -5874.87). Current reward after update: -1543.68, Optimal reward -1254.87
Iteration 197 took 2.27 seconds (mean sampled reward: -5037.42). Current reward after update: -1460.96, Optimal reward -1254.87
Iteration 198 took 2.27 seconds (mean sampled reward: -5373.75). Current reward after update: -1489.80, Optimal reward -1254.87
Iteration 199 took 2.32 seconds (mean sampled reward: -5536.59). Current reward after update: -1461.96, Optimal reward -1254.87
Iteration 200 took 2.31 seconds (mean sampled reward: -6247.55). Current reward after update: -1941.68, Optimal reward -1254.87
Iteration 1 took 2.42 seconds (mean sampled reward: -7598.86). Current reward after update: -6269.68, Optimal reward -6269.68
Iteration 2 took 2.41 seconds (mean sampled reward: -7348.54). Current reward after update: -4298.87, Optimal reward -4298.87
Iteration 3 took 2.42 seconds (mean sampled reward: -6852.72). Current reward after update: -3031.68, Optimal reward -3031.68
Iteration 4 took 2.39 seconds (mean sampled reward: -5848.50). Current reward after update: -2355.62, Optimal reward -2355.62
Iteration 5 took 2.28 seconds (mean sampled reward: -6246.58). Current reward after update: -1740.01, Optimal reward -1740.01
Iteration 6 took 2.43 seconds (mean sampled reward: -4646.00). Current reward after update: -1384.44, Optimal reward -1384.44
Iteration 7 took 2.27 seconds (mean sampled reward: -4501.93). Current reward after update: -1564.26, Optimal reward -1384.44
Iteration 8 took 2.56 seconds (mean sampled reward: -4958.79). Current reward after update: -1463.11, Optimal reward -1384.44
Iteration 9 took 2.29 seconds (mean sampled reward: -5078.11). Current reward after update: -1540.81, Optimal reward -1384.44
Iteration 10 took 2.31 seconds (mean sampled reward: -5343.20). Current reward after update: -1167.03, Optimal reward -1167.03
Iteration 11 took 2.32 seconds (mean sampled reward: -5825.86). Current reward after update: -969.81, Optimal reward -969.81
Iteration 12 took 2.24 seconds (mean sampled reward: -5425.52). Current reward after update: -1281.05, Optimal reward -969.81
Iteration 13 took 2.22 seconds (mean sampled reward: -4927.59). Current reward after update: -1192.02, Optimal reward -969.81
Iteration 14 took 2.14 seconds (mean sampled reward: -4645.86). Current reward after update: -1103.77, Optimal reward -969.81
Iteration 15 took 2.28 seconds (mean sampled reward: -4658.65). Current reward after update: -1175.46, Optimal reward -969.81
Iteration 16 took 2.16 seconds (mean sampled reward: -3234.66). Current reward after update: -1300.48, Optimal reward -969.81
Iteration 17 took 2.36 seconds (mean sampled reward: -3415.29). Current reward after update: -1741.78, Optimal reward -969.81
Iteration 18 took 2.36 seconds (mean sampled reward: -3153.18). Current reward after update: -1258.43, Optimal reward -969.81
Iteration 19 took 2.39 seconds (mean sampled reward: -4720.09). Current reward after update: -1558.60, Optimal reward -969.81
Iteration 20 took 2.26 seconds (mean sampled reward: -4105.09). Current reward after update: -1199.12, Optimal reward -969.81
Iteration 21 took 2.33 seconds (mean sampled reward: -3922.90). Current reward after update: -1394.81, Optimal reward -969.81
Iteration 22 took 2.36 seconds (mean sampled reward: -3453.09). Current reward after update: -1130.43, Optimal reward -969.81
Iteration 23 took 2.26 seconds (mean sampled reward: -4023.58). Current reward after update: -859.42, Optimal reward -859.42
Iteration 24 took 2.36 seconds (mean sampled reward: -3711.50). Current reward after update: -876.66, Optimal reward -859.42
Iteration 25 took 2.19 seconds (mean sampled reward: -3337.02). Current reward after update: -931.62, Optimal reward -859.42
Iteration 26 took 2.40 seconds (mean sampled reward: -3548.03). Current reward after update: -1062.01, Optimal reward -859.42
Iteration 27 took 2.34 seconds (mean sampled reward: -4470.00). Current reward after update: -1231.10, Optimal reward -859.42
Iteration 28 took 2.30 seconds (mean sampled reward: -4870.29). Current reward after update: -1308.90, Optimal reward -859.42
Iteration 29 took 2.41 seconds (mean sampled reward: -5652.91). Current reward after update: -1409.67, Optimal reward -859.42
Iteration 30 took 2.39 seconds (mean sampled reward: -5368.99). Current reward after update: -1321.21, Optimal reward -859.42
Iteration 31 took 2.43 seconds (mean sampled reward: -3598.79). Current reward after update: -1217.61, Optimal reward -859.42
Iteration 32 took 2.22 seconds (mean sampled reward: -3224.27). Current reward after update: -1305.51, Optimal reward -859.42
Iteration 33 took 2.24 seconds (mean sampled reward: -3115.83). Current reward after update: -1355.00, Optimal reward -859.42
Iteration 34 took 2.23 seconds (mean sampled reward: -3935.40). Current reward after update: -1076.26, Optimal reward -859.42
Iteration 35 took 2.41 seconds (mean sampled reward: -4041.95). Current reward after update: -657.08, Optimal reward -657.08
Iteration 36 took 2.34 seconds (mean sampled reward: -4790.99). Current reward after update: -1103.05, Optimal reward -657.08
Iteration 37 took 2.28 seconds (mean sampled reward: -5407.58). Current reward after update: -953.59, Optimal reward -657.08
Iteration 38 took 2.31 seconds (mean sampled reward: -5428.89). Current reward after update: -1126.07, Optimal reward -657.08
Iteration 39 took 2.36 seconds (mean sampled reward: -5044.93). Current reward after update: -982.87, Optimal reward -657.08
Iteration 40 took 2.34 seconds (mean sampled reward: -4446.69). Current reward after update: -1043.90, Optimal reward -657.08
Iteration 41 took 2.53 seconds (mean sampled reward: -4025.51). Current reward after update: -4762.18, Optimal reward -657.08
Iteration 42 took 2.29 seconds (mean sampled reward: -4105.37). Current reward after update: -633.74, Optimal reward -633.74
Iteration 43 took 2.28 seconds (mean sampled reward: -5202.95). Current reward after update: -867.82, Optimal reward -633.74
Iteration 44 took 2.26 seconds (mean sampled reward: -5143.04). Current reward after update: -1021.68, Optimal reward -633.74
Iteration 45 took 2.46 seconds (mean sampled reward: -4387.36). Current reward after update: -933.64, Optimal reward -633.74
Iteration 46 took 2.37 seconds (mean sampled reward: -4031.82). Current reward after update: -1123.54, Optimal reward -633.74
Iteration 47 took 2.48 seconds (mean sampled reward: -4167.59). Current reward after update: -1202.42, Optimal reward -633.74
Iteration 48 took 2.31 seconds (mean sampled reward: -3486.64). Current reward after update: -1020.56, Optimal reward -633.74
Iteration 49 took 2.36 seconds (mean sampled reward: -3201.69). Current reward after update: -936.76, Optimal reward -633.74
Iteration 50 took 2.31 seconds (mean sampled reward: -2956.44). Current reward after update: -1088.47, Optimal reward -633.74
Iteration 51 took 2.30 seconds (mean sampled reward: -3422.33). Current reward after update: -936.47, Optimal reward -633.74
Iteration 52 took 2.28 seconds (mean sampled reward: -2711.94). Current reward after update: -1990.28, Optimal reward -633.74
Iteration 53 took 2.42 seconds (mean sampled reward: -3927.46). Current reward after update: -1225.29, Optimal reward -633.74
Iteration 54 took 2.34 seconds (mean sampled reward: -5075.39). Current reward after update: -1122.14, Optimal reward -633.74
Iteration 55 took 2.36 seconds (mean sampled reward: -2561.61). Current reward after update: -957.41, Optimal reward -633.74
Iteration 56 took 2.33 seconds (mean sampled reward: -3641.90). Current reward after update: -913.44, Optimal reward -633.74
Iteration 57 took 2.32 seconds (mean sampled reward: -3681.56). Current reward after update: -1755.19, Optimal reward -633.74
Iteration 58 took 2.37 seconds (mean sampled reward: -4237.56). Current reward after update: -903.51, Optimal reward -633.74
Iteration 59 took 2.49 seconds (mean sampled reward: -2909.20). Current reward after update: -754.32, Optimal reward -633.74
Iteration 60 took 2.35 seconds (mean sampled reward: -3145.50). Current reward after update: -708.89, Optimal reward -633.74
Iteration 61 took 2.34 seconds (mean sampled reward: -3369.54). Current reward after update: -883.68, Optimal reward -633.74
Iteration 62 took 2.34 seconds (mean sampled reward: -3361.72). Current reward after update: -644.26, Optimal reward -633.74
Iteration 63 took 2.50 seconds (mean sampled reward: -3301.47). Current reward after update: -775.42, Optimal reward -633.74
Iteration 64 took 2.35 seconds (mean sampled reward: -2973.86). Current reward after update: -795.45, Optimal reward -633.74
Iteration 65 took 2.35 seconds (mean sampled reward: -2686.07). Current reward after update: -752.69, Optimal reward -633.74
Iteration 66 took 2.34 seconds (mean sampled reward: -2851.14). Current reward after update: -2142.34, Optimal reward -633.74
Iteration 67 took 2.26 seconds (mean sampled reward: -3261.55). Current reward after update: -737.51, Optimal reward -633.74
Iteration 68 took 2.27 seconds (mean sampled reward: -3196.60). Current reward after update: -675.13, Optimal reward -633.74
Iteration 69 took 2.31 seconds (mean sampled reward: -2816.61). Current reward after update: -673.80, Optimal reward -633.74
Iteration 70 took 2.32 seconds (mean sampled reward: -2975.91). Current reward after update: -596.48, Optimal reward -596.48
Iteration 71 took 2.35 seconds (mean sampled reward: -3795.55). Current reward after update: -862.45, Optimal reward -596.48
Iteration 72 took 2.33 seconds (mean sampled reward: -2871.70). Current reward after update: -822.72, Optimal reward -596.48
Iteration 73 took 2.28 seconds (mean sampled reward: -3042.48). Current reward after update: -904.61, Optimal reward -596.48
Iteration 74 took 2.29 seconds (mean sampled reward: -3646.22). Current reward after update: -865.21, Optimal reward -596.48
Iteration 75 took 2.32 seconds (mean sampled reward: -4088.13). Current reward after update: -830.27, Optimal reward -596.48
Iteration 76 took 2.31 seconds (mean sampled reward: -4262.36). Current reward after update: -963.36, Optimal reward -596.48
Iteration 77 took 2.31 seconds (mean sampled reward: -4047.96). Current reward after update: -911.77, Optimal reward -596.48
Iteration 78 took 2.44 seconds (mean sampled reward: -4398.65). Current reward after update: -974.96, Optimal reward -596.48
Iteration 79 took 2.37 seconds (mean sampled reward: -3558.11). Current reward after update: -782.82, Optimal reward -596.48
Iteration 80 took 2.32 seconds (mean sampled reward: -3592.91). Current reward after update: -732.40, Optimal reward -596.48
Iteration 81 took 2.39 seconds (mean sampled reward: -3215.54). Current reward after update: -2181.19, Optimal reward -596.48
Iteration 82 took 2.36 seconds (mean sampled reward: -3231.89). Current reward after update: -633.18, Optimal reward -596.48
Iteration 83 took 2.35 seconds (mean sampled reward: -3242.26). Current reward after update: -628.09, Optimal reward -596.48
Iteration 84 took 2.29 seconds (mean sampled reward: -3324.71). Current reward after update: -761.67, Optimal reward -596.48
Iteration 85 took 2.28 seconds (mean sampled reward: -3326.77). Current reward after update: -751.05, Optimal reward -596.48
Iteration 86 took 2.30 seconds (mean sampled reward: -2960.07). Current reward after update: -672.69, Optimal reward -596.48
Iteration 87 took 2.30 seconds (mean sampled reward: -2911.62). Current reward after update: -703.32, Optimal reward -596.48
Iteration 88 took 2.34 seconds (mean sampled reward: -2696.48). Current reward after update: -960.98, Optimal reward -596.48
Iteration 89 took 2.32 seconds (mean sampled reward: -3102.08). Current reward after update: -911.21, Optimal reward -596.48
Iteration 90 took 2.35 seconds (mean sampled reward: -3240.19). Current reward after update: -621.27, Optimal reward -596.48
Iteration 91 took 2.50 seconds (mean sampled reward: -3615.51). Current reward after update: -703.70, Optimal reward -596.48
Iteration 92 took 2.36 seconds (mean sampled reward: -3123.12). Current reward after update: -778.81, Optimal reward -596.48
Iteration 93 took 2.37 seconds (mean sampled reward: -2858.78). Current reward after update: -823.54, Optimal reward -596.48
Iteration 94 took 2.35 seconds (mean sampled reward: -2941.13). Current reward after update: -660.99, Optimal reward -596.48
Iteration 95 took 2.37 seconds (mean sampled reward: -2963.88). Current reward after update: -636.78, Optimal reward -596.48
Iteration 96 took 2.33 seconds (mean sampled reward: -3063.32). Current reward after update: -2203.60, Optimal reward -596.48
Iteration 97 took 2.34 seconds (mean sampled reward: -3297.17). Current reward after update: -718.91, Optimal reward -596.48
Iteration 98 took 2.35 seconds (mean sampled reward: -2688.77). Current reward after update: -598.35, Optimal reward -596.48
Iteration 99 took 2.46 seconds (mean sampled reward: -4035.36). Current reward after update: -1426.50, Optimal reward -596.48
Iteration 100 took 2.30 seconds (mean sampled reward: -2777.65). Current reward after update: -907.30, Optimal reward -596.48
Iteration 101 took 2.33 seconds (mean sampled reward: -3820.39). Current reward after update: -749.36, Optimal reward -596.48
Iteration 102 took 2.38 seconds (mean sampled reward: -4091.67). Current reward after update: -654.17, Optimal reward -596.48
Iteration 103 took 2.32 seconds (mean sampled reward: -3830.51). Current reward after update: -701.26, Optimal reward -596.48
Iteration 104 took 2.40 seconds (mean sampled reward: -4606.34). Current reward after update: -904.02, Optimal reward -596.48
Iteration 105 took 2.27 seconds (mean sampled reward: -2865.64). Current reward after update: -694.21, Optimal reward -596.48
Iteration 106 took 2.41 seconds (mean sampled reward: -2712.72). Current reward after update: -838.98, Optimal reward -596.48
Iteration 107 took 2.31 seconds (mean sampled reward: -2617.01). Current reward after update: -1114.55, Optimal reward -596.48
Iteration 108 took 2.48 seconds (mean sampled reward: -3098.47). Current reward after update: -877.85, Optimal reward -596.48
Iteration 109 took 2.51 seconds (mean sampled reward: -4242.76). Current reward after update: -651.34, Optimal reward -596.48
Iteration 110 took 2.46 seconds (mean sampled reward: -4330.92). Current reward after update: -692.97, Optimal reward -596.48
Iteration 111 took 2.64 seconds (mean sampled reward: -5043.50). Current reward after update: -963.15, Optimal reward -596.48
Iteration 112 took 2.39 seconds (mean sampled reward: -3529.86). Current reward after update: -880.54, Optimal reward -596.48
Iteration 113 took 2.41 seconds (mean sampled reward: -3616.48). Current reward after update: -827.58, Optimal reward -596.48
Iteration 114 took 2.58 seconds (mean sampled reward: -2700.31). Current reward after update: -1301.20, Optimal reward -596.48
Iteration 115 took 2.49 seconds (mean sampled reward: -3128.54). Current reward after update: -861.73, Optimal reward -596.48
Iteration 116 took 2.35 seconds (mean sampled reward: -3326.45). Current reward after update: -921.06, Optimal reward -596.48
Iteration 117 took 2.34 seconds (mean sampled reward: -3776.42). Current reward after update: -968.74, Optimal reward -596.48
Iteration 118 took 2.31 seconds (mean sampled reward: -3672.07). Current reward after update: -984.32, Optimal reward -596.48
Iteration 119 took 2.36 seconds (mean sampled reward: -2408.56). Current reward after update: -2523.62, Optimal reward -596.48
Iteration 120 took 2.61 seconds (mean sampled reward: -2943.93). Current reward after update: -757.35, Optimal reward -596.48
Iteration 121 took 2.48 seconds (mean sampled reward: -2265.81). Current reward after update: -817.66, Optimal reward -596.48
Iteration 122 took 2.49 seconds (mean sampled reward: -2488.16). Current reward after update: -829.88, Optimal reward -596.48
Iteration 123 took 2.42 seconds (mean sampled reward: -3088.93). Current reward after update: -799.01, Optimal reward -596.48
Iteration 124 took 2.44 seconds (mean sampled reward: -3511.42). Current reward after update: -782.09, Optimal reward -596.48
Iteration 125 took 2.34 seconds (mean sampled reward: -2800.28). Current reward after update: -1754.07, Optimal reward -596.48
Iteration 126 took 2.33 seconds (mean sampled reward: -2901.64). Current reward after update: -744.29, Optimal reward -596.48
Iteration 127 took 2.36 seconds (mean sampled reward: -3192.57). Current reward after update: -657.84, Optimal reward -596.48
Iteration 128 took 2.30 seconds (mean sampled reward: -2616.41). Current reward after update: -797.16, Optimal reward -596.48
Iteration 129 took 2.38 seconds (mean sampled reward: -2700.80). Current reward after update: -489.52, Optimal reward -489.52
Iteration 130 took 2.36 seconds (mean sampled reward: -3026.03). Current reward after update: -2081.16, Optimal reward -489.52
Iteration 131 took 2.46 seconds (mean sampled reward: -4009.75). Current reward after update: -819.03, Optimal reward -489.52
Iteration 132 took 2.41 seconds (mean sampled reward: -3805.46). Current reward after update: -1112.97, Optimal reward -489.52
Iteration 133 took 2.41 seconds (mean sampled reward: -2776.96). Current reward after update: -789.45, Optimal reward -489.52
Iteration 134 took 2.37 seconds (mean sampled reward: -2685.21). Current reward after update: -837.83, Optimal reward -489.52
Iteration 135 took 2.33 seconds (mean sampled reward: -3293.55). Current reward after update: -814.97, Optimal reward -489.52
Iteration 136 took 2.38 seconds (mean sampled reward: -2584.91). Current reward after update: -781.25, Optimal reward -489.52
Iteration 137 took 2.33 seconds (mean sampled reward: -2394.17). Current reward after update: -768.80, Optimal reward -489.52
Iteration 138 took 2.35 seconds (mean sampled reward: -2523.98). Current reward after update: -897.83, Optimal reward -489.52
Iteration 139 took 2.34 seconds (mean sampled reward: -2801.46). Current reward after update: -880.99, Optimal reward -489.52
Iteration 140 took 2.38 seconds (mean sampled reward: -2779.70). Current reward after update: -860.33, Optimal reward -489.52
Iteration 141 took 2.37 seconds (mean sampled reward: -2666.44). Current reward after update: -756.09, Optimal reward -489.52
Iteration 142 took 2.37 seconds (mean sampled reward: -2799.58). Current reward after update: -884.34, Optimal reward -489.52
Iteration 143 took 2.38 seconds (mean sampled reward: -2692.69). Current reward after update: -856.62, Optimal reward -489.52
Iteration 144 took 2.30 seconds (mean sampled reward: -3089.56). Current reward after update: -1628.73, Optimal reward -489.52
Iteration 145 took 2.34 seconds (mean sampled reward: -3166.98). Current reward after update: -811.04, Optimal reward -489.52
Iteration 146 took 2.35 seconds (mean sampled reward: -2655.62). Current reward after update: -879.84, Optimal reward -489.52
Iteration 147 took 2.35 seconds (mean sampled reward: -2669.49). Current reward after update: -1593.57, Optimal reward -489.52
Iteration 148 took 2.40 seconds (mean sampled reward: -3320.82). Current reward after update: -835.44, Optimal reward -489.52
Iteration 149 took 2.32 seconds (mean sampled reward: -3482.91). Current reward after update: -879.73, Optimal reward -489.52
Iteration 150 took 2.27 seconds (mean sampled reward: -3341.36). Current reward after update: -762.22, Optimal reward -489.52
Iteration 151 took 2.31 seconds (mean sampled reward: -3980.11). Current reward after update: -864.78, Optimal reward -489.52
Iteration 152 took 2.35 seconds (mean sampled reward: -4345.64). Current reward after update: -936.07, Optimal reward -489.52
Iteration 153 took 2.36 seconds (mean sampled reward: -4389.67). Current reward after update: -976.84, Optimal reward -489.52
Iteration 154 took 2.39 seconds (mean sampled reward: -3259.11). Current reward after update: -1747.68, Optimal reward -489.52
Iteration 155 took 2.36 seconds (mean sampled reward: -3084.91). Current reward after update: -928.48, Optimal reward -489.52
Iteration 156 took 2.35 seconds (mean sampled reward: -4307.78). Current reward after update: -881.57, Optimal reward -489.52
Iteration 157 took 2.40 seconds (mean sampled reward: -3287.30). Current reward after update: -831.76, Optimal reward -489.52
Iteration 158 took 2.42 seconds (mean sampled reward: -4126.42). Current reward after update: -1163.28, Optimal reward -489.52
Iteration 159 took 2.29 seconds (mean sampled reward: -5435.88). Current reward after update: -823.51, Optimal reward -489.52
Iteration 160 took 2.33 seconds (mean sampled reward: -4063.22). Current reward after update: -938.70, Optimal reward -489.52
Iteration 161 took 2.33 seconds (mean sampled reward: -3108.11). Current reward after update: -942.77, Optimal reward -489.52
Iteration 162 took 2.41 seconds (mean sampled reward: -3314.13). Current reward after update: -911.69, Optimal reward -489.52
Iteration 163 took 2.29 seconds (mean sampled reward: -3210.37). Current reward after update: -2617.26, Optimal reward -489.52
Iteration 164 took 2.31 seconds (mean sampled reward: -2493.20). Current reward after update: -901.21, Optimal reward -489.52
Iteration 165 took 2.37 seconds (mean sampled reward: -2363.84). Current reward after update: -997.92, Optimal reward -489.52
Iteration 166 took 2.31 seconds (mean sampled reward: -2647.88). Current reward after update: -838.51, Optimal reward -489.52
Iteration 167 took 2.36 seconds (mean sampled reward: -2500.35). Current reward after update: -875.69, Optimal reward -489.52
Iteration 168 took 2.27 seconds (mean sampled reward: -3242.31). Current reward after update: -1183.06, Optimal reward -489.52
Iteration 169 took 2.24 seconds (mean sampled reward: -3953.84). Current reward after update: -1492.40, Optimal reward -489.52
Iteration 170 took 2.23 seconds (mean sampled reward: -3889.48). Current reward after update: -756.08, Optimal reward -489.52
Iteration 171 took 2.28 seconds (mean sampled reward: -5043.23). Current reward after update: -811.98, Optimal reward -489.52
Iteration 172 took 2.38 seconds (mean sampled reward: -4214.57). Current reward after update: -853.73, Optimal reward -489.52
Iteration 173 took 2.33 seconds (mean sampled reward: -3598.32). Current reward after update: -1114.40, Optimal reward -489.52
Iteration 174 took 2.36 seconds (mean sampled reward: -3316.39). Current reward after update: -906.28, Optimal reward -489.52
Iteration 175 took 2.25 seconds (mean sampled reward: -4453.93). Current reward after update: -909.77, Optimal reward -489.52
Iteration 176 took 2.30 seconds (mean sampled reward: -4649.34). Current reward after update: -1073.54, Optimal reward -489.52
Iteration 177 took 2.26 seconds (mean sampled reward: -4610.08). Current reward after update: -1508.18, Optimal reward -489.52
Iteration 178 took 2.31 seconds (mean sampled reward: -4380.75). Current reward after update: -921.50, Optimal reward -489.52
Iteration 179 took 2.29 seconds (mean sampled reward: -4201.13). Current reward after update: -801.60, Optimal reward -489.52
Iteration 180 took 2.27 seconds (mean sampled reward: -3913.33). Current reward after update: -912.48, Optimal reward -489.52
Iteration 181 took 2.28 seconds (mean sampled reward: -5108.96). Current reward after update: -831.72, Optimal reward -489.52
Iteration 182 took 2.24 seconds (mean sampled reward: -5230.70). Current reward after update: -872.87, Optimal reward -489.52
Iteration 183 took 2.30 seconds (mean sampled reward: -4747.73). Current reward after update: -2097.72, Optimal reward -489.52
Iteration 184 took 2.22 seconds (mean sampled reward: -5074.38). Current reward after update: -877.93, Optimal reward -489.52
Iteration 185 took 2.33 seconds (mean sampled reward: -2961.88). Current reward after update: -819.86, Optimal reward -489.52
Iteration 186 took 2.31 seconds (mean sampled reward: -2732.55). Current reward after update: -917.02, Optimal reward -489.52
Iteration 187 took 2.31 seconds (mean sampled reward: -2197.62). Current reward after update: -1618.86, Optimal reward -489.52
Iteration 188 took 2.28 seconds (mean sampled reward: -2599.68). Current reward after update: -805.91, Optimal reward -489.52
Iteration 189 took 2.25 seconds (mean sampled reward: -2988.57). Current reward after update: -903.27, Optimal reward -489.52
Iteration 190 took 2.24 seconds (mean sampled reward: -2344.06). Current reward after update: -606.10, Optimal reward -489.52
Iteration 191 took 2.31 seconds (mean sampled reward: -2347.53). Current reward after update: -740.46, Optimal reward -489.52
Iteration 192 took 2.40 seconds (mean sampled reward: -3057.78). Current reward after update: -805.17, Optimal reward -489.52
Iteration 193 took 2.20 seconds (mean sampled reward: -4942.85). Current reward after update: -727.82, Optimal reward -489.52
Iteration 194 took 2.22 seconds (mean sampled reward: -5137.10). Current reward after update: -940.04, Optimal reward -489.52
Iteration 195 took 2.25 seconds (mean sampled reward: -4828.13). Current reward after update: -6443.91, Optimal reward -489.52
Iteration 196 took 2.24 seconds (mean sampled reward: -4879.09). Current reward after update: -907.63, Optimal reward -489.52
Iteration 197 took 2.22 seconds (mean sampled reward: -5487.49). Current reward after update: -2380.35, Optimal reward -489.52
Iteration 198 took 2.29 seconds (mean sampled reward: -5116.80). Current reward after update: -1370.69, Optimal reward -489.52
Iteration 199 took 2.28 seconds (mean sampled reward: -5166.91). Current reward after update: -958.64, Optimal reward -489.52
Iteration 200 took 2.28 seconds (mean sampled reward: -5550.01). Current reward after update: -1202.81, Optimal reward -489.52
Max force: 100 Sigma: 0.4 mean rewards: -722.7284172639174, best rewards:-423.7900551266849

Iteration 1 took 2.41 seconds (mean sampled reward: -7480.57). Current reward after update: -4658.60, Optimal reward -4658.60
Iteration 2 took 2.38 seconds (mean sampled reward: -6917.43). Current reward after update: -3470.27, Optimal reward -3470.27
Iteration 3 took 2.26 seconds (mean sampled reward: -6388.77). Current reward after update: -3493.95, Optimal reward -3470.27
Iteration 4 took 2.39 seconds (mean sampled reward: -5719.99). Current reward after update: -3481.28, Optimal reward -3470.27
Iteration 5 took 2.35 seconds (mean sampled reward: -6341.68). Current reward after update: -2942.61, Optimal reward -2942.61
Iteration 6 took 2.26 seconds (mean sampled reward: -6816.37). Current reward after update: -3312.44, Optimal reward -2942.61
Iteration 7 took 2.30 seconds (mean sampled reward: -6242.34). Current reward after update: -2970.26, Optimal reward -2942.61
Iteration 8 took 2.24 seconds (mean sampled reward: -6402.06). Current reward after update: -2633.10, Optimal reward -2633.10
Iteration 9 took 2.18 seconds (mean sampled reward: -6125.18). Current reward after update: -2919.28, Optimal reward -2633.10
Iteration 10 took 2.38 seconds (mean sampled reward: -6015.81). Current reward after update: -2531.97, Optimal reward -2531.97
Iteration 11 took 2.30 seconds (mean sampled reward: -5931.62). Current reward after update: -2212.59, Optimal reward -2212.59
Iteration 12 took 2.27 seconds (mean sampled reward: -6602.86). Current reward after update: -1922.84, Optimal reward -1922.84
Iteration 13 took 2.24 seconds (mean sampled reward: -6680.20). Current reward after update: -1461.19, Optimal reward -1461.19
Iteration 14 took 2.50 seconds (mean sampled reward: -6439.21). Current reward after update: -1662.15, Optimal reward -1461.19
Iteration 15 took 2.49 seconds (mean sampled reward: -6129.70). Current reward after update: -1411.99, Optimal reward -1411.99
Iteration 16 took 2.62 seconds (mean sampled reward: -6112.33). Current reward after update: -1572.62, Optimal reward -1411.99
Iteration 17 took 2.49 seconds (mean sampled reward: -6276.06). Current reward after update: -1389.42, Optimal reward -1389.42
Iteration 18 took 2.45 seconds (mean sampled reward: -6685.82). Current reward after update: -1085.41, Optimal reward -1085.41
Iteration 19 took 2.39 seconds (mean sampled reward: -7008.06). Current reward after update: -1180.08, Optimal reward -1085.41
Iteration 20 took 2.41 seconds (mean sampled reward: -6503.36). Current reward after update: -1314.49, Optimal reward -1085.41
Iteration 21 took 2.42 seconds (mean sampled reward: -6755.29). Current reward after update: -913.74, Optimal reward -913.74
Iteration 22 took 2.26 seconds (mean sampled reward: -6732.83). Current reward after update: -1114.28, Optimal reward -913.74
Iteration 23 took 2.30 seconds (mean sampled reward: -6350.58). Current reward after update: -642.23, Optimal reward -642.23
Iteration 24 took 2.35 seconds (mean sampled reward: -5983.09). Current reward after update: -980.46, Optimal reward -642.23
Iteration 25 took 2.49 seconds (mean sampled reward: -6026.41). Current reward after update: -696.27, Optimal reward -642.23
Iteration 26 took 2.32 seconds (mean sampled reward: -5385.50). Current reward after update: -614.05, Optimal reward -614.05
Iteration 27 took 2.36 seconds (mean sampled reward: -5593.48). Current reward after update: -547.76, Optimal reward -547.76
Iteration 28 took 2.44 seconds (mean sampled reward: -6182.96). Current reward after update: -733.35, Optimal reward -547.76
Iteration 29 took 2.37 seconds (mean sampled reward: -5349.13). Current reward after update: -577.71, Optimal reward -547.76
Iteration 30 took 2.41 seconds (mean sampled reward: -5936.46). Current reward after update: -436.84, Optimal reward -436.84
Iteration 31 took 2.32 seconds (mean sampled reward: -5435.02). Current reward after update: -550.83, Optimal reward -436.84
Iteration 32 took 2.29 seconds (mean sampled reward: -5841.36). Current reward after update: -503.94, Optimal reward -436.84
Iteration 33 took 2.32 seconds (mean sampled reward: -6329.37). Current reward after update: -671.75, Optimal reward -436.84
Iteration 34 took 2.30 seconds (mean sampled reward: -5949.88). Current reward after update: -653.50, Optimal reward -436.84
Iteration 35 took 2.28 seconds (mean sampled reward: -6283.29). Current reward after update: -971.97, Optimal reward -436.84
Iteration 36 took 2.27 seconds (mean sampled reward: -5729.05). Current reward after update: -518.82, Optimal reward -436.84
Iteration 37 took 2.23 seconds (mean sampled reward: -5142.62). Current reward after update: -718.84, Optimal reward -436.84
Iteration 38 took 2.38 seconds (mean sampled reward: -3855.52). Current reward after update: -757.93, Optimal reward -436.84
Iteration 39 took 2.25 seconds (mean sampled reward: -4360.28). Current reward after update: -648.80, Optimal reward -436.84
Iteration 40 took 2.29 seconds (mean sampled reward: -2935.88). Current reward after update: -569.29, Optimal reward -436.84
Iteration 41 took 2.36 seconds (mean sampled reward: -3578.90). Current reward after update: -488.84, Optimal reward -436.84
Iteration 42 took 2.29 seconds (mean sampled reward: -2767.12). Current reward after update: -450.15, Optimal reward -436.84
Iteration 43 took 2.30 seconds (mean sampled reward: -2794.23). Current reward after update: -495.62, Optimal reward -436.84
Iteration 44 took 2.31 seconds (mean sampled reward: -1668.11). Current reward after update: -392.94, Optimal reward -392.94
Iteration 45 took 2.29 seconds (mean sampled reward: -1744.49). Current reward after update: -374.44, Optimal reward -374.44
Iteration 46 took 2.25 seconds (mean sampled reward: -1872.83). Current reward after update: -1066.75, Optimal reward -374.44
Iteration 47 took 2.32 seconds (mean sampled reward: -2324.99). Current reward after update: -455.56, Optimal reward -374.44
Iteration 48 took 2.20 seconds (mean sampled reward: -2086.22). Current reward after update: -548.21, Optimal reward -374.44
Iteration 49 took 2.33 seconds (mean sampled reward: -1599.63). Current reward after update: -712.82, Optimal reward -374.44
Iteration 50 took 2.25 seconds (mean sampled reward: -1590.64). Current reward after update: -450.00, Optimal reward -374.44
Iteration 51 took 2.19 seconds (mean sampled reward: -1637.78). Current reward after update: -406.57, Optimal reward -374.44
Iteration 52 took 2.27 seconds (mean sampled reward: -1712.71). Current reward after update: -378.25, Optimal reward -374.44
Iteration 53 took 2.29 seconds (mean sampled reward: -3865.06). Current reward after update: -400.65, Optimal reward -374.44
Iteration 54 took 2.59 seconds (mean sampled reward: -2714.64). Current reward after update: -396.32, Optimal reward -374.44
Iteration 55 took 2.30 seconds (mean sampled reward: -3315.74). Current reward after update: -360.56, Optimal reward -360.56
Iteration 56 took 2.46 seconds (mean sampled reward: -4498.85). Current reward after update: -561.65, Optimal reward -360.56
Iteration 57 took 2.19 seconds (mean sampled reward: -3794.90). Current reward after update: -456.99, Optimal reward -360.56
Iteration 58 took 2.28 seconds (mean sampled reward: -3904.03). Current reward after update: -480.96, Optimal reward -360.56
Iteration 59 took 2.27 seconds (mean sampled reward: -2124.46). Current reward after update: -391.07, Optimal reward -360.56
Iteration 60 took 2.37 seconds (mean sampled reward: -2502.13). Current reward after update: -546.27, Optimal reward -360.56
Iteration 61 took 2.27 seconds (mean sampled reward: -2780.22). Current reward after update: -480.22, Optimal reward -360.56
Iteration 62 took 2.28 seconds (mean sampled reward: -2717.77). Current reward after update: -596.78, Optimal reward -360.56
Iteration 63 took 2.27 seconds (mean sampled reward: -3449.90). Current reward after update: -417.02, Optimal reward -360.56
Iteration 64 took 2.26 seconds (mean sampled reward: -2591.14). Current reward after update: -433.68, Optimal reward -360.56
Iteration 65 took 2.18 seconds (mean sampled reward: -3399.70). Current reward after update: -438.22, Optimal reward -360.56
Iteration 66 took 2.19 seconds (mean sampled reward: -3627.82). Current reward after update: -528.69, Optimal reward -360.56
Iteration 67 took 2.24 seconds (mean sampled reward: -2911.13). Current reward after update: -840.93, Optimal reward -360.56
Iteration 68 took 2.23 seconds (mean sampled reward: -2513.22). Current reward after update: -564.48, Optimal reward -360.56
Iteration 69 took 2.25 seconds (mean sampled reward: -2838.52). Current reward after update: -1623.43, Optimal reward -360.56
Iteration 70 took 2.21 seconds (mean sampled reward: -1936.88). Current reward after update: -1267.48, Optimal reward -360.56
Iteration 71 took 2.24 seconds (mean sampled reward: -4054.77). Current reward after update: -691.22, Optimal reward -360.56
Iteration 72 took 2.24 seconds (mean sampled reward: -4004.23). Current reward after update: -653.80, Optimal reward -360.56
Iteration 73 took 2.22 seconds (mean sampled reward: -3888.38). Current reward after update: -780.47, Optimal reward -360.56
Iteration 74 took 2.23 seconds (mean sampled reward: -2517.26). Current reward after update: -595.64, Optimal reward -360.56
Iteration 75 took 2.36 seconds (mean sampled reward: -2617.77). Current reward after update: -551.97, Optimal reward -360.56
Iteration 76 took 2.26 seconds (mean sampled reward: -2952.15). Current reward after update: -478.67, Optimal reward -360.56
Iteration 77 took 2.24 seconds (mean sampled reward: -2695.46). Current reward after update: -622.53, Optimal reward -360.56
Iteration 78 took 2.28 seconds (mean sampled reward: -2168.97). Current reward after update: -599.30, Optimal reward -360.56
Iteration 79 took 2.32 seconds (mean sampled reward: -2891.79). Current reward after update: -1954.99, Optimal reward -360.56
Iteration 80 took 2.35 seconds (mean sampled reward: -3899.83). Current reward after update: -646.19, Optimal reward -360.56
Iteration 81 took 2.33 seconds (mean sampled reward: -3856.80). Current reward after update: -815.85, Optimal reward -360.56
Iteration 82 took 2.27 seconds (mean sampled reward: -3705.55). Current reward after update: -533.61, Optimal reward -360.56
Iteration 83 took 2.30 seconds (mean sampled reward: -2507.50). Current reward after update: -427.18, Optimal reward -360.56
Iteration 84 took 2.28 seconds (mean sampled reward: -2794.98). Current reward after update: -472.30, Optimal reward -360.56
Iteration 85 took 2.26 seconds (mean sampled reward: -4002.24). Current reward after update: -411.07, Optimal reward -360.56
Iteration 86 took 2.45 seconds (mean sampled reward: -4642.61). Current reward after update: -648.10, Optimal reward -360.56
Iteration 87 took 2.28 seconds (mean sampled reward: -2707.96). Current reward after update: -500.49, Optimal reward -360.56
Iteration 88 took 2.38 seconds (mean sampled reward: -2285.48). Current reward after update: -410.70, Optimal reward -360.56
Iteration 89 took 2.33 seconds (mean sampled reward: -3911.64). Current reward after update: -455.37, Optimal reward -360.56
Iteration 90 took 2.34 seconds (mean sampled reward: -3188.49). Current reward after update: -482.45, Optimal reward -360.56
Iteration 91 took 2.36 seconds (mean sampled reward: -3706.38). Current reward after update: -616.68, Optimal reward -360.56
Iteration 92 took 2.36 seconds (mean sampled reward: -2858.24). Current reward after update: -516.55, Optimal reward -360.56
Iteration 93 took 2.44 seconds (mean sampled reward: -2683.45). Current reward after update: -445.69, Optimal reward -360.56
Iteration 94 took 2.39 seconds (mean sampled reward: -4440.53). Current reward after update: -1093.49, Optimal reward -360.56
Iteration 95 took 2.37 seconds (mean sampled reward: -4491.99). Current reward after update: -675.70, Optimal reward -360.56
Iteration 96 took 2.41 seconds (mean sampled reward: -5358.28). Current reward after update: -540.34, Optimal reward -360.56
Iteration 97 took 2.40 seconds (mean sampled reward: -4669.62). Current reward after update: -1692.18, Optimal reward -360.56
Iteration 98 took 2.32 seconds (mean sampled reward: -3912.76). Current reward after update: -518.51, Optimal reward -360.56
Iteration 99 took 2.42 seconds (mean sampled reward: -4740.31). Current reward after update: -550.75, Optimal reward -360.56
Iteration 100 took 2.39 seconds (mean sampled reward: -3763.26). Current reward after update: -516.54, Optimal reward -360.56
Iteration 101 took 2.54 seconds (mean sampled reward: -2842.75). Current reward after update: -400.09, Optimal reward -360.56
Iteration 102 took 2.38 seconds (mean sampled reward: -2438.39). Current reward after update: -588.27, Optimal reward -360.56
Iteration 103 took 2.30 seconds (mean sampled reward: -2574.97). Current reward after update: -393.69, Optimal reward -360.56
Iteration 104 took 2.41 seconds (mean sampled reward: -3748.59). Current reward after update: -397.13, Optimal reward -360.56
Iteration 105 took 2.34 seconds (mean sampled reward: -4614.94). Current reward after update: -428.03, Optimal reward -360.56
Iteration 106 took 2.43 seconds (mean sampled reward: -4097.82). Current reward after update: -406.42, Optimal reward -360.56
Iteration 107 took 2.37 seconds (mean sampled reward: -4872.27). Current reward after update: -345.24, Optimal reward -345.24
Iteration 108 took 2.42 seconds (mean sampled reward: -4992.43). Current reward after update: -407.77, Optimal reward -345.24
Iteration 109 took 2.53 seconds (mean sampled reward: -4117.32). Current reward after update: -313.12, Optimal reward -313.12
Iteration 110 took 2.40 seconds (mean sampled reward: -4423.16). Current reward after update: -453.75, Optimal reward -313.12
Iteration 111 took 2.31 seconds (mean sampled reward: -4309.02). Current reward after update: -536.16, Optimal reward -313.12
Iteration 112 took 2.27 seconds (mean sampled reward: -4513.74). Current reward after update: -389.24, Optimal reward -313.12
Iteration 113 took 2.27 seconds (mean sampled reward: -5259.96). Current reward after update: -766.89, Optimal reward -313.12
Iteration 114 took 2.29 seconds (mean sampled reward: -4560.19). Current reward after update: -526.85, Optimal reward -313.12
Iteration 115 took 2.31 seconds (mean sampled reward: -3659.24). Current reward after update: -1389.37, Optimal reward -313.12
Iteration 116 took 2.26 seconds (mean sampled reward: -3044.63). Current reward after update: -342.77, Optimal reward -313.12
Iteration 117 took 2.36 seconds (mean sampled reward: -3204.10). Current reward after update: -440.36, Optimal reward -313.12
Iteration 118 took 2.31 seconds (mean sampled reward: -3376.47). Current reward after update: -410.61, Optimal reward -313.12
Iteration 119 took 2.31 seconds (mean sampled reward: -2636.64). Current reward after update: -353.21, Optimal reward -313.12
Iteration 120 took 2.25 seconds (mean sampled reward: -2201.08). Current reward after update: -419.23, Optimal reward -313.12
Iteration 121 took 2.38 seconds (mean sampled reward: -2025.10). Current reward after update: -340.03, Optimal reward -313.12
Iteration 122 took 2.38 seconds (mean sampled reward: -2111.24). Current reward after update: -379.36, Optimal reward -313.12
Iteration 123 took 2.46 seconds (mean sampled reward: -2292.86). Current reward after update: -375.28, Optimal reward -313.12
Iteration 124 took 2.20 seconds (mean sampled reward: -1591.22). Current reward after update: -356.42, Optimal reward -313.12
Iteration 125 took 2.21 seconds (mean sampled reward: -1732.82). Current reward after update: -423.16, Optimal reward -313.12
Iteration 126 took 2.23 seconds (mean sampled reward: -1682.56). Current reward after update: -427.98, Optimal reward -313.12
Iteration 127 took 2.24 seconds (mean sampled reward: -1588.99). Current reward after update: -370.87, Optimal reward -313.12
Iteration 128 took 2.27 seconds (mean sampled reward: -1935.91). Current reward after update: -452.84, Optimal reward -313.12
Iteration 129 took 2.28 seconds (mean sampled reward: -2326.93). Current reward after update: -333.58, Optimal reward -313.12
Iteration 130 took 2.37 seconds (mean sampled reward: -3086.42). Current reward after update: -322.63, Optimal reward -313.12
Iteration 131 took 2.40 seconds (mean sampled reward: -2517.39). Current reward after update: -423.96, Optimal reward -313.12
Iteration 132 took 2.39 seconds (mean sampled reward: -2513.28). Current reward after update: -312.12, Optimal reward -312.12
Iteration 133 took 2.26 seconds (mean sampled reward: -2031.24). Current reward after update: -424.76, Optimal reward -312.12
Iteration 134 took 2.29 seconds (mean sampled reward: -2026.01). Current reward after update: -557.78, Optimal reward -312.12
Iteration 135 took 2.24 seconds (mean sampled reward: -1879.52). Current reward after update: -340.54, Optimal reward -312.12
Iteration 136 took 2.34 seconds (mean sampled reward: -3430.34). Current reward after update: -603.08, Optimal reward -312.12
Iteration 137 took 2.41 seconds (mean sampled reward: -4615.96). Current reward after update: -464.88, Optimal reward -312.12
Iteration 138 took 2.42 seconds (mean sampled reward: -4894.48). Current reward after update: -570.12, Optimal reward -312.12
Iteration 139 took 2.37 seconds (mean sampled reward: -4893.72). Current reward after update: -430.62, Optimal reward -312.12
Iteration 140 took 2.39 seconds (mean sampled reward: -4664.58). Current reward after update: -540.18, Optimal reward -312.12
Iteration 141 took 2.28 seconds (mean sampled reward: -4577.54). Current reward after update: -445.18, Optimal reward -312.12
Iteration 142 took 2.27 seconds (mean sampled reward: -4561.41). Current reward after update: -427.85, Optimal reward -312.12
Iteration 143 took 2.29 seconds (mean sampled reward: -3591.05). Current reward after update: -431.60, Optimal reward -312.12
Iteration 144 took 2.26 seconds (mean sampled reward: -3335.49). Current reward after update: -444.00, Optimal reward -312.12
Iteration 145 took 2.28 seconds (mean sampled reward: -3183.69). Current reward after update: -456.74, Optimal reward -312.12
Iteration 146 took 2.24 seconds (mean sampled reward: -3169.90). Current reward after update: -499.98, Optimal reward -312.12
Iteration 147 took 2.30 seconds (mean sampled reward: -4227.79). Current reward after update: -512.44, Optimal reward -312.12
Iteration 148 took 2.27 seconds (mean sampled reward: -3653.00). Current reward after update: -437.78, Optimal reward -312.12
Iteration 149 took 2.23 seconds (mean sampled reward: -5136.13). Current reward after update: -699.61, Optimal reward -312.12
Iteration 150 took 2.28 seconds (mean sampled reward: -3684.37). Current reward after update: -580.98, Optimal reward -312.12
Iteration 151 took 2.25 seconds (mean sampled reward: -2671.88). Current reward after update: -556.10, Optimal reward -312.12
Iteration 152 took 2.28 seconds (mean sampled reward: -2217.92). Current reward after update: -520.64, Optimal reward -312.12
Iteration 153 took 2.23 seconds (mean sampled reward: -2381.70). Current reward after update: -561.39, Optimal reward -312.12
Iteration 154 took 2.31 seconds (mean sampled reward: -2162.62). Current reward after update: -542.18, Optimal reward -312.12
Iteration 155 took 2.26 seconds (mean sampled reward: -2219.42). Current reward after update: -619.84, Optimal reward -312.12
Iteration 156 took 2.29 seconds (mean sampled reward: -2317.69). Current reward after update: -509.56, Optimal reward -312.12
Iteration 157 took 2.26 seconds (mean sampled reward: -2881.51). Current reward after update: -438.96, Optimal reward -312.12
Iteration 158 took 2.35 seconds (mean sampled reward: -3516.50). Current reward after update: -545.30, Optimal reward -312.12
Iteration 159 took 2.39 seconds (mean sampled reward: -3276.53). Current reward after update: -504.91, Optimal reward -312.12
Iteration 160 took 2.40 seconds (mean sampled reward: -5118.58). Current reward after update: -551.61, Optimal reward -312.12
Iteration 161 took 2.30 seconds (mean sampled reward: -3288.82). Current reward after update: -586.06, Optimal reward -312.12
Iteration 162 took 2.28 seconds (mean sampled reward: -4774.09). Current reward after update: -596.70, Optimal reward -312.12
Iteration 163 took 2.29 seconds (mean sampled reward: -2666.96). Current reward after update: -638.25, Optimal reward -312.12
Iteration 164 took 2.31 seconds (mean sampled reward: -4761.81). Current reward after update: -699.66, Optimal reward -312.12
Iteration 165 took 2.27 seconds (mean sampled reward: -4717.84). Current reward after update: -1003.77, Optimal reward -312.12
Iteration 166 took 2.18 seconds (mean sampled reward: -4797.73). Current reward after update: -956.45, Optimal reward -312.12
Iteration 167 took 2.21 seconds (mean sampled reward: -4992.41). Current reward after update: -855.25, Optimal reward -312.12
Iteration 168 took 2.27 seconds (mean sampled reward: -4247.10). Current reward after update: -698.98, Optimal reward -312.12
Iteration 169 took 2.41 seconds (mean sampled reward: -2435.49). Current reward after update: -473.30, Optimal reward -312.12
Iteration 170 took 2.21 seconds (mean sampled reward: -4420.31). Current reward after update: -442.91, Optimal reward -312.12
Iteration 171 took 2.28 seconds (mean sampled reward: -3439.52). Current reward after update: -493.55, Optimal reward -312.12
Iteration 172 took 2.25 seconds (mean sampled reward: -4361.79). Current reward after update: -542.83, Optimal reward -312.12
Iteration 173 took 2.30 seconds (mean sampled reward: -3294.90). Current reward after update: -522.77, Optimal reward -312.12
Iteration 174 took 2.15 seconds (mean sampled reward: -4526.43). Current reward after update: -460.51, Optimal reward -312.12
Iteration 175 took 2.29 seconds (mean sampled reward: -3552.03). Current reward after update: -459.41, Optimal reward -312.12
Iteration 176 took 2.21 seconds (mean sampled reward: -4927.35). Current reward after update: -765.87, Optimal reward -312.12
Iteration 177 took 2.23 seconds (mean sampled reward: -4295.51). Current reward after update: -574.45, Optimal reward -312.12
Iteration 178 took 2.24 seconds (mean sampled reward: -3563.39). Current reward after update: -510.34, Optimal reward -312.12
Iteration 179 took 2.33 seconds (mean sampled reward: -3534.10). Current reward after update: -547.22, Optimal reward -312.12
Iteration 180 took 2.21 seconds (mean sampled reward: -2994.35). Current reward after update: -666.20, Optimal reward -312.12
Iteration 181 took 2.23 seconds (mean sampled reward: -4021.05). Current reward after update: -633.86, Optimal reward -312.12
Iteration 182 took 2.18 seconds (mean sampled reward: -4330.40). Current reward after update: -527.82, Optimal reward -312.12
Iteration 183 took 2.17 seconds (mean sampled reward: -4980.71). Current reward after update: -460.47, Optimal reward -312.12
Iteration 184 took 2.22 seconds (mean sampled reward: -3757.50). Current reward after update: -449.22, Optimal reward -312.12
Iteration 185 took 2.23 seconds (mean sampled reward: -3398.68). Current reward after update: -694.53, Optimal reward -312.12
Iteration 186 took 2.22 seconds (mean sampled reward: -4546.66). Current reward after update: -701.24, Optimal reward -312.12
Iteration 187 took 2.24 seconds (mean sampled reward: -3471.11). Current reward after update: -532.82, Optimal reward -312.12
Iteration 188 took 2.22 seconds (mean sampled reward: -2939.86). Current reward after update: -537.33, Optimal reward -312.12
Iteration 189 took 2.23 seconds (mean sampled reward: -3017.66). Current reward after update: -887.83, Optimal reward -312.12
Iteration 190 took 2.30 seconds (mean sampled reward: -3089.72). Current reward after update: -843.83, Optimal reward -312.12
Iteration 191 took 2.28 seconds (mean sampled reward: -2890.22). Current reward after update: -782.53, Optimal reward -312.12
Iteration 192 took 2.23 seconds (mean sampled reward: -3545.83). Current reward after update: -542.84, Optimal reward -312.12
Iteration 193 took 2.35 seconds (mean sampled reward: -2965.33). Current reward after update: -563.41, Optimal reward -312.12
Iteration 194 took 2.29 seconds (mean sampled reward: -3322.06). Current reward after update: -681.28, Optimal reward -312.12
Iteration 195 took 2.36 seconds (mean sampled reward: -3194.11). Current reward after update: -665.59, Optimal reward -312.12
Iteration 196 took 2.39 seconds (mean sampled reward: -4098.15). Current reward after update: -587.17, Optimal reward -312.12
Iteration 197 took 2.27 seconds (mean sampled reward: -2684.39). Current reward after update: -991.68, Optimal reward -312.12
Iteration 198 took 2.33 seconds (mean sampled reward: -2292.01). Current reward after update: -601.36, Optimal reward -312.12
Iteration 199 took 2.37 seconds (mean sampled reward: -2811.77). Current reward after update: -601.15, Optimal reward -312.12
Iteration 200 took 2.24 seconds (mean sampled reward: -2510.17). Current reward after update: -606.88, Optimal reward -312.12
Iteration 1 took 2.40 seconds (mean sampled reward: -7449.79). Current reward after update: -4396.49, Optimal reward -4396.49
Iteration 2 took 2.27 seconds (mean sampled reward: -6825.58). Current reward after update: -2379.84, Optimal reward -2379.84
Iteration 3 took 2.22 seconds (mean sampled reward: -6141.56). Current reward after update: -3848.66, Optimal reward -2379.84
Iteration 4 took 2.20 seconds (mean sampled reward: -4986.53). Current reward after update: -1806.83, Optimal reward -1806.83
Iteration 5 took 2.36 seconds (mean sampled reward: -5646.23). Current reward after update: -1674.10, Optimal reward -1674.10
Iteration 6 took 2.59 seconds (mean sampled reward: -6025.70). Current reward after update: -2409.29, Optimal reward -1674.10
Iteration 7 took 2.20 seconds (mean sampled reward: -5529.42). Current reward after update: -2245.45, Optimal reward -1674.10
Iteration 8 took 2.36 seconds (mean sampled reward: -4975.40). Current reward after update: -2059.27, Optimal reward -1674.10
Iteration 9 took 2.37 seconds (mean sampled reward: -4369.73). Current reward after update: -1779.54, Optimal reward -1674.10
Iteration 10 took 2.37 seconds (mean sampled reward: -5069.67). Current reward after update: -1488.79, Optimal reward -1488.79
Iteration 11 took 2.20 seconds (mean sampled reward: -5577.55). Current reward after update: -1663.80, Optimal reward -1488.79
Iteration 12 took 2.40 seconds (mean sampled reward: -5746.13). Current reward after update: -1725.94, Optimal reward -1488.79
Iteration 13 took 2.30 seconds (mean sampled reward: -5388.00). Current reward after update: -1658.11, Optimal reward -1488.79
Iteration 14 took 2.30 seconds (mean sampled reward: -4944.54). Current reward after update: -1636.35, Optimal reward -1488.79
Iteration 15 took 2.29 seconds (mean sampled reward: -4413.52). Current reward after update: -1585.18, Optimal reward -1488.79
Iteration 16 took 2.29 seconds (mean sampled reward: -5022.27). Current reward after update: -1744.35, Optimal reward -1488.79
Iteration 17 took 2.33 seconds (mean sampled reward: -4368.27). Current reward after update: -1261.70, Optimal reward -1261.70
Iteration 18 took 2.29 seconds (mean sampled reward: -4622.59). Current reward after update: -1345.53, Optimal reward -1261.70
Iteration 19 took 2.14 seconds (mean sampled reward: -4404.91). Current reward after update: -1040.86, Optimal reward -1040.86
Iteration 20 took 2.46 seconds (mean sampled reward: -4876.73). Current reward after update: -1160.86, Optimal reward -1040.86
Iteration 21 took 2.18 seconds (mean sampled reward: -5328.99). Current reward after update: -1159.37, Optimal reward -1040.86
Iteration 22 took 2.18 seconds (mean sampled reward: -4700.62). Current reward after update: -1359.43, Optimal reward -1040.86
Iteration 23 took 2.16 seconds (mean sampled reward: -4482.83). Current reward after update: -941.87, Optimal reward -941.87
Iteration 24 took 2.13 seconds (mean sampled reward: -4961.83). Current reward after update: -1133.94, Optimal reward -941.87
Iteration 25 took 2.30 seconds (mean sampled reward: -5276.40). Current reward after update: -1017.78, Optimal reward -941.87
Iteration 26 took 2.27 seconds (mean sampled reward: -4671.97). Current reward after update: -1154.33, Optimal reward -941.87
Iteration 27 took 2.11 seconds (mean sampled reward: -5594.78). Current reward after update: -1389.71, Optimal reward -941.87
Iteration 28 took 2.27 seconds (mean sampled reward: -5642.26). Current reward after update: -1019.62, Optimal reward -941.87
Iteration 29 took 2.24 seconds (mean sampled reward: -5136.95). Current reward after update: -1135.26, Optimal reward -941.87
Iteration 30 took 2.56 seconds (mean sampled reward: -4831.07). Current reward after update: -993.14, Optimal reward -941.87
Iteration 31 took 2.53 seconds (mean sampled reward: -5586.87). Current reward after update: -872.62, Optimal reward -872.62
Iteration 32 took 2.25 seconds (mean sampled reward: -4859.31). Current reward after update: -656.97, Optimal reward -656.97
Iteration 33 took 2.28 seconds (mean sampled reward: -4502.50). Current reward after update: -757.22, Optimal reward -656.97
Iteration 34 took 2.41 seconds (mean sampled reward: -3424.20). Current reward after update: -792.85, Optimal reward -656.97
Iteration 35 took 2.36 seconds (mean sampled reward: -3889.44). Current reward after update: -869.20, Optimal reward -656.97
Iteration 36 took 2.28 seconds (mean sampled reward: -3585.39). Current reward after update: -800.54, Optimal reward -656.97
Iteration 37 took 2.27 seconds (mean sampled reward: -4965.99). Current reward after update: -799.31, Optimal reward -656.97
Iteration 38 took 2.41 seconds (mean sampled reward: -4485.78). Current reward after update: -656.79, Optimal reward -656.79
Iteration 39 took 2.36 seconds (mean sampled reward: -6050.56). Current reward after update: -764.49, Optimal reward -656.79
Iteration 40 took 2.27 seconds (mean sampled reward: -4812.89). Current reward after update: -751.40, Optimal reward -656.79
Iteration 41 took 2.31 seconds (mean sampled reward: -5488.18). Current reward after update: -997.89, Optimal reward -656.79
Iteration 42 took 2.30 seconds (mean sampled reward: -5541.38). Current reward after update: -1007.77, Optimal reward -656.79
Iteration 43 took 2.39 seconds (mean sampled reward: -4854.13). Current reward after update: -903.33, Optimal reward -656.79
Iteration 44 took 2.38 seconds (mean sampled reward: -4813.07). Current reward after update: -967.23, Optimal reward -656.79
Iteration 45 took 2.31 seconds (mean sampled reward: -4869.28). Current reward after update: -769.75, Optimal reward -656.79
Iteration 46 took 2.36 seconds (mean sampled reward: -4633.65). Current reward after update: -919.52, Optimal reward -656.79
Iteration 47 took 2.30 seconds (mean sampled reward: -4388.86). Current reward after update: -759.97, Optimal reward -656.79
Iteration 48 took 2.30 seconds (mean sampled reward: -4810.59). Current reward after update: -737.68, Optimal reward -656.79
Iteration 49 took 2.33 seconds (mean sampled reward: -3680.96). Current reward after update: -953.51, Optimal reward -656.79
Iteration 50 took 2.37 seconds (mean sampled reward: -3452.31). Current reward after update: -729.98, Optimal reward -656.79
Iteration 51 took 2.43 seconds (mean sampled reward: -4027.40). Current reward after update: -704.52, Optimal reward -656.79
Iteration 52 took 2.32 seconds (mean sampled reward: -4613.69). Current reward after update: -721.85, Optimal reward -656.79
Iteration 53 took 2.33 seconds (mean sampled reward: -5468.40). Current reward after update: -819.75, Optimal reward -656.79
Iteration 54 took 2.36 seconds (mean sampled reward: -5585.16). Current reward after update: -803.30, Optimal reward -656.79
Iteration 55 took 2.54 seconds (mean sampled reward: -5471.96). Current reward after update: -707.77, Optimal reward -656.79
Iteration 56 took 2.33 seconds (mean sampled reward: -5128.69). Current reward after update: -755.44, Optimal reward -656.79
Iteration 57 took 2.38 seconds (mean sampled reward: -5163.10). Current reward after update: -982.22, Optimal reward -656.79
Iteration 58 took 2.36 seconds (mean sampled reward: -3893.65). Current reward after update: -677.91, Optimal reward -656.79
Iteration 59 took 2.27 seconds (mean sampled reward: -5464.91). Current reward after update: -705.48, Optimal reward -656.79
Iteration 60 took 2.37 seconds (mean sampled reward: -5334.98). Current reward after update: -619.35, Optimal reward -619.35
Iteration 61 took 2.30 seconds (mean sampled reward: -4732.59). Current reward after update: -750.76, Optimal reward -619.35
Iteration 62 took 2.30 seconds (mean sampled reward: -4730.45). Current reward after update: -591.66, Optimal reward -591.66
Iteration 63 took 2.31 seconds (mean sampled reward: -4362.22). Current reward after update: -7020.21, Optimal reward -591.66
Iteration 64 took 2.31 seconds (mean sampled reward: -5447.85). Current reward after update: -878.87, Optimal reward -591.66
Iteration 65 took 2.40 seconds (mean sampled reward: -5145.16). Current reward after update: -743.99, Optimal reward -591.66
Iteration 66 took 2.29 seconds (mean sampled reward: -4770.57). Current reward after update: -693.25, Optimal reward -591.66
Iteration 67 took 2.30 seconds (mean sampled reward: -4103.41). Current reward after update: -589.69, Optimal reward -589.69
Iteration 68 took 2.33 seconds (mean sampled reward: -3832.45). Current reward after update: -586.99, Optimal reward -586.99
Iteration 69 took 2.28 seconds (mean sampled reward: -4562.72). Current reward after update: -1723.62, Optimal reward -586.99
Iteration 70 took 2.30 seconds (mean sampled reward: -4396.12). Current reward after update: -900.83, Optimal reward -586.99
Iteration 71 took 2.28 seconds (mean sampled reward: -4247.92). Current reward after update: -1044.65, Optimal reward -586.99
Iteration 72 took 2.32 seconds (mean sampled reward: -5175.88). Current reward after update: -581.16, Optimal reward -581.16
Iteration 73 took 2.31 seconds (mean sampled reward: -4472.21). Current reward after update: -581.84, Optimal reward -581.16
Iteration 74 took 2.27 seconds (mean sampled reward: -5228.85). Current reward after update: -710.57, Optimal reward -581.16
Iteration 75 took 2.29 seconds (mean sampled reward: -4508.12). Current reward after update: -645.95, Optimal reward -581.16
Iteration 76 took 2.29 seconds (mean sampled reward: -5518.61). Current reward after update: -653.88, Optimal reward -581.16
Iteration 77 took 2.25 seconds (mean sampled reward: -5755.18). Current reward after update: -7239.36, Optimal reward -581.16
Iteration 78 took 2.33 seconds (mean sampled reward: -5241.92). Current reward after update: -604.90, Optimal reward -581.16
Iteration 79 took 2.25 seconds (mean sampled reward: -6073.44). Current reward after update: -685.56, Optimal reward -581.16
Iteration 80 took 2.39 seconds (mean sampled reward: -4629.48). Current reward after update: -663.30, Optimal reward -581.16
Iteration 81 took 2.32 seconds (mean sampled reward: -3122.81). Current reward after update: -535.28, Optimal reward -535.28
Iteration 82 took 2.26 seconds (mean sampled reward: -3345.92). Current reward after update: -1140.44, Optimal reward -535.28
Iteration 83 took 2.30 seconds (mean sampled reward: -3625.14). Current reward after update: -7308.44, Optimal reward -535.28
Iteration 84 took 2.26 seconds (mean sampled reward: -4444.82). Current reward after update: -554.38, Optimal reward -535.28
Iteration 85 took 2.26 seconds (mean sampled reward: -4426.87). Current reward after update: -631.87, Optimal reward -535.28
Iteration 86 took 2.28 seconds (mean sampled reward: -5215.32). Current reward after update: -2070.01, Optimal reward -535.28
Iteration 87 took 2.28 seconds (mean sampled reward: -5740.05). Current reward after update: -466.02, Optimal reward -466.02
Iteration 88 took 2.28 seconds (mean sampled reward: -4968.40). Current reward after update: -622.15, Optimal reward -466.02
Iteration 89 took 2.32 seconds (mean sampled reward: -4616.82). Current reward after update: -598.77, Optimal reward -466.02
Iteration 90 took 2.28 seconds (mean sampled reward: -4616.87). Current reward after update: -564.60, Optimal reward -466.02
Iteration 91 took 2.26 seconds (mean sampled reward: -3363.24). Current reward after update: -680.86, Optimal reward -466.02
Iteration 92 took 2.26 seconds (mean sampled reward: -3209.90). Current reward after update: -550.04, Optimal reward -466.02
Iteration 93 took 2.32 seconds (mean sampled reward: -2693.19). Current reward after update: -1685.59, Optimal reward -466.02
Iteration 94 took 2.29 seconds (mean sampled reward: -3436.57). Current reward after update: -555.81, Optimal reward -466.02
Iteration 95 took 2.32 seconds (mean sampled reward: -2245.01). Current reward after update: -445.55, Optimal reward -445.55
Iteration 96 took 2.37 seconds (mean sampled reward: -2235.05). Current reward after update: -488.22, Optimal reward -445.55
Iteration 97 took 2.32 seconds (mean sampled reward: -2288.02). Current reward after update: -428.88, Optimal reward -428.88
Iteration 98 took 2.30 seconds (mean sampled reward: -3353.30). Current reward after update: -1529.97, Optimal reward -428.88
Iteration 99 took 2.33 seconds (mean sampled reward: -2520.29). Current reward after update: -422.04, Optimal reward -422.04
Iteration 100 took 2.37 seconds (mean sampled reward: -3140.99). Current reward after update: -6954.91, Optimal reward -422.04
Iteration 101 took 2.35 seconds (mean sampled reward: -3818.60). Current reward after update: -511.64, Optimal reward -422.04
Iteration 102 took 2.43 seconds (mean sampled reward: -3436.51). Current reward after update: -483.86, Optimal reward -422.04
Iteration 103 took 2.29 seconds (mean sampled reward: -4682.73). Current reward after update: -551.31, Optimal reward -422.04
Iteration 104 took 2.31 seconds (mean sampled reward: -3266.10). Current reward after update: -485.79, Optimal reward -422.04
Iteration 105 took 2.33 seconds (mean sampled reward: -3458.32). Current reward after update: -470.61, Optimal reward -422.04
Iteration 106 took 2.37 seconds (mean sampled reward: -3414.22). Current reward after update: -416.06, Optimal reward -416.06
Iteration 107 took 2.37 seconds (mean sampled reward: -4317.19). Current reward after update: -392.48, Optimal reward -392.48
Iteration 108 took 2.31 seconds (mean sampled reward: -4250.55). Current reward after update: -424.28, Optimal reward -392.48
Iteration 109 took 2.29 seconds (mean sampled reward: -3451.01). Current reward after update: -478.71, Optimal reward -392.48
Iteration 110 took 2.44 seconds (mean sampled reward: -2509.67). Current reward after update: -385.92, Optimal reward -385.92
Iteration 111 took 2.44 seconds (mean sampled reward: -2337.75). Current reward after update: -1174.34, Optimal reward -385.92
Iteration 112 took 2.34 seconds (mean sampled reward: -2128.79). Current reward after update: -444.63, Optimal reward -385.92
Iteration 113 took 2.43 seconds (mean sampled reward: -2029.38). Current reward after update: -483.68, Optimal reward -385.92
Iteration 114 took 2.29 seconds (mean sampled reward: -2453.79). Current reward after update: -399.04, Optimal reward -385.92
Iteration 115 took 2.35 seconds (mean sampled reward: -3301.92). Current reward after update: -555.10, Optimal reward -385.92
Iteration 116 took 2.36 seconds (mean sampled reward: -3820.91). Current reward after update: -495.66, Optimal reward -385.92
Iteration 117 took 2.42 seconds (mean sampled reward: -3290.39). Current reward after update: -1697.57, Optimal reward -385.92
Iteration 118 took 2.30 seconds (mean sampled reward: -2825.43). Current reward after update: -555.18, Optimal reward -385.92
Iteration 119 took 2.31 seconds (mean sampled reward: -3309.72). Current reward after update: -563.98, Optimal reward -385.92
Iteration 120 took 2.31 seconds (mean sampled reward: -2756.77). Current reward after update: -605.86, Optimal reward -385.92
Iteration 121 took 2.32 seconds (mean sampled reward: -3193.88). Current reward after update: -680.01, Optimal reward -385.92
Iteration 122 took 2.33 seconds (mean sampled reward: -3092.50). Current reward after update: -699.75, Optimal reward -385.92
Iteration 123 took 2.31 seconds (mean sampled reward: -3360.64). Current reward after update: -593.45, Optimal reward -385.92
Iteration 124 took 2.34 seconds (mean sampled reward: -3913.97). Current reward after update: -658.99, Optimal reward -385.92
Iteration 125 took 2.42 seconds (mean sampled reward: -3598.65). Current reward after update: -563.46, Optimal reward -385.92
Iteration 126 took 2.32 seconds (mean sampled reward: -3339.40). Current reward after update: -607.12, Optimal reward -385.92
Iteration 127 took 2.37 seconds (mean sampled reward: -3531.14). Current reward after update: -603.80, Optimal reward -385.92
Iteration 128 took 2.41 seconds (mean sampled reward: -3247.71). Current reward after update: -614.30, Optimal reward -385.92
Iteration 129 took 2.44 seconds (mean sampled reward: -3298.37). Current reward after update: -654.53, Optimal reward -385.92
Iteration 130 took 2.28 seconds (mean sampled reward: -3474.98). Current reward after update: -599.42, Optimal reward -385.92
Iteration 131 took 2.27 seconds (mean sampled reward: -5060.37). Current reward after update: -6905.86, Optimal reward -385.92
Iteration 132 took 2.29 seconds (mean sampled reward: -4735.16). Current reward after update: -728.83, Optimal reward -385.92
Iteration 133 took 2.31 seconds (mean sampled reward: -3730.10). Current reward after update: -527.92, Optimal reward -385.92
Iteration 134 took 2.29 seconds (mean sampled reward: -4690.69). Current reward after update: -6983.74, Optimal reward -385.92
Iteration 135 took 2.27 seconds (mean sampled reward: -5023.01). Current reward after update: -7261.39, Optimal reward -385.92
Iteration 136 took 2.28 seconds (mean sampled reward: -5062.59). Current reward after update: -1193.11, Optimal reward -385.92
Iteration 137 took 2.33 seconds (mean sampled reward: -3584.03). Current reward after update: -498.72, Optimal reward -385.92
Iteration 138 took 2.29 seconds (mean sampled reward: -4891.66). Current reward after update: -574.20, Optimal reward -385.92
Iteration 139 took 2.28 seconds (mean sampled reward: -4545.12). Current reward after update: -678.64, Optimal reward -385.92
Iteration 140 took 2.30 seconds (mean sampled reward: -4350.24). Current reward after update: -1756.38, Optimal reward -385.92
Iteration 141 took 2.31 seconds (mean sampled reward: -4397.37). Current reward after update: -597.17, Optimal reward -385.92
Iteration 142 took 2.30 seconds (mean sampled reward: -2761.41). Current reward after update: -551.18, Optimal reward -385.92
Iteration 143 took 2.38 seconds (mean sampled reward: -2423.04). Current reward after update: -7301.55, Optimal reward -385.92
Iteration 144 took 2.31 seconds (mean sampled reward: -2769.55). Current reward after update: -1707.65, Optimal reward -385.92
Iteration 145 took 2.29 seconds (mean sampled reward: -4070.16). Current reward after update: -611.22, Optimal reward -385.92
Iteration 146 took 2.32 seconds (mean sampled reward: -3159.50). Current reward after update: -649.34, Optimal reward -385.92
Iteration 147 took 2.29 seconds (mean sampled reward: -3787.74). Current reward after update: -607.08, Optimal reward -385.92
Iteration 148 took 2.28 seconds (mean sampled reward: -4051.15). Current reward after update: -720.82, Optimal reward -385.92
Iteration 149 took 2.31 seconds (mean sampled reward: -2499.77). Current reward after update: -567.09, Optimal reward -385.92
Iteration 150 took 2.43 seconds (mean sampled reward: -3348.33). Current reward after update: -584.63, Optimal reward -385.92
Iteration 151 took 2.34 seconds (mean sampled reward: -2386.70). Current reward after update: -588.18, Optimal reward -385.92
Iteration 152 took 2.34 seconds (mean sampled reward: -2262.96). Current reward after update: -610.82, Optimal reward -385.92
Iteration 153 took 2.36 seconds (mean sampled reward: -1974.09). Current reward after update: -1594.02, Optimal reward -385.92
Iteration 154 took 2.33 seconds (mean sampled reward: -1737.17). Current reward after update: -560.26, Optimal reward -385.92
Iteration 155 took 2.35 seconds (mean sampled reward: -2147.61). Current reward after update: -580.26, Optimal reward -385.92
Iteration 156 took 2.40 seconds (mean sampled reward: -1729.03). Current reward after update: -583.81, Optimal reward -385.92
Iteration 157 took 2.34 seconds (mean sampled reward: -2163.48). Current reward after update: -548.58, Optimal reward -385.92
Iteration 158 took 2.40 seconds (mean sampled reward: -1986.28). Current reward after update: -564.81, Optimal reward -385.92
Iteration 159 took 2.37 seconds (mean sampled reward: -1889.67). Current reward after update: -525.02, Optimal reward -385.92
Iteration 160 took 2.36 seconds (mean sampled reward: -2184.89). Current reward after update: -647.59, Optimal reward -385.92
Iteration 161 took 2.36 seconds (mean sampled reward: -2214.39). Current reward after update: -604.88, Optimal reward -385.92
Iteration 162 took 2.38 seconds (mean sampled reward: -2554.19). Current reward after update: -712.37, Optimal reward -385.92
Iteration 163 took 2.42 seconds (mean sampled reward: -2434.22). Current reward after update: -577.89, Optimal reward -385.92
Iteration 164 took 2.46 seconds (mean sampled reward: -2403.90). Current reward after update: -558.31, Optimal reward -385.92
Iteration 165 took 2.42 seconds (mean sampled reward: -2276.84). Current reward after update: -548.08, Optimal reward -385.92
Iteration 166 took 2.44 seconds (mean sampled reward: -1834.93). Current reward after update: -527.35, Optimal reward -385.92
Iteration 167 took 2.34 seconds (mean sampled reward: -1920.15). Current reward after update: -518.31, Optimal reward -385.92
Iteration 168 took 2.40 seconds (mean sampled reward: -2161.89). Current reward after update: -593.38, Optimal reward -385.92
Iteration 169 took 2.34 seconds (mean sampled reward: -2181.82). Current reward after update: -532.51, Optimal reward -385.92
Iteration 170 took 2.36 seconds (mean sampled reward: -2127.79). Current reward after update: -506.49, Optimal reward -385.92
Iteration 171 took 2.41 seconds (mean sampled reward: -2344.37). Current reward after update: -545.23, Optimal reward -385.92
Iteration 172 took 2.34 seconds (mean sampled reward: -2399.79). Current reward after update: -473.14, Optimal reward -385.92
Iteration 173 took 2.34 seconds (mean sampled reward: -2790.33). Current reward after update: -488.82, Optimal reward -385.92
Iteration 174 took 2.33 seconds (mean sampled reward: -2412.51). Current reward after update: -521.58, Optimal reward -385.92
Iteration 175 took 2.37 seconds (mean sampled reward: -2351.59). Current reward after update: -687.36, Optimal reward -385.92
Iteration 176 took 2.38 seconds (mean sampled reward: -1863.68). Current reward after update: -546.74, Optimal reward -385.92
Iteration 177 took 2.32 seconds (mean sampled reward: -1944.73). Current reward after update: -534.86, Optimal reward -385.92
Iteration 178 took 2.36 seconds (mean sampled reward: -2517.83). Current reward after update: -655.22, Optimal reward -385.92
Iteration 179 took 2.35 seconds (mean sampled reward: -3123.88). Current reward after update: -765.09, Optimal reward -385.92
Iteration 180 took 2.40 seconds (mean sampled reward: -2782.40). Current reward after update: -516.34, Optimal reward -385.92
Iteration 181 took 2.40 seconds (mean sampled reward: -2553.34). Current reward after update: -525.57, Optimal reward -385.92
Iteration 182 took 2.34 seconds (mean sampled reward: -2538.92). Current reward after update: -488.58, Optimal reward -385.92
Iteration 183 took 2.37 seconds (mean sampled reward: -2511.20). Current reward after update: -581.64, Optimal reward -385.92
Iteration 184 took 2.35 seconds (mean sampled reward: -3088.43). Current reward after update: -727.22, Optimal reward -385.92
Iteration 185 took 2.37 seconds (mean sampled reward: -2755.49). Current reward after update: -786.35, Optimal reward -385.92
Iteration 186 took 2.38 seconds (mean sampled reward: -2583.33). Current reward after update: -1903.74, Optimal reward -385.92
Iteration 187 took 2.38 seconds (mean sampled reward: -2254.55). Current reward after update: -598.93, Optimal reward -385.92
Iteration 188 took 2.40 seconds (mean sampled reward: -2075.42). Current reward after update: -1626.27, Optimal reward -385.92
Iteration 189 took 2.35 seconds (mean sampled reward: -2109.40). Current reward after update: -587.32, Optimal reward -385.92
Iteration 190 took 2.33 seconds (mean sampled reward: -2189.85). Current reward after update: -556.52, Optimal reward -385.92
Iteration 191 took 2.35 seconds (mean sampled reward: -2005.49). Current reward after update: -603.99, Optimal reward -385.92
Iteration 192 took 2.36 seconds (mean sampled reward: -2015.21). Current reward after update: -668.78, Optimal reward -385.92
Iteration 193 took 2.35 seconds (mean sampled reward: -2106.46). Current reward after update: -575.89, Optimal reward -385.92
Iteration 194 took 2.34 seconds (mean sampled reward: -2150.70). Current reward after update: -988.22, Optimal reward -385.92
Iteration 195 took 2.37 seconds (mean sampled reward: -2510.33). Current reward after update: -1058.01, Optimal reward -385.92
Iteration 196 took 2.35 seconds (mean sampled reward: -2604.30). Current reward after update: -617.10, Optimal reward -385.92
Iteration 197 took 2.32 seconds (mean sampled reward: -2536.07). Current reward after update: -721.33, Optimal reward -385.92
Iteration 198 took 2.33 seconds (mean sampled reward: -2479.12). Current reward after update: -636.13, Optimal reward -385.92
Iteration 199 took 2.35 seconds (mean sampled reward: -2440.08). Current reward after update: -591.69, Optimal reward -385.92
Iteration 200 took 2.37 seconds (mean sampled reward: -2911.42). Current reward after update: -589.58, Optimal reward -385.92
Iteration 1 took 2.43 seconds (mean sampled reward: -7417.77). Current reward after update: -4095.24, Optimal reward -4095.24
Iteration 2 took 2.32 seconds (mean sampled reward: -6404.88). Current reward after update: -2939.56, Optimal reward -2939.56
Iteration 3 took 2.25 seconds (mean sampled reward: -5988.55). Current reward after update: -2796.92, Optimal reward -2796.92
Iteration 4 took 2.35 seconds (mean sampled reward: -6045.25). Current reward after update: -2406.08, Optimal reward -2406.08
Iteration 5 took 2.31 seconds (mean sampled reward: -5485.64). Current reward after update: -2042.63, Optimal reward -2042.63
Iteration 6 took 2.22 seconds (mean sampled reward: -5332.47). Current reward after update: -1836.27, Optimal reward -1836.27
Iteration 7 took 2.15 seconds (mean sampled reward: -5644.40). Current reward after update: -1313.78, Optimal reward -1313.78
Iteration 8 took 2.24 seconds (mean sampled reward: -5263.78). Current reward after update: -1610.11, Optimal reward -1313.78
Iteration 9 took 2.14 seconds (mean sampled reward: -5259.37). Current reward after update: -1795.87, Optimal reward -1313.78
Iteration 10 took 2.26 seconds (mean sampled reward: -5906.64). Current reward after update: -1344.67, Optimal reward -1313.78
Iteration 11 took 2.32 seconds (mean sampled reward: -6102.37). Current reward after update: -1707.65, Optimal reward -1313.78
Iteration 12 took 2.24 seconds (mean sampled reward: -5594.11). Current reward after update: -1678.57, Optimal reward -1313.78
Iteration 13 took 2.35 seconds (mean sampled reward: -5044.19). Current reward after update: -1617.78, Optimal reward -1313.78
Iteration 14 took 2.22 seconds (mean sampled reward: -5306.27). Current reward after update: -1643.00, Optimal reward -1313.78
Iteration 15 took 2.44 seconds (mean sampled reward: -5856.78). Current reward after update: -1350.33, Optimal reward -1313.78
Iteration 16 took 2.27 seconds (mean sampled reward: -6108.45). Current reward after update: -1666.37, Optimal reward -1313.78
Iteration 17 took 2.46 seconds (mean sampled reward: -5609.73). Current reward after update: -1140.66, Optimal reward -1140.66
Iteration 18 took 2.31 seconds (mean sampled reward: -5084.34). Current reward after update: -885.67, Optimal reward -885.67
Iteration 19 took 2.28 seconds (mean sampled reward: -5342.13). Current reward after update: -931.14, Optimal reward -885.67
Iteration 20 took 2.30 seconds (mean sampled reward: -4941.96). Current reward after update: -949.06, Optimal reward -885.67
Iteration 21 took 2.53 seconds (mean sampled reward: -6061.67). Current reward after update: -943.85, Optimal reward -885.67
Iteration 22 took 2.21 seconds (mean sampled reward: -5222.56). Current reward after update: -1189.27, Optimal reward -885.67
Iteration 23 took 2.20 seconds (mean sampled reward: -5487.72). Current reward after update: -1102.17, Optimal reward -885.67
Iteration 24 took 2.22 seconds (mean sampled reward: -5323.26). Current reward after update: -1282.10, Optimal reward -885.67
Iteration 25 took 2.20 seconds (mean sampled reward: -5167.80). Current reward after update: -1082.12, Optimal reward -885.67
Iteration 26 took 2.28 seconds (mean sampled reward: -5429.85). Current reward after update: -1035.78, Optimal reward -885.67
Iteration 27 took 2.43 seconds (mean sampled reward: -5163.81). Current reward after update: -1091.82, Optimal reward -885.67
Iteration 28 took 2.25 seconds (mean sampled reward: -5931.77). Current reward after update: -993.69, Optimal reward -885.67
Iteration 29 took 2.41 seconds (mean sampled reward: -5585.92). Current reward after update: -839.30, Optimal reward -839.30
Iteration 30 took 2.35 seconds (mean sampled reward: -5575.85). Current reward after update: -5109.00, Optimal reward -839.30
Iteration 31 took 2.27 seconds (mean sampled reward: -5602.14). Current reward after update: -951.84, Optimal reward -839.30
Iteration 32 took 2.33 seconds (mean sampled reward: -5157.88). Current reward after update: -1050.04, Optimal reward -839.30
Iteration 33 took 2.26 seconds (mean sampled reward: -5349.21). Current reward after update: -859.85, Optimal reward -839.30
Iteration 34 took 2.29 seconds (mean sampled reward: -5578.87). Current reward after update: -868.68, Optimal reward -839.30
Iteration 35 took 2.26 seconds (mean sampled reward: -5080.08). Current reward after update: -1057.89, Optimal reward -839.30
Iteration 36 took 2.31 seconds (mean sampled reward: -5346.63). Current reward after update: -1067.45, Optimal reward -839.30
Iteration 37 took 2.33 seconds (mean sampled reward: -5485.99). Current reward after update: -842.86, Optimal reward -839.30
Iteration 38 took 2.28 seconds (mean sampled reward: -5979.77). Current reward after update: -1513.90, Optimal reward -839.30
Iteration 39 took 2.28 seconds (mean sampled reward: -6069.23). Current reward after update: -956.18, Optimal reward -839.30
Iteration 40 took 2.27 seconds (mean sampled reward: -5392.28). Current reward after update: -1055.53, Optimal reward -839.30
Iteration 41 took 2.40 seconds (mean sampled reward: -5496.51). Current reward after update: -2158.61, Optimal reward -839.30
Iteration 42 took 2.27 seconds (mean sampled reward: -5019.80). Current reward after update: -1171.89, Optimal reward -839.30
Iteration 43 took 2.44 seconds (mean sampled reward: -5207.77). Current reward after update: -905.78, Optimal reward -839.30
Iteration 44 took 2.25 seconds (mean sampled reward: -4972.13). Current reward after update: -807.11, Optimal reward -807.11
Iteration 45 took 2.31 seconds (mean sampled reward: -5111.59). Current reward after update: -761.17, Optimal reward -761.17
Iteration 46 took 2.23 seconds (mean sampled reward: -5199.46). Current reward after update: -6098.85, Optimal reward -761.17
Iteration 47 took 2.29 seconds (mean sampled reward: -5477.91). Current reward after update: -760.05, Optimal reward -760.05
Iteration 48 took 2.27 seconds (mean sampled reward: -5072.07). Current reward after update: -1050.65, Optimal reward -760.05
Iteration 49 took 2.27 seconds (mean sampled reward: -5027.94). Current reward after update: -1105.04, Optimal reward -760.05
Iteration 50 took 2.34 seconds (mean sampled reward: -5248.45). Current reward after update: -711.83, Optimal reward -711.83
Iteration 51 took 2.30 seconds (mean sampled reward: -4791.13). Current reward after update: -931.43, Optimal reward -711.83
Iteration 52 took 2.24 seconds (mean sampled reward: -4862.26). Current reward after update: -1190.03, Optimal reward -711.83
Iteration 53 took 2.38 seconds (mean sampled reward: -4602.53). Current reward after update: -1173.79, Optimal reward -711.83
Iteration 54 took 2.30 seconds (mean sampled reward: -5320.08). Current reward after update: -1331.87, Optimal reward -711.83
Iteration 55 took 2.57 seconds (mean sampled reward: -5856.11). Current reward after update: -1432.41, Optimal reward -711.83
Iteration 56 took 2.43 seconds (mean sampled reward: -6030.73). Current reward after update: -1219.19, Optimal reward -711.83
Iteration 57 took 2.37 seconds (mean sampled reward: -5986.67). Current reward after update: -1124.18, Optimal reward -711.83
Iteration 58 took 2.27 seconds (mean sampled reward: -6319.48). Current reward after update: -1033.73, Optimal reward -711.83
Iteration 59 took 2.27 seconds (mean sampled reward: -6157.90). Current reward after update: -1274.19, Optimal reward -711.83
Iteration 60 took 2.33 seconds (mean sampled reward: -5161.00). Current reward after update: -903.19, Optimal reward -711.83
Iteration 61 took 2.46 seconds (mean sampled reward: -5078.04). Current reward after update: -1016.72, Optimal reward -711.83
Iteration 62 took 2.40 seconds (mean sampled reward: -5556.90). Current reward after update: -978.54, Optimal reward -711.83
Iteration 63 took 2.35 seconds (mean sampled reward: -6178.11). Current reward after update: -1042.50, Optimal reward -711.83
Iteration 64 took 2.25 seconds (mean sampled reward: -5919.35). Current reward after update: -1090.85, Optimal reward -711.83
Iteration 65 took 2.38 seconds (mean sampled reward: -5822.20). Current reward after update: -1042.23, Optimal reward -711.83
Iteration 66 took 2.24 seconds (mean sampled reward: -5195.61). Current reward after update: -916.68, Optimal reward -711.83
Iteration 67 took 2.33 seconds (mean sampled reward: -5363.87). Current reward after update: -910.41, Optimal reward -711.83
Iteration 68 took 2.32 seconds (mean sampled reward: -5707.05). Current reward after update: -926.19, Optimal reward -711.83
Iteration 69 took 2.34 seconds (mean sampled reward: -5843.45). Current reward after update: -747.56, Optimal reward -711.83
Iteration 70 took 2.41 seconds (mean sampled reward: -6180.53). Current reward after update: -942.85, Optimal reward -711.83
Iteration 71 took 2.27 seconds (mean sampled reward: -5643.07). Current reward after update: -696.14, Optimal reward -696.14
Iteration 72 took 2.35 seconds (mean sampled reward: -6571.36). Current reward after update: -1283.88, Optimal reward -696.14
Iteration 73 took 2.26 seconds (mean sampled reward: -6596.14). Current reward after update: -2221.02, Optimal reward -696.14
Iteration 74 took 2.25 seconds (mean sampled reward: -5975.93). Current reward after update: -1107.90, Optimal reward -696.14
Iteration 75 took 2.23 seconds (mean sampled reward: -5418.10). Current reward after update: -658.67, Optimal reward -658.67
Iteration 76 took 2.27 seconds (mean sampled reward: -5696.04). Current reward after update: -637.46, Optimal reward -637.46
Iteration 77 took 2.24 seconds (mean sampled reward: -5371.09). Current reward after update: -903.59, Optimal reward -637.46
Iteration 78 took 2.32 seconds (mean sampled reward: -5568.01). Current reward after update: -873.89, Optimal reward -637.46
Iteration 79 took 2.31 seconds (mean sampled reward: -5857.49). Current reward after update: -965.20, Optimal reward -637.46
Iteration 80 took 2.25 seconds (mean sampled reward: -5481.33). Current reward after update: -905.42, Optimal reward -637.46
Iteration 81 took 2.22 seconds (mean sampled reward: -4885.12). Current reward after update: -782.89, Optimal reward -637.46
Iteration 82 took 2.29 seconds (mean sampled reward: -5009.66). Current reward after update: -756.63, Optimal reward -637.46
Iteration 83 took 2.26 seconds (mean sampled reward: -5852.50). Current reward after update: -796.03, Optimal reward -637.46
Iteration 84 took 2.23 seconds (mean sampled reward: -5391.03). Current reward after update: -524.36, Optimal reward -524.36
Iteration 85 took 2.39 seconds (mean sampled reward: -4696.34). Current reward after update: -664.09, Optimal reward -524.36
Iteration 86 took 2.23 seconds (mean sampled reward: -4699.62). Current reward after update: -753.09, Optimal reward -524.36
Iteration 87 took 2.38 seconds (mean sampled reward: -4824.18). Current reward after update: -799.19, Optimal reward -524.36
Iteration 88 took 2.23 seconds (mean sampled reward: -4639.78). Current reward after update: -609.90, Optimal reward -524.36
Iteration 89 took 2.20 seconds (mean sampled reward: -4636.84). Current reward after update: -665.15, Optimal reward -524.36
Iteration 90 took 2.24 seconds (mean sampled reward: -4475.72). Current reward after update: -559.19, Optimal reward -524.36
Iteration 91 took 2.34 seconds (mean sampled reward: -4353.65). Current reward after update: -675.20, Optimal reward -524.36
Iteration 92 took 2.36 seconds (mean sampled reward: -5705.38). Current reward after update: -743.90, Optimal reward -524.36
Iteration 93 took 2.22 seconds (mean sampled reward: -5704.99). Current reward after update: -990.24, Optimal reward -524.36
Iteration 94 took 2.24 seconds (mean sampled reward: -5882.88). Current reward after update: -645.07, Optimal reward -524.36
Iteration 95 took 2.29 seconds (mean sampled reward: -5474.67). Current reward after update: -955.98, Optimal reward -524.36
Iteration 96 took 2.25 seconds (mean sampled reward: -5787.45). Current reward after update: -928.58, Optimal reward -524.36
Iteration 97 took 2.31 seconds (mean sampled reward: -5675.76). Current reward after update: -981.41, Optimal reward -524.36
Iteration 98 took 2.22 seconds (mean sampled reward: -4643.14). Current reward after update: -872.61, Optimal reward -524.36
Iteration 99 took 2.22 seconds (mean sampled reward: -5545.03). Current reward after update: -648.98, Optimal reward -524.36
Iteration 100 took 2.21 seconds (mean sampled reward: -5399.46). Current reward after update: -986.53, Optimal reward -524.36
Iteration 101 took 2.24 seconds (mean sampled reward: -5588.88). Current reward after update: -1039.20, Optimal reward -524.36
Iteration 102 took 2.25 seconds (mean sampled reward: -5891.00). Current reward after update: -915.23, Optimal reward -524.36
Iteration 103 took 2.32 seconds (mean sampled reward: -5081.66). Current reward after update: -752.69, Optimal reward -524.36
Iteration 104 took 2.35 seconds (mean sampled reward: -5274.15). Current reward after update: -1879.27, Optimal reward -524.36
Iteration 105 took 2.32 seconds (mean sampled reward: -5222.00). Current reward after update: -1247.32, Optimal reward -524.36
Iteration 106 took 2.35 seconds (mean sampled reward: -4876.75). Current reward after update: -696.77, Optimal reward -524.36
Iteration 107 took 2.24 seconds (mean sampled reward: -4958.77). Current reward after update: -1061.52, Optimal reward -524.36
Iteration 108 took 2.52 seconds (mean sampled reward: -5199.85). Current reward after update: -891.28, Optimal reward -524.36
Iteration 109 took 2.26 seconds (mean sampled reward: -5138.02). Current reward after update: -820.92, Optimal reward -524.36
Iteration 110 took 2.31 seconds (mean sampled reward: -5072.32). Current reward after update: -1036.45, Optimal reward -524.36
Iteration 111 took 2.25 seconds (mean sampled reward: -5021.47). Current reward after update: -1131.98, Optimal reward -524.36
Iteration 112 took 2.21 seconds (mean sampled reward: -5432.71). Current reward after update: -1035.19, Optimal reward -524.36
Iteration 113 took 2.34 seconds (mean sampled reward: -5770.30). Current reward after update: -1699.19, Optimal reward -524.36
Iteration 114 took 2.37 seconds (mean sampled reward: -5865.65). Current reward after update: -819.33, Optimal reward -524.36
Iteration 115 took 2.33 seconds (mean sampled reward: -5650.56). Current reward after update: -2357.59, Optimal reward -524.36
Iteration 116 took 2.23 seconds (mean sampled reward: -5070.09). Current reward after update: -1364.68, Optimal reward -524.36
Iteration 117 took 2.19 seconds (mean sampled reward: -5558.56). Current reward after update: -1225.49, Optimal reward -524.36
Iteration 118 took 2.19 seconds (mean sampled reward: -5920.36). Current reward after update: -943.67, Optimal reward -524.36
Iteration 119 took 2.17 seconds (mean sampled reward: -5088.13). Current reward after update: -948.80, Optimal reward -524.36
Iteration 120 took 2.20 seconds (mean sampled reward: -5178.45). Current reward after update: -1115.78, Optimal reward -524.36
Iteration 121 took 2.29 seconds (mean sampled reward: -5304.16). Current reward after update: -1213.52, Optimal reward -524.36
Iteration 122 took 2.19 seconds (mean sampled reward: -5392.88). Current reward after update: -927.72, Optimal reward -524.36
Iteration 123 took 2.24 seconds (mean sampled reward: -5410.94). Current reward after update: -1054.97, Optimal reward -524.36
Iteration 124 took 2.20 seconds (mean sampled reward: -5650.33). Current reward after update: -1012.19, Optimal reward -524.36
Iteration 125 took 2.22 seconds (mean sampled reward: -5403.77). Current reward after update: -1033.17, Optimal reward -524.36
Iteration 126 took 2.21 seconds (mean sampled reward: -5067.92). Current reward after update: -898.61, Optimal reward -524.36
Iteration 127 took 2.20 seconds (mean sampled reward: -5523.94). Current reward after update: -1705.39, Optimal reward -524.36
Iteration 128 took 2.22 seconds (mean sampled reward: -5971.39). Current reward after update: -1180.94, Optimal reward -524.36
Iteration 129 took 2.23 seconds (mean sampled reward: -5624.23). Current reward after update: -758.37, Optimal reward -524.36
Iteration 130 took 2.27 seconds (mean sampled reward: -5285.93). Current reward after update: -764.56, Optimal reward -524.36
Iteration 131 took 2.25 seconds (mean sampled reward: -5918.85). Current reward after update: -1018.03, Optimal reward -524.36
Iteration 132 took 2.27 seconds (mean sampled reward: -5641.36). Current reward after update: -896.90, Optimal reward -524.36
Iteration 133 took 2.40 seconds (mean sampled reward: -5229.52). Current reward after update: -1229.20, Optimal reward -524.36
Iteration 134 took 2.27 seconds (mean sampled reward: -5214.22). Current reward after update: -1017.10, Optimal reward -524.36
Iteration 135 took 2.35 seconds (mean sampled reward: -5292.36). Current reward after update: -733.54, Optimal reward -524.36
Iteration 136 took 2.26 seconds (mean sampled reward: -5538.08). Current reward after update: -1088.22, Optimal reward -524.36
Iteration 137 took 2.25 seconds (mean sampled reward: -5036.88). Current reward after update: -1110.27, Optimal reward -524.36
Iteration 138 took 2.29 seconds (mean sampled reward: -5439.49). Current reward after update: -879.98, Optimal reward -524.36
Iteration 139 took 2.29 seconds (mean sampled reward: -5262.02). Current reward after update: -990.60, Optimal reward -524.36
Iteration 140 took 2.40 seconds (mean sampled reward: -5506.35). Current reward after update: -985.82, Optimal reward -524.36
Iteration 141 took 2.32 seconds (mean sampled reward: -5515.39). Current reward after update: -974.32, Optimal reward -524.36
Iteration 142 took 2.30 seconds (mean sampled reward: -5261.17). Current reward after update: -881.28, Optimal reward -524.36
Iteration 143 took 2.41 seconds (mean sampled reward: -5908.60). Current reward after update: -996.27, Optimal reward -524.36
Iteration 144 took 2.28 seconds (mean sampled reward: -5053.04). Current reward after update: -785.48, Optimal reward -524.36
Iteration 145 took 2.31 seconds (mean sampled reward: -4428.37). Current reward after update: -960.96, Optimal reward -524.36
Iteration 146 took 2.41 seconds (mean sampled reward: -5377.39). Current reward after update: -915.43, Optimal reward -524.36
Iteration 147 took 2.36 seconds (mean sampled reward: -5647.99). Current reward after update: -997.46, Optimal reward -524.36
Iteration 148 took 2.34 seconds (mean sampled reward: -5845.55). Current reward after update: -964.80, Optimal reward -524.36
Iteration 149 took 2.28 seconds (mean sampled reward: -4840.08). Current reward after update: -1077.44, Optimal reward -524.36
Iteration 150 took 2.30 seconds (mean sampled reward: -4467.35). Current reward after update: -1568.87, Optimal reward -524.36
Iteration 151 took 2.30 seconds (mean sampled reward: -5531.31). Current reward after update: -1192.31, Optimal reward -524.36
Iteration 152 took 2.30 seconds (mean sampled reward: -4759.58). Current reward after update: -1293.69, Optimal reward -524.36
Iteration 153 took 2.26 seconds (mean sampled reward: -5009.89). Current reward after update: -1050.90, Optimal reward -524.36
Iteration 154 took 2.26 seconds (mean sampled reward: -5399.72). Current reward after update: -1280.10, Optimal reward -524.36
Iteration 155 took 2.25 seconds (mean sampled reward: -4439.28). Current reward after update: -1100.50, Optimal reward -524.36
Iteration 156 took 2.29 seconds (mean sampled reward: -4634.93). Current reward after update: -6919.15, Optimal reward -524.36
Iteration 157 took 2.38 seconds (mean sampled reward: -4884.82). Current reward after update: -1433.80, Optimal reward -524.36
Iteration 158 took 2.50 seconds (mean sampled reward: -4770.50). Current reward after update: -1230.66, Optimal reward -524.36
Iteration 159 took 2.37 seconds (mean sampled reward: -4748.07). Current reward after update: -1154.41, Optimal reward -524.36
Iteration 160 took 2.44 seconds (mean sampled reward: -5047.65). Current reward after update: -1189.42, Optimal reward -524.36
Iteration 161 took 2.29 seconds (mean sampled reward: -4240.46). Current reward after update: -987.69, Optimal reward -524.36
Iteration 162 took 2.39 seconds (mean sampled reward: -4202.61). Current reward after update: -1030.96, Optimal reward -524.36
Iteration 163 took 2.34 seconds (mean sampled reward: -4593.52). Current reward after update: -1387.59, Optimal reward -524.36
Iteration 164 took 2.34 seconds (mean sampled reward: -4997.79). Current reward after update: -1015.53, Optimal reward -524.36
Iteration 165 took 2.27 seconds (mean sampled reward: -4742.20). Current reward after update: -1133.94, Optimal reward -524.36
Iteration 166 took 2.32 seconds (mean sampled reward: -4285.54). Current reward after update: -1297.25, Optimal reward -524.36
Iteration 167 took 2.33 seconds (mean sampled reward: -4539.84). Current reward after update: -858.92, Optimal reward -524.36
Iteration 168 took 2.28 seconds (mean sampled reward: -4087.63). Current reward after update: -2283.42, Optimal reward -524.36
Iteration 169 took 2.29 seconds (mean sampled reward: -3647.04). Current reward after update: -1153.44, Optimal reward -524.36
Iteration 170 took 2.32 seconds (mean sampled reward: -4680.06). Current reward after update: -2361.18, Optimal reward -524.36
Iteration 171 took 2.32 seconds (mean sampled reward: -4498.13). Current reward after update: -1208.65, Optimal reward -524.36
Iteration 172 took 2.29 seconds (mean sampled reward: -4061.06). Current reward after update: -1095.98, Optimal reward -524.36
Iteration 173 took 2.26 seconds (mean sampled reward: -3627.50). Current reward after update: -766.44, Optimal reward -524.36
Iteration 174 took 2.33 seconds (mean sampled reward: -3914.72). Current reward after update: -769.43, Optimal reward -524.36
Iteration 175 took 2.31 seconds (mean sampled reward: -4178.90). Current reward after update: -1081.04, Optimal reward -524.36
Iteration 176 took 2.34 seconds (mean sampled reward: -4259.29). Current reward after update: -1574.03, Optimal reward -524.36
Iteration 177 took 2.36 seconds (mean sampled reward: -4045.07). Current reward after update: -1235.36, Optimal reward -524.36
Iteration 178 took 2.32 seconds (mean sampled reward: -4124.79). Current reward after update: -1162.95, Optimal reward -524.36
Iteration 179 took 2.33 seconds (mean sampled reward: -4268.18). Current reward after update: -899.93, Optimal reward -524.36
Iteration 180 took 2.37 seconds (mean sampled reward: -4455.80). Current reward after update: -1215.62, Optimal reward -524.36
Iteration 181 took 2.32 seconds (mean sampled reward: -3805.88). Current reward after update: -1320.82, Optimal reward -524.36
Iteration 182 took 2.30 seconds (mean sampled reward: -4260.10). Current reward after update: -2213.87, Optimal reward -524.36
Iteration 183 took 2.39 seconds (mean sampled reward: -3864.58). Current reward after update: -1092.58, Optimal reward -524.36
Iteration 184 took 2.36 seconds (mean sampled reward: -3445.59). Current reward after update: -1310.21, Optimal reward -524.36
Iteration 185 took 2.26 seconds (mean sampled reward: -3971.86). Current reward after update: -1424.18, Optimal reward -524.36
Iteration 186 took 2.26 seconds (mean sampled reward: -3633.05). Current reward after update: -1488.35, Optimal reward -524.36
Iteration 187 took 2.30 seconds (mean sampled reward: -4267.65). Current reward after update: -962.28, Optimal reward -524.36
Iteration 188 took 2.26 seconds (mean sampled reward: -3517.49). Current reward after update: -824.26, Optimal reward -524.36
Iteration 189 took 2.24 seconds (mean sampled reward: -3190.42). Current reward after update: -1123.18, Optimal reward -524.36
Iteration 190 took 2.24 seconds (mean sampled reward: -4054.48). Current reward after update: -1214.08, Optimal reward -524.36
Iteration 191 took 2.26 seconds (mean sampled reward: -3089.05). Current reward after update: -1834.45, Optimal reward -524.36
Iteration 192 took 2.28 seconds (mean sampled reward: -3770.91). Current reward after update: -2948.95, Optimal reward -524.36
Iteration 193 took 2.23 seconds (mean sampled reward: -4247.24). Current reward after update: -1122.26, Optimal reward -524.36
Iteration 194 took 2.27 seconds (mean sampled reward: -4195.71). Current reward after update: -1115.67, Optimal reward -524.36
Iteration 195 took 2.28 seconds (mean sampled reward: -4001.13). Current reward after update: -1290.52, Optimal reward -524.36
Iteration 196 took 2.38 seconds (mean sampled reward: -3345.00). Current reward after update: -1095.78, Optimal reward -524.36
Iteration 197 took 2.41 seconds (mean sampled reward: -3749.85). Current reward after update: -1152.23, Optimal reward -524.36
Iteration 198 took 2.38 seconds (mean sampled reward: -5064.58). Current reward after update: -1309.24, Optimal reward -524.36
Iteration 199 took 2.37 seconds (mean sampled reward: -5206.07). Current reward after update: -1644.99, Optimal reward -524.36
Iteration 200 took 2.34 seconds (mean sampled reward: -6234.43). Current reward after update: -1439.45, Optimal reward -524.36
Max force: 100 Sigma: 0.8 mean rewards: -407.4679270129541, best rewards:-312.1225883012718

argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
pybullet build time: Jan 28 2022 20:17:22
Iteration 1 took 3.78 seconds (mean sampled reward: -6146.39). Current reward after update: -4143.44, Optimal reward -4143.44
Iteration 2 took 3.64 seconds (mean sampled reward: -5911.58). Current reward after update: -3489.36, Optimal reward -3489.36
Iteration 3 took 3.49 seconds (mean sampled reward: -5582.97). Current reward after update: -2511.43, Optimal reward -2511.43
Iteration 4 took 3.57 seconds (mean sampled reward: -5242.03). Current reward after update: -2309.04, Optimal reward -2309.04
Iteration 5 took 3.65 seconds (mean sampled reward: -4637.86). Current reward after update: -1494.79, Optimal reward -1494.79
Iteration 6 took 3.58 seconds (mean sampled reward: -4907.37). Current reward after update: -1434.40, Optimal reward -1434.40
Iteration 7 took 3.45 seconds (mean sampled reward: -4830.27). Current reward after update: -1481.46, Optimal reward -1434.40
Iteration 8 took 3.39 seconds (mean sampled reward: -4534.30). Current reward after update: -1351.45, Optimal reward -1351.45
Iteration 9 took 3.37 seconds (mean sampled reward: -4232.11). Current reward after update: -904.52, Optimal reward -904.52
Iteration 10 took 3.48 seconds (mean sampled reward: -4766.26). Current reward after update: -807.83, Optimal reward -807.83
Iteration 11 took 3.73 seconds (mean sampled reward: -4511.95). Current reward after update: -915.20, Optimal reward -807.83
Iteration 12 took 3.38 seconds (mean sampled reward: -4055.60). Current reward after update: -782.69, Optimal reward -782.69
Iteration 13 took 3.43 seconds (mean sampled reward: -3663.35). Current reward after update: -992.11, Optimal reward -782.69
Iteration 14 took 3.42 seconds (mean sampled reward: -4164.89). Current reward after update: -960.75, Optimal reward -782.69
Iteration 15 took 3.80 seconds (mean sampled reward: -4542.66). Current reward after update: -1068.70, Optimal reward -782.69
Iteration 16 took 3.69 seconds (mean sampled reward: -4697.27). Current reward after update: -898.99, Optimal reward -782.69
Iteration 17 took 3.94 seconds (mean sampled reward: -4518.56). Current reward after update: -1178.46, Optimal reward -782.69
Iteration 18 took 3.65 seconds (mean sampled reward: -4568.61). Current reward after update: -1317.42, Optimal reward -782.69
Iteration 19 took 3.50 seconds (mean sampled reward: -4125.61). Current reward after update: -889.00, Optimal reward -782.69
Iteration 20 took 3.43 seconds (mean sampled reward: -3864.61). Current reward after update: -905.14, Optimal reward -782.69
Iteration 21 took 3.52 seconds (mean sampled reward: -4290.11). Current reward after update: -1027.42, Optimal reward -782.69
Iteration 22 took 3.45 seconds (mean sampled reward: -4128.85). Current reward after update: -876.57, Optimal reward -782.69
Iteration 23 took 3.26 seconds (mean sampled reward: -3129.08). Current reward after update: -863.77, Optimal reward -782.69
Iteration 24 took 3.62 seconds (mean sampled reward: -3881.12). Current reward after update: -726.37, Optimal reward -726.37
Iteration 25 took 3.54 seconds (mean sampled reward: -3617.42). Current reward after update: -593.20, Optimal reward -593.20
Iteration 26 took 3.50 seconds (mean sampled reward: -3983.76). Current reward after update: -742.40, Optimal reward -593.20
Iteration 27 took 3.75 seconds (mean sampled reward: -4405.25). Current reward after update: -718.27, Optimal reward -593.20
Iteration 28 took 3.92 seconds (mean sampled reward: -4311.55). Current reward after update: -663.33, Optimal reward -593.20
Iteration 29 took 3.53 seconds (mean sampled reward: -3775.77). Current reward after update: -677.44, Optimal reward -593.20
Iteration 30 took 3.40 seconds (mean sampled reward: -3354.16). Current reward after update: -806.35, Optimal reward -593.20
Iteration 31 took 3.44 seconds (mean sampled reward: -3399.87). Current reward after update: -876.88, Optimal reward -593.20
Iteration 32 took 3.43 seconds (mean sampled reward: -3597.19). Current reward after update: -797.68, Optimal reward -593.20
Iteration 33 took 3.41 seconds (mean sampled reward: -4223.46). Current reward after update: -635.94, Optimal reward -593.20
Iteration 34 took 3.46 seconds (mean sampled reward: -4108.63). Current reward after update: -648.90, Optimal reward -593.20
Iteration 35 took 3.55 seconds (mean sampled reward: -4693.33). Current reward after update: -943.81, Optimal reward -593.20
Iteration 36 took 3.49 seconds (mean sampled reward: -4124.93). Current reward after update: -594.48, Optimal reward -593.20
Iteration 37 took 3.49 seconds (mean sampled reward: -4611.27). Current reward after update: -856.86, Optimal reward -593.20
Iteration 38 took 3.48 seconds (mean sampled reward: -4380.47). Current reward after update: -859.78, Optimal reward -593.20
Iteration 39 took 3.52 seconds (mean sampled reward: -4302.55). Current reward after update: -835.70, Optimal reward -593.20
Iteration 40 took 3.43 seconds (mean sampled reward: -4693.88). Current reward after update: -773.57, Optimal reward -593.20
Iteration 41 took 3.40 seconds (mean sampled reward: -3433.65). Current reward after update: -683.77, Optimal reward -593.20
Iteration 42 took 3.41 seconds (mean sampled reward: -4942.54). Current reward after update: -716.82, Optimal reward -593.20
Iteration 43 took 3.37 seconds (mean sampled reward: -4350.54). Current reward after update: -711.04, Optimal reward -593.20
Iteration 44 took 3.36 seconds (mean sampled reward: -5004.24). Current reward after update: -642.07, Optimal reward -593.20
Iteration 45 took 3.40 seconds (mean sampled reward: -4939.04). Current reward after update: -964.56, Optimal reward -593.20
Iteration 46 took 3.35 seconds (mean sampled reward: -3674.79). Current reward after update: -541.52, Optimal reward -541.52
Iteration 47 took 3.37 seconds (mean sampled reward: -4039.16). Current reward after update: -442.63, Optimal reward -442.63
Iteration 48 took 3.44 seconds (mean sampled reward: -4907.58). Current reward after update: -572.67, Optimal reward -442.63
Iteration 49 took 3.50 seconds (mean sampled reward: -4052.50). Current reward after update: -607.39, Optimal reward -442.63
Iteration 50 took 3.49 seconds (mean sampled reward: -4014.70). Current reward after update: -978.35, Optimal reward -442.63
Iteration 51 took 3.55 seconds (mean sampled reward: -4050.81). Current reward after update: -509.43, Optimal reward -442.63
Iteration 52 took 3.43 seconds (mean sampled reward: -3966.33). Current reward after update: -409.67, Optimal reward -409.67
Iteration 53 took 3.60 seconds (mean sampled reward: -4616.59). Current reward after update: -497.82, Optimal reward -409.67
Iteration 54 took 3.35 seconds (mean sampled reward: -3563.77). Current reward after update: -506.84, Optimal reward -409.67
Iteration 55 took 3.58 seconds (mean sampled reward: -4598.69). Current reward after update: -387.61, Optimal reward -387.61
Iteration 56 took 3.72 seconds (mean sampled reward: -5056.78). Current reward after update: -556.62, Optimal reward -387.61
Iteration 57 took 3.45 seconds (mean sampled reward: -3851.61). Current reward after update: -597.07, Optimal reward -387.61
Iteration 58 took 3.59 seconds (mean sampled reward: -4733.18). Current reward after update: -674.36, Optimal reward -387.61
Iteration 59 took 3.56 seconds (mean sampled reward: -4463.74). Current reward after update: -519.97, Optimal reward -387.61
Iteration 60 took 3.54 seconds (mean sampled reward: -3680.07). Current reward after update: -504.74, Optimal reward -387.61
Iteration 61 took 3.41 seconds (mean sampled reward: -4026.24). Current reward after update: -492.84, Optimal reward -387.61
Iteration 62 took 3.45 seconds (mean sampled reward: -3632.03). Current reward after update: -535.40, Optimal reward -387.61
Iteration 63 took 3.49 seconds (mean sampled reward: -3494.49). Current reward after update: -522.54, Optimal reward -387.61
Iteration 64 took 3.49 seconds (mean sampled reward: -3578.28). Current reward after update: -485.76, Optimal reward -387.61
Iteration 65 took 3.90 seconds (mean sampled reward: -3832.74). Current reward after update: -654.75, Optimal reward -387.61
Iteration 66 took 3.53 seconds (mean sampled reward: -3091.72). Current reward after update: -408.27, Optimal reward -387.61
Iteration 67 took 3.51 seconds (mean sampled reward: -3627.80). Current reward after update: -500.26, Optimal reward -387.61
Iteration 68 took 3.35 seconds (mean sampled reward: -2385.71). Current reward after update: -426.74, Optimal reward -387.61
Iteration 69 took 3.45 seconds (mean sampled reward: -2627.03). Current reward after update: -439.60, Optimal reward -387.61
Iteration 70 took 3.64 seconds (mean sampled reward: -2890.26). Current reward after update: -363.76, Optimal reward -363.76
Iteration 71 took 3.35 seconds (mean sampled reward: -1684.61). Current reward after update: -353.59, Optimal reward -353.59
Iteration 72 took 3.69 seconds (mean sampled reward: -2785.41). Current reward after update: -364.44, Optimal reward -353.59
Iteration 73 took 3.51 seconds (mean sampled reward: -3735.03). Current reward after update: -408.59, Optimal reward -353.59
Iteration 74 took 3.83 seconds (mean sampled reward: -4484.32). Current reward after update: -445.55, Optimal reward -353.59
Iteration 75 took 3.71 seconds (mean sampled reward: -3041.30). Current reward after update: -441.80, Optimal reward -353.59
Iteration 76 took 3.59 seconds (mean sampled reward: -4505.66). Current reward after update: -849.46, Optimal reward -353.59
Iteration 77 took 3.42 seconds (mean sampled reward: -4217.23). Current reward after update: -507.32, Optimal reward -353.59
Iteration 78 took 3.46 seconds (mean sampled reward: -4288.31). Current reward after update: -540.21, Optimal reward -353.59
Iteration 79 took 3.47 seconds (mean sampled reward: -2764.56). Current reward after update: -383.19, Optimal reward -353.59
Iteration 80 took 3.40 seconds (mean sampled reward: -3797.57). Current reward after update: -523.94, Optimal reward -353.59
Iteration 81 took 3.31 seconds (mean sampled reward: -3416.55). Current reward after update: -405.83, Optimal reward -353.59
Iteration 82 took 3.27 seconds (mean sampled reward: -3662.95). Current reward after update: -438.22, Optimal reward -353.59
Iteration 83 took 3.36 seconds (mean sampled reward: -4302.77). Current reward after update: -584.64, Optimal reward -353.59
Iteration 84 took 3.49 seconds (mean sampled reward: -4864.70). Current reward after update: -609.69, Optimal reward -353.59
Iteration 85 took 3.34 seconds (mean sampled reward: -4247.10). Current reward after update: -474.82, Optimal reward -353.59
Iteration 86 took 3.29 seconds (mean sampled reward: -3472.96). Current reward after update: -537.47, Optimal reward -353.59
Iteration 87 took 3.28 seconds (mean sampled reward: -3503.48). Current reward after update: -585.69, Optimal reward -353.59
Iteration 88 took 3.49 seconds (mean sampled reward: -4642.43). Current reward after update: -527.93, Optimal reward -353.59
Iteration 89 took 3.55 seconds (mean sampled reward: -5015.70). Current reward after update: -691.02, Optimal reward -353.59
Iteration 90 took 3.39 seconds (mean sampled reward: -3975.46). Current reward after update: -586.48, Optimal reward -353.59
Iteration 91 took 3.60 seconds (mean sampled reward: -3707.54). Current reward after update: -517.75, Optimal reward -353.59
Iteration 92 took 3.47 seconds (mean sampled reward: -3110.31). Current reward after update: -593.56, Optimal reward -353.59
Iteration 93 took 3.36 seconds (mean sampled reward: -2672.48). Current reward after update: -585.73, Optimal reward -353.59
Iteration 94 took 3.27 seconds (mean sampled reward: -2447.92). Current reward after update: -436.00, Optimal reward -353.59
Iteration 95 took 3.37 seconds (mean sampled reward: -2558.01). Current reward after update: -521.32, Optimal reward -353.59
Iteration 96 took 3.54 seconds (mean sampled reward: -3995.62). Current reward after update: -506.29, Optimal reward -353.59
Iteration 97 took 3.52 seconds (mean sampled reward: -3474.44). Current reward after update: -445.72, Optimal reward -353.59
Iteration 98 took 3.47 seconds (mean sampled reward: -3149.72). Current reward after update: -447.67, Optimal reward -353.59
Iteration 99 took 3.46 seconds (mean sampled reward: -3737.00). Current reward after update: -472.42, Optimal reward -353.59
Iteration 100 took 3.50 seconds (mean sampled reward: -3621.74). Current reward after update: -442.19, Optimal reward -353.59
Iteration 101 took 3.25 seconds (mean sampled reward: -1817.48). Current reward after update: -410.82, Optimal reward -353.59
Iteration 102 took 3.30 seconds (mean sampled reward: -2093.94). Current reward after update: -441.89, Optimal reward -353.59
Iteration 103 took 3.34 seconds (mean sampled reward: -1739.77). Current reward after update: -416.04, Optimal reward -353.59
Iteration 104 took 3.20 seconds (mean sampled reward: -1713.82). Current reward after update: -457.68, Optimal reward -353.59
Iteration 105 took 3.24 seconds (mean sampled reward: -1691.82). Current reward after update: -485.86, Optimal reward -353.59
Iteration 106 took 3.37 seconds (mean sampled reward: -1702.49). Current reward after update: -412.10, Optimal reward -353.59
Iteration 107 took 3.21 seconds (mean sampled reward: -1738.44). Current reward after update: -405.39, Optimal reward -353.59
Iteration 108 took 3.52 seconds (mean sampled reward: -1594.81). Current reward after update: -396.79, Optimal reward -353.59
Iteration 109 took 3.30 seconds (mean sampled reward: -1774.51). Current reward after update: -606.94, Optimal reward -353.59
Iteration 110 took 3.20 seconds (mean sampled reward: -1792.03). Current reward after update: -444.25, Optimal reward -353.59
Iteration 111 took 3.28 seconds (mean sampled reward: -1823.42). Current reward after update: -461.84, Optimal reward -353.59
Iteration 112 took 3.23 seconds (mean sampled reward: -1593.80). Current reward after update: -473.98, Optimal reward -353.59
Iteration 113 took 3.28 seconds (mean sampled reward: -1615.56). Current reward after update: -429.46, Optimal reward -353.59
Iteration 114 took 3.25 seconds (mean sampled reward: -1626.74). Current reward after update: -420.07, Optimal reward -353.59
Iteration 115 took 3.22 seconds (mean sampled reward: -1739.45). Current reward after update: -371.74, Optimal reward -353.59
Iteration 116 took 3.19 seconds (mean sampled reward: -1727.87). Current reward after update: -461.71, Optimal reward -353.59
Iteration 117 took 3.27 seconds (mean sampled reward: -2152.54). Current reward after update: -486.42, Optimal reward -353.59
Iteration 118 took 3.31 seconds (mean sampled reward: -2042.43). Current reward after update: -397.48, Optimal reward -353.59
Iteration 119 took 3.27 seconds (mean sampled reward: -1973.73). Current reward after update: -463.33, Optimal reward -353.59
Iteration 120 took 3.52 seconds (mean sampled reward: -3733.39). Current reward after update: -388.79, Optimal reward -353.59
Iteration 121 took 3.31 seconds (mean sampled reward: -1870.68). Current reward after update: -462.87, Optimal reward -353.59
Iteration 122 took 3.42 seconds (mean sampled reward: -2819.86). Current reward after update: -460.13, Optimal reward -353.59
Iteration 123 took 3.46 seconds (mean sampled reward: -3481.95). Current reward after update: -536.08, Optimal reward -353.59
Iteration 124 took 3.51 seconds (mean sampled reward: -3753.37). Current reward after update: -552.50, Optimal reward -353.59
Iteration 125 took 3.36 seconds (mean sampled reward: -2871.05). Current reward after update: -458.44, Optimal reward -353.59
Iteration 126 took 3.56 seconds (mean sampled reward: -3408.08). Current reward after update: -401.79, Optimal reward -353.59
Iteration 127 took 3.49 seconds (mean sampled reward: -3692.06). Current reward after update: -446.50, Optimal reward -353.59
Iteration 128 took 3.63 seconds (mean sampled reward: -3897.69). Current reward after update: -467.63, Optimal reward -353.59
Iteration 129 took 3.54 seconds (mean sampled reward: -3458.68). Current reward after update: -429.42, Optimal reward -353.59
Iteration 130 took 3.53 seconds (mean sampled reward: -3686.63). Current reward after update: -514.97, Optimal reward -353.59
Iteration 131 took 3.67 seconds (mean sampled reward: -4359.22). Current reward after update: -507.13, Optimal reward -353.59
Iteration 132 took 3.71 seconds (mean sampled reward: -3884.33). Current reward after update: -399.05, Optimal reward -353.59
Iteration 133 took 3.60 seconds (mean sampled reward: -3288.47). Current reward after update: -404.59, Optimal reward -353.59
Iteration 134 took 3.59 seconds (mean sampled reward: -3264.24). Current reward after update: -438.92, Optimal reward -353.59
Iteration 135 took 3.47 seconds (mean sampled reward: -3255.06). Current reward after update: -386.87, Optimal reward -353.59
Iteration 136 took 3.56 seconds (mean sampled reward: -3884.59). Current reward after update: -454.10, Optimal reward -353.59
Iteration 137 took 3.60 seconds (mean sampled reward: -4110.31). Current reward after update: -745.32, Optimal reward -353.59
Iteration 138 took 3.46 seconds (mean sampled reward: -3444.63). Current reward after update: -497.18, Optimal reward -353.59
Iteration 139 took 3.49 seconds (mean sampled reward: -3844.30). Current reward after update: -451.44, Optimal reward -353.59
Iteration 140 took 3.62 seconds (mean sampled reward: -4307.19). Current reward after update: -474.95, Optimal reward -353.59
Iteration 141 took 3.66 seconds (mean sampled reward: -4081.67). Current reward after update: -395.82, Optimal reward -353.59
Iteration 142 took 3.67 seconds (mean sampled reward: -4346.90). Current reward after update: -462.97, Optimal reward -353.59
Iteration 143 took 3.68 seconds (mean sampled reward: -4281.04). Current reward after update: -601.47, Optimal reward -353.59
Iteration 144 took 3.66 seconds (mean sampled reward: -4374.50). Current reward after update: -467.66, Optimal reward -353.59
Iteration 145 took 3.60 seconds (mean sampled reward: -4296.01). Current reward after update: -461.65, Optimal reward -353.59
Iteration 146 took 3.63 seconds (mean sampled reward: -4650.70). Current reward after update: -479.00, Optimal reward -353.59
Iteration 147 took 3.73 seconds (mean sampled reward: -5374.67). Current reward after update: -529.86, Optimal reward -353.59
Iteration 148 took 3.72 seconds (mean sampled reward: -5138.92). Current reward after update: -406.09, Optimal reward -353.59
Iteration 149 took 3.63 seconds (mean sampled reward: -5624.16). Current reward after update: -413.26, Optimal reward -353.59
Iteration 150 took 3.65 seconds (mean sampled reward: -4534.23). Current reward after update: -356.76, Optimal reward -353.59
Iteration 151 took 3.63 seconds (mean sampled reward: -4273.28). Current reward after update: -363.20, Optimal reward -353.59
Iteration 152 took 3.47 seconds (mean sampled reward: -2877.32). Current reward after update: -331.63, Optimal reward -331.63
Iteration 153 took 3.62 seconds (mean sampled reward: -3839.07). Current reward after update: -420.03, Optimal reward -331.63
Iteration 154 took 3.50 seconds (mean sampled reward: -3254.80). Current reward after update: -413.51, Optimal reward -331.63
Iteration 155 took 3.48 seconds (mean sampled reward: -3519.88). Current reward after update: -361.24, Optimal reward -331.63
Iteration 156 took 3.37 seconds (mean sampled reward: -2492.72). Current reward after update: -437.96, Optimal reward -331.63
Iteration 157 took 3.28 seconds (mean sampled reward: -2236.44). Current reward after update: -380.02, Optimal reward -331.63
Iteration 158 took 3.27 seconds (mean sampled reward: -1885.83). Current reward after update: -513.40, Optimal reward -331.63
Iteration 159 took 3.31 seconds (mean sampled reward: -1951.76). Current reward after update: -389.25, Optimal reward -331.63
Iteration 160 took 3.40 seconds (mean sampled reward: -2234.49). Current reward after update: -422.64, Optimal reward -331.63
Iteration 161 took 3.31 seconds (mean sampled reward: -1994.18). Current reward after update: -410.18, Optimal reward -331.63
Iteration 162 took 3.38 seconds (mean sampled reward: -2016.28). Current reward after update: -349.12, Optimal reward -331.63
Iteration 163 took 3.53 seconds (mean sampled reward: -3898.49). Current reward after update: -413.04, Optimal reward -331.63
Iteration 164 took 3.73 seconds (mean sampled reward: -4444.24). Current reward after update: -458.68, Optimal reward -331.63
Iteration 165 took 3.53 seconds (mean sampled reward: -3217.62). Current reward after update: -344.51, Optimal reward -331.63
Iteration 166 took 3.51 seconds (mean sampled reward: -3311.88). Current reward after update: -338.94, Optimal reward -331.63
Iteration 167 took 3.48 seconds (mean sampled reward: -2823.90). Current reward after update: -299.71, Optimal reward -299.71
Iteration 168 took 3.57 seconds (mean sampled reward: -4066.81). Current reward after update: -330.99, Optimal reward -299.71
Iteration 169 took 3.47 seconds (mean sampled reward: -3117.70). Current reward after update: -372.10, Optimal reward -299.71
Iteration 170 took 3.55 seconds (mean sampled reward: -4216.15). Current reward after update: -474.22, Optimal reward -299.71
Iteration 171 took 3.43 seconds (mean sampled reward: -2912.31). Current reward after update: -401.22, Optimal reward -299.71
Iteration 172 took 3.57 seconds (mean sampled reward: -4184.70). Current reward after update: -389.14, Optimal reward -299.71
Iteration 173 took 3.45 seconds (mean sampled reward: -3194.98). Current reward after update: -409.74, Optimal reward -299.71
Iteration 174 took 3.50 seconds (mean sampled reward: -3779.61). Current reward after update: -387.25, Optimal reward -299.71
Iteration 175 took 3.66 seconds (mean sampled reward: -4258.58). Current reward after update: -352.29, Optimal reward -299.71
Iteration 176 took 3.66 seconds (mean sampled reward: -4691.40). Current reward after update: -515.80, Optimal reward -299.71
Iteration 177 took 3.52 seconds (mean sampled reward: -3895.29). Current reward after update: -453.74, Optimal reward -299.71
Iteration 178 took 3.61 seconds (mean sampled reward: -4246.62). Current reward after update: -554.19, Optimal reward -299.71
Iteration 179 took 3.52 seconds (mean sampled reward: -3862.56). Current reward after update: -365.83, Optimal reward -299.71
Iteration 180 took 3.60 seconds (mean sampled reward: -3977.24). Current reward after update: -378.85, Optimal reward -299.71
Iteration 181 took 3.58 seconds (mean sampled reward: -3820.78). Current reward after update: -264.66, Optimal reward -264.66
Iteration 182 took 3.49 seconds (mean sampled reward: -3422.77). Current reward after update: -258.25, Optimal reward -258.25
Iteration 183 took 3.48 seconds (mean sampled reward: -2996.93). Current reward after update: -315.50, Optimal reward -258.25
Iteration 184 took 3.49 seconds (mean sampled reward: -3185.85). Current reward after update: -278.94, Optimal reward -258.25
Iteration 185 took 3.39 seconds (mean sampled reward: -2645.64). Current reward after update: -268.45, Optimal reward -258.25
Iteration 186 took 3.37 seconds (mean sampled reward: -2546.98). Current reward after update: -291.40, Optimal reward -258.25
Iteration 187 took 3.43 seconds (mean sampled reward: -2820.04). Current reward after update: -261.22, Optimal reward -258.25
Iteration 188 took 3.45 seconds (mean sampled reward: -3273.59). Current reward after update: -266.10, Optimal reward -258.25
Iteration 189 took 3.46 seconds (mean sampled reward: -3294.80). Current reward after update: -308.44, Optimal reward -258.25
Iteration 190 took 3.54 seconds (mean sampled reward: -3953.65). Current reward after update: -341.27, Optimal reward -258.25
Iteration 191 took 3.81 seconds (mean sampled reward: -4958.33). Current reward after update: -476.49, Optimal reward -258.25
Iteration 192 took 3.75 seconds (mean sampled reward: -5026.94). Current reward after update: -442.30, Optimal reward -258.25
Iteration 193 took 3.50 seconds (mean sampled reward: -3459.60). Current reward after update: -449.47, Optimal reward -258.25
Iteration 194 took 3.52 seconds (mean sampled reward: -3552.61). Current reward after update: -420.38, Optimal reward -258.25
Iteration 195 took 3.56 seconds (mean sampled reward: -3660.68). Current reward after update: -353.37, Optimal reward -258.25
Iteration 196 took 3.50 seconds (mean sampled reward: -3645.78). Current reward after update: -366.06, Optimal reward -258.25
Iteration 197 took 3.53 seconds (mean sampled reward: -3601.65). Current reward after update: -453.59, Optimal reward -258.25
Iteration 198 took 3.47 seconds (mean sampled reward: -3341.81). Current reward after update: -307.08, Optimal reward -258.25
Iteration 199 took 3.56 seconds (mean sampled reward: -3960.86). Current reward after update: -323.53, Optimal reward -258.25
Iteration 200 took 3.68 seconds (mean sampled reward: -4005.73). Current reward after update: -353.58, Optimal reward -258.25
Iteration 201 took 3.70 seconds (mean sampled reward: -4677.07). Current reward after update: -506.89, Optimal reward -258.25
Iteration 202 took 3.77 seconds (mean sampled reward: -5101.15). Current reward after update: -488.74, Optimal reward -258.25
Iteration 203 took 3.73 seconds (mean sampled reward: -4596.79). Current reward after update: -370.08, Optimal reward -258.25
Iteration 204 took 3.76 seconds (mean sampled reward: -4667.49). Current reward after update: -404.42, Optimal reward -258.25
Iteration 205 took 3.66 seconds (mean sampled reward: -4203.17). Current reward after update: -450.41, Optimal reward -258.25
Iteration 206 took 3.66 seconds (mean sampled reward: -4717.82). Current reward after update: -472.00, Optimal reward -258.25
Iteration 207 took 3.72 seconds (mean sampled reward: -4987.49). Current reward after update: -554.26, Optimal reward -258.25
Iteration 208 took 3.50 seconds (mean sampled reward: -3247.70). Current reward after update: -358.45, Optimal reward -258.25
Iteration 209 took 3.52 seconds (mean sampled reward: -3592.48). Current reward after update: -439.58, Optimal reward -258.25
Iteration 210 took 3.59 seconds (mean sampled reward: -3604.04). Current reward after update: -426.97, Optimal reward -258.25
Iteration 211 took 3.55 seconds (mean sampled reward: -3613.72). Current reward after update: -428.63, Optimal reward -258.25
Iteration 212 took 3.79 seconds (mean sampled reward: -3346.34). Current reward after update: -522.16, Optimal reward -258.25
Iteration 213 took 3.58 seconds (mean sampled reward: -3420.79). Current reward after update: -490.74, Optimal reward -258.25
Iteration 214 took 3.76 seconds (mean sampled reward: -3867.59). Current reward after update: -444.83, Optimal reward -258.25
Iteration 215 took 3.68 seconds (mean sampled reward: -3656.85). Current reward after update: -432.00, Optimal reward -258.25
Iteration 216 took 3.68 seconds (mean sampled reward: -4024.56). Current reward after update: -478.46, Optimal reward -258.25
Iteration 217 took 3.54 seconds (mean sampled reward: -3259.72). Current reward after update: -431.50, Optimal reward -258.25
Iteration 218 took 3.61 seconds (mean sampled reward: -3853.60). Current reward after update: -402.15, Optimal reward -258.25
Iteration 219 took 3.60 seconds (mean sampled reward: -3788.16). Current reward after update: -393.56, Optimal reward -258.25
Iteration 220 took 3.80 seconds (mean sampled reward: -4378.73). Current reward after update: -581.43, Optimal reward -258.25
Iteration 221 took 3.59 seconds (mean sampled reward: -3984.51). Current reward after update: -441.09, Optimal reward -258.25
Iteration 222 took 3.55 seconds (mean sampled reward: -3515.07). Current reward after update: -422.56, Optimal reward -258.25
Iteration 223 took 3.67 seconds (mean sampled reward: -4372.42). Current reward after update: -440.14, Optimal reward -258.25
Iteration 224 took 3.53 seconds (mean sampled reward: -3851.94). Current reward after update: -487.53, Optimal reward -258.25
Iteration 225 took 4.10 seconds (mean sampled reward: -4300.40). Current reward after update: -434.55, Optimal reward -258.25
Iteration 226 took 3.47 seconds (mean sampled reward: -3166.72). Current reward after update: -477.34, Optimal reward -258.25
Iteration 227 took 3.64 seconds (mean sampled reward: -4112.96). Current reward after update: -665.75, Optimal reward -258.25
Iteration 228 took 3.53 seconds (mean sampled reward: -3404.98). Current reward after update: -508.06, Optimal reward -258.25
Iteration 229 took 3.53 seconds (mean sampled reward: -3458.19). Current reward after update: -517.73, Optimal reward -258.25
Iteration 230 took 3.57 seconds (mean sampled reward: -3148.64). Current reward after update: -400.91, Optimal reward -258.25
Iteration 231 took 3.51 seconds (mean sampled reward: -3265.45). Current reward after update: -441.71, Optimal reward -258.25
Iteration 232 took 3.65 seconds (mean sampled reward: -4079.02). Current reward after update: -402.56, Optimal reward -258.25
Iteration 233 took 3.66 seconds (mean sampled reward: -3805.91). Current reward after update: -500.87, Optimal reward -258.25
Iteration 234 took 3.64 seconds (mean sampled reward: -3810.22). Current reward after update: -412.48, Optimal reward -258.25
Iteration 235 took 3.63 seconds (mean sampled reward: -3379.49). Current reward after update: -567.94, Optimal reward -258.25
Iteration 236 took 3.60 seconds (mean sampled reward: -3422.27). Current reward after update: -571.24, Optimal reward -258.25
Iteration 237 took 3.66 seconds (mean sampled reward: -3842.67). Current reward after update: -532.49, Optimal reward -258.25
Iteration 238 took 3.93 seconds (mean sampled reward: -4109.57). Current reward after update: -600.55, Optimal reward -258.25
Iteration 239 took 3.70 seconds (mean sampled reward: -4098.49). Current reward after update: -669.67, Optimal reward -258.25
Iteration 240 took 4.02 seconds (mean sampled reward: -4539.19). Current reward after update: -569.87, Optimal reward -258.25
Iteration 241 took 3.85 seconds (mean sampled reward: -5283.20). Current reward after update: -508.23, Optimal reward -258.25
Iteration 242 took 3.88 seconds (mean sampled reward: -5048.82). Current reward after update: -551.57, Optimal reward -258.25
Iteration 243 took 3.87 seconds (mean sampled reward: -5378.28). Current reward after update: -550.21, Optimal reward -258.25
Iteration 244 took 3.72 seconds (mean sampled reward: -4184.14). Current reward after update: -458.70, Optimal reward -258.25
Iteration 245 took 3.64 seconds (mean sampled reward: -3914.79). Current reward after update: -480.80, Optimal reward -258.25
Iteration 246 took 3.75 seconds (mean sampled reward: -4625.06). Current reward after update: -641.10, Optimal reward -258.25
Iteration 247 took 3.81 seconds (mean sampled reward: -5242.45). Current reward after update: -778.98, Optimal reward -258.25
Iteration 248 took 3.80 seconds (mean sampled reward: -5321.34). Current reward after update: -904.01, Optimal reward -258.25
Iteration 249 took 3.78 seconds (mean sampled reward: -4992.48). Current reward after update: -509.81, Optimal reward -258.25
Iteration 250 took 3.84 seconds (mean sampled reward: -5069.57). Current reward after update: -477.53, Optimal reward -258.25
Iteration 251 took 3.83 seconds (mean sampled reward: -5367.99). Current reward after update: -528.85, Optimal reward -258.25
Iteration 252 took 3.82 seconds (mean sampled reward: -5109.05). Current reward after update: -474.18, Optimal reward -258.25
Iteration 253 took 3.65 seconds (mean sampled reward: -4298.90). Current reward after update: -421.51, Optimal reward -258.25
Iteration 254 took 3.62 seconds (mean sampled reward: -3867.50). Current reward after update: -429.16, Optimal reward -258.25
Iteration 255 took 3.79 seconds (mean sampled reward: -4741.47). Current reward after update: -451.85, Optimal reward -258.25
Iteration 256 took 3.79 seconds (mean sampled reward: -4715.12). Current reward after update: -531.89, Optimal reward -258.25
Iteration 257 took 3.54 seconds (mean sampled reward: -3237.13). Current reward after update: -395.54, Optimal reward -258.25
Iteration 258 took 3.57 seconds (mean sampled reward: -3076.39). Current reward after update: -831.08, Optimal reward -258.25
Iteration 259 took 3.59 seconds (mean sampled reward: -3030.24). Current reward after update: -436.63, Optimal reward -258.25
Iteration 260 took 3.40 seconds (mean sampled reward: -2309.65). Current reward after update: -452.12, Optimal reward -258.25
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
Iteration 261 took 3.43 seconds (mean sampled reward: -2865.90). Current reward after update: -485.26, Optimal reward -258.25
Iteration 262 took 3.39 seconds (mean sampled reward: -2843.36). Current reward after update: -416.51, Optimal reward -258.25
Iteration 263 took 3.60 seconds (mean sampled reward: -3216.41). Current reward after update: -506.11, Optimal reward -258.25
Iteration 264 took 3.53 seconds (mean sampled reward: -3522.47). Current reward after update: -436.19, Optimal reward -258.25
Iteration 265 took 3.41 seconds (mean sampled reward: -2588.20). Current reward after update: -535.16, Optimal reward -258.25
Iteration 266 took 3.53 seconds (mean sampled reward: -3097.56). Current reward after update: -414.31, Optimal reward -258.25
Iteration 267 took 3.73 seconds (mean sampled reward: -4311.02). Current reward after update: -513.09, Optimal reward -258.25
Iteration 268 took 3.67 seconds (mean sampled reward: -3741.10). Current reward after update: -483.30, Optimal reward -258.25
Iteration 269 took 3.44 seconds (mean sampled reward: -2543.55). Current reward after update: -1188.69, Optimal reward -258.25
Iteration 270 took 3.41 seconds (mean sampled reward: -2534.15). Current reward after update: -942.68, Optimal reward -258.25
Iteration 271 took 3.41 seconds (mean sampled reward: -2224.30). Current reward after update: -654.20, Optimal reward -258.25
Iteration 272 took 3.33 seconds (mean sampled reward: -2180.05). Current reward after update: -607.66, Optimal reward -258.25
Iteration 273 took 3.33 seconds (mean sampled reward: -2052.89). Current reward after update: -655.03, Optimal reward -258.25
Iteration 274 took 3.34 seconds (mean sampled reward: -2017.06). Current reward after update: -1371.84, Optimal reward -258.25
Iteration 275 took 3.59 seconds (mean sampled reward: -3078.74). Current reward after update: -630.07, Optimal reward -258.25
Iteration 276 took 3.43 seconds (mean sampled reward: -2534.44). Current reward after update: -581.32, Optimal reward -258.25
Iteration 277 took 3.46 seconds (mean sampled reward: -2742.14). Current reward after update: -832.40, Optimal reward -258.25
Iteration 278 took 3.47 seconds (mean sampled reward: -2904.12). Current reward after update: -497.32, Optimal reward -258.25
Iteration 279 took 3.40 seconds (mean sampled reward: -2732.37). Current reward after update: -568.66, Optimal reward -258.25
Iteration 280 took 3.44 seconds (mean sampled reward: -2813.10). Current reward after update: -523.45, Optimal reward -258.25
Iteration 281 took 3.44 seconds (mean sampled reward: -2675.54). Current reward after update: -552.94, Optimal reward -258.25
Iteration 282 took 3.64 seconds (mean sampled reward: -3664.39). Current reward after update: -474.90, Optimal reward -258.25
Iteration 283 took 3.60 seconds (mean sampled reward: -3340.47). Current reward after update: -587.86, Optimal reward -258.25
Iteration 284 took 3.38 seconds (mean sampled reward: -2567.23). Current reward after update: -520.28, Optimal reward -258.25
Iteration 285 took 3.43 seconds (mean sampled reward: -2194.91). Current reward after update: -444.05, Optimal reward -258.25
Iteration 286 took 3.38 seconds (mean sampled reward: -2113.80). Current reward after update: -437.05, Optimal reward -258.25
Iteration 287 took 3.50 seconds (mean sampled reward: -2675.13). Current reward after update: -497.68, Optimal reward -258.25
Iteration 288 took 3.57 seconds (mean sampled reward: -2797.83). Current reward after update: -512.83, Optimal reward -258.25
Iteration 289 took 3.61 seconds (mean sampled reward: -3251.44). Current reward after update: -513.21, Optimal reward -258.25
Iteration 290 took 3.71 seconds (mean sampled reward: -4164.55). Current reward after update: -613.64, Optimal reward -258.25
Iteration 291 took 3.76 seconds (mean sampled reward: -4176.78). Current reward after update: -657.82, Optimal reward -258.25
Iteration 292 took 3.67 seconds (mean sampled reward: -3760.93). Current reward after update: -482.80, Optimal reward -258.25
Iteration 293 took 3.63 seconds (mean sampled reward: -3829.63). Current reward after update: -736.84, Optimal reward -258.25
Iteration 294 took 3.44 seconds (mean sampled reward: -2804.26). Current reward after update: -439.77, Optimal reward -258.25
Iteration 295 took 3.56 seconds (mean sampled reward: -4012.20). Current reward after update: -444.04, Optimal reward -258.25
Iteration 296 took 3.73 seconds (mean sampled reward: -4418.28). Current reward after update: -559.16, Optimal reward -258.25
Iteration 297 took 3.75 seconds (mean sampled reward: -4665.65). Current reward after update: -558.74, Optimal reward -258.25
Iteration 298 took 3.69 seconds (mean sampled reward: -4513.06). Current reward after update: -688.54, Optimal reward -258.25
Iteration 299 took 3.64 seconds (mean sampled reward: -3472.06). Current reward after update: -581.25, Optimal reward -258.25
Iteration 300 took 3.59 seconds (mean sampled reward: -4058.08). Current reward after update: -588.53, Optimal reward -258.25
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
Iteration 1 took 3.90 seconds (mean sampled reward: -6136.95). Current reward after update: -3916.01, Optimal reward -3916.01
Iteration 2 took 3.55 seconds (mean sampled reward: -5356.94). Current reward after update: -1721.95, Optimal reward -1721.95
Iteration 3 took 3.73 seconds (mean sampled reward: -4791.94). Current reward after update: -1493.46, Optimal reward -1493.46
Iteration 4 took 3.65 seconds (mean sampled reward: -4626.12). Current reward after update: -1433.36, Optimal reward -1433.36
Iteration 5 took 3.61 seconds (mean sampled reward: -4685.32). Current reward after update: -2243.00, Optimal reward -1433.36
Iteration 6 took 3.89 seconds (mean sampled reward: -4936.42). Current reward after update: -1605.77, Optimal reward -1433.36
Iteration 7 took 3.64 seconds (mean sampled reward: -4767.52). Current reward after update: -1386.61, Optimal reward -1386.61
Iteration 8 took 3.84 seconds (mean sampled reward: -4856.06). Current reward after update: -1370.44, Optimal reward -1370.44
Iteration 9 took 3.64 seconds (mean sampled reward: -4838.78). Current reward after update: -1385.10, Optimal reward -1370.44
Iteration 10 took 3.73 seconds (mean sampled reward: -4981.39). Current reward after update: -1255.84, Optimal reward -1255.84
Iteration 11 took 3.82 seconds (mean sampled reward: -5168.02). Current reward after update: -1125.41, Optimal reward -1125.41
Iteration 12 took 3.87 seconds (mean sampled reward: -5209.10). Current reward after update: -733.92, Optimal reward -733.92
Iteration 13 took 3.61 seconds (mean sampled reward: -5093.67). Current reward after update: -902.07, Optimal reward -733.92
Iteration 14 took 3.62 seconds (mean sampled reward: -4783.63). Current reward after update: -606.25, Optimal reward -606.25
Iteration 15 took 3.61 seconds (mean sampled reward: -4850.71). Current reward after update: -518.50, Optimal reward -518.50
Iteration 16 took 4.10 seconds (mean sampled reward: -5278.32). Current reward after update: -721.77, Optimal reward -518.50
Iteration 17 took 3.89 seconds (mean sampled reward: -5431.44). Current reward after update: -751.78, Optimal reward -518.50
Iteration 18 took 3.83 seconds (mean sampled reward: -5112.62). Current reward after update: -768.06, Optimal reward -518.50
Iteration 19 took 3.73 seconds (mean sampled reward: -4927.00). Current reward after update: -631.14, Optimal reward -518.50
Iteration 20 took 3.65 seconds (mean sampled reward: -4912.95). Current reward after update: -874.42, Optimal reward -518.50
Iteration 21 took 3.66 seconds (mean sampled reward: -5328.18). Current reward after update: -1000.76, Optimal reward -518.50
Iteration 22 took 3.74 seconds (mean sampled reward: -5239.87). Current reward after update: -826.81, Optimal reward -518.50
Iteration 23 took 3.65 seconds (mean sampled reward: -4900.12). Current reward after update: -1044.91, Optimal reward -518.50
Iteration 24 took 3.64 seconds (mean sampled reward: -5138.82). Current reward after update: -1096.15, Optimal reward -518.50
Iteration 25 took 3.76 seconds (mean sampled reward: -5280.75). Current reward after update: -988.51, Optimal reward -518.50
Iteration 26 took 3.71 seconds (mean sampled reward: -5087.60). Current reward after update: -1534.75, Optimal reward -518.50
Iteration 27 took 3.74 seconds (mean sampled reward: -4972.84). Current reward after update: -997.89, Optimal reward -518.50
Iteration 28 took 3.74 seconds (mean sampled reward: -4889.55). Current reward after update: -678.24, Optimal reward -518.50
Iteration 29 took 3.92 seconds (mean sampled reward: -4764.91). Current reward after update: -838.65, Optimal reward -518.50
Iteration 30 took 3.64 seconds (mean sampled reward: -4648.74). Current reward after update: -855.10, Optimal reward -518.50
Iteration 31 took 4.00 seconds (mean sampled reward: -4735.12). Current reward after update: -1027.99, Optimal reward -518.50
Iteration 32 took 3.53 seconds (mean sampled reward: -4855.34). Current reward after update: -878.32, Optimal reward -518.50
Iteration 33 took 3.59 seconds (mean sampled reward: -4442.63). Current reward after update: -902.54, Optimal reward -518.50
Iteration 34 took 3.52 seconds (mean sampled reward: -4268.40). Current reward after update: -948.10, Optimal reward -518.50
Iteration 35 took 3.71 seconds (mean sampled reward: -4465.52). Current reward after update: -603.11, Optimal reward -518.50
Iteration 36 took 3.68 seconds (mean sampled reward: -4096.78). Current reward after update: -627.35, Optimal reward -518.50
Iteration 37 took 3.62 seconds (mean sampled reward: -4317.37). Current reward after update: -663.16, Optimal reward -518.50
Iteration 38 took 3.67 seconds (mean sampled reward: -4518.55). Current reward after update: -769.70, Optimal reward -518.50
Iteration 39 took 3.60 seconds (mean sampled reward: -4644.54). Current reward after update: -661.19, Optimal reward -518.50
Iteration 40 took 3.82 seconds (mean sampled reward: -4823.85). Current reward after update: -594.09, Optimal reward -518.50
Iteration 41 took 3.77 seconds (mean sampled reward: -4825.51). Current reward after update: -1038.47, Optimal reward -518.50
Iteration 42 took 3.78 seconds (mean sampled reward: -4803.23). Current reward after update: -675.74, Optimal reward -518.50
Iteration 43 took 3.57 seconds (mean sampled reward: -5063.05). Current reward after update: -730.80, Optimal reward -518.50
Iteration 44 took 3.60 seconds (mean sampled reward: -4785.36). Current reward after update: -838.12, Optimal reward -518.50
Iteration 45 took 3.46 seconds (mean sampled reward: -4723.18). Current reward after update: -840.65, Optimal reward -518.50
Iteration 46 took 3.54 seconds (mean sampled reward: -4704.05). Current reward after update: -849.26, Optimal reward -518.50
Iteration 47 took 3.62 seconds (mean sampled reward: -5301.11). Current reward after update: -991.65, Optimal reward -518.50
Iteration 48 took 3.67 seconds (mean sampled reward: -5425.20). Current reward after update: -845.24, Optimal reward -518.50
Iteration 49 took 3.62 seconds (mean sampled reward: -5040.96). Current reward after update: -811.94, Optimal reward -518.50
Iteration 50 took 3.43 seconds (mean sampled reward: -5108.82). Current reward after update: -784.32, Optimal reward -518.50
Iteration 51 took 3.44 seconds (mean sampled reward: -5460.81). Current reward after update: -1036.48, Optimal reward -518.50
Iteration 52 took 3.50 seconds (mean sampled reward: -4700.82). Current reward after update: -994.88, Optimal reward -518.50
Iteration 53 took 3.43 seconds (mean sampled reward: -5231.80). Current reward after update: -1028.14, Optimal reward -518.50
Iteration 54 took 3.46 seconds (mean sampled reward: -5061.80). Current reward after update: -1016.42, Optimal reward -518.50
Iteration 55 took 3.51 seconds (mean sampled reward: -4779.09). Current reward after update: -1225.13, Optimal reward -518.50
Iteration 56 took 3.45 seconds (mean sampled reward: -4539.68). Current reward after update: -969.34, Optimal reward -518.50
Iteration 57 took 3.43 seconds (mean sampled reward: -4362.59). Current reward after update: -938.97, Optimal reward -518.50
Iteration 58 took 3.41 seconds (mean sampled reward: -4202.47). Current reward after update: -1175.53, Optimal reward -518.50
Iteration 59 took 3.53 seconds (mean sampled reward: -4870.74). Current reward after update: -991.92, Optimal reward -518.50
Iteration 60 took 3.48 seconds (mean sampled reward: -5155.09). Current reward after update: -766.39, Optimal reward -518.50
Iteration 61 took 3.60 seconds (mean sampled reward: -4711.94). Current reward after update: -745.55, Optimal reward -518.50
Iteration 62 took 3.50 seconds (mean sampled reward: -4836.74). Current reward after update: -667.71, Optimal reward -518.50
Iteration 63 took 3.50 seconds (mean sampled reward: -4864.09). Current reward after update: -772.20, Optimal reward -518.50
Iteration 64 took 3.42 seconds (mean sampled reward: -5132.59). Current reward after update: -608.71, Optimal reward -518.50
Iteration 65 took 3.53 seconds (mean sampled reward: -5382.96). Current reward after update: -611.96, Optimal reward -518.50
Iteration 66 took 3.61 seconds (mean sampled reward: -5102.93). Current reward after update: -835.93, Optimal reward -518.50
Iteration 67 took 3.46 seconds (mean sampled reward: -5158.13). Current reward after update: -590.83, Optimal reward -518.50
Iteration 68 took 3.52 seconds (mean sampled reward: -5084.37). Current reward after update: -685.88, Optimal reward -518.50
Iteration 69 took 3.58 seconds (mean sampled reward: -5042.63). Current reward after update: -666.21, Optimal reward -518.50
Iteration 70 took 3.53 seconds (mean sampled reward: -4870.37). Current reward after update: -591.27, Optimal reward -518.50
Iteration 71 took 3.57 seconds (mean sampled reward: -5003.33). Current reward after update: -546.39, Optimal reward -518.50
Iteration 72 took 3.57 seconds (mean sampled reward: -5458.54). Current reward after update: -515.19, Optimal reward -515.19
Iteration 73 took 3.57 seconds (mean sampled reward: -5766.09). Current reward after update: -922.91, Optimal reward -515.19
Iteration 74 took 3.53 seconds (mean sampled reward: -5573.41). Current reward after update: -678.73, Optimal reward -515.19
Iteration 75 took 3.46 seconds (mean sampled reward: -5191.21). Current reward after update: -515.32, Optimal reward -515.19
Iteration 76 took 3.50 seconds (mean sampled reward: -5074.29). Current reward after update: -5317.95, Optimal reward -515.19
Iteration 77 took 3.53 seconds (mean sampled reward: -5452.95). Current reward after update: -4270.00, Optimal reward -515.19
Iteration 78 took 3.62 seconds (mean sampled reward: -5211.42). Current reward after update: -1112.06, Optimal reward -515.19
Iteration 79 took 3.69 seconds (mean sampled reward: -5365.55). Current reward after update: -679.80, Optimal reward -515.19
Iteration 80 took 3.60 seconds (mean sampled reward: -5027.54). Current reward after update: -1074.89, Optimal reward -515.19
Iteration 81 took 3.64 seconds (mean sampled reward: -5049.99). Current reward after update: -1144.95, Optimal reward -515.19
Iteration 82 took 3.55 seconds (mean sampled reward: -5201.50). Current reward after update: -873.60, Optimal reward -515.19
Iteration 83 took 3.45 seconds (mean sampled reward: -4668.00). Current reward after update: -847.96, Optimal reward -515.19
Iteration 84 took 3.50 seconds (mean sampled reward: -5011.86). Current reward after update: -726.45, Optimal reward -515.19
Iteration 85 took 3.43 seconds (mean sampled reward: -4295.87). Current reward after update: -791.57, Optimal reward -515.19
Iteration 86 took 3.45 seconds (mean sampled reward: -4882.46). Current reward after update: -685.34, Optimal reward -515.19
Iteration 87 took 3.51 seconds (mean sampled reward: -4578.68). Current reward after update: -565.56, Optimal reward -515.19
Iteration 88 took 3.50 seconds (mean sampled reward: -4797.82). Current reward after update: -1152.61, Optimal reward -515.19
Iteration 89 took 3.50 seconds (mean sampled reward: -4517.46). Current reward after update: -609.93, Optimal reward -515.19
Iteration 90 took 3.52 seconds (mean sampled reward: -4587.47). Current reward after update: -542.28, Optimal reward -515.19
Iteration 91 took 3.56 seconds (mean sampled reward: -4408.84). Current reward after update: -690.15, Optimal reward -515.19
Iteration 92 took 3.60 seconds (mean sampled reward: -4384.11). Current reward after update: -554.26, Optimal reward -515.19
Iteration 93 took 3.56 seconds (mean sampled reward: -3981.99). Current reward after update: -496.45, Optimal reward -496.45
Iteration 94 took 3.59 seconds (mean sampled reward: -4094.95). Current reward after update: -633.37, Optimal reward -496.45
Iteration 95 took 3.61 seconds (mean sampled reward: -4769.32). Current reward after update: -746.86, Optimal reward -496.45
Iteration 96 took 3.54 seconds (mean sampled reward: -4811.18). Current reward after update: -892.60, Optimal reward -496.45
Iteration 97 took 3.50 seconds (mean sampled reward: -3589.12). Current reward after update: -623.21, Optimal reward -496.45
Iteration 98 took 3.58 seconds (mean sampled reward: -3607.01). Current reward after update: -595.87, Optimal reward -496.45
Iteration 99 took 3.53 seconds (mean sampled reward: -3767.17). Current reward after update: -517.94, Optimal reward -496.45
Iteration 100 took 3.64 seconds (mean sampled reward: -4473.82). Current reward after update: -404.80, Optimal reward -404.80
Iteration 101 took 3.57 seconds (mean sampled reward: -4377.37). Current reward after update: -453.34, Optimal reward -404.80
Iteration 102 took 3.53 seconds (mean sampled reward: -3596.24). Current reward after update: -448.61, Optimal reward -404.80
Iteration 103 took 3.51 seconds (mean sampled reward: -4499.95). Current reward after update: -385.08, Optimal reward -385.08
Iteration 104 took 3.58 seconds (mean sampled reward: -4879.36). Current reward after update: -640.06, Optimal reward -385.08
Iteration 105 took 3.58 seconds (mean sampled reward: -5084.41). Current reward after update: -420.81, Optimal reward -385.08
Iteration 106 took 3.69 seconds (mean sampled reward: -5037.33). Current reward after update: -544.55, Optimal reward -385.08
Iteration 107 took 3.64 seconds (mean sampled reward: -5419.18). Current reward after update: -469.66, Optimal reward -385.08
Iteration 108 took 3.62 seconds (mean sampled reward: -5220.55). Current reward after update: -480.27, Optimal reward -385.08
Iteration 109 took 3.58 seconds (mean sampled reward: -4674.83). Current reward after update: -404.92, Optimal reward -385.08
Iteration 110 took 3.50 seconds (mean sampled reward: -4886.78). Current reward after update: -456.04, Optimal reward -385.08
Iteration 111 took 3.51 seconds (mean sampled reward: -5203.10). Current reward after update: -400.58, Optimal reward -385.08
Iteration 112 took 3.73 seconds (mean sampled reward: -5146.93). Current reward after update: -390.41, Optimal reward -385.08
Iteration 113 took 3.54 seconds (mean sampled reward: -5162.96). Current reward after update: -449.11, Optimal reward -385.08
Iteration 114 took 3.57 seconds (mean sampled reward: -4514.02). Current reward after update: -424.59, Optimal reward -385.08
Iteration 115 took 3.49 seconds (mean sampled reward: -5067.49). Current reward after update: -494.05, Optimal reward -385.08
Iteration 116 took 3.58 seconds (mean sampled reward: -5016.01). Current reward after update: -387.40, Optimal reward -385.08
Iteration 117 took 3.55 seconds (mean sampled reward: -4876.58). Current reward after update: -412.20, Optimal reward -385.08
Iteration 118 took 3.48 seconds (mean sampled reward: -4553.20). Current reward after update: -411.57, Optimal reward -385.08
Iteration 119 took 3.60 seconds (mean sampled reward: -4491.82). Current reward after update: -406.04, Optimal reward -385.08
Iteration 120 took 3.52 seconds (mean sampled reward: -3580.50). Current reward after update: -383.79, Optimal reward -383.79
Iteration 121 took 3.48 seconds (mean sampled reward: -4679.46). Current reward after update: -371.89, Optimal reward -371.89
Iteration 122 took 3.63 seconds (mean sampled reward: -4756.82). Current reward after update: -464.87, Optimal reward -371.89
Iteration 123 took 3.53 seconds (mean sampled reward: -4502.75). Current reward after update: -391.27, Optimal reward -371.89
Iteration 124 took 3.49 seconds (mean sampled reward: -3660.51). Current reward after update: -479.45, Optimal reward -371.89
Iteration 125 took 3.45 seconds (mean sampled reward: -4305.96). Current reward after update: -405.10, Optimal reward -371.89
Iteration 126 took 3.55 seconds (mean sampled reward: -4736.60). Current reward after update: -389.99, Optimal reward -371.89
Iteration 127 took 3.57 seconds (mean sampled reward: -4562.99). Current reward after update: -370.06, Optimal reward -370.06
Iteration 128 took 3.59 seconds (mean sampled reward: -3898.80). Current reward after update: -350.18, Optimal reward -350.18
Iteration 129 took 3.46 seconds (mean sampled reward: -4560.77). Current reward after update: -369.67, Optimal reward -350.18
Iteration 130 took 3.59 seconds (mean sampled reward: -4841.43). Current reward after update: -473.52, Optimal reward -350.18
Iteration 131 took 3.58 seconds (mean sampled reward: -4747.31). Current reward after update: -391.50, Optimal reward -350.18
Iteration 132 took 3.48 seconds (mean sampled reward: -5095.46). Current reward after update: -381.92, Optimal reward -350.18
Iteration 133 took 3.49 seconds (mean sampled reward: -5107.96). Current reward after update: -366.08, Optimal reward -350.18
Iteration 134 took 3.43 seconds (mean sampled reward: -5176.30). Current reward after update: -405.72, Optimal reward -350.18
Iteration 135 took 3.55 seconds (mean sampled reward: -4461.82). Current reward after update: -361.99, Optimal reward -350.18
Iteration 136 took 3.46 seconds (mean sampled reward: -4975.05). Current reward after update: -363.44, Optimal reward -350.18
Iteration 137 took 3.46 seconds (mean sampled reward: -4618.37). Current reward after update: -360.76, Optimal reward -350.18
Iteration 138 took 3.63 seconds (mean sampled reward: -5198.30). Current reward after update: -382.61, Optimal reward -350.18
Iteration 139 took 3.53 seconds (mean sampled reward: -5155.88). Current reward after update: -387.10, Optimal reward -350.18
Iteration 140 took 3.51 seconds (mean sampled reward: -4342.25). Current reward after update: -359.31, Optimal reward -350.18
Iteration 141 took 3.41 seconds (mean sampled reward: -4836.71). Current reward after update: -408.34, Optimal reward -350.18
Iteration 142 took 3.39 seconds (mean sampled reward: -4970.25). Current reward after update: -435.52, Optimal reward -350.18
Iteration 143 took 3.55 seconds (mean sampled reward: -4437.85). Current reward after update: -440.51, Optimal reward -350.18
Iteration 144 took 3.48 seconds (mean sampled reward: -4600.34). Current reward after update: -413.03, Optimal reward -350.18
Iteration 145 took 3.44 seconds (mean sampled reward: -4108.47). Current reward after update: -431.37, Optimal reward -350.18
Iteration 146 took 3.47 seconds (mean sampled reward: -4006.60). Current reward after update: -472.69, Optimal reward -350.18
Iteration 147 took 3.43 seconds (mean sampled reward: -3554.37). Current reward after update: -491.05, Optimal reward -350.18
Iteration 148 took 3.45 seconds (mean sampled reward: -4233.97). Current reward after update: -580.98, Optimal reward -350.18
Iteration 149 took 3.46 seconds (mean sampled reward: -3520.72). Current reward after update: -479.42, Optimal reward -350.18
Iteration 150 took 3.59 seconds (mean sampled reward: -4526.25). Current reward after update: -491.85, Optimal reward -350.18
Iteration 151 took 3.48 seconds (mean sampled reward: -4235.53). Current reward after update: -454.68, Optimal reward -350.18
Iteration 152 took 3.46 seconds (mean sampled reward: -4763.77). Current reward after update: -530.29, Optimal reward -350.18
Iteration 153 took 3.47 seconds (mean sampled reward: -4672.43). Current reward after update: -347.54, Optimal reward -347.54
Iteration 154 took 3.53 seconds (mean sampled reward: -4429.51). Current reward after update: -344.53, Optimal reward -344.53
Iteration 155 took 3.56 seconds (mean sampled reward: -4184.68). Current reward after update: -355.72, Optimal reward -344.53
Iteration 156 took 3.44 seconds (mean sampled reward: -3917.78). Current reward after update: -330.69, Optimal reward -330.69
Iteration 157 took 3.42 seconds (mean sampled reward: -4447.28). Current reward after update: -389.03, Optimal reward -330.69
Iteration 158 took 3.40 seconds (mean sampled reward: -4326.59). Current reward after update: -431.27, Optimal reward -330.69
Iteration 159 took 3.44 seconds (mean sampled reward: -4662.24). Current reward after update: -334.16, Optimal reward -330.69
Iteration 160 took 3.48 seconds (mean sampled reward: -4476.21). Current reward after update: -310.53, Optimal reward -310.53
Iteration 161 took 3.40 seconds (mean sampled reward: -4194.32). Current reward after update: -346.62, Optimal reward -310.53
Iteration 162 took 3.50 seconds (mean sampled reward: -5107.94). Current reward after update: -399.55, Optimal reward -310.53
Iteration 163 took 3.49 seconds (mean sampled reward: -4663.67). Current reward after update: -370.31, Optimal reward -310.53
Iteration 164 took 3.56 seconds (mean sampled reward: -4440.32). Current reward after update: -310.84, Optimal reward -310.53
Iteration 165 took 3.41 seconds (mean sampled reward: -4596.21). Current reward after update: -303.47, Optimal reward -303.47
Iteration 166 took 3.49 seconds (mean sampled reward: -3958.44). Current reward after update: -303.22, Optimal reward -303.22
Iteration 167 took 3.45 seconds (mean sampled reward: -4301.31). Current reward after update: -325.53, Optimal reward -303.22
Iteration 168 took 3.43 seconds (mean sampled reward: -4026.95). Current reward after update: -282.03, Optimal reward -282.03
Iteration 169 took 3.49 seconds (mean sampled reward: -4721.59). Current reward after update: -387.48, Optimal reward -282.03
Iteration 170 took 3.48 seconds (mean sampled reward: -4901.18). Current reward after update: -337.22, Optimal reward -282.03
Iteration 171 took 3.44 seconds (mean sampled reward: -4448.32). Current reward after update: -332.39, Optimal reward -282.03
Iteration 172 took 3.44 seconds (mean sampled reward: -4188.10). Current reward after update: -292.74, Optimal reward -282.03
Iteration 173 took 3.46 seconds (mean sampled reward: -4636.44). Current reward after update: -981.50, Optimal reward -282.03
Iteration 174 took 3.49 seconds (mean sampled reward: -4375.38). Current reward after update: -319.29, Optimal reward -282.03
Iteration 175 took 3.57 seconds (mean sampled reward: -4773.34). Current reward after update: -465.99, Optimal reward -282.03
Iteration 176 took 3.57 seconds (mean sampled reward: -4684.98). Current reward after update: -386.91, Optimal reward -282.03
Iteration 177 took 3.44 seconds (mean sampled reward: -4898.51). Current reward after update: -350.47, Optimal reward -282.03
Iteration 178 took 3.46 seconds (mean sampled reward: -4646.29). Current reward after update: -314.13, Optimal reward -282.03
Iteration 179 took 3.47 seconds (mean sampled reward: -4634.11). Current reward after update: -360.40, Optimal reward -282.03
Iteration 180 took 3.50 seconds (mean sampled reward: -4322.58). Current reward after update: -384.23, Optimal reward -282.03
Iteration 181 took 3.44 seconds (mean sampled reward: -4446.46). Current reward after update: -382.06, Optimal reward -282.03
Iteration 182 took 3.46 seconds (mean sampled reward: -4239.34). Current reward after update: -421.80, Optimal reward -282.03
Iteration 183 took 3.49 seconds (mean sampled reward: -3579.27). Current reward after update: -301.16, Optimal reward -282.03
Iteration 184 took 3.43 seconds (mean sampled reward: -4042.98). Current reward after update: -364.88, Optimal reward -282.03
Iteration 185 took 3.52 seconds (mean sampled reward: -4207.75). Current reward after update: -338.21, Optimal reward -282.03
Iteration 186 took 3.44 seconds (mean sampled reward: -3909.07). Current reward after update: -314.90, Optimal reward -282.03
Iteration 187 took 3.53 seconds (mean sampled reward: -4249.33). Current reward after update: -370.05, Optimal reward -282.03
Iteration 188 took 3.47 seconds (mean sampled reward: -4369.60). Current reward after update: -411.15, Optimal reward -282.03
Iteration 189 took 3.41 seconds (mean sampled reward: -4456.27). Current reward after update: -307.39, Optimal reward -282.03
Iteration 190 took 3.40 seconds (mean sampled reward: -4855.48). Current reward after update: -343.21, Optimal reward -282.03
Iteration 191 took 3.45 seconds (mean sampled reward: -5095.48). Current reward after update: -355.66, Optimal reward -282.03
Iteration 192 took 3.52 seconds (mean sampled reward: -4834.88). Current reward after update: -478.70, Optimal reward -282.03
Iteration 193 took 3.44 seconds (mean sampled reward: -5067.33). Current reward after update: -357.74, Optimal reward -282.03
Iteration 194 took 3.41 seconds (mean sampled reward: -4718.10). Current reward after update: -352.25, Optimal reward -282.03
Iteration 195 took 3.45 seconds (mean sampled reward: -4504.66). Current reward after update: -325.82, Optimal reward -282.03
Iteration 196 took 3.42 seconds (mean sampled reward: -4801.69). Current reward after update: -351.53, Optimal reward -282.03
Iteration 197 took 3.49 seconds (mean sampled reward: -4225.77). Current reward after update: -323.32, Optimal reward -282.03
Iteration 198 took 3.51 seconds (mean sampled reward: -4767.62). Current reward after update: -304.53, Optimal reward -282.03
Iteration 199 took 3.55 seconds (mean sampled reward: -5246.89). Current reward after update: -308.22, Optimal reward -282.03
Iteration 200 took 3.52 seconds (mean sampled reward: -5034.11). Current reward after update: -304.01, Optimal reward -282.03
Iteration 201 took 3.60 seconds (mean sampled reward: -5160.29). Current reward after update: -346.01, Optimal reward -282.03
Iteration 202 took 3.55 seconds (mean sampled reward: -5038.65). Current reward after update: -313.07, Optimal reward -282.03
Iteration 203 took 3.65 seconds (mean sampled reward: -5254.64). Current reward after update: -344.42, Optimal reward -282.03
Iteration 204 took 3.62 seconds (mean sampled reward: -5407.37). Current reward after update: -461.01, Optimal reward -282.03
Iteration 205 took 3.62 seconds (mean sampled reward: -5094.91). Current reward after update: -318.49, Optimal reward -282.03
Iteration 206 took 3.59 seconds (mean sampled reward: -4987.72). Current reward after update: -315.69, Optimal reward -282.03
Iteration 207 took 3.63 seconds (mean sampled reward: -4748.84). Current reward after update: -294.50, Optimal reward -282.03
Iteration 208 took 3.61 seconds (mean sampled reward: -4947.69). Current reward after update: -399.02, Optimal reward -282.03
Iteration 209 took 3.62 seconds (mean sampled reward: -4670.44). Current reward after update: -324.04, Optimal reward -282.03
Iteration 210 took 3.55 seconds (mean sampled reward: -4206.96). Current reward after update: -284.76, Optimal reward -282.03
Iteration 211 took 3.57 seconds (mean sampled reward: -4864.90). Current reward after update: -329.09, Optimal reward -282.03
Iteration 212 took 3.99 seconds (mean sampled reward: -4824.77). Current reward after update: -388.63, Optimal reward -282.03
Iteration 213 took 4.02 seconds (mean sampled reward: -5039.31). Current reward after update: -324.29, Optimal reward -282.03
Iteration 214 took 3.84 seconds (mean sampled reward: -4046.96). Current reward after update: -313.29, Optimal reward -282.03
Iteration 215 took 3.91 seconds (mean sampled reward: -4506.44). Current reward after update: -296.60, Optimal reward -282.03
Iteration 216 took 3.79 seconds (mean sampled reward: -4997.38). Current reward after update: -313.70, Optimal reward -282.03
Iteration 217 took 3.92 seconds (mean sampled reward: -4773.08). Current reward after update: -316.19, Optimal reward -282.03
Iteration 218 took 3.82 seconds (mean sampled reward: -4973.07). Current reward after update: -339.35, Optimal reward -282.03
Iteration 219 took 3.61 seconds (mean sampled reward: -4733.76). Current reward after update: -332.12, Optimal reward -282.03
Iteration 220 took 3.64 seconds (mean sampled reward: -5054.19). Current reward after update: -325.22, Optimal reward -282.03
Iteration 221 took 3.52 seconds (mean sampled reward: -4733.38). Current reward after update: -306.67, Optimal reward -282.03
Iteration 222 took 3.54 seconds (mean sampled reward: -4557.75). Current reward after update: -311.90, Optimal reward -282.03
Iteration 223 took 3.61 seconds (mean sampled reward: -5000.37). Current reward after update: -337.12, Optimal reward -282.03
Iteration 224 took 3.54 seconds (mean sampled reward: -5035.83). Current reward after update: -360.85, Optimal reward -282.03
Iteration 225 took 3.51 seconds (mean sampled reward: -4329.42). Current reward after update: -383.74, Optimal reward -282.03
Iteration 226 took 3.63 seconds (mean sampled reward: -4189.18). Current reward after update: -339.16, Optimal reward -282.03
Iteration 227 took 3.49 seconds (mean sampled reward: -4101.77). Current reward after update: -1821.45, Optimal reward -282.03
Iteration 228 took 3.60 seconds (mean sampled reward: -4921.21). Current reward after update: -384.72, Optimal reward -282.03
Iteration 229 took 3.54 seconds (mean sampled reward: -4664.94). Current reward after update: -343.85, Optimal reward -282.03
Iteration 230 took 3.50 seconds (mean sampled reward: -4650.24). Current reward after update: -348.29, Optimal reward -282.03
Iteration 231 took 3.49 seconds (mean sampled reward: -4840.51). Current reward after update: -350.75, Optimal reward -282.03
Iteration 232 took 3.53 seconds (mean sampled reward: -4487.67). Current reward after update: -344.42, Optimal reward -282.03
Iteration 233 took 3.54 seconds (mean sampled reward: -4881.71). Current reward after update: -326.93, Optimal reward -282.03
Iteration 234 took 3.56 seconds (mean sampled reward: -4380.36). Current reward after update: -345.27, Optimal reward -282.03
Iteration 235 took 3.46 seconds (mean sampled reward: -4768.83). Current reward after update: -363.18, Optimal reward -282.03
Iteration 236 took 3.59 seconds (mean sampled reward: -4135.40). Current reward after update: -470.48, Optimal reward -282.03
Iteration 237 took 3.57 seconds (mean sampled reward: -3409.19). Current reward after update: -306.11, Optimal reward -282.03
Iteration 238 took 3.52 seconds (mean sampled reward: -3667.27). Current reward after update: -345.47, Optimal reward -282.03
Iteration 239 took 3.55 seconds (mean sampled reward: -3619.09). Current reward after update: -331.12, Optimal reward -282.03
Iteration 240 took 3.55 seconds (mean sampled reward: -3523.84). Current reward after update: -363.82, Optimal reward -282.03
Iteration 241 took 3.45 seconds (mean sampled reward: -3512.76). Current reward after update: -307.50, Optimal reward -282.03
Iteration 242 took 3.50 seconds (mean sampled reward: -3654.44). Current reward after update: -337.36, Optimal reward -282.03
Iteration 243 took 3.51 seconds (mean sampled reward: -3599.29). Current reward after update: -380.73, Optimal reward -282.03
Iteration 244 took 3.52 seconds (mean sampled reward: -3923.98). Current reward after update: -358.29, Optimal reward -282.03
Iteration 245 took 3.55 seconds (mean sampled reward: -4141.64). Current reward after update: -425.61, Optimal reward -282.03
Iteration 246 took 3.48 seconds (mean sampled reward: -4007.11). Current reward after update: -348.98, Optimal reward -282.03
Iteration 247 took 3.49 seconds (mean sampled reward: -3576.06). Current reward after update: -379.43, Optimal reward -282.03
Iteration 248 took 3.55 seconds (mean sampled reward: -3605.65). Current reward after update: -311.96, Optimal reward -282.03
Iteration 249 took 3.53 seconds (mean sampled reward: -3682.22). Current reward after update: -390.60, Optimal reward -282.03
Iteration 250 took 3.60 seconds (mean sampled reward: -4651.87). Current reward after update: -353.93, Optimal reward -282.03
Iteration 251 took 3.57 seconds (mean sampled reward: -3846.77). Current reward after update: -390.90, Optimal reward -282.03
Iteration 252 took 3.55 seconds (mean sampled reward: -3885.03). Current reward after update: -366.56, Optimal reward -282.03
Iteration 253 took 3.56 seconds (mean sampled reward: -4005.48). Current reward after update: -401.43, Optimal reward -282.03
Iteration 254 took 3.59 seconds (mean sampled reward: -3689.55). Current reward after update: -407.35, Optimal reward -282.03
Iteration 255 took 3.55 seconds (mean sampled reward: -3748.12). Current reward after update: -326.12, Optimal reward -282.03
Iteration 256 took 3.59 seconds (mean sampled reward: -4067.33). Current reward after update: -370.90, Optimal reward -282.03
Iteration 257 took 3.58 seconds (mean sampled reward: -4216.38). Current reward after update: -362.99, Optimal reward -282.03
Iteration 258 took 3.52 seconds (mean sampled reward: -4470.30). Current reward after update: -350.49, Optimal reward -282.03
Iteration 259 took 3.56 seconds (mean sampled reward: -4402.97). Current reward after update: -966.52, Optimal reward -282.03
Iteration 260 took 3.56 seconds (mean sampled reward: -4001.55). Current reward after update: -345.97, Optimal reward -282.03
Iteration 261 took 3.64 seconds (mean sampled reward: -4540.20). Current reward after update: -337.60, Optimal reward -282.03
Iteration 262 took 3.62 seconds (mean sampled reward: -4928.94). Current reward after update: -562.78, Optimal reward -282.03
Iteration 263 took 3.56 seconds (mean sampled reward: -4028.25). Current reward after update: -356.38, Optimal reward -282.03
Iteration 264 took 3.54 seconds (mean sampled reward: -3857.07). Current reward after update: -393.89, Optimal reward -282.03
Iteration 265 took 3.51 seconds (mean sampled reward: -4720.74). Current reward after update: -363.82, Optimal reward -282.03
Iteration 266 took 3.55 seconds (mean sampled reward: -4604.56). Current reward after update: -375.09, Optimal reward -282.03
Iteration 267 took 3.58 seconds (mean sampled reward: -4227.77). Current reward after update: -308.17, Optimal reward -282.03
Iteration 268 took 3.56 seconds (mean sampled reward: -3993.32). Current reward after update: -447.20, Optimal reward -282.03
Iteration 269 took 3.63 seconds (mean sampled reward: -3639.13). Current reward after update: -342.00, Optimal reward -282.03
Iteration 270 took 3.49 seconds (mean sampled reward: -4016.15). Current reward after update: -418.33, Optimal reward -282.03
Iteration 271 took 3.51 seconds (mean sampled reward: -3897.27). Current reward after update: -420.97, Optimal reward -282.03
Iteration 272 took 3.50 seconds (mean sampled reward: -4049.05). Current reward after update: -418.72, Optimal reward -282.03
Iteration 273 took 3.51 seconds (mean sampled reward: -4003.80). Current reward after update: -429.30, Optimal reward -282.03
Iteration 274 took 3.55 seconds (mean sampled reward: -3895.88). Current reward after update: -399.67, Optimal reward -282.03
Iteration 275 took 3.54 seconds (mean sampled reward: -3819.87). Current reward after update: -426.76, Optimal reward -282.03
Iteration 276 took 3.46 seconds (mean sampled reward: -4163.05). Current reward after update: -356.72, Optimal reward -282.03
Iteration 277 took 3.49 seconds (mean sampled reward: -3728.28). Current reward after update: -384.77, Optimal reward -282.03
Iteration 278 took 3.47 seconds (mean sampled reward: -4087.33). Current reward after update: -336.68, Optimal reward -282.03
Iteration 279 took 3.53 seconds (mean sampled reward: -4351.76). Current reward after update: -397.99, Optimal reward -282.03
Iteration 280 took 3.51 seconds (mean sampled reward: -3842.50). Current reward after update: -356.86, Optimal reward -282.03
Iteration 281 took 3.57 seconds (mean sampled reward: -3957.37). Current reward after update: -402.75, Optimal reward -282.03
Iteration 282 took 3.56 seconds (mean sampled reward: -3933.86). Current reward after update: -348.80, Optimal reward -282.03
Iteration 283 took 3.57 seconds (mean sampled reward: -3703.51). Current reward after update: -306.37, Optimal reward -282.03
Iteration 284 took 3.64 seconds (mean sampled reward: -3956.07). Current reward after update: -453.74, Optimal reward -282.03
Iteration 285 took 3.59 seconds (mean sampled reward: -4064.99). Current reward after update: -449.19, Optimal reward -282.03
Iteration 286 took 3.58 seconds (mean sampled reward: -3695.26). Current reward after update: -307.98, Optimal reward -282.03
Iteration 287 took 3.60 seconds (mean sampled reward: -4234.60). Current reward after update: -498.21, Optimal reward -282.03
Iteration 288 took 3.71 seconds (mean sampled reward: -4347.16). Current reward after update: -524.59, Optimal reward -282.03
Iteration 289 took 3.62 seconds (mean sampled reward: -4574.42). Current reward after update: -454.27, Optimal reward -282.03
Iteration 290 took 3.63 seconds (mean sampled reward: -4246.19). Current reward after update: -464.81, Optimal reward -282.03
Iteration 291 took 3.64 seconds (mean sampled reward: -4649.01). Current reward after update: -329.95, Optimal reward -282.03
Iteration 292 took 3.61 seconds (mean sampled reward: -4298.13). Current reward after update: -362.27, Optimal reward -282.03
Iteration 293 took 3.63 seconds (mean sampled reward: -3774.03). Current reward after update: -415.46, Optimal reward -282.03
Iteration 294 took 3.62 seconds (mean sampled reward: -4209.19). Current reward after update: -337.71, Optimal reward -282.03
Iteration 295 took 3.58 seconds (mean sampled reward: -4207.90). Current reward after update: -411.08, Optimal reward -282.03
Iteration 296 took 3.49 seconds (mean sampled reward: -3854.02). Current reward after update: -427.98, Optimal reward -282.03
Iteration 297 took 3.54 seconds (mean sampled reward: -3941.20). Current reward after update: -493.79, Optimal reward -282.03
Iteration 298 took 3.55 seconds (mean sampled reward: -3596.94). Current reward after update: -469.36, Optimal reward -282.03
Iteration 299 took 3.56 seconds (mean sampled reward: -3726.91). Current reward after update: -427.62, Optimal reward -282.03
Iteration 300 took 3.58 seconds (mean sampled reward: -3916.46). Current reward after update: -374.41, Optimal reward -282.03
Iteration 1 took 3.86 seconds (mean sampled reward: -6133.01). Current reward after update: -3619.05, Optimal reward -3619.05
Iteration 2 took 3.68 seconds (mean sampled reward: -5464.94). Current reward after update: -2088.47, Optimal reward -2088.47
Iteration 3 took 4.11 seconds (mean sampled reward: -5203.66). Current reward after update: -3039.39, Optimal reward -2088.47
Iteration 4 took 4.01 seconds (mean sampled reward: -5409.68). Current reward after update: -1600.00, Optimal reward -1600.00
Iteration 5 took 4.06 seconds (mean sampled reward: -5244.18). Current reward after update: -1263.71, Optimal reward -1263.71
Iteration 6 took 3.96 seconds (mean sampled reward: -5317.89). Current reward after update: -1114.82, Optimal reward -1114.82
Iteration 7 took 4.23 seconds (mean sampled reward: -5297.16). Current reward after update: -1500.18, Optimal reward -1114.82
Iteration 8 took 3.91 seconds (mean sampled reward: -5267.06). Current reward after update: -1419.87, Optimal reward -1114.82
Iteration 9 took 3.81 seconds (mean sampled reward: -5087.47). Current reward after update: -1505.96, Optimal reward -1114.82
Iteration 10 took 3.62 seconds (mean sampled reward: -4342.29). Current reward after update: -1382.17, Optimal reward -1114.82
Iteration 11 took 3.89 seconds (mean sampled reward: -4644.55). Current reward after update: -1445.45, Optimal reward -1114.82
Iteration 12 took 3.52 seconds (mean sampled reward: -4538.89). Current reward after update: -1143.34, Optimal reward -1114.82
Iteration 13 took 3.79 seconds (mean sampled reward: -4810.60). Current reward after update: -1136.48, Optimal reward -1114.82
Iteration 14 took 3.72 seconds (mean sampled reward: -4271.73). Current reward after update: -1252.83, Optimal reward -1114.82
Iteration 15 took 3.79 seconds (mean sampled reward: -4205.72). Current reward after update: -1262.12, Optimal reward -1114.82
Iteration 16 took 3.95 seconds (mean sampled reward: -4392.34). Current reward after update: -1254.27, Optimal reward -1114.82
Iteration 17 took 3.63 seconds (mean sampled reward: -4615.63). Current reward after update: -1079.80, Optimal reward -1079.80
Iteration 18 took 3.63 seconds (mean sampled reward: -4529.66). Current reward after update: -1291.53, Optimal reward -1079.80
Iteration 19 took 3.68 seconds (mean sampled reward: -4335.55). Current reward after update: -1174.35, Optimal reward -1079.80
Iteration 20 took 3.70 seconds (mean sampled reward: -4875.56). Current reward after update: -1293.18, Optimal reward -1079.80
Iteration 21 took 3.58 seconds (mean sampled reward: -4261.72). Current reward after update: -1019.94, Optimal reward -1019.94
Iteration 22 took 3.57 seconds (mean sampled reward: -3383.62). Current reward after update: -1134.25, Optimal reward -1019.94
Iteration 23 took 3.45 seconds (mean sampled reward: -3406.44). Current reward after update: -1175.21, Optimal reward -1019.94
Iteration 24 took 3.64 seconds (mean sampled reward: -3795.23). Current reward after update: -1051.54, Optimal reward -1019.94
Iteration 25 took 3.73 seconds (mean sampled reward: -3893.43). Current reward after update: -4516.15, Optimal reward -1019.94
Iteration 26 took 3.69 seconds (mean sampled reward: -4152.33). Current reward after update: -1146.97, Optimal reward -1019.94
Iteration 27 took 3.65 seconds (mean sampled reward: -3782.81). Current reward after update: -1099.75, Optimal reward -1019.94
Iteration 28 took 3.54 seconds (mean sampled reward: -3195.85). Current reward after update: -1081.94, Optimal reward -1019.94
Iteration 29 took 3.80 seconds (mean sampled reward: -3748.53). Current reward after update: -1268.21, Optimal reward -1019.94
Iteration 30 took 3.68 seconds (mean sampled reward: -3658.86). Current reward after update: -1138.07, Optimal reward -1019.94
Iteration 31 took 3.79 seconds (mean sampled reward: -3913.72). Current reward after update: -1268.81, Optimal reward -1019.94
Iteration 32 took 3.86 seconds (mean sampled reward: -3430.08). Current reward after update: -1002.64, Optimal reward -1002.64
Iteration 33 took 3.55 seconds (mean sampled reward: -4069.56). Current reward after update: -1039.52, Optimal reward -1002.64
Iteration 34 took 3.54 seconds (mean sampled reward: -4012.78). Current reward after update: -974.40, Optimal reward -974.40
Iteration 35 took 3.61 seconds (mean sampled reward: -4220.13). Current reward after update: -942.87, Optimal reward -942.87
Iteration 36 took 3.80 seconds (mean sampled reward: -3897.52). Current reward after update: -1054.74, Optimal reward -942.87
Iteration 37 took 3.83 seconds (mean sampled reward: -3667.87). Current reward after update: -837.01, Optimal reward -837.01
Iteration 38 took 3.52 seconds (mean sampled reward: -4398.96). Current reward after update: -898.51, Optimal reward -837.01
Iteration 39 took 3.63 seconds (mean sampled reward: -4899.99). Current reward after update: -823.17, Optimal reward -823.17
Iteration 40 took 3.51 seconds (mean sampled reward: -4288.58). Current reward after update: -816.91, Optimal reward -816.91
Iteration 41 took 3.51 seconds (mean sampled reward: -4871.53). Current reward after update: -869.29, Optimal reward -816.91
Iteration 42 took 3.52 seconds (mean sampled reward: -5030.05). Current reward after update: -896.51, Optimal reward -816.91
Iteration 43 took 3.51 seconds (mean sampled reward: -5205.41). Current reward after update: -783.64, Optimal reward -783.64
Iteration 44 took 3.52 seconds (mean sampled reward: -5060.51). Current reward after update: -874.20, Optimal reward -783.64
Iteration 45 took 3.57 seconds (mean sampled reward: -4788.57). Current reward after update: -867.45, Optimal reward -783.64
Iteration 46 took 3.65 seconds (mean sampled reward: -4946.84). Current reward after update: -841.62, Optimal reward -783.64
Iteration 47 took 3.55 seconds (mean sampled reward: -4885.43). Current reward after update: -899.67, Optimal reward -783.64
Iteration 48 took 3.60 seconds (mean sampled reward: -4494.63). Current reward after update: -754.83, Optimal reward -754.83
Iteration 49 took 3.68 seconds (mean sampled reward: -3667.96). Current reward after update: -816.89, Optimal reward -754.83
Iteration 50 took 3.39 seconds (mean sampled reward: -3294.71). Current reward after update: -839.63, Optimal reward -754.83
Iteration 51 took 3.41 seconds (mean sampled reward: -3217.01). Current reward after update: -832.92, Optimal reward -754.83
Iteration 52 took 3.73 seconds (mean sampled reward: -3755.95). Current reward after update: -827.41, Optimal reward -754.83
Iteration 53 took 3.57 seconds (mean sampled reward: -4818.72). Current reward after update: -953.20, Optimal reward -754.83
Iteration 54 took 3.56 seconds (mean sampled reward: -4711.52). Current reward after update: -889.48, Optimal reward -754.83
Iteration 55 took 3.55 seconds (mean sampled reward: -3827.89). Current reward after update: -870.11, Optimal reward -754.83
Iteration 56 took 3.72 seconds (mean sampled reward: -3439.24). Current reward after update: -773.62, Optimal reward -754.83
Iteration 57 took 3.52 seconds (mean sampled reward: -3245.67). Current reward after update: -748.56, Optimal reward -748.56
Iteration 58 took 3.70 seconds (mean sampled reward: -3722.96). Current reward after update: -801.90, Optimal reward -748.56
Iteration 59 took 3.44 seconds (mean sampled reward: -3542.25). Current reward after update: -792.33, Optimal reward -748.56
Iteration 60 took 3.51 seconds (mean sampled reward: -3312.20). Current reward after update: -877.76, Optimal reward -748.56
Iteration 61 took 3.56 seconds (mean sampled reward: -3136.83). Current reward after update: -773.32, Optimal reward -748.56
Iteration 62 took 3.49 seconds (mean sampled reward: -2831.82). Current reward after update: -845.14, Optimal reward -748.56
Iteration 63 took 3.46 seconds (mean sampled reward: -2889.35). Current reward after update: -787.41, Optimal reward -748.56
Iteration 64 took 3.43 seconds (mean sampled reward: -3126.45). Current reward after update: -1002.89, Optimal reward -748.56
Iteration 65 took 3.38 seconds (mean sampled reward: -3027.08). Current reward after update: -935.86, Optimal reward -748.56
Iteration 66 took 3.44 seconds (mean sampled reward: -3032.91). Current reward after update: -945.94, Optimal reward -748.56
Iteration 67 took 3.57 seconds (mean sampled reward: -4161.41). Current reward after update: -1023.40, Optimal reward -748.56
Iteration 68 took 3.48 seconds (mean sampled reward: -3109.40). Current reward after update: -935.66, Optimal reward -748.56
Iteration 69 took 3.49 seconds (mean sampled reward: -2925.68). Current reward after update: -1070.16, Optimal reward -748.56
Iteration 70 took 3.53 seconds (mean sampled reward: -3155.51). Current reward after update: -1023.63, Optimal reward -748.56
Iteration 71 took 3.54 seconds (mean sampled reward: -2609.70). Current reward after update: -918.44, Optimal reward -748.56
Iteration 72 took 3.58 seconds (mean sampled reward: -3252.68). Current reward after update: -1058.61, Optimal reward -748.56
Iteration 73 took 3.63 seconds (mean sampled reward: -3585.23). Current reward after update: -1090.42, Optimal reward -748.56
Iteration 74 took 3.59 seconds (mean sampled reward: -3972.53). Current reward after update: -939.23, Optimal reward -748.56
Iteration 75 took 3.58 seconds (mean sampled reward: -3432.62). Current reward after update: -989.98, Optimal reward -748.56
Iteration 76 took 3.71 seconds (mean sampled reward: -4356.05). Current reward after update: -968.52, Optimal reward -748.56
Iteration 77 took 3.59 seconds (mean sampled reward: -4726.77). Current reward after update: -879.43, Optimal reward -748.56
Iteration 78 took 3.61 seconds (mean sampled reward: -4855.00). Current reward after update: -1009.94, Optimal reward -748.56
Iteration 79 took 3.55 seconds (mean sampled reward: -4120.53). Current reward after update: -931.34, Optimal reward -748.56
Iteration 80 took 3.77 seconds (mean sampled reward: -5029.16). Current reward after update: -842.64, Optimal reward -748.56
Iteration 81 took 3.64 seconds (mean sampled reward: -4096.56). Current reward after update: -949.47, Optimal reward -748.56
Iteration 82 took 3.82 seconds (mean sampled reward: -3901.95). Current reward after update: -932.84, Optimal reward -748.56
Iteration 83 took 3.58 seconds (mean sampled reward: -3579.19). Current reward after update: -931.14, Optimal reward -748.56
Iteration 84 took 3.59 seconds (mean sampled reward: -5121.03). Current reward after update: -998.45, Optimal reward -748.56
Iteration 85 took 3.54 seconds (mean sampled reward: -3639.10). Current reward after update: -877.32, Optimal reward -748.56
Iteration 86 took 3.70 seconds (mean sampled reward: -3236.17). Current reward after update: -795.87, Optimal reward -748.56
Iteration 87 took 3.70 seconds (mean sampled reward: -3482.80). Current reward after update: -778.65, Optimal reward -748.56
Iteration 88 took 3.71 seconds (mean sampled reward: -3772.96). Current reward after update: -834.54, Optimal reward -748.56
Iteration 89 took 3.75 seconds (mean sampled reward: -2980.95). Current reward after update: -770.82, Optimal reward -748.56
Iteration 90 took 3.86 seconds (mean sampled reward: -3461.56). Current reward after update: -757.32, Optimal reward -748.56
Iteration 91 took 3.93 seconds (mean sampled reward: -4390.64). Current reward after update: -954.71, Optimal reward -748.56
Iteration 92 took 3.81 seconds (mean sampled reward: -2965.93). Current reward after update: -684.75, Optimal reward -684.75
Iteration 93 took 3.81 seconds (mean sampled reward: -2847.79). Current reward after update: -655.97, Optimal reward -655.97
Iteration 94 took 3.79 seconds (mean sampled reward: -3107.92). Current reward after update: -760.41, Optimal reward -655.97
Iteration 95 took 3.86 seconds (mean sampled reward: -3202.53). Current reward after update: -652.02, Optimal reward -652.02
Iteration 96 took 3.73 seconds (mean sampled reward: -3252.29). Current reward after update: -759.82, Optimal reward -652.02
Iteration 97 took 3.93 seconds (mean sampled reward: -3892.04). Current reward after update: -848.19, Optimal reward -652.02
Iteration 98 took 4.02 seconds (mean sampled reward: -4713.79). Current reward after update: -865.39, Optimal reward -652.02
Iteration 99 took 3.87 seconds (mean sampled reward: -3124.37). Current reward after update: -834.80, Optimal reward -652.02
Iteration 100 took 3.88 seconds (mean sampled reward: -3208.34). Current reward after update: -806.38, Optimal reward -652.02
Iteration 101 took 4.02 seconds (mean sampled reward: -4971.01). Current reward after update: -844.34, Optimal reward -652.02
Iteration 102 took 3.95 seconds (mean sampled reward: -4325.08). Current reward after update: -770.14, Optimal reward -652.02
Iteration 103 took 3.84 seconds (mean sampled reward: -4459.25). Current reward after update: -743.81, Optimal reward -652.02
Iteration 104 took 3.96 seconds (mean sampled reward: -4535.74). Current reward after update: -1036.02, Optimal reward -652.02
Iteration 105 took 3.96 seconds (mean sampled reward: -5023.05). Current reward after update: -602.87, Optimal reward -602.87
Iteration 106 took 3.99 seconds (mean sampled reward: -4169.12). Current reward after update: -833.61, Optimal reward -602.87
Iteration 107 took 4.02 seconds (mean sampled reward: -2922.73). Current reward after update: -635.65, Optimal reward -602.87
Iteration 108 took 4.11 seconds (mean sampled reward: -2987.25). Current reward after update: -691.77, Optimal reward -602.87
Iteration 109 took 3.87 seconds (mean sampled reward: -3306.22). Current reward after update: -690.12, Optimal reward -602.87
Iteration 110 took 3.84 seconds (mean sampled reward: -3368.63). Current reward after update: -695.95, Optimal reward -602.87
Iteration 111 took 3.79 seconds (mean sampled reward: -3371.75). Current reward after update: -724.99, Optimal reward -602.87
Iteration 112 took 4.05 seconds (mean sampled reward: -3951.27). Current reward after update: -622.56, Optimal reward -602.87
Iteration 113 took 3.89 seconds (mean sampled reward: -3120.80). Current reward after update: -584.81, Optimal reward -584.81
Iteration 114 took 3.81 seconds (mean sampled reward: -3450.70). Current reward after update: -515.52, Optimal reward -515.52
Iteration 115 took 3.66 seconds (mean sampled reward: -3510.46). Current reward after update: -569.44, Optimal reward -515.52
Iteration 116 took 3.50 seconds (mean sampled reward: -3010.36). Current reward after update: -546.64, Optimal reward -515.52
Iteration 117 took 3.61 seconds (mean sampled reward: -3257.02). Current reward after update: -578.28, Optimal reward -515.52
Iteration 118 took 3.78 seconds (mean sampled reward: -2883.32). Current reward after update: -581.15, Optimal reward -515.52
Iteration 119 took 3.89 seconds (mean sampled reward: -3205.64). Current reward after update: -960.55, Optimal reward -515.52
Iteration 120 took 3.97 seconds (mean sampled reward: -3727.47). Current reward after update: -613.43, Optimal reward -515.52
Iteration 121 took 3.80 seconds (mean sampled reward: -3309.59). Current reward after update: -678.25, Optimal reward -515.52
Iteration 122 took 4.02 seconds (mean sampled reward: -4808.39). Current reward after update: -606.73, Optimal reward -515.52
Iteration 123 took 3.75 seconds (mean sampled reward: -2945.13). Current reward after update: -614.43, Optimal reward -515.52
Iteration 124 took 3.83 seconds (mean sampled reward: -3195.72). Current reward after update: -577.01, Optimal reward -515.52
Iteration 125 took 3.77 seconds (mean sampled reward: -3177.45). Current reward after update: -546.46, Optimal reward -515.52
Iteration 126 took 3.78 seconds (mean sampled reward: -3146.51). Current reward after update: -584.75, Optimal reward -515.52
Iteration 127 took 3.88 seconds (mean sampled reward: -4386.61). Current reward after update: -672.47, Optimal reward -515.52
Iteration 128 took 3.76 seconds (mean sampled reward: -3748.43). Current reward after update: -625.94, Optimal reward -515.52
Iteration 129 took 3.70 seconds (mean sampled reward: -3180.19). Current reward after update: -628.63, Optimal reward -515.52
Iteration 130 took 3.86 seconds (mean sampled reward: -4211.55). Current reward after update: -687.77, Optimal reward -515.52
Iteration 131 took 3.86 seconds (mean sampled reward: -4001.00). Current reward after update: -660.78, Optimal reward -515.52
Iteration 132 took 3.97 seconds (mean sampled reward: -4620.34). Current reward after update: -604.33, Optimal reward -515.52
Iteration 133 took 3.90 seconds (mean sampled reward: -3714.93). Current reward after update: -652.20, Optimal reward -515.52
Iteration 134 took 3.85 seconds (mean sampled reward: -3528.96). Current reward after update: -611.71, Optimal reward -515.52
Iteration 135 took 4.04 seconds (mean sampled reward: -4086.10). Current reward after update: -531.69, Optimal reward -515.52
Iteration 136 took 3.81 seconds (mean sampled reward: -3467.21). Current reward after update: -563.02, Optimal reward -515.52
Iteration 137 took 3.88 seconds (mean sampled reward: -3840.20). Current reward after update: -577.22, Optimal reward -515.52
Iteration 138 took 3.93 seconds (mean sampled reward: -4211.31). Current reward after update: -541.78, Optimal reward -515.52
Iteration 139 took 3.78 seconds (mean sampled reward: -2967.62). Current reward after update: -605.98, Optimal reward -515.52
Iteration 140 took 3.65 seconds (mean sampled reward: -2409.51). Current reward after update: -526.60, Optimal reward -515.52
Iteration 141 took 3.89 seconds (mean sampled reward: -4118.07). Current reward after update: -526.24, Optimal reward -515.52
Iteration 142 took 3.98 seconds (mean sampled reward: -5079.37). Current reward after update: -477.31, Optimal reward -477.31
Iteration 143 took 3.95 seconds (mean sampled reward: -4625.07). Current reward after update: -614.78, Optimal reward -477.31
Iteration 144 took 3.91 seconds (mean sampled reward: -3513.94). Current reward after update: -528.69, Optimal reward -477.31
Iteration 145 took 4.07 seconds (mean sampled reward: -5478.42). Current reward after update: -621.70, Optimal reward -477.31
Iteration 146 took 4.00 seconds (mean sampled reward: -4441.93). Current reward after update: -649.58, Optimal reward -477.31
Iteration 147 took 3.74 seconds (mean sampled reward: -3433.04). Current reward after update: -493.02, Optimal reward -477.31
Iteration 148 took 3.80 seconds (mean sampled reward: -3768.78). Current reward after update: -553.27, Optimal reward -477.31
Iteration 149 took 3.89 seconds (mean sampled reward: -4322.61). Current reward after update: -605.96, Optimal reward -477.31
Iteration 150 took 3.77 seconds (mean sampled reward: -2923.07). Current reward after update: -592.06, Optimal reward -477.31
Iteration 151 took 3.94 seconds (mean sampled reward: -3676.06). Current reward after update: -552.72, Optimal reward -477.31
Iteration 152 took 3.89 seconds (mean sampled reward: -3442.45). Current reward after update: -551.92, Optimal reward -477.31
Iteration 153 took 3.72 seconds (mean sampled reward: -2712.32). Current reward after update: -503.23, Optimal reward -477.31
Iteration 154 took 3.81 seconds (mean sampled reward: -3860.96). Current reward after update: -540.35, Optimal reward -477.31
Iteration 155 took 3.72 seconds (mean sampled reward: -3175.43). Current reward after update: -482.70, Optimal reward -477.31
Iteration 156 took 3.70 seconds (mean sampled reward: -3125.00). Current reward after update: -688.06, Optimal reward -477.31
Iteration 157 took 3.68 seconds (mean sampled reward: -3353.16). Current reward after update: -668.00, Optimal reward -477.31
Iteration 158 took 3.65 seconds (mean sampled reward: -2877.18). Current reward after update: -569.20, Optimal reward -477.31
Iteration 159 took 3.83 seconds (mean sampled reward: -2944.80). Current reward after update: -551.89, Optimal reward -477.31
Iteration 160 took 3.83 seconds (mean sampled reward: -3603.11). Current reward after update: -564.00, Optimal reward -477.31
Iteration 161 took 3.45 seconds (mean sampled reward: -2854.62). Current reward after update: -699.47, Optimal reward -477.31
Iteration 162 took 3.41 seconds (mean sampled reward: -2950.33). Current reward after update: -656.54, Optimal reward -477.31
Iteration 163 took 3.48 seconds (mean sampled reward: -3272.37). Current reward after update: -604.24, Optimal reward -477.31
Iteration 164 took 3.41 seconds (mean sampled reward: -2707.76). Current reward after update: -843.60, Optimal reward -477.31
Iteration 165 took 3.47 seconds (mean sampled reward: -2933.06). Current reward after update: -688.38, Optimal reward -477.31
Iteration 166 took 3.53 seconds (mean sampled reward: -2840.52). Current reward after update: -5715.10, Optimal reward -477.31
Iteration 167 took 3.71 seconds (mean sampled reward: -4009.98). Current reward after update: -697.75, Optimal reward -477.31
Iteration 168 took 3.49 seconds (mean sampled reward: -3332.44). Current reward after update: -590.31, Optimal reward -477.31
Iteration 169 took 3.54 seconds (mean sampled reward: -3171.31). Current reward after update: -887.95, Optimal reward -477.31
Iteration 170 took 3.60 seconds (mean sampled reward: -4295.54). Current reward after update: -710.26, Optimal reward -477.31
Iteration 171 took 3.81 seconds (mean sampled reward: -4639.64). Current reward after update: -979.03, Optimal reward -477.31
Iteration 172 took 3.59 seconds (mean sampled reward: -4504.85). Current reward after update: -663.82, Optimal reward -477.31
Iteration 173 took 3.61 seconds (mean sampled reward: -4436.18). Current reward after update: -729.52, Optimal reward -477.31
Iteration 174 took 3.62 seconds (mean sampled reward: -4852.78). Current reward after update: -692.56, Optimal reward -477.31
Iteration 175 took 3.57 seconds (mean sampled reward: -3723.38). Current reward after update: -639.15, Optimal reward -477.31
Iteration 176 took 3.54 seconds (mean sampled reward: -2680.03). Current reward after update: -625.86, Optimal reward -477.31
Iteration 177 took 3.48 seconds (mean sampled reward: -2740.33). Current reward after update: -654.19, Optimal reward -477.31
Iteration 178 took 3.47 seconds (mean sampled reward: -2608.80). Current reward after update: -644.09, Optimal reward -477.31
Iteration 179 took 3.71 seconds (mean sampled reward: -3791.92). Current reward after update: -734.24, Optimal reward -477.31
Iteration 180 took 3.47 seconds (mean sampled reward: -3324.97). Current reward after update: -779.24, Optimal reward -477.31
Iteration 181 took 3.49 seconds (mean sampled reward: -3477.79). Current reward after update: -769.42, Optimal reward -477.31
Iteration 182 took 3.58 seconds (mean sampled reward: -3085.44). Current reward after update: -702.54, Optimal reward -477.31
Iteration 183 took 3.56 seconds (mean sampled reward: -3975.06). Current reward after update: -721.82, Optimal reward -477.31
Iteration 184 took 3.66 seconds (mean sampled reward: -4912.41). Current reward after update: -907.89, Optimal reward -477.31
Iteration 185 took 3.70 seconds (mean sampled reward: -4754.23). Current reward after update: -893.47, Optimal reward -477.31
Iteration 186 took 3.68 seconds (mean sampled reward: -4610.06). Current reward after update: -680.84, Optimal reward -477.31
Iteration 187 took 3.72 seconds (mean sampled reward: -3537.37). Current reward after update: -609.98, Optimal reward -477.31
Iteration 188 took 3.55 seconds (mean sampled reward: -3207.22). Current reward after update: -689.21, Optimal reward -477.31
Iteration 189 took 3.52 seconds (mean sampled reward: -3093.67). Current reward after update: -693.01, Optimal reward -477.31
Iteration 190 took 3.53 seconds (mean sampled reward: -2773.99). Current reward after update: -697.78, Optimal reward -477.31
Iteration 191 took 3.61 seconds (mean sampled reward: -2901.43). Current reward after update: -704.42, Optimal reward -477.31
Iteration 192 took 3.63 seconds (mean sampled reward: -3528.82). Current reward after update: -616.85, Optimal reward -477.31
Iteration 193 took 3.50 seconds (mean sampled reward: -3177.13). Current reward after update: -717.78, Optimal reward -477.31
Iteration 194 took 3.48 seconds (mean sampled reward: -2821.84). Current reward after update: -637.11, Optimal reward -477.31
Iteration 195 took 3.48 seconds (mean sampled reward: -3137.15). Current reward after update: -661.54, Optimal reward -477.31
Iteration 196 took 3.58 seconds (mean sampled reward: -3617.49). Current reward after update: -598.95, Optimal reward -477.31
Iteration 197 took 3.72 seconds (mean sampled reward: -4941.18). Current reward after update: -675.65, Optimal reward -477.31
Iteration 198 took 3.67 seconds (mean sampled reward: -5160.51). Current reward after update: -665.54, Optimal reward -477.31
Iteration 199 took 3.54 seconds (mean sampled reward: -3481.46). Current reward after update: -686.63, Optimal reward -477.31
Iteration 200 took 3.41 seconds (mean sampled reward: -2610.17). Current reward after update: -710.97, Optimal reward -477.31
Iteration 201 took 3.66 seconds (mean sampled reward: -4352.76). Current reward after update: -714.30, Optimal reward -477.31
Iteration 202 took 3.57 seconds (mean sampled reward: -2622.26). Current reward after update: -678.46, Optimal reward -477.31
Iteration 203 took 3.48 seconds (mean sampled reward: -3222.75). Current reward after update: -612.94, Optimal reward -477.31
Iteration 204 took 3.48 seconds (mean sampled reward: -2583.38). Current reward after update: -687.69, Optimal reward -477.31
Iteration 205 took 3.50 seconds (mean sampled reward: -2605.46). Current reward after update: -638.55, Optimal reward -477.31
Iteration 206 took 3.55 seconds (mean sampled reward: -2898.78). Current reward after update: -622.89, Optimal reward -477.31
Iteration 207 took 3.46 seconds (mean sampled reward: -2597.31). Current reward after update: -642.49, Optimal reward -477.31
Iteration 208 took 3.53 seconds (mean sampled reward: -2867.92). Current reward after update: -592.82, Optimal reward -477.31
Iteration 209 took 3.54 seconds (mean sampled reward: -2699.01). Current reward after update: -661.31, Optimal reward -477.31
Iteration 210 took 3.44 seconds (mean sampled reward: -2726.17). Current reward after update: -629.10, Optimal reward -477.31
Iteration 211 took 3.53 seconds (mean sampled reward: -2836.77). Current reward after update: -545.67, Optimal reward -477.31
Iteration 212 took 3.78 seconds (mean sampled reward: -3077.67). Current reward after update: -658.88, Optimal reward -477.31
Iteration 213 took 3.46 seconds (mean sampled reward: -2618.75). Current reward after update: -656.35, Optimal reward -477.31
Iteration 214 took 3.56 seconds (mean sampled reward: -2683.36). Current reward after update: -5742.67, Optimal reward -477.31
Iteration 215 took 3.50 seconds (mean sampled reward: -3248.22). Current reward after update: -705.86, Optimal reward -477.31
Iteration 216 took 3.38 seconds (mean sampled reward: -2699.05). Current reward after update: -751.39, Optimal reward -477.31
Iteration 217 took 3.60 seconds (mean sampled reward: -2977.01). Current reward after update: -689.24, Optimal reward -477.31
Iteration 218 took 3.61 seconds (mean sampled reward: -3468.18). Current reward after update: -621.44, Optimal reward -477.31
Iteration 219 took 3.61 seconds (mean sampled reward: -3349.73). Current reward after update: -624.26, Optimal reward -477.31
Iteration 220 took 3.78 seconds (mean sampled reward: -4572.41). Current reward after update: -723.54, Optimal reward -477.31
Iteration 221 took 3.62 seconds (mean sampled reward: -3658.32). Current reward after update: -646.49, Optimal reward -477.31
Iteration 222 took 3.63 seconds (mean sampled reward: -3629.84). Current reward after update: -688.03, Optimal reward -477.31
Iteration 223 took 3.65 seconds (mean sampled reward: -3506.19). Current reward after update: -657.14, Optimal reward -477.31
Iteration 224 took 3.75 seconds (mean sampled reward: -4196.47). Current reward after update: -799.14, Optimal reward -477.31
Iteration 225 took 3.67 seconds (mean sampled reward: -3187.72). Current reward after update: -753.18, Optimal reward -477.31
Iteration 226 took 3.59 seconds (mean sampled reward: -3262.65). Current reward after update: -728.26, Optimal reward -477.31
Iteration 227 took 3.58 seconds (mean sampled reward: -4489.53). Current reward after update: -738.99, Optimal reward -477.31
Iteration 228 took 3.58 seconds (mean sampled reward: -4332.95). Current reward after update: -704.64, Optimal reward -477.31
Iteration 229 took 3.57 seconds (mean sampled reward: -4136.25). Current reward after update: -743.77, Optimal reward -477.31
Iteration 230 took 3.63 seconds (mean sampled reward: -4437.00). Current reward after update: -717.05, Optimal reward -477.31
Iteration 231 took 3.52 seconds (mean sampled reward: -4461.94). Current reward after update: -729.85, Optimal reward -477.31
Iteration 232 took 3.50 seconds (mean sampled reward: -3125.16). Current reward after update: -699.13, Optimal reward -477.31
Iteration 233 took 3.46 seconds (mean sampled reward: -2820.74). Current reward after update: -766.50, Optimal reward -477.31
Iteration 234 took 3.45 seconds (mean sampled reward: -2953.91). Current reward after update: -669.43, Optimal reward -477.31
Iteration 235 took 3.57 seconds (mean sampled reward: -3591.99). Current reward after update: -653.83, Optimal reward -477.31
Iteration 236 took 3.50 seconds (mean sampled reward: -4560.84). Current reward after update: -819.90, Optimal reward -477.31
Iteration 237 took 3.56 seconds (mean sampled reward: -3597.62). Current reward after update: -705.81, Optimal reward -477.31
Iteration 238 took 3.55 seconds (mean sampled reward: -4265.32). Current reward after update: -639.55, Optimal reward -477.31
Iteration 239 took 3.58 seconds (mean sampled reward: -3726.26). Current reward after update: -762.31, Optimal reward -477.31
Iteration 240 took 3.46 seconds (mean sampled reward: -2953.33). Current reward after update: -714.91, Optimal reward -477.31
Iteration 241 took 3.49 seconds (mean sampled reward: -3051.99). Current reward after update: -683.48, Optimal reward -477.31
Iteration 242 took 3.54 seconds (mean sampled reward: -3543.48). Current reward after update: -775.55, Optimal reward -477.31
Iteration 243 took 3.42 seconds (mean sampled reward: -3187.63). Current reward after update: -660.15, Optimal reward -477.31
Iteration 244 took 3.38 seconds (mean sampled reward: -3226.11). Current reward after update: -774.34, Optimal reward -477.31
Iteration 245 took 3.75 seconds (mean sampled reward: -4552.27). Current reward after update: -864.08, Optimal reward -477.31
Iteration 246 took 3.47 seconds (mean sampled reward: -3474.10). Current reward after update: -648.50, Optimal reward -477.31
Iteration 247 took 3.41 seconds (mean sampled reward: -3297.39). Current reward after update: -692.60, Optimal reward -477.31
Iteration 248 took 3.43 seconds (mean sampled reward: -3080.10). Current reward after update: -579.13, Optimal reward -477.31
Iteration 249 took 3.48 seconds (mean sampled reward: -3862.94). Current reward after update: -855.93, Optimal reward -477.31
Iteration 250 took 3.78 seconds (mean sampled reward: -3485.77). Current reward after update: -652.47, Optimal reward -477.31
Iteration 251 took 3.54 seconds (mean sampled reward: -3453.78). Current reward after update: -757.36, Optimal reward -477.31
Iteration 252 took 3.48 seconds (mean sampled reward: -3014.79). Current reward after update: -741.74, Optimal reward -477.31
Iteration 253 took 3.50 seconds (mean sampled reward: -3507.24). Current reward after update: -985.31, Optimal reward -477.31
Iteration 254 took 3.49 seconds (mean sampled reward: -3460.50). Current reward after update: -654.85, Optimal reward -477.31
Iteration 255 took 3.60 seconds (mean sampled reward: -3642.23). Current reward after update: -701.73, Optimal reward -477.31
Iteration 256 took 3.45 seconds (mean sampled reward: -3612.52). Current reward after update: -586.83, Optimal reward -477.31
Iteration 257 took 3.46 seconds (mean sampled reward: -2782.79). Current reward after update: -611.55, Optimal reward -477.31
Iteration 258 took 3.47 seconds (mean sampled reward: -3065.58). Current reward after update: -771.01, Optimal reward -477.31
Iteration 259 took 3.43 seconds (mean sampled reward: -3020.22). Current reward after update: -674.13, Optimal reward -477.31
Iteration 260 took 3.43 seconds (mean sampled reward: -3153.23). Current reward after update: -711.93, Optimal reward -477.31
Iteration 261 took 3.54 seconds (mean sampled reward: -3247.07). Current reward after update: -743.79, Optimal reward -477.31
Iteration 262 took 3.55 seconds (mean sampled reward: -3545.96). Current reward after update: -710.71, Optimal reward -477.31
Iteration 263 took 3.60 seconds (mean sampled reward: -3507.42). Current reward after update: -772.52, Optimal reward -477.31
Iteration 264 took 3.70 seconds (mean sampled reward: -4605.64). Current reward after update: -816.58, Optimal reward -477.31
Iteration 265 took 3.50 seconds (mean sampled reward: -3250.25). Current reward after update: -763.91, Optimal reward -477.31
Iteration 266 took 3.53 seconds (mean sampled reward: -4441.62). Current reward after update: -828.88, Optimal reward -477.31
Iteration 267 took 3.59 seconds (mean sampled reward: -4342.70). Current reward after update: -962.35, Optimal reward -477.31
Iteration 268 took 3.51 seconds (mean sampled reward: -4493.00). Current reward after update: -915.28, Optimal reward -477.31
Iteration 269 took 3.53 seconds (mean sampled reward: -3994.40). Current reward after update: -892.45, Optimal reward -477.31
Iteration 270 took 3.55 seconds (mean sampled reward: -4479.60). Current reward after update: -840.02, Optimal reward -477.31
Iteration 271 took 3.59 seconds (mean sampled reward: -4875.07). Current reward after update: -844.42, Optimal reward -477.31
Iteration 272 took 3.57 seconds (mean sampled reward: -4052.57). Current reward after update: -698.63, Optimal reward -477.31
Iteration 273 took 3.68 seconds (mean sampled reward: -4510.92). Current reward after update: -920.21, Optimal reward -477.31
Iteration 274 took 3.63 seconds (mean sampled reward: -4823.11). Current reward after update: -934.55, Optimal reward -477.31
Iteration 275 took 3.62 seconds (mean sampled reward: -4489.04). Current reward after update: -739.61, Optimal reward -477.31
Iteration 276 took 3.66 seconds (mean sampled reward: -3857.27). Current reward after update: -921.30, Optimal reward -477.31
Iteration 277 took 3.59 seconds (mean sampled reward: -3630.63). Current reward after update: -806.65, Optimal reward -477.31
Iteration 278 took 3.59 seconds (mean sampled reward: -4317.52). Current reward after update: -763.56, Optimal reward -477.31
Iteration 279 took 3.61 seconds (mean sampled reward: -4361.38). Current reward after update: -748.33, Optimal reward -477.31
Iteration 280 took 3.59 seconds (mean sampled reward: -3437.35). Current reward after update: -851.45, Optimal reward -477.31
Iteration 281 took 3.60 seconds (mean sampled reward: -3876.32). Current reward after update: -862.02, Optimal reward -477.31
Iteration 282 took 3.55 seconds (mean sampled reward: -3049.77). Current reward after update: -647.95, Optimal reward -477.31
Iteration 283 took 3.54 seconds (mean sampled reward: -3301.74). Current reward after update: -845.23, Optimal reward -477.31
Iteration 284 took 3.56 seconds (mean sampled reward: -3483.85). Current reward after update: -842.81, Optimal reward -477.31
Iteration 285 took 3.49 seconds (mean sampled reward: -3212.52). Current reward after update: -761.08, Optimal reward -477.31
Iteration 286 took 3.51 seconds (mean sampled reward: -3023.16). Current reward after update: -831.64, Optimal reward -477.31
Iteration 287 took 3.60 seconds (mean sampled reward: -2993.82). Current reward after update: -838.78, Optimal reward -477.31
Iteration 288 took 3.53 seconds (mean sampled reward: -3056.64). Current reward after update: -784.92, Optimal reward -477.31
Iteration 289 took 3.48 seconds (mean sampled reward: -3546.80). Current reward after update: -936.52, Optimal reward -477.31
Iteration 290 took 3.48 seconds (mean sampled reward: -3168.78). Current reward after update: -694.55, Optimal reward -477.31
Iteration 291 took 3.52 seconds (mean sampled reward: -3301.01). Current reward after update: -716.16, Optimal reward -477.31
Iteration 292 took 3.56 seconds (mean sampled reward: -3124.98). Current reward after update: -640.38, Optimal reward -477.31
Iteration 293 took 3.60 seconds (mean sampled reward: -3347.57). Current reward after update: -650.80, Optimal reward -477.31
Iteration 294 took 3.57 seconds (mean sampled reward: -4157.38). Current reward after update: -770.50, Optimal reward -477.31
Iteration 295 took 3.46 seconds (mean sampled reward: -3541.56). Current reward after update: -734.28, Optimal reward -477.31
Iteration 296 took 3.63 seconds (mean sampled reward: -3802.61). Current reward after update: -603.01, Optimal reward -477.31
Iteration 297 took 3.45 seconds (mean sampled reward: -3354.99). Current reward after update: -664.27, Optimal reward -477.31
Iteration 298 took 3.63 seconds (mean sampled reward: -4691.02). Current reward after update: -729.25, Optimal reward -477.31
Iteration 299 took 3.60 seconds (mean sampled reward: -4725.03). Current reward after update: -713.40, Optimal reward -477.31
Iteration 300 took 3.54 seconds (mean sampled reward: -3952.57). Current reward after update: -660.44, Optimal reward -477.31
Iteration 1 took 3.86 seconds (mean sampled reward: -6145.44). Current reward after update: -3161.22, Optimal reward -3161.22
Iteration 2 took 3.70 seconds (mean sampled reward: -6022.79). Current reward after update: -2913.75, Optimal reward -2913.75
Iteration 3 took 3.95 seconds (mean sampled reward: -5301.84). Current reward after update: -2256.68, Optimal reward -2256.68
Iteration 4 took 3.83 seconds (mean sampled reward: -5518.56). Current reward after update: -1354.24, Optimal reward -1354.24
Iteration 5 took 3.68 seconds (mean sampled reward: -5279.68). Current reward after update: -1584.83, Optimal reward -1354.24
Iteration 6 took 3.98 seconds (mean sampled reward: -4853.55). Current reward after update: -1240.49, Optimal reward -1240.49
Iteration 7 took 3.53 seconds (mean sampled reward: -4910.11). Current reward after update: -1345.32, Optimal reward -1240.49
Iteration 8 took 3.46 seconds (mean sampled reward: -4455.87). Current reward after update: -907.51, Optimal reward -907.51
Iteration 9 took 3.41 seconds (mean sampled reward: -4765.53). Current reward after update: -715.70, Optimal reward -715.70
Iteration 10 took 3.71 seconds (mean sampled reward: -3852.71). Current reward after update: -582.06, Optimal reward -582.06
Iteration 11 took 3.48 seconds (mean sampled reward: -4375.13). Current reward after update: -719.60, Optimal reward -582.06
Iteration 12 took 3.54 seconds (mean sampled reward: -3952.01). Current reward after update: -604.32, Optimal reward -582.06
Iteration 13 took 3.58 seconds (mean sampled reward: -3807.20). Current reward after update: -555.06, Optimal reward -555.06
Iteration 14 took 3.53 seconds (mean sampled reward: -3426.71). Current reward after update: -441.94, Optimal reward -441.94
Iteration 15 took 3.74 seconds (mean sampled reward: -4143.89). Current reward after update: -477.03, Optimal reward -441.94
Iteration 16 took 3.55 seconds (mean sampled reward: -4136.89). Current reward after update: -569.52, Optimal reward -441.94
Iteration 17 took 3.62 seconds (mean sampled reward: -4464.33). Current reward after update: -470.07, Optimal reward -441.94
Iteration 18 took 3.68 seconds (mean sampled reward: -4493.71). Current reward after update: -514.20, Optimal reward -441.94
Iteration 19 took 3.62 seconds (mean sampled reward: -4619.35). Current reward after update: -414.62, Optimal reward -414.62
Iteration 20 took 3.79 seconds (mean sampled reward: -4224.84). Current reward after update: -419.18, Optimal reward -414.62
Iteration 21 took 3.70 seconds (mean sampled reward: -4884.89). Current reward after update: -596.77, Optimal reward -414.62
Iteration 22 took 3.66 seconds (mean sampled reward: -4931.52). Current reward after update: -474.46, Optimal reward -414.62
Iteration 23 took 3.66 seconds (mean sampled reward: -4269.25). Current reward after update: -452.13, Optimal reward -414.62
Iteration 24 took 3.63 seconds (mean sampled reward: -4648.08). Current reward after update: -483.08, Optimal reward -414.62
Iteration 25 took 3.56 seconds (mean sampled reward: -4295.01). Current reward after update: -597.10, Optimal reward -414.62
Iteration 26 took 3.46 seconds (mean sampled reward: -3204.25). Current reward after update: -507.62, Optimal reward -414.62
Iteration 27 took 3.61 seconds (mean sampled reward: -3776.29). Current reward after update: -506.42, Optimal reward -414.62
Iteration 28 took 3.77 seconds (mean sampled reward: -3262.02). Current reward after update: -520.14, Optimal reward -414.62
Iteration 29 took 3.92 seconds (mean sampled reward: -3185.48). Current reward after update: -459.92, Optimal reward -414.62
Iteration 30 took 3.83 seconds (mean sampled reward: -3026.02). Current reward after update: -595.08, Optimal reward -414.62
Iteration 31 took 3.60 seconds (mean sampled reward: -3086.40). Current reward after update: -340.14, Optimal reward -340.14
Iteration 32 took 3.64 seconds (mean sampled reward: -3599.21). Current reward after update: -502.23, Optimal reward -340.14
Iteration 33 took 3.64 seconds (mean sampled reward: -3837.83). Current reward after update: -593.27, Optimal reward -340.14
Iteration 34 took 3.57 seconds (mean sampled reward: -3684.04). Current reward after update: -572.75, Optimal reward -340.14
Iteration 35 took 3.54 seconds (mean sampled reward: -4047.81). Current reward after update: -644.37, Optimal reward -340.14
Iteration 36 took 3.67 seconds (mean sampled reward: -3780.02). Current reward after update: -539.09, Optimal reward -340.14
Iteration 37 took 3.54 seconds (mean sampled reward: -3496.02). Current reward after update: -616.01, Optimal reward -340.14
Iteration 38 took 3.51 seconds (mean sampled reward: -3512.39). Current reward after update: -617.26, Optimal reward -340.14
Iteration 39 took 3.50 seconds (mean sampled reward: -4094.73). Current reward after update: -600.98, Optimal reward -340.14
Iteration 40 took 3.65 seconds (mean sampled reward: -4397.66). Current reward after update: -598.40, Optimal reward -340.14
Iteration 41 took 3.61 seconds (mean sampled reward: -3710.58). Current reward after update: -562.80, Optimal reward -340.14
Iteration 42 took 3.55 seconds (mean sampled reward: -3707.87). Current reward after update: -514.65, Optimal reward -340.14
Iteration 43 took 3.53 seconds (mean sampled reward: -3141.17). Current reward after update: -621.51, Optimal reward -340.14
Iteration 44 took 3.73 seconds (mean sampled reward: -3676.12). Current reward after update: -538.88, Optimal reward -340.14
Iteration 45 took 3.78 seconds (mean sampled reward: -3830.13). Current reward after update: -705.22, Optimal reward -340.14
Iteration 46 took 3.87 seconds (mean sampled reward: -4052.12). Current reward after update: -568.73, Optimal reward -340.14
Iteration 47 took 3.68 seconds (mean sampled reward: -3995.86). Current reward after update: -627.54, Optimal reward -340.14
Iteration 48 took 3.70 seconds (mean sampled reward: -3416.69). Current reward after update: -577.98, Optimal reward -340.14
Iteration 49 took 3.76 seconds (mean sampled reward: -3576.06). Current reward after update: -576.98, Optimal reward -340.14
Iteration 50 took 3.63 seconds (mean sampled reward: -2938.22). Current reward after update: -578.03, Optimal reward -340.14
Iteration 51 took 3.67 seconds (mean sampled reward: -4020.10). Current reward after update: -558.18, Optimal reward -340.14
Iteration 52 took 3.68 seconds (mean sampled reward: -3957.09). Current reward after update: -422.24, Optimal reward -340.14
Iteration 53 took 3.77 seconds (mean sampled reward: -3238.01). Current reward after update: -465.95, Optimal reward -340.14
Iteration 54 took 3.81 seconds (mean sampled reward: -4443.88). Current reward after update: -436.01, Optimal reward -340.14
Iteration 55 took 3.70 seconds (mean sampled reward: -3918.65). Current reward after update: -416.65, Optimal reward -340.14
Iteration 56 took 3.92 seconds (mean sampled reward: -4086.32). Current reward after update: -444.42, Optimal reward -340.14
Iteration 57 took 3.95 seconds (mean sampled reward: -4961.23). Current reward after update: -376.56, Optimal reward -340.14
Iteration 58 took 3.83 seconds (mean sampled reward: -4222.00). Current reward after update: -411.90, Optimal reward -340.14
Iteration 59 took 3.81 seconds (mean sampled reward: -4686.80). Current reward after update: -548.32, Optimal reward -340.14
Iteration 60 took 3.85 seconds (mean sampled reward: -4781.74). Current reward after update: -494.34, Optimal reward -340.14
Iteration 61 took 4.07 seconds (mean sampled reward: -4468.49). Current reward after update: -689.80, Optimal reward -340.14
Iteration 62 took 3.65 seconds (mean sampled reward: -3082.83). Current reward after update: -1111.20, Optimal reward -340.14
Iteration 63 took 3.77 seconds (mean sampled reward: -3244.35). Current reward after update: -403.68, Optimal reward -340.14
Iteration 64 took 3.66 seconds (mean sampled reward: -2935.01). Current reward after update: -242.12, Optimal reward -242.12
Iteration 65 took 3.73 seconds (mean sampled reward: -3696.91). Current reward after update: -368.67, Optimal reward -242.12
Iteration 66 took 3.85 seconds (mean sampled reward: -4718.89). Current reward after update: -438.03, Optimal reward -242.12
Iteration 67 took 3.84 seconds (mean sampled reward: -4283.37). Current reward after update: -356.27, Optimal reward -242.12
Iteration 68 took 3.96 seconds (mean sampled reward: -4387.83). Current reward after update: -328.92, Optimal reward -242.12
Iteration 69 took 3.84 seconds (mean sampled reward: -4613.32). Current reward after update: -415.00, Optimal reward -242.12
Iteration 70 took 3.83 seconds (mean sampled reward: -4394.44). Current reward after update: -337.47, Optimal reward -242.12
Iteration 71 took 3.85 seconds (mean sampled reward: -4735.90). Current reward after update: -345.52, Optimal reward -242.12
Iteration 72 took 3.80 seconds (mean sampled reward: -4664.25). Current reward after update: -516.07, Optimal reward -242.12
Iteration 73 took 3.89 seconds (mean sampled reward: -4411.40). Current reward after update: -461.25, Optimal reward -242.12
Iteration 74 took 3.75 seconds (mean sampled reward: -3527.19). Current reward after update: -391.95, Optimal reward -242.12
Iteration 75 took 4.01 seconds (mean sampled reward: -4453.49). Current reward after update: -405.92, Optimal reward -242.12
Iteration 76 took 3.94 seconds (mean sampled reward: -3835.70). Current reward after update: -429.20, Optimal reward -242.12
Iteration 77 took 3.97 seconds (mean sampled reward: -4258.43). Current reward after update: -558.02, Optimal reward -242.12
Iteration 78 took 4.01 seconds (mean sampled reward: -4221.12). Current reward after update: -495.44, Optimal reward -242.12
Iteration 79 took 4.06 seconds (mean sampled reward: -4562.50). Current reward after update: -437.77, Optimal reward -242.12
Iteration 80 took 4.06 seconds (mean sampled reward: -4418.31). Current reward after update: -403.29, Optimal reward -242.12
Iteration 81 took 4.00 seconds (mean sampled reward: -5010.65). Current reward after update: -542.32, Optimal reward -242.12
Iteration 82 took 3.87 seconds (mean sampled reward: -5047.37). Current reward after update: -495.79, Optimal reward -242.12
Iteration 83 took 3.85 seconds (mean sampled reward: -4881.48). Current reward after update: -590.84, Optimal reward -242.12
Iteration 84 took 3.94 seconds (mean sampled reward: -4477.50). Current reward after update: -536.88, Optimal reward -242.12
Iteration 85 took 3.90 seconds (mean sampled reward: -4594.54). Current reward after update: -394.42, Optimal reward -242.12
Iteration 86 took 3.96 seconds (mean sampled reward: -4675.04). Current reward after update: -559.02, Optimal reward -242.12
Iteration 87 took 3.82 seconds (mean sampled reward: -4243.35). Current reward after update: -456.41, Optimal reward -242.12
Iteration 88 took 3.96 seconds (mean sampled reward: -4810.35). Current reward after update: -480.13, Optimal reward -242.12
Iteration 89 took 3.95 seconds (mean sampled reward: -4437.67). Current reward after update: -539.39, Optimal reward -242.12
Iteration 90 took 3.94 seconds (mean sampled reward: -4856.27). Current reward after update: -518.98, Optimal reward -242.12
Iteration 91 took 3.94 seconds (mean sampled reward: -4920.39). Current reward after update: -600.38, Optimal reward -242.12
Iteration 92 took 3.82 seconds (mean sampled reward: -4490.36). Current reward after update: -482.78, Optimal reward -242.12
Iteration 93 took 3.78 seconds (mean sampled reward: -4706.16). Current reward after update: -5570.64, Optimal reward -242.12
Iteration 94 took 3.95 seconds (mean sampled reward: -5280.65). Current reward after update: -558.27, Optimal reward -242.12
Iteration 95 took 3.97 seconds (mean sampled reward: -5159.98). Current reward after update: -585.12, Optimal reward -242.12
Iteration 96 took 4.01 seconds (mean sampled reward: -4833.38). Current reward after update: -505.04, Optimal reward -242.12
Iteration 97 took 3.94 seconds (mean sampled reward: -4981.64). Current reward after update: -575.52, Optimal reward -242.12
Iteration 98 took 3.86 seconds (mean sampled reward: -4534.32). Current reward after update: -540.79, Optimal reward -242.12
Iteration 99 took 4.01 seconds (mean sampled reward: -5243.17). Current reward after update: -779.58, Optimal reward -242.12
Iteration 100 took 3.98 seconds (mean sampled reward: -4631.84). Current reward after update: -545.99, Optimal reward -242.12
Iteration 101 took 4.05 seconds (mean sampled reward: -4385.02). Current reward after update: -507.68, Optimal reward -242.12
Iteration 102 took 3.97 seconds (mean sampled reward: -4572.69). Current reward after update: -540.85, Optimal reward -242.12
Iteration 103 took 4.03 seconds (mean sampled reward: -4673.81). Current reward after update: -501.01, Optimal reward -242.12
Iteration 104 took 3.87 seconds (mean sampled reward: -4354.36). Current reward after update: -519.13, Optimal reward -242.12
Iteration 105 took 3.83 seconds (mean sampled reward: -4410.58). Current reward after update: -531.94, Optimal reward -242.12
Iteration 106 took 3.97 seconds (mean sampled reward: -4482.34). Current reward after update: -533.71, Optimal reward -242.12
Iteration 107 took 3.74 seconds (mean sampled reward: -3917.68). Current reward after update: -630.95, Optimal reward -242.12
Iteration 108 took 4.01 seconds (mean sampled reward: -4014.48). Current reward after update: -466.96, Optimal reward -242.12
Iteration 109 took 3.57 seconds (mean sampled reward: -4138.81). Current reward after update: -576.69, Optimal reward -242.12
Iteration 110 took 3.84 seconds (mean sampled reward: -4571.15). Current reward after update: -473.12, Optimal reward -242.12
Iteration 111 took 3.80 seconds (mean sampled reward: -4433.98). Current reward after update: -516.42, Optimal reward -242.12
Iteration 112 took 3.78 seconds (mean sampled reward: -4492.30). Current reward after update: -712.60, Optimal reward -242.12
Iteration 113 took 3.81 seconds (mean sampled reward: -4271.23). Current reward after update: -584.42, Optimal reward -242.12
Iteration 114 took 3.92 seconds (mean sampled reward: -4201.18). Current reward after update: -445.93, Optimal reward -242.12
Iteration 115 took 3.83 seconds (mean sampled reward: -3622.23). Current reward after update: -469.23, Optimal reward -242.12
Iteration 116 took 3.87 seconds (mean sampled reward: -3623.50). Current reward after update: -466.21, Optimal reward -242.12
Iteration 117 took 3.98 seconds (mean sampled reward: -3643.04). Current reward after update: -512.95, Optimal reward -242.12
Iteration 118 took 3.80 seconds (mean sampled reward: -3129.41). Current reward after update: -418.80, Optimal reward -242.12
Iteration 119 took 3.89 seconds (mean sampled reward: -4223.80). Current reward after update: -502.50, Optimal reward -242.12
Iteration 120 took 3.88 seconds (mean sampled reward: -4828.98). Current reward after update: -516.68, Optimal reward -242.12
Iteration 121 took 3.84 seconds (mean sampled reward: -4662.33). Current reward after update: -459.36, Optimal reward -242.12
Iteration 122 took 3.69 seconds (mean sampled reward: -3972.82). Current reward after update: -522.47, Optimal reward -242.12
Iteration 123 took 3.75 seconds (mean sampled reward: -4021.01). Current reward after update: -526.20, Optimal reward -242.12
Iteration 124 took 3.70 seconds (mean sampled reward: -4270.42). Current reward after update: -487.53, Optimal reward -242.12
Iteration 125 took 3.66 seconds (mean sampled reward: -3287.43). Current reward after update: -515.10, Optimal reward -242.12
Iteration 126 took 3.66 seconds (mean sampled reward: -3403.92). Current reward after update: -495.31, Optimal reward -242.12
Iteration 127 took 3.72 seconds (mean sampled reward: -3603.86). Current reward after update: -468.12, Optimal reward -242.12
Iteration 128 took 3.67 seconds (mean sampled reward: -3657.74). Current reward after update: -442.45, Optimal reward -242.12
Iteration 129 took 3.80 seconds (mean sampled reward: -4537.53). Current reward after update: -442.22, Optimal reward -242.12
Iteration 130 took 3.76 seconds (mean sampled reward: -4234.53). Current reward after update: -434.11, Optimal reward -242.12
Iteration 131 took 3.73 seconds (mean sampled reward: -4205.57). Current reward after update: -479.46, Optimal reward -242.12
Iteration 132 took 3.75 seconds (mean sampled reward: -4407.81). Current reward after update: -542.21, Optimal reward -242.12
Iteration 133 took 3.79 seconds (mean sampled reward: -4652.46). Current reward after update: -508.95, Optimal reward -242.12
Iteration 134 took 3.80 seconds (mean sampled reward: -4694.35). Current reward after update: -474.25, Optimal reward -242.12
Iteration 135 took 3.80 seconds (mean sampled reward: -4240.66). Current reward after update: -558.20, Optimal reward -242.12
Iteration 136 took 3.83 seconds (mean sampled reward: -4546.71). Current reward after update: -559.38, Optimal reward -242.12
Iteration 137 took 3.79 seconds (mean sampled reward: -3977.43). Current reward after update: -528.49, Optimal reward -242.12
Iteration 138 took 3.61 seconds (mean sampled reward: -3387.58). Current reward after update: -520.35, Optimal reward -242.12
Iteration 139 took 3.80 seconds (mean sampled reward: -4215.75). Current reward after update: -615.09, Optimal reward -242.12
Iteration 140 took 3.74 seconds (mean sampled reward: -3656.24). Current reward after update: -554.84, Optimal reward -242.12
Iteration 141 took 3.71 seconds (mean sampled reward: -3777.72). Current reward after update: -574.12, Optimal reward -242.12
Iteration 142 took 3.65 seconds (mean sampled reward: -4084.39). Current reward after update: -615.78, Optimal reward -242.12
Iteration 143 took 3.65 seconds (mean sampled reward: -4180.65). Current reward after update: -612.08, Optimal reward -242.12
Iteration 144 took 3.69 seconds (mean sampled reward: -4053.05). Current reward after update: -516.66, Optimal reward -242.12
Iteration 145 took 3.72 seconds (mean sampled reward: -4687.02). Current reward after update: -627.43, Optimal reward -242.12
Iteration 146 took 3.69 seconds (mean sampled reward: -4270.22). Current reward after update: -546.81, Optimal reward -242.12
Iteration 147 took 3.79 seconds (mean sampled reward: -5140.44). Current reward after update: -620.62, Optimal reward -242.12
Iteration 148 took 3.72 seconds (mean sampled reward: -4513.62). Current reward after update: -1908.23, Optimal reward -242.12
Iteration 149 took 3.68 seconds (mean sampled reward: -4717.37). Current reward after update: -550.12, Optimal reward -242.12
Iteration 150 took 3.75 seconds (mean sampled reward: -5160.25). Current reward after update: -518.66, Optimal reward -242.12
Iteration 151 took 3.59 seconds (mean sampled reward: -3442.22). Current reward after update: -573.21, Optimal reward -242.12
Iteration 152 took 3.64 seconds (mean sampled reward: -4504.08). Current reward after update: -619.60, Optimal reward -242.12
Iteration 153 took 3.62 seconds (mean sampled reward: -3522.80). Current reward after update: -496.08, Optimal reward -242.12
Iteration 154 took 3.58 seconds (mean sampled reward: -2922.92). Current reward after update: -499.17, Optimal reward -242.12
Iteration 155 took 3.62 seconds (mean sampled reward: -2670.01). Current reward after update: -3229.38, Optimal reward -242.12
Iteration 156 took 3.68 seconds (mean sampled reward: -2977.96). Current reward after update: -509.85, Optimal reward -242.12
Iteration 157 took 3.60 seconds (mean sampled reward: -3282.87). Current reward after update: -454.84, Optimal reward -242.12
Iteration 158 took 3.64 seconds (mean sampled reward: -3590.07). Current reward after update: -470.74, Optimal reward -242.12
Iteration 159 took 3.60 seconds (mean sampled reward: -3521.44). Current reward after update: -519.47, Optimal reward -242.12
Iteration 160 took 3.64 seconds (mean sampled reward: -4458.57). Current reward after update: -479.64, Optimal reward -242.12
Iteration 161 took 3.77 seconds (mean sampled reward: -4868.94). Current reward after update: -455.67, Optimal reward -242.12
Iteration 162 took 3.80 seconds (mean sampled reward: -4092.23). Current reward after update: -438.97, Optimal reward -242.12
Iteration 163 took 3.75 seconds (mean sampled reward: -4094.51). Current reward after update: -474.38, Optimal reward -242.12
Iteration 164 took 3.79 seconds (mean sampled reward: -3696.82). Current reward after update: -440.00, Optimal reward -242.12
Iteration 165 took 3.72 seconds (mean sampled reward: -4436.90). Current reward after update: -513.33, Optimal reward -242.12
Iteration 166 took 3.72 seconds (mean sampled reward: -4125.73). Current reward after update: -533.25, Optimal reward -242.12
Iteration 167 took 3.75 seconds (mean sampled reward: -4409.07). Current reward after update: -607.07, Optimal reward -242.12
Iteration 168 took 3.73 seconds (mean sampled reward: -4604.66). Current reward after update: -526.09, Optimal reward -242.12
Iteration 169 took 3.61 seconds (mean sampled reward: -3555.93). Current reward after update: -504.44, Optimal reward -242.12
Iteration 170 took 3.65 seconds (mean sampled reward: -4446.14). Current reward after update: -469.87, Optimal reward -242.12
Iteration 171 took 3.63 seconds (mean sampled reward: -3972.41). Current reward after update: -421.07, Optimal reward -242.12
Iteration 172 took 3.68 seconds (mean sampled reward: -4331.75). Current reward after update: -391.15, Optimal reward -242.12
Iteration 173 took 3.76 seconds (mean sampled reward: -4240.99). Current reward after update: -404.11, Optimal reward -242.12
Iteration 174 took 3.70 seconds (mean sampled reward: -4521.43). Current reward after update: -408.88, Optimal reward -242.12
Iteration 175 took 3.67 seconds (mean sampled reward: -4497.83). Current reward after update: -479.47, Optimal reward -242.12
Iteration 176 took 3.58 seconds (mean sampled reward: -4392.91). Current reward after update: -444.80, Optimal reward -242.12
Iteration 177 took 3.60 seconds (mean sampled reward: -3976.93). Current reward after update: -434.05, Optimal reward -242.12
Iteration 178 took 3.65 seconds (mean sampled reward: -3914.94). Current reward after update: -512.84, Optimal reward -242.12
Iteration 179 took 3.73 seconds (mean sampled reward: -3721.20). Current reward after update: -392.80, Optimal reward -242.12
Iteration 180 took 3.71 seconds (mean sampled reward: -4504.72). Current reward after update: -599.59, Optimal reward -242.12
Iteration 181 took 3.76 seconds (mean sampled reward: -4383.30). Current reward after update: -541.64, Optimal reward -242.12
Iteration 182 took 3.64 seconds (mean sampled reward: -4388.17). Current reward after update: -549.56, Optimal reward -242.12
Iteration 183 took 3.63 seconds (mean sampled reward: -3896.93). Current reward after update: -500.53, Optimal reward -242.12
Iteration 184 took 3.69 seconds (mean sampled reward: -3825.76). Current reward after update: -384.10, Optimal reward -242.12
Iteration 185 took 3.70 seconds (mean sampled reward: -4138.35). Current reward after update: -464.49, Optimal reward -242.12
Iteration 186 took 3.58 seconds (mean sampled reward: -3847.85). Current reward after update: -380.94, Optimal reward -242.12
Iteration 187 took 3.65 seconds (mean sampled reward: -4365.44). Current reward after update: -515.75, Optimal reward -242.12
Iteration 188 took 3.72 seconds (mean sampled reward: -4630.26). Current reward after update: -474.79, Optimal reward -242.12
Iteration 189 took 3.84 seconds (mean sampled reward: -4705.35). Current reward after update: -490.24, Optimal reward -242.12
Iteration 190 took 3.65 seconds (mean sampled reward: -4758.50). Current reward after update: -445.33, Optimal reward -242.12
Iteration 191 took 3.79 seconds (mean sampled reward: -5128.74). Current reward after update: -533.80, Optimal reward -242.12
Iteration 192 took 3.77 seconds (mean sampled reward: -5604.38). Current reward after update: -651.92, Optimal reward -242.12
Iteration 193 took 3.72 seconds (mean sampled reward: -5059.59). Current reward after update: -565.76, Optimal reward -242.12
Iteration 194 took 3.78 seconds (mean sampled reward: -5446.17). Current reward after update: -530.11, Optimal reward -242.12
Iteration 195 took 3.77 seconds (mean sampled reward: -5441.98). Current reward after update: -565.85, Optimal reward -242.12
Iteration 196 took 3.78 seconds (mean sampled reward: -4845.93). Current reward after update: -502.16, Optimal reward -242.12
Iteration 197 took 3.73 seconds (mean sampled reward: -4487.17). Current reward after update: -616.34, Optimal reward -242.12
Iteration 198 took 3.66 seconds (mean sampled reward: -4353.66). Current reward after update: -469.89, Optimal reward -242.12
Iteration 199 took 3.77 seconds (mean sampled reward: -4900.70). Current reward after update: -525.60, Optimal reward -242.12
Iteration 200 took 3.80 seconds (mean sampled reward: -4916.69). Current reward after update: -558.90, Optimal reward -242.12
Iteration 201 took 3.66 seconds (mean sampled reward: -4722.99). Current reward after update: -570.40, Optimal reward -242.12
Iteration 202 took 3.69 seconds (mean sampled reward: -4735.47). Current reward after update: -525.05, Optimal reward -242.12
Iteration 203 took 3.66 seconds (mean sampled reward: -4017.41). Current reward after update: -513.36, Optimal reward -242.12
Iteration 204 took 3.75 seconds (mean sampled reward: -4032.91). Current reward after update: -518.84, Optimal reward -242.12
Iteration 205 took 3.65 seconds (mean sampled reward: -4323.78). Current reward after update: -522.83, Optimal reward -242.12
Iteration 206 took 3.64 seconds (mean sampled reward: -4322.51). Current reward after update: -449.35, Optimal reward -242.12
Iteration 207 took 3.71 seconds (mean sampled reward: -4479.83). Current reward after update: -487.00, Optimal reward -242.12
Iteration 208 took 3.58 seconds (mean sampled reward: -4668.94). Current reward after update: -564.17, Optimal reward -242.12
Iteration 209 took 3.50 seconds (mean sampled reward: -3977.49). Current reward after update: -606.00, Optimal reward -242.12
Iteration 210 took 3.61 seconds (mean sampled reward: -3889.00). Current reward after update: -1185.52, Optimal reward -242.12
Iteration 211 took 3.69 seconds (mean sampled reward: -4197.48). Current reward after update: -483.57, Optimal reward -242.12
Iteration 212 took 3.93 seconds (mean sampled reward: -4662.22). Current reward after update: -578.46, Optimal reward -242.12
Iteration 213 took 3.75 seconds (mean sampled reward: -3924.70). Current reward after update: -513.20, Optimal reward -242.12
Iteration 214 took 3.64 seconds (mean sampled reward: -4465.32). Current reward after update: -512.08, Optimal reward -242.12
Iteration 215 took 3.44 seconds (mean sampled reward: -3705.83). Current reward after update: -2674.02, Optimal reward -242.12
Iteration 216 took 3.48 seconds (mean sampled reward: -3464.72). Current reward after update: -505.96, Optimal reward -242.12
Iteration 217 took 3.64 seconds (mean sampled reward: -4066.98). Current reward after update: -503.07, Optimal reward -242.12
Iteration 218 took 3.64 seconds (mean sampled reward: -3562.33). Current reward after update: -497.10, Optimal reward -242.12
Iteration 219 took 3.70 seconds (mean sampled reward: -3589.40). Current reward after update: -475.52, Optimal reward -242.12
Iteration 220 took 3.80 seconds (mean sampled reward: -3571.40). Current reward after update: -480.18, Optimal reward -242.12
Iteration 221 took 3.74 seconds (mean sampled reward: -3361.84). Current reward after update: -448.18, Optimal reward -242.12
Iteration 222 took 3.76 seconds (mean sampled reward: -3856.23). Current reward after update: -461.56, Optimal reward -242.12
Iteration 223 took 3.71 seconds (mean sampled reward: -3704.93). Current reward after update: -440.84, Optimal reward -242.12
Iteration 224 took 3.70 seconds (mean sampled reward: -3601.32). Current reward after update: -478.51, Optimal reward -242.12
Iteration 225 took 3.66 seconds (mean sampled reward: -3470.54). Current reward after update: -2915.00, Optimal reward -242.12
Iteration 226 took 3.65 seconds (mean sampled reward: -3328.85). Current reward after update: -465.05, Optimal reward -242.12
Iteration 227 took 3.67 seconds (mean sampled reward: -3159.95). Current reward after update: -344.83, Optimal reward -242.12
Iteration 228 took 3.73 seconds (mean sampled reward: -3078.72). Current reward after update: -415.28, Optimal reward -242.12
Iteration 229 took 3.71 seconds (mean sampled reward: -3974.50). Current reward after update: -408.55, Optimal reward -242.12
Iteration 230 took 3.82 seconds (mean sampled reward: -3670.87). Current reward after update: -406.20, Optimal reward -242.12
Iteration 231 took 3.66 seconds (mean sampled reward: -2947.14). Current reward after update: -415.44, Optimal reward -242.12
Iteration 232 took 3.96 seconds (mean sampled reward: -3704.52). Current reward after update: -409.77, Optimal reward -242.12
Iteration 233 took 3.75 seconds (mean sampled reward: -3123.25). Current reward after update: -395.59, Optimal reward -242.12
Iteration 234 took 3.68 seconds (mean sampled reward: -3620.33). Current reward after update: -371.78, Optimal reward -242.12
Iteration 235 took 3.61 seconds (mean sampled reward: -3621.40). Current reward after update: -387.31, Optimal reward -242.12
Iteration 236 took 3.67 seconds (mean sampled reward: -3491.95). Current reward after update: -473.56, Optimal reward -242.12
Iteration 237 took 3.91 seconds (mean sampled reward: -3666.35). Current reward after update: -432.33, Optimal reward -242.12
Iteration 238 took 3.74 seconds (mean sampled reward: -3231.59). Current reward after update: -353.41, Optimal reward -242.12
Iteration 239 took 3.77 seconds (mean sampled reward: -3334.50). Current reward after update: -348.59, Optimal reward -242.12
Iteration 240 took 3.77 seconds (mean sampled reward: -3484.07). Current reward after update: -394.50, Optimal reward -242.12
Iteration 241 took 3.79 seconds (mean sampled reward: -4189.44). Current reward after update: -384.23, Optimal reward -242.12
Iteration 242 took 3.73 seconds (mean sampled reward: -4026.09). Current reward after update: -316.17, Optimal reward -242.12
Iteration 243 took 3.70 seconds (mean sampled reward: -3987.04). Current reward after update: -347.47, Optimal reward -242.12
Iteration 244 took 3.83 seconds (mean sampled reward: -3958.46). Current reward after update: -340.01, Optimal reward -242.12
Iteration 245 took 4.01 seconds (mean sampled reward: -4011.12). Current reward after update: -482.55, Optimal reward -242.12
Iteration 246 took 3.76 seconds (mean sampled reward: -2892.88). Current reward after update: -442.72, Optimal reward -242.12
Iteration 247 took 3.78 seconds (mean sampled reward: -3041.79). Current reward after update: -383.40, Optimal reward -242.12
Iteration 248 took 3.66 seconds (mean sampled reward: -2836.78). Current reward after update: -2212.40, Optimal reward -242.12
Iteration 249 took 3.75 seconds (mean sampled reward: -3193.81). Current reward after update: -433.64, Optimal reward -242.12
Iteration 250 took 3.69 seconds (mean sampled reward: -2822.85). Current reward after update: -422.51, Optimal reward -242.12
Iteration 251 took 3.72 seconds (mean sampled reward: -3116.91). Current reward after update: -394.01, Optimal reward -242.12
Iteration 252 took 3.78 seconds (mean sampled reward: -3526.76). Current reward after update: -362.30, Optimal reward -242.12
Iteration 253 took 3.63 seconds (mean sampled reward: -2877.49). Current reward after update: -403.13, Optimal reward -242.12
Iteration 254 took 3.70 seconds (mean sampled reward: -2683.13). Current reward after update: -374.94, Optimal reward -242.12
Iteration 255 took 3.67 seconds (mean sampled reward: -2699.53). Current reward after update: -381.31, Optimal reward -242.12
Iteration 256 took 3.60 seconds (mean sampled reward: -2503.61). Current reward after update: -411.92, Optimal reward -242.12
Iteration 257 took 3.58 seconds (mean sampled reward: -2260.77). Current reward after update: -383.33, Optimal reward -242.12
Iteration 258 took 3.63 seconds (mean sampled reward: -2265.29). Current reward after update: -386.13, Optimal reward -242.12
Iteration 259 took 3.48 seconds (mean sampled reward: -2034.35). Current reward after update: -376.88, Optimal reward -242.12
Iteration 260 took 3.63 seconds (mean sampled reward: -2514.33). Current reward after update: -393.83, Optimal reward -242.12
Iteration 261 took 3.84 seconds (mean sampled reward: -3267.74). Current reward after update: -417.11, Optimal reward -242.12
Iteration 262 took 3.78 seconds (mean sampled reward: -3371.03). Current reward after update: -378.76, Optimal reward -242.12
Iteration 263 took 3.66 seconds (mean sampled reward: -2851.74). Current reward after update: -957.95, Optimal reward -242.12
Iteration 264 took 3.58 seconds (mean sampled reward: -2283.76). Current reward after update: -314.25, Optimal reward -242.12
Iteration 265 took 3.78 seconds (mean sampled reward: -2543.14). Current reward after update: -305.54, Optimal reward -242.12
Iteration 266 took 3.69 seconds (mean sampled reward: -2014.85). Current reward after update: -340.00, Optimal reward -242.12
Iteration 267 took 3.47 seconds (mean sampled reward: -1641.50). Current reward after update: -333.81, Optimal reward -242.12
Iteration 268 took 3.51 seconds (mean sampled reward: -1745.60). Current reward after update: -367.70, Optimal reward -242.12
Iteration 269 took 3.54 seconds (mean sampled reward: -2036.52). Current reward after update: -338.45, Optimal reward -242.12
Iteration 270 took 3.63 seconds (mean sampled reward: -2274.32). Current reward after update: -386.06, Optimal reward -242.12
Iteration 271 took 3.56 seconds (mean sampled reward: -1936.83). Current reward after update: -308.60, Optimal reward -242.12
Iteration 272 took 3.51 seconds (mean sampled reward: -1754.04). Current reward after update: -318.19, Optimal reward -242.12
Iteration 273 took 3.54 seconds (mean sampled reward: -1644.40). Current reward after update: -311.73, Optimal reward -242.12
Iteration 274 took 3.50 seconds (mean sampled reward: -1663.70). Current reward after update: -336.42, Optimal reward -242.12
Iteration 275 took 3.50 seconds (mean sampled reward: -1669.79). Current reward after update: -312.49, Optimal reward -242.12
Iteration 276 took 3.58 seconds (mean sampled reward: -2006.50). Current reward after update: -316.45, Optimal reward -242.12
Iteration 277 took 3.57 seconds (mean sampled reward: -1953.34). Current reward after update: -294.78, Optimal reward -242.12
Iteration 278 took 3.55 seconds (mean sampled reward: -2037.52). Current reward after update: -319.34, Optimal reward -242.12
Iteration 279 took 3.60 seconds (mean sampled reward: -2321.76). Current reward after update: -355.72, Optimal reward -242.12
Iteration 280 took 3.62 seconds (mean sampled reward: -2270.49). Current reward after update: -377.57, Optimal reward -242.12
Iteration 281 took 3.57 seconds (mean sampled reward: -1945.62). Current reward after update: -312.30, Optimal reward -242.12
Iteration 282 took 3.53 seconds (mean sampled reward: -1676.99). Current reward after update: -350.30, Optimal reward -242.12
Iteration 283 took 3.48 seconds (mean sampled reward: -1463.04). Current reward after update: -324.81, Optimal reward -242.12
Iteration 284 took 3.44 seconds (mean sampled reward: -1494.66). Current reward after update: -334.73, Optimal reward -242.12
Iteration 285 took 3.49 seconds (mean sampled reward: -1489.47). Current reward after update: -311.75, Optimal reward -242.12
Iteration 286 took 3.46 seconds (mean sampled reward: -1678.98). Current reward after update: -329.52, Optimal reward -242.12
Iteration 287 took 3.48 seconds (mean sampled reward: -1912.97). Current reward after update: -629.63, Optimal reward -242.12
Iteration 288 took 3.54 seconds (mean sampled reward: -2265.71). Current reward after update: -358.00, Optimal reward -242.12
Iteration 289 took 3.52 seconds (mean sampled reward: -1970.88). Current reward after update: -381.18, Optimal reward -242.12
Iteration 290 took 3.51 seconds (mean sampled reward: -1789.97). Current reward after update: -355.02, Optimal reward -242.12
Iteration 291 took 3.53 seconds (mean sampled reward: -2297.12). Current reward after update: -388.91, Optimal reward -242.12
Iteration 292 took 3.76 seconds (mean sampled reward: -4292.49). Current reward after update: -416.60, Optimal reward -242.12
Iteration 293 took 3.70 seconds (mean sampled reward: -4005.82). Current reward after update: -381.66, Optimal reward -242.12
Iteration 294 took 3.60 seconds (mean sampled reward: -3292.38). Current reward after update: -410.16, Optimal reward -242.12
Iteration 295 took 3.59 seconds (mean sampled reward: -3460.38). Current reward after update: -409.41, Optimal reward -242.12
Iteration 296 took 3.64 seconds (mean sampled reward: -4058.34). Current reward after update: -421.97, Optimal reward -242.12
Iteration 297 took 3.47 seconds (mean sampled reward: -3316.70). Current reward after update: -424.37, Optimal reward -242.12
Iteration 298 took 3.37 seconds (mean sampled reward: -2457.94). Current reward after update: -387.09, Optimal reward -242.12
Iteration 299 took 3.33 seconds (mean sampled reward: -3123.90). Current reward after update: -418.01, Optimal reward -242.12
Iteration 300 took 3.38 seconds (mean sampled reward: -2414.38). Current reward after update: -416.96, Optimal reward -242.12
Iteration 1 took 3.85 seconds (mean sampled reward: -6143.78). Current reward after update: -3894.18, Optimal reward -3894.18
Iteration 2 took 3.72 seconds (mean sampled reward: -5813.33). Current reward after update: -2510.64, Optimal reward -2510.64
Iteration 3 took 3.68 seconds (mean sampled reward: -5349.75). Current reward after update: -2357.33, Optimal reward -2357.33
Iteration 4 took 3.68 seconds (mean sampled reward: -5368.99). Current reward after update: -2084.76, Optimal reward -2084.76
Iteration 5 took 3.57 seconds (mean sampled reward: -5089.69). Current reward after update: -1611.65, Optimal reward -1611.65
Iteration 6 took 3.68 seconds (mean sampled reward: -4917.38). Current reward after update: -1135.84, Optimal reward -1135.84
Iteration 7 took 3.81 seconds (mean sampled reward: -4788.52). Current reward after update: -1059.18, Optimal reward -1059.18
Iteration 8 took 3.76 seconds (mean sampled reward: -4667.09). Current reward after update: -1177.00, Optimal reward -1059.18
Iteration 9 took 3.68 seconds (mean sampled reward: -4087.64). Current reward after update: -809.19, Optimal reward -809.19
Iteration 10 took 3.60 seconds (mean sampled reward: -3879.32). Current reward after update: -672.90, Optimal reward -672.90
Iteration 11 took 4.08 seconds (mean sampled reward: -4125.55). Current reward after update: -699.95, Optimal reward -672.90
Iteration 12 took 3.80 seconds (mean sampled reward: -4225.57). Current reward after update: -691.91, Optimal reward -672.90
Iteration 13 took 3.88 seconds (mean sampled reward: -4029.06). Current reward after update: -850.00, Optimal reward -672.90
Iteration 14 took 3.79 seconds (mean sampled reward: -4131.47). Current reward after update: -653.40, Optimal reward -653.40
Iteration 15 took 3.92 seconds (mean sampled reward: -4537.81). Current reward after update: -677.66, Optimal reward -653.40
Iteration 16 took 3.71 seconds (mean sampled reward: -4561.25). Current reward after update: -660.31, Optimal reward -653.40
Iteration 17 took 4.29 seconds (mean sampled reward: -4442.64). Current reward after update: -756.51, Optimal reward -653.40
Iteration 18 took 3.84 seconds (mean sampled reward: -3842.90). Current reward after update: -583.07, Optimal reward -583.07
Iteration 19 took 3.83 seconds (mean sampled reward: -4160.10). Current reward after update: -671.84, Optimal reward -583.07
Iteration 20 took 3.84 seconds (mean sampled reward: -4785.43). Current reward after update: -857.16, Optimal reward -583.07
Iteration 21 took 3.87 seconds (mean sampled reward: -4442.08). Current reward after update: -628.36, Optimal reward -583.07
Iteration 22 took 4.09 seconds (mean sampled reward: -4522.21). Current reward after update: -476.92, Optimal reward -476.92
Iteration 23 took 3.78 seconds (mean sampled reward: -4780.56). Current reward after update: -676.02, Optimal reward -476.92
Iteration 24 took 3.81 seconds (mean sampled reward: -4767.07). Current reward after update: -777.86, Optimal reward -476.92
Iteration 25 took 3.87 seconds (mean sampled reward: -4343.23). Current reward after update: -706.87, Optimal reward -476.92
Iteration 26 took 3.65 seconds (mean sampled reward: -4752.62). Current reward after update: -783.31, Optimal reward -476.92
Iteration 27 took 3.80 seconds (mean sampled reward: -4795.73). Current reward after update: -699.89, Optimal reward -476.92
Iteration 28 took 3.96 seconds (mean sampled reward: -4872.92). Current reward after update: -679.21, Optimal reward -476.92
Iteration 29 took 4.01 seconds (mean sampled reward: -5005.45). Current reward after update: -658.22, Optimal reward -476.92
Iteration 30 took 3.99 seconds (mean sampled reward: -4845.72). Current reward after update: -727.25, Optimal reward -476.92
Iteration 31 took 3.82 seconds (mean sampled reward: -4516.85). Current reward after update: -684.51, Optimal reward -476.92
Iteration 32 took 3.84 seconds (mean sampled reward: -3444.67). Current reward after update: -604.19, Optimal reward -476.92
Iteration 33 took 3.83 seconds (mean sampled reward: -4106.34). Current reward after update: -544.99, Optimal reward -476.92
Iteration 34 took 3.72 seconds (mean sampled reward: -3967.38). Current reward after update: -625.33, Optimal reward -476.92
Iteration 35 took 3.85 seconds (mean sampled reward: -4282.01). Current reward after update: -477.01, Optimal reward -476.92
Iteration 36 took 3.82 seconds (mean sampled reward: -4453.07). Current reward after update: -570.81, Optimal reward -476.92
Iteration 37 took 4.02 seconds (mean sampled reward: -4124.49). Current reward after update: -587.97, Optimal reward -476.92
Iteration 38 took 3.90 seconds (mean sampled reward: -3837.60). Current reward after update: -543.34, Optimal reward -476.92
Iteration 39 took 4.05 seconds (mean sampled reward: -3871.37). Current reward after update: -496.06, Optimal reward -476.92
Iteration 40 took 3.96 seconds (mean sampled reward: -4432.37). Current reward after update: -920.47, Optimal reward -476.92
Iteration 41 took 3.94 seconds (mean sampled reward: -4687.16). Current reward after update: -599.47, Optimal reward -476.92
Iteration 42 took 3.89 seconds (mean sampled reward: -4697.32). Current reward after update: -615.02, Optimal reward -476.92
Iteration 43 took 3.81 seconds (mean sampled reward: -4677.91). Current reward after update: -641.56, Optimal reward -476.92
Iteration 44 took 3.87 seconds (mean sampled reward: -4802.73). Current reward after update: -529.89, Optimal reward -476.92
Iteration 45 took 3.86 seconds (mean sampled reward: -4827.88). Current reward after update: -620.82, Optimal reward -476.92
Iteration 46 took 3.88 seconds (mean sampled reward: -4559.59). Current reward after update: -597.17, Optimal reward -476.92
Iteration 47 took 3.81 seconds (mean sampled reward: -4437.37). Current reward after update: -473.15, Optimal reward -473.15
Iteration 48 took 3.77 seconds (mean sampled reward: -4289.41). Current reward after update: -455.56, Optimal reward -455.56
Iteration 49 took 3.76 seconds (mean sampled reward: -4277.71). Current reward after update: -438.61, Optimal reward -438.61
Iteration 50 took 3.84 seconds (mean sampled reward: -4256.86). Current reward after update: -416.14, Optimal reward -416.14
Iteration 51 took 3.78 seconds (mean sampled reward: -3313.96). Current reward after update: -326.70, Optimal reward -326.70
Iteration 52 took 3.82 seconds (mean sampled reward: -3444.17). Current reward after update: -328.17, Optimal reward -326.70
Iteration 53 took 3.85 seconds (mean sampled reward: -3206.16). Current reward after update: -395.97, Optimal reward -326.70
Iteration 54 took 3.92 seconds (mean sampled reward: -2991.22). Current reward after update: -356.27, Optimal reward -326.70
Iteration 55 took 4.01 seconds (mean sampled reward: -3556.10). Current reward after update: -348.27, Optimal reward -326.70
Iteration 56 took 4.03 seconds (mean sampled reward: -3583.83). Current reward after update: -360.19, Optimal reward -326.70
Iteration 57 took 3.95 seconds (mean sampled reward: -3887.87). Current reward after update: -299.69, Optimal reward -299.69
Iteration 58 took 3.79 seconds (mean sampled reward: -4412.67). Current reward after update: -401.03, Optimal reward -299.69
Iteration 59 took 3.78 seconds (mean sampled reward: -3362.09). Current reward after update: -333.67, Optimal reward -299.69
Iteration 60 took 3.94 seconds (mean sampled reward: -3217.50). Current reward after update: -402.23, Optimal reward -299.69
Iteration 61 took 3.81 seconds (mean sampled reward: -4068.80). Current reward after update: -448.61, Optimal reward -299.69
Iteration 62 took 3.81 seconds (mean sampled reward: -4268.82). Current reward after update: -695.65, Optimal reward -299.69
Iteration 63 took 3.72 seconds (mean sampled reward: -4094.60). Current reward after update: -351.17, Optimal reward -299.69
Iteration 64 took 3.72 seconds (mean sampled reward: -3902.06). Current reward after update: -318.08, Optimal reward -299.69
Iteration 65 took 3.76 seconds (mean sampled reward: -4547.89). Current reward after update: -336.03, Optimal reward -299.69
Iteration 66 took 3.77 seconds (mean sampled reward: -4491.82). Current reward after update: -411.20, Optimal reward -299.69
Iteration 67 took 3.67 seconds (mean sampled reward: -4474.53). Current reward after update: -514.51, Optimal reward -299.69
Iteration 68 took 3.65 seconds (mean sampled reward: -4034.26). Current reward after update: -502.37, Optimal reward -299.69
Iteration 69 took 3.61 seconds (mean sampled reward: -4276.15). Current reward after update: -563.59, Optimal reward -299.69
Iteration 70 took 3.76 seconds (mean sampled reward: -4816.67). Current reward after update: -729.76, Optimal reward -299.69
Iteration 71 took 3.64 seconds (mean sampled reward: -4322.12). Current reward after update: -352.75, Optimal reward -299.69
Iteration 72 took 3.59 seconds (mean sampled reward: -3798.43). Current reward after update: -334.60, Optimal reward -299.69
Iteration 73 took 3.59 seconds (mean sampled reward: -4018.06). Current reward after update: -593.61, Optimal reward -299.69
Iteration 74 took 3.61 seconds (mean sampled reward: -4358.87). Current reward after update: -261.26, Optimal reward -261.26
Iteration 75 took 3.70 seconds (mean sampled reward: -4055.77). Current reward after update: -405.07, Optimal reward -261.26
Iteration 76 took 3.67 seconds (mean sampled reward: -3748.73). Current reward after update: -485.69, Optimal reward -261.26
Iteration 77 took 3.72 seconds (mean sampled reward: -4019.87). Current reward after update: -606.80, Optimal reward -261.26
Iteration 78 took 3.67 seconds (mean sampled reward: -3821.94). Current reward after update: -674.92, Optimal reward -261.26
Iteration 79 took 3.63 seconds (mean sampled reward: -3810.16). Current reward after update: -572.86, Optimal reward -261.26
Iteration 80 took 3.76 seconds (mean sampled reward: -3732.96). Current reward after update: -524.78, Optimal reward -261.26
Iteration 81 took 3.62 seconds (mean sampled reward: -4414.56). Current reward after update: -474.83, Optimal reward -261.26
Iteration 82 took 3.63 seconds (mean sampled reward: -3628.20). Current reward after update: -522.46, Optimal reward -261.26
Iteration 83 took 3.68 seconds (mean sampled reward: -3696.35). Current reward after update: -536.39, Optimal reward -261.26
Iteration 84 took 3.67 seconds (mean sampled reward: -3831.69). Current reward after update: -389.40, Optimal reward -261.26
Iteration 85 took 3.60 seconds (mean sampled reward: -3418.00). Current reward after update: -486.74, Optimal reward -261.26
Iteration 86 took 3.63 seconds (mean sampled reward: -3613.73). Current reward after update: -612.54, Optimal reward -261.26
Iteration 87 took 3.67 seconds (mean sampled reward: -3748.47). Current reward after update: -360.88, Optimal reward -261.26
Iteration 88 took 3.67 seconds (mean sampled reward: -3632.11). Current reward after update: -359.41, Optimal reward -261.26
Iteration 89 took 3.57 seconds (mean sampled reward: -3539.06). Current reward after update: -454.98, Optimal reward -261.26
Iteration 90 took 3.53 seconds (mean sampled reward: -3636.54). Current reward after update: -580.48, Optimal reward -261.26
Iteration 91 took 3.56 seconds (mean sampled reward: -4295.74). Current reward after update: -669.49, Optimal reward -261.26
Iteration 92 took 3.70 seconds (mean sampled reward: -4019.07). Current reward after update: -502.68, Optimal reward -261.26
Iteration 93 took 3.56 seconds (mean sampled reward: -4394.73). Current reward after update: -476.24, Optimal reward -261.26
Iteration 94 took 3.56 seconds (mean sampled reward: -5077.47). Current reward after update: -642.69, Optimal reward -261.26
Iteration 95 took 3.65 seconds (mean sampled reward: -4682.43). Current reward after update: -739.57, Optimal reward -261.26
Iteration 96 took 3.64 seconds (mean sampled reward: -5054.03). Current reward after update: -697.27, Optimal reward -261.26
Iteration 97 took 3.63 seconds (mean sampled reward: -5240.19). Current reward after update: -666.36, Optimal reward -261.26
Iteration 98 took 3.57 seconds (mean sampled reward: -4727.65). Current reward after update: -545.85, Optimal reward -261.26
Iteration 99 took 3.43 seconds (mean sampled reward: -4611.28). Current reward after update: -422.79, Optimal reward -261.26
Iteration 100 took 3.48 seconds (mean sampled reward: -4715.90). Current reward after update: -396.72, Optimal reward -261.26
Iteration 101 took 3.43 seconds (mean sampled reward: -4862.15). Current reward after update: -426.16, Optimal reward -261.26
Iteration 102 took 3.50 seconds (mean sampled reward: -4963.22). Current reward after update: -582.87, Optimal reward -261.26
Iteration 103 took 3.46 seconds (mean sampled reward: -4338.07). Current reward after update: -251.64, Optimal reward -251.64
Iteration 104 took 3.52 seconds (mean sampled reward: -4527.57). Current reward after update: -276.38, Optimal reward -251.64
Iteration 105 took 3.42 seconds (mean sampled reward: -4386.54). Current reward after update: -643.75, Optimal reward -251.64
Iteration 106 took 3.59 seconds (mean sampled reward: -4325.19). Current reward after update: -548.66, Optimal reward -251.64
Iteration 107 took 3.67 seconds (mean sampled reward: -4194.38). Current reward after update: -580.84, Optimal reward -251.64
Iteration 108 took 3.50 seconds (mean sampled reward: -4892.51). Current reward after update: -671.13, Optimal reward -251.64
Iteration 109 took 3.70 seconds (mean sampled reward: -4542.71). Current reward after update: -685.58, Optimal reward -251.64
Iteration 110 took 3.67 seconds (mean sampled reward: -4741.61). Current reward after update: -637.45, Optimal reward -251.64
Iteration 111 took 3.40 seconds (mean sampled reward: -4372.67). Current reward after update: -666.07, Optimal reward -251.64
Iteration 112 took 3.45 seconds (mean sampled reward: -4297.16). Current reward after update: -559.39, Optimal reward -251.64
Iteration 113 took 3.66 seconds (mean sampled reward: -4017.13). Current reward after update: -620.42, Optimal reward -251.64
Iteration 114 took 3.47 seconds (mean sampled reward: -4317.53). Current reward after update: -508.51, Optimal reward -251.64
Iteration 115 took 3.47 seconds (mean sampled reward: -4060.57). Current reward after update: -632.73, Optimal reward -251.64
Iteration 116 took 3.57 seconds (mean sampled reward: -4531.76). Current reward after update: -696.45, Optimal reward -251.64
Iteration 117 took 3.49 seconds (mean sampled reward: -5294.18). Current reward after update: -734.93, Optimal reward -251.64
Iteration 118 took 3.47 seconds (mean sampled reward: -5017.70). Current reward after update: -701.33, Optimal reward -251.64
Iteration 119 took 3.48 seconds (mean sampled reward: -4880.33). Current reward after update: -573.35, Optimal reward -251.64
Iteration 120 took 3.42 seconds (mean sampled reward: -4414.41). Current reward after update: -488.24, Optimal reward -251.64
Iteration 121 took 3.41 seconds (mean sampled reward: -3545.00). Current reward after update: -603.19, Optimal reward -251.64
Iteration 122 took 3.44 seconds (mean sampled reward: -3981.24). Current reward after update: -477.11, Optimal reward -251.64
Iteration 123 took 3.39 seconds (mean sampled reward: -3856.59). Current reward after update: -450.25, Optimal reward -251.64
Iteration 124 took 3.43 seconds (mean sampled reward: -4001.18). Current reward after update: -537.94, Optimal reward -251.64
Iteration 125 took 3.49 seconds (mean sampled reward: -4884.11). Current reward after update: -568.56, Optimal reward -251.64
Iteration 126 took 3.39 seconds (mean sampled reward: -3912.45). Current reward after update: -534.82, Optimal reward -251.64
Iteration 127 took 3.48 seconds (mean sampled reward: -4099.96). Current reward after update: -549.99, Optimal reward -251.64
Iteration 128 took 3.51 seconds (mean sampled reward: -4526.49). Current reward after update: -572.18, Optimal reward -251.64
Iteration 129 took 3.52 seconds (mean sampled reward: -4958.76). Current reward after update: -519.92, Optimal reward -251.64
Iteration 130 took 3.52 seconds (mean sampled reward: -4592.17). Current reward after update: -551.91, Optimal reward -251.64
Iteration 131 took 3.57 seconds (mean sampled reward: -5044.17). Current reward after update: -700.17, Optimal reward -251.64
Iteration 132 took 3.50 seconds (mean sampled reward: -5025.95). Current reward after update: -733.60, Optimal reward -251.64
Iteration 133 took 3.48 seconds (mean sampled reward: -5518.47). Current reward after update: -503.59, Optimal reward -251.64
Iteration 134 took 3.52 seconds (mean sampled reward: -4825.99). Current reward after update: -422.15, Optimal reward -251.64
Iteration 135 took 3.51 seconds (mean sampled reward: -4797.45). Current reward after update: -405.74, Optimal reward -251.64
Iteration 136 took 3.52 seconds (mean sampled reward: -4832.74). Current reward after update: -614.54, Optimal reward -251.64
Iteration 137 took 3.48 seconds (mean sampled reward: -5127.72). Current reward after update: -595.04, Optimal reward -251.64
Iteration 138 took 3.51 seconds (mean sampled reward: -4593.86). Current reward after update: -515.73, Optimal reward -251.64
Iteration 139 took 3.57 seconds (mean sampled reward: -5257.06). Current reward after update: -438.14, Optimal reward -251.64
Iteration 140 took 3.52 seconds (mean sampled reward: -5414.10). Current reward after update: -585.29, Optimal reward -251.64
Iteration 141 took 3.49 seconds (mean sampled reward: -5281.05). Current reward after update: -442.90, Optimal reward -251.64
Iteration 142 took 3.55 seconds (mean sampled reward: -5376.03). Current reward after update: -637.42, Optimal reward -251.64
Iteration 143 took 3.48 seconds (mean sampled reward: -5300.89). Current reward after update: -457.11, Optimal reward -251.64
Iteration 144 took 3.47 seconds (mean sampled reward: -5434.40). Current reward after update: -469.58, Optimal reward -251.64
Iteration 145 took 3.47 seconds (mean sampled reward: -5355.49). Current reward after update: -657.01, Optimal reward -251.64
Iteration 146 took 3.44 seconds (mean sampled reward: -5007.12). Current reward after update: -578.09, Optimal reward -251.64
Iteration 147 took 3.51 seconds (mean sampled reward: -4693.83). Current reward after update: -525.98, Optimal reward -251.64
Iteration 148 took 3.52 seconds (mean sampled reward: -4917.98). Current reward after update: -621.53, Optimal reward -251.64
Iteration 149 took 3.53 seconds (mean sampled reward: -5141.74). Current reward after update: -702.92, Optimal reward -251.64
Iteration 150 took 3.55 seconds (mean sampled reward: -4634.97). Current reward after update: -505.25, Optimal reward -251.64
Iteration 151 took 3.55 seconds (mean sampled reward: -4579.50). Current reward after update: -449.25, Optimal reward -251.64
Iteration 152 took 3.59 seconds (mean sampled reward: -4730.24). Current reward after update: -529.90, Optimal reward -251.64
Iteration 153 took 3.55 seconds (mean sampled reward: -4535.84). Current reward after update: -376.79, Optimal reward -251.64
Iteration 154 took 3.51 seconds (mean sampled reward: -5003.92). Current reward after update: -542.73, Optimal reward -251.64
Iteration 155 took 3.58 seconds (mean sampled reward: -4666.21). Current reward after update: -303.06, Optimal reward -251.64
Iteration 156 took 3.52 seconds (mean sampled reward: -4563.16). Current reward after update: -368.70, Optimal reward -251.64
Iteration 157 took 3.54 seconds (mean sampled reward: -4140.34). Current reward after update: -642.97, Optimal reward -251.64
Iteration 158 took 3.68 seconds (mean sampled reward: -4729.59). Current reward after update: -621.10, Optimal reward -251.64
Iteration 159 took 3.68 seconds (mean sampled reward: -4974.55). Current reward after update: -838.76, Optimal reward -251.64
Iteration 160 took 3.54 seconds (mean sampled reward: -4396.54). Current reward after update: -685.07, Optimal reward -251.64
Iteration 161 took 3.65 seconds (mean sampled reward: -4318.43). Current reward after update: -552.88, Optimal reward -251.64
Iteration 162 took 3.54 seconds (mean sampled reward: -4964.21). Current reward after update: -564.06, Optimal reward -251.64
Iteration 163 took 3.48 seconds (mean sampled reward: -4386.91). Current reward after update: -726.73, Optimal reward -251.64
Iteration 164 took 3.66 seconds (mean sampled reward: -4532.04). Current reward after update: -759.10, Optimal reward -251.64
Iteration 165 took 3.62 seconds (mean sampled reward: -4811.07). Current reward after update: -635.46, Optimal reward -251.64
Iteration 166 took 3.55 seconds (mean sampled reward: -4497.44). Current reward after update: -642.06, Optimal reward -251.64
Iteration 167 took 3.57 seconds (mean sampled reward: -4582.35). Current reward after update: -539.18, Optimal reward -251.64
Iteration 168 took 3.52 seconds (mean sampled reward: -5027.63). Current reward after update: -499.77, Optimal reward -251.64
Iteration 169 took 3.55 seconds (mean sampled reward: -4724.27). Current reward after update: -571.78, Optimal reward -251.64
Iteration 170 took 3.52 seconds (mean sampled reward: -4932.86). Current reward after update: -563.62, Optimal reward -251.64
Iteration 171 took 3.55 seconds (mean sampled reward: -4568.46). Current reward after update: -745.46, Optimal reward -251.64
Iteration 172 took 3.57 seconds (mean sampled reward: -4647.45). Current reward after update: -568.70, Optimal reward -251.64
Iteration 173 took 3.55 seconds (mean sampled reward: -4660.13). Current reward after update: -682.49, Optimal reward -251.64
Iteration 174 took 3.58 seconds (mean sampled reward: -4378.82). Current reward after update: -626.52, Optimal reward -251.64
Iteration 175 took 3.56 seconds (mean sampled reward: -5012.49). Current reward after update: -585.86, Optimal reward -251.64
Iteration 176 took 3.56 seconds (mean sampled reward: -4858.68). Current reward after update: -542.58, Optimal reward -251.64
Iteration 177 took 3.60 seconds (mean sampled reward: -4487.97). Current reward after update: -603.56, Optimal reward -251.64
Iteration 178 took 3.52 seconds (mean sampled reward: -4431.30). Current reward after update: -649.03, Optimal reward -251.64
Iteration 179 took 3.50 seconds (mean sampled reward: -4552.53). Current reward after update: -553.53, Optimal reward -251.64
Iteration 180 took 3.55 seconds (mean sampled reward: -4697.79). Current reward after update: -653.44, Optimal reward -251.64
Iteration 181 took 3.54 seconds (mean sampled reward: -5366.79). Current reward after update: -695.22, Optimal reward -251.64
Iteration 182 took 3.57 seconds (mean sampled reward: -4660.78). Current reward after update: -555.52, Optimal reward -251.64
Iteration 183 took 3.53 seconds (mean sampled reward: -5098.69). Current reward after update: -618.60, Optimal reward -251.64
Iteration 184 took 3.50 seconds (mean sampled reward: -4582.52). Current reward after update: -592.74, Optimal reward -251.64
Iteration 185 took 3.59 seconds (mean sampled reward: -4846.45). Current reward after update: -589.64, Optimal reward -251.64
Iteration 186 took 3.46 seconds (mean sampled reward: -4025.41). Current reward after update: -614.82, Optimal reward -251.64
Iteration 187 took 3.48 seconds (mean sampled reward: -4253.54). Current reward after update: -670.50, Optimal reward -251.64
Iteration 188 took 3.49 seconds (mean sampled reward: -4290.41). Current reward after update: -536.78, Optimal reward -251.64
Iteration 189 took 3.54 seconds (mean sampled reward: -4350.26). Current reward after update: -532.78, Optimal reward -251.64
Iteration 190 took 3.55 seconds (mean sampled reward: -4853.35). Current reward after update: -559.88, Optimal reward -251.64
Iteration 191 took 3.53 seconds (mean sampled reward: -4574.67). Current reward after update: -511.12, Optimal reward -251.64
Iteration 192 took 3.55 seconds (mean sampled reward: -4267.50). Current reward after update: -581.74, Optimal reward -251.64
Iteration 193 took 3.55 seconds (mean sampled reward: -4364.78). Current reward after update: -596.75, Optimal reward -251.64
Iteration 194 took 3.55 seconds (mean sampled reward: -4053.69). Current reward after update: -555.78, Optimal reward -251.64
Iteration 195 took 3.56 seconds (mean sampled reward: -4675.69). Current reward after update: -609.50, Optimal reward -251.64
Iteration 196 took 3.46 seconds (mean sampled reward: -5375.15). Current reward after update: -719.68, Optimal reward -251.64
Iteration 197 took 3.44 seconds (mean sampled reward: -4962.62). Current reward after update: -703.05, Optimal reward -251.64
Iteration 198 took 3.43 seconds (mean sampled reward: -4876.83). Current reward after update: -659.96, Optimal reward -251.64
Iteration 199 took 3.52 seconds (mean sampled reward: -3728.08). Current reward after update: -668.26, Optimal reward -251.64
Iteration 200 took 3.51 seconds (mean sampled reward: -4145.11). Current reward after update: -564.98, Optimal reward -251.64
Iteration 201 took 3.55 seconds (mean sampled reward: -4791.28). Current reward after update: -538.04, Optimal reward -251.64
Iteration 202 took 3.53 seconds (mean sampled reward: -4641.88). Current reward after update: -638.33, Optimal reward -251.64
Iteration 203 took 3.49 seconds (mean sampled reward: -4865.99). Current reward after update: -583.56, Optimal reward -251.64
Iteration 204 took 3.54 seconds (mean sampled reward: -4172.23). Current reward after update: -610.14, Optimal reward -251.64
Iteration 205 took 3.55 seconds (mean sampled reward: -4662.83). Current reward after update: -529.92, Optimal reward -251.64
Iteration 206 took 3.59 seconds (mean sampled reward: -4937.70). Current reward after update: -645.59, Optimal reward -251.64
Iteration 207 took 3.60 seconds (mean sampled reward: -5152.46). Current reward after update: -602.42, Optimal reward -251.64
Iteration 208 took 3.54 seconds (mean sampled reward: -4593.87). Current reward after update: -728.73, Optimal reward -251.64
Iteration 209 took 3.59 seconds (mean sampled reward: -5251.72). Current reward after update: -593.83, Optimal reward -251.64
Iteration 210 took 3.57 seconds (mean sampled reward: -4760.41). Current reward after update: -695.98, Optimal reward -251.64
Iteration 211 took 3.52 seconds (mean sampled reward: -4756.25). Current reward after update: -590.21, Optimal reward -251.64
Iteration 212 took 3.84 seconds (mean sampled reward: -4416.60). Current reward after update: -658.38, Optimal reward -251.64
Iteration 213 took 3.55 seconds (mean sampled reward: -4383.63). Current reward after update: -583.85, Optimal reward -251.64
Iteration 214 took 3.99 seconds (mean sampled reward: -3756.06). Current reward after update: -602.36, Optimal reward -251.64
Iteration 215 took 3.71 seconds (mean sampled reward: -4076.31). Current reward after update: -528.99, Optimal reward -251.64
Iteration 216 took 3.56 seconds (mean sampled reward: -4884.54). Current reward after update: -524.65, Optimal reward -251.64
Iteration 217 took 3.70 seconds (mean sampled reward: -4929.93). Current reward after update: -520.24, Optimal reward -251.64
Iteration 218 took 3.58 seconds (mean sampled reward: -4829.60). Current reward after update: -913.09, Optimal reward -251.64
Iteration 219 took 3.53 seconds (mean sampled reward: -4601.91). Current reward after update: -554.71, Optimal reward -251.64
Iteration 220 took 3.55 seconds (mean sampled reward: -4906.37). Current reward after update: -538.25, Optimal reward -251.64
Iteration 221 took 3.66 seconds (mean sampled reward: -5124.97). Current reward after update: -786.24, Optimal reward -251.64
Iteration 222 took 3.77 seconds (mean sampled reward: -4666.18). Current reward after update: -629.79, Optimal reward -251.64
Iteration 223 took 3.64 seconds (mean sampled reward: -4666.27). Current reward after update: -539.74, Optimal reward -251.64
Iteration 224 took 3.70 seconds (mean sampled reward: -4602.21). Current reward after update: -530.96, Optimal reward -251.64
Iteration 225 took 3.62 seconds (mean sampled reward: -4575.33). Current reward after update: -631.59, Optimal reward -251.64
Iteration 226 took 3.87 seconds (mean sampled reward: -4243.90). Current reward after update: -623.18, Optimal reward -251.64
Iteration 227 took 3.68 seconds (mean sampled reward: -5256.93). Current reward after update: -541.87, Optimal reward -251.64
Iteration 228 took 3.71 seconds (mean sampled reward: -4872.26). Current reward after update: -565.59, Optimal reward -251.64
Iteration 229 took 3.82 seconds (mean sampled reward: -4810.38). Current reward after update: -503.82, Optimal reward -251.64
Iteration 230 took 3.66 seconds (mean sampled reward: -4941.28). Current reward after update: -540.82, Optimal reward -251.64
Iteration 231 took 3.73 seconds (mean sampled reward: -4991.71). Current reward after update: -729.55, Optimal reward -251.64
Iteration 232 took 3.92 seconds (mean sampled reward: -4814.53). Current reward after update: -545.52, Optimal reward -251.64
Iteration 233 took 3.60 seconds (mean sampled reward: -4616.90). Current reward after update: -625.87, Optimal reward -251.64
Iteration 234 took 3.59 seconds (mean sampled reward: -4870.23). Current reward after update: -676.95, Optimal reward -251.64
Iteration 235 took 3.59 seconds (mean sampled reward: -3824.83). Current reward after update: -676.44, Optimal reward -251.64
Iteration 236 took 3.49 seconds (mean sampled reward: -3508.27). Current reward after update: -532.60, Optimal reward -251.64
Iteration 237 took 3.77 seconds (mean sampled reward: -4512.72). Current reward after update: -545.43, Optimal reward -251.64
Iteration 238 took 3.52 seconds (mean sampled reward: -4534.10). Current reward after update: -491.20, Optimal reward -251.64
Iteration 239 took 3.66 seconds (mean sampled reward: -4679.28). Current reward after update: -573.01, Optimal reward -251.64
Iteration 240 took 3.53 seconds (mean sampled reward: -4064.61). Current reward after update: -558.60, Optimal reward -251.64
Iteration 241 took 3.55 seconds (mean sampled reward: -4462.34). Current reward after update: -721.41, Optimal reward -251.64
Iteration 242 took 3.57 seconds (mean sampled reward: -4206.85). Current reward after update: -520.19, Optimal reward -251.64
Iteration 243 took 3.66 seconds (mean sampled reward: -4560.16). Current reward after update: -546.52, Optimal reward -251.64
Iteration 244 took 3.46 seconds (mean sampled reward: -4524.55). Current reward after update: -546.41, Optimal reward -251.64
Iteration 245 took 3.57 seconds (mean sampled reward: -4542.74). Current reward after update: -521.86, Optimal reward -251.64
Iteration 246 took 3.48 seconds (mean sampled reward: -4684.08). Current reward after update: -544.25, Optimal reward -251.64
Iteration 247 took 3.52 seconds (mean sampled reward: -4869.22). Current reward after update: -557.26, Optimal reward -251.64
Iteration 248 took 3.71 seconds (mean sampled reward: -4607.13). Current reward after update: -643.41, Optimal reward -251.64
Iteration 249 took 3.65 seconds (mean sampled reward: -4790.77). Current reward after update: -702.69, Optimal reward -251.64
Iteration 250 took 3.52 seconds (mean sampled reward: -4277.86). Current reward after update: -538.96, Optimal reward -251.64
Iteration 251 took 3.58 seconds (mean sampled reward: -4458.38). Current reward after update: -543.31, Optimal reward -251.64
Iteration 252 took 3.57 seconds (mean sampled reward: -4188.73). Current reward after update: -594.61, Optimal reward -251.64
Iteration 253 took 3.53 seconds (mean sampled reward: -4109.57). Current reward after update: -635.55, Optimal reward -251.64
Iteration 254 took 3.49 seconds (mean sampled reward: -3923.28). Current reward after update: -558.29, Optimal reward -251.64
Iteration 255 took 3.49 seconds (mean sampled reward: -4401.27). Current reward after update: -646.71, Optimal reward -251.64
Iteration 256 took 3.50 seconds (mean sampled reward: -4183.49). Current reward after update: -637.05, Optimal reward -251.64
Iteration 257 took 3.56 seconds (mean sampled reward: -4314.83). Current reward after update: -739.99, Optimal reward -251.64
Iteration 258 took 3.60 seconds (mean sampled reward: -4299.40). Current reward after update: -785.51, Optimal reward -251.64
Iteration 259 took 3.77 seconds (mean sampled reward: -4410.40). Current reward after update: -743.15, Optimal reward -251.64
Iteration 260 took 3.56 seconds (mean sampled reward: -4324.52). Current reward after update: -602.34, Optimal reward -251.64
Iteration 261 took 3.57 seconds (mean sampled reward: -4156.73). Current reward after update: -524.79, Optimal reward -251.64
Iteration 262 took 3.54 seconds (mean sampled reward: -4589.17). Current reward after update: -540.15, Optimal reward -251.64
Iteration 263 took 3.57 seconds (mean sampled reward: -4443.48). Current reward after update: -614.34, Optimal reward -251.64
Iteration 264 took 3.49 seconds (mean sampled reward: -4206.94). Current reward after update: -571.34, Optimal reward -251.64
Iteration 265 took 3.45 seconds (mean sampled reward: -4496.76). Current reward after update: -564.19, Optimal reward -251.64
Iteration 266 took 3.63 seconds (mean sampled reward: -4506.78). Current reward after update: -527.12, Optimal reward -251.64
Iteration 267 took 3.60 seconds (mean sampled reward: -4012.18). Current reward after update: -544.42, Optimal reward -251.64
Iteration 268 took 3.64 seconds (mean sampled reward: -3566.23). Current reward after update: -589.75, Optimal reward -251.64
Iteration 269 took 3.50 seconds (mean sampled reward: -3756.01). Current reward after update: -443.03, Optimal reward -251.64
Iteration 270 took 3.52 seconds (mean sampled reward: -4515.66). Current reward after update: -596.04, Optimal reward -251.64
Iteration 271 took 3.56 seconds (mean sampled reward: -4231.69). Current reward after update: -635.37, Optimal reward -251.64
Iteration 272 took 3.53 seconds (mean sampled reward: -3958.02). Current reward after update: -580.42, Optimal reward -251.64
Iteration 273 took 3.54 seconds (mean sampled reward: -3697.04). Current reward after update: -585.48, Optimal reward -251.64
Iteration 274 took 3.62 seconds (mean sampled reward: -4357.13). Current reward after update: -578.57, Optimal reward -251.64
Iteration 275 took 3.55 seconds (mean sampled reward: -3790.80). Current reward after update: -564.11, Optimal reward -251.64
Iteration 276 took 3.53 seconds (mean sampled reward: -3871.15). Current reward after update: -581.88, Optimal reward -251.64
Iteration 277 took 3.52 seconds (mean sampled reward: -4061.30). Current reward after update: -548.99, Optimal reward -251.64
Iteration 278 took 3.56 seconds (mean sampled reward: -4178.79). Current reward after update: -608.70, Optimal reward -251.64
Iteration 279 took 3.50 seconds (mean sampled reward: -3495.65). Current reward after update: -575.21, Optimal reward -251.64
Iteration 280 took 3.50 seconds (mean sampled reward: -4142.42). Current reward after update: -576.23, Optimal reward -251.64
Iteration 281 took 3.58 seconds (mean sampled reward: -4853.83). Current reward after update: -577.89, Optimal reward -251.64
Iteration 282 took 3.56 seconds (mean sampled reward: -4274.54). Current reward after update: -523.20, Optimal reward -251.64
Iteration 283 took 3.44 seconds (mean sampled reward: -4223.73). Current reward after update: -613.55, Optimal reward -251.64
Iteration 284 took 3.50 seconds (mean sampled reward: -4216.58). Current reward after update: -521.77, Optimal reward -251.64
Iteration 285 took 3.46 seconds (mean sampled reward: -4501.82). Current reward after update: -551.11, Optimal reward -251.64
Iteration 286 took 3.43 seconds (mean sampled reward: -4745.33). Current reward after update: -658.56, Optimal reward -251.64
Iteration 287 took 3.41 seconds (mean sampled reward: -4382.16). Current reward after update: -593.38, Optimal reward -251.64
Iteration 288 took 3.51 seconds (mean sampled reward: -3510.26). Current reward after update: -769.53, Optimal reward -251.64
Iteration 289 took 3.54 seconds (mean sampled reward: -4031.40). Current reward after update: -672.92, Optimal reward -251.64
Iteration 290 took 3.57 seconds (mean sampled reward: -4226.79). Current reward after update: -604.62, Optimal reward -251.64
Iteration 291 took 3.56 seconds (mean sampled reward: -4378.95). Current reward after update: -569.42, Optimal reward -251.64
Iteration 292 took 3.55 seconds (mean sampled reward: -3834.51). Current reward after update: -556.51, Optimal reward -251.64
Iteration 293 took 3.53 seconds (mean sampled reward: -4472.16). Current reward after update: -612.26, Optimal reward -251.64
Iteration 294 took 3.50 seconds (mean sampled reward: -3676.92). Current reward after update: -563.12, Optimal reward -251.64
Iteration 295 took 3.47 seconds (mean sampled reward: -3428.57). Current reward after update: -551.79, Optimal reward -251.64
Iteration 296 took 3.48 seconds (mean sampled reward: -3880.26). Current reward after update: -557.73, Optimal reward -251.64
Iteration 297 took 3.49 seconds (mean sampled reward: -3512.68). Current reward after update: -487.78, Optimal reward -251.64
Iteration 298 took 3.52 seconds (mean sampled reward: -3573.35). Current reward after update: -521.49, Optimal reward -251.64
Iteration 299 took 3.54 seconds (mean sampled reward: -3882.12). Current reward after update: -525.54, Optimal reward -251.64
Iteration 300 took 3.59 seconds (mean sampled reward: -4539.58). Current reward after update: -499.22, Optimal reward -251.64
Max force: 50 Sigma: 0.8 mean rewards: -302.26863826050726, best rewards:-242.11576839933946

argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
pybullet build time: Jan 28 2022 20:17:22
Iteration 1 took 3.76 seconds (mean sampled reward: -7460.64). Current reward after update: -2794.50, Optimal reward -2794.50
Iteration 2 took 3.55 seconds (mean sampled reward: -6775.55). Current reward after update: -2039.22, Optimal reward -2039.22
Iteration 3 took 3.54 seconds (mean sampled reward: -6618.08). Current reward after update: -2478.02, Optimal reward -2039.22
Iteration 4 took 3.54 seconds (mean sampled reward: -6486.90). Current reward after update: -2195.62, Optimal reward -2039.22
Iteration 5 took 3.44 seconds (mean sampled reward: -6046.23). Current reward after update: -1307.23, Optimal reward -1307.23
Iteration 6 took 3.45 seconds (mean sampled reward: -5849.66). Current reward after update: -962.06, Optimal reward -962.06
Iteration 7 took 3.54 seconds (mean sampled reward: -6139.98). Current reward after update: -1277.38, Optimal reward -962.06
Iteration 8 took 3.81 seconds (mean sampled reward: -6329.83). Current reward after update: -1140.87, Optimal reward -962.06
Iteration 9 took 3.48 seconds (mean sampled reward: -5656.38). Current reward after update: -1021.87, Optimal reward -962.06
Iteration 10 took 3.50 seconds (mean sampled reward: -5502.64). Current reward after update: -1114.28, Optimal reward -962.06
Iteration 11 took 3.68 seconds (mean sampled reward: -6248.35). Current reward after update: -1100.89, Optimal reward -962.06
Iteration 12 took 3.58 seconds (mean sampled reward: -5403.18). Current reward after update: -1092.04, Optimal reward -962.06
Iteration 13 took 3.55 seconds (mean sampled reward: -5718.09). Current reward after update: -1229.72, Optimal reward -962.06
Iteration 14 took 3.75 seconds (mean sampled reward: -6652.21). Current reward after update: -1282.89, Optimal reward -962.06
Iteration 15 took 3.68 seconds (mean sampled reward: -5718.03). Current reward after update: -1068.72, Optimal reward -962.06
Iteration 16 took 3.57 seconds (mean sampled reward: -5464.18). Current reward after update: -1020.26, Optimal reward -962.06
Iteration 17 took 3.56 seconds (mean sampled reward: -5687.27). Current reward after update: -909.12, Optimal reward -909.12
Iteration 18 took 3.46 seconds (mean sampled reward: -5233.70). Current reward after update: -1045.98, Optimal reward -909.12
Iteration 19 took 3.42 seconds (mean sampled reward: -5204.68). Current reward after update: -999.07, Optimal reward -909.12
Iteration 20 took 3.45 seconds (mean sampled reward: -5082.11). Current reward after update: -812.91, Optimal reward -812.91
Iteration 21 took 3.40 seconds (mean sampled reward: -5355.30). Current reward after update: -970.05, Optimal reward -812.91
Iteration 22 took 3.40 seconds (mean sampled reward: -4978.06). Current reward after update: -941.75, Optimal reward -812.91
Iteration 23 took 3.28 seconds (mean sampled reward: -5220.87). Current reward after update: -576.09, Optimal reward -576.09
Iteration 24 took 3.37 seconds (mean sampled reward: -5436.79). Current reward after update: -788.93, Optimal reward -576.09
Iteration 25 took 3.26 seconds (mean sampled reward: -4349.51). Current reward after update: -694.70, Optimal reward -576.09
Iteration 26 took 3.38 seconds (mean sampled reward: -5602.83). Current reward after update: -825.29, Optimal reward -576.09
Iteration 27 took 3.41 seconds (mean sampled reward: -5519.02). Current reward after update: -891.58, Optimal reward -576.09
Iteration 28 took 3.44 seconds (mean sampled reward: -5368.56). Current reward after update: -703.26, Optimal reward -576.09
Iteration 29 took 3.83 seconds (mean sampled reward: -6322.19). Current reward after update: -1004.19, Optimal reward -576.09
Iteration 30 took 3.45 seconds (mean sampled reward: -4776.96). Current reward after update: -793.04, Optimal reward -576.09
Iteration 31 took 3.42 seconds (mean sampled reward: -5588.82). Current reward after update: -780.55, Optimal reward -576.09
Iteration 32 took 3.48 seconds (mean sampled reward: -6073.46). Current reward after update: -732.75, Optimal reward -576.09
Iteration 33 took 3.52 seconds (mean sampled reward: -6142.85). Current reward after update: -836.74, Optimal reward -576.09
Iteration 34 took 3.47 seconds (mean sampled reward: -6624.86). Current reward after update: -920.00, Optimal reward -576.09
Iteration 35 took 3.57 seconds (mean sampled reward: -6473.65). Current reward after update: -975.77, Optimal reward -576.09
Iteration 36 took 3.46 seconds (mean sampled reward: -6511.62). Current reward after update: -857.51, Optimal reward -576.09
Iteration 37 took 3.53 seconds (mean sampled reward: -6253.66). Current reward after update: -668.00, Optimal reward -576.09
Iteration 38 took 3.52 seconds (mean sampled reward: -6041.05). Current reward after update: -686.76, Optimal reward -576.09
Iteration 39 took 3.56 seconds (mean sampled reward: -6190.19). Current reward after update: -451.55, Optimal reward -451.55
Iteration 40 took 3.51 seconds (mean sampled reward: -5962.53). Current reward after update: -599.96, Optimal reward -451.55
Iteration 41 took 3.52 seconds (mean sampled reward: -5962.42). Current reward after update: -694.73, Optimal reward -451.55
Iteration 42 took 3.52 seconds (mean sampled reward: -6530.13). Current reward after update: -724.76, Optimal reward -451.55
Iteration 43 took 3.42 seconds (mean sampled reward: -5637.16). Current reward after update: -646.96, Optimal reward -451.55
Iteration 44 took 3.50 seconds (mean sampled reward: -6108.47). Current reward after update: -502.62, Optimal reward -451.55
Iteration 45 took 3.57 seconds (mean sampled reward: -6566.31). Current reward after update: -561.00, Optimal reward -451.55
Iteration 46 took 3.59 seconds (mean sampled reward: -6552.53). Current reward after update: -467.71, Optimal reward -451.55
Iteration 47 took 3.58 seconds (mean sampled reward: -6349.62). Current reward after update: -598.07, Optimal reward -451.55
Iteration 48 took 3.57 seconds (mean sampled reward: -6152.47). Current reward after update: -636.07, Optimal reward -451.55
Iteration 49 took 3.56 seconds (mean sampled reward: -6588.22). Current reward after update: -647.69, Optimal reward -451.55
Iteration 50 took 3.55 seconds (mean sampled reward: -6354.85). Current reward after update: -496.55, Optimal reward -451.55
Iteration 51 took 3.51 seconds (mean sampled reward: -6409.30). Current reward after update: -418.87, Optimal reward -418.87
Iteration 52 took 3.54 seconds (mean sampled reward: -6902.76). Current reward after update: -704.73, Optimal reward -418.87
Iteration 53 took 3.55 seconds (mean sampled reward: -6381.82). Current reward after update: -525.09, Optimal reward -418.87
Iteration 54 took 3.74 seconds (mean sampled reward: -6499.46). Current reward after update: -362.96, Optimal reward -362.96
Iteration 55 took 3.62 seconds (mean sampled reward: -6349.47). Current reward after update: -742.16, Optimal reward -362.96
Iteration 56 took 3.60 seconds (mean sampled reward: -6621.30). Current reward after update: -813.47, Optimal reward -362.96
Iteration 57 took 3.67 seconds (mean sampled reward: -6074.85). Current reward after update: -509.71, Optimal reward -362.96
Iteration 58 took 3.67 seconds (mean sampled reward: -6439.77). Current reward after update: -609.51, Optimal reward -362.96
Iteration 59 took 3.52 seconds (mean sampled reward: -6148.04). Current reward after update: -584.59, Optimal reward -362.96
Iteration 60 took 3.55 seconds (mean sampled reward: -6626.97). Current reward after update: -576.79, Optimal reward -362.96
Iteration 61 took 3.54 seconds (mean sampled reward: -6575.44). Current reward after update: -385.16, Optimal reward -362.96
Iteration 62 took 3.54 seconds (mean sampled reward: -6514.76). Current reward after update: -634.19, Optimal reward -362.96
Iteration 63 took 3.57 seconds (mean sampled reward: -6397.33). Current reward after update: -547.08, Optimal reward -362.96
Iteration 64 took 3.88 seconds (mean sampled reward: -6429.17). Current reward after update: -585.11, Optimal reward -362.96
Iteration 65 took 3.61 seconds (mean sampled reward: -6456.47). Current reward after update: -485.21, Optimal reward -362.96
Iteration 66 took 3.59 seconds (mean sampled reward: -6726.67). Current reward after update: -716.63, Optimal reward -362.96
Iteration 67 took 3.59 seconds (mean sampled reward: -6845.45). Current reward after update: -1238.83, Optimal reward -362.96
Iteration 68 took 3.53 seconds (mean sampled reward: -7259.79). Current reward after update: -859.20, Optimal reward -362.96
Iteration 69 took 3.70 seconds (mean sampled reward: -6986.60). Current reward after update: -926.19, Optimal reward -362.96
Iteration 70 took 3.52 seconds (mean sampled reward: -6948.45). Current reward after update: -532.05, Optimal reward -362.96
Iteration 71 took 3.46 seconds (mean sampled reward: -6777.24). Current reward after update: -461.84, Optimal reward -362.96
Iteration 72 took 3.48 seconds (mean sampled reward: -6776.71). Current reward after update: -695.43, Optimal reward -362.96
Iteration 73 took 3.56 seconds (mean sampled reward: -6866.85). Current reward after update: -536.53, Optimal reward -362.96
Iteration 74 took 3.31 seconds (mean sampled reward: -6688.88). Current reward after update: -455.22, Optimal reward -362.96
Iteration 75 took 3.38 seconds (mean sampled reward: -7157.77). Current reward after update: -792.48, Optimal reward -362.96
Iteration 76 took 3.44 seconds (mean sampled reward: -6430.86). Current reward after update: -422.70, Optimal reward -362.96
Iteration 77 took 3.47 seconds (mean sampled reward: -6852.12). Current reward after update: -360.88, Optimal reward -360.88
Iteration 78 took 3.58 seconds (mean sampled reward: -6311.00). Current reward after update: -491.29, Optimal reward -360.88
Iteration 79 took 3.57 seconds (mean sampled reward: -6335.75). Current reward after update: -528.97, Optimal reward -360.88
Iteration 80 took 3.52 seconds (mean sampled reward: -5964.21). Current reward after update: -456.10, Optimal reward -360.88
Iteration 81 took 3.39 seconds (mean sampled reward: -6009.98). Current reward after update: -331.62, Optimal reward -331.62
Iteration 82 took 3.40 seconds (mean sampled reward: -6406.40). Current reward after update: -669.66, Optimal reward -331.62
Iteration 83 took 3.61 seconds (mean sampled reward: -6331.53). Current reward after update: -525.97, Optimal reward -331.62
Iteration 84 took 3.44 seconds (mean sampled reward: -6040.72). Current reward after update: -687.41, Optimal reward -331.62
Iteration 85 took 3.36 seconds (mean sampled reward: -6516.74). Current reward after update: -759.72, Optimal reward -331.62
Iteration 86 took 3.34 seconds (mean sampled reward: -5851.30). Current reward after update: -545.16, Optimal reward -331.62
Iteration 87 took 3.34 seconds (mean sampled reward: -6155.32). Current reward after update: -652.39, Optimal reward -331.62
Iteration 88 took 3.39 seconds (mean sampled reward: -6218.62). Current reward after update: -500.29, Optimal reward -331.62
Iteration 89 took 3.45 seconds (mean sampled reward: -6074.19). Current reward after update: -575.14, Optimal reward -331.62
Iteration 90 took 3.37 seconds (mean sampled reward: -6311.42). Current reward after update: -636.55, Optimal reward -331.62
Iteration 91 took 3.35 seconds (mean sampled reward: -6422.87). Current reward after update: -557.68, Optimal reward -331.62
Iteration 92 took 3.44 seconds (mean sampled reward: -6364.39). Current reward after update: -603.39, Optimal reward -331.62
Iteration 93 took 3.38 seconds (mean sampled reward: -6316.15). Current reward after update: -720.35, Optimal reward -331.62
Iteration 94 took 3.42 seconds (mean sampled reward: -6270.11). Current reward after update: -823.42, Optimal reward -331.62
Iteration 95 took 3.43 seconds (mean sampled reward: -6697.33). Current reward after update: -566.41, Optimal reward -331.62
Iteration 96 took 3.50 seconds (mean sampled reward: -6387.18). Current reward after update: -552.78, Optimal reward -331.62
Iteration 97 took 3.43 seconds (mean sampled reward: -6421.41). Current reward after update: -625.94, Optimal reward -331.62
Iteration 98 took 3.52 seconds (mean sampled reward: -6469.15). Current reward after update: -541.51, Optimal reward -331.62
Iteration 99 took 3.55 seconds (mean sampled reward: -6803.13). Current reward after update: -611.98, Optimal reward -331.62
Iteration 100 took 3.54 seconds (mean sampled reward: -6930.80). Current reward after update: -711.84, Optimal reward -331.62
Iteration 101 took 3.56 seconds (mean sampled reward: -6553.76). Current reward after update: -587.49, Optimal reward -331.62
Iteration 102 took 3.63 seconds (mean sampled reward: -6438.22). Current reward after update: -620.09, Optimal reward -331.62
Iteration 103 took 3.62 seconds (mean sampled reward: -6801.33). Current reward after update: -728.13, Optimal reward -331.62
Iteration 104 took 3.61 seconds (mean sampled reward: -6468.17). Current reward after update: -742.68, Optimal reward -331.62
Iteration 105 took 3.63 seconds (mean sampled reward: -6838.48). Current reward after update: -788.76, Optimal reward -331.62
Iteration 106 took 3.74 seconds (mean sampled reward: -6751.27). Current reward after update: -619.04, Optimal reward -331.62
Iteration 107 took 3.62 seconds (mean sampled reward: -7080.24). Current reward after update: -617.40, Optimal reward -331.62
Iteration 108 took 3.75 seconds (mean sampled reward: -6184.68). Current reward after update: -542.09, Optimal reward -331.62
Iteration 109 took 3.93 seconds (mean sampled reward: -6367.14). Current reward after update: -726.27, Optimal reward -331.62
Iteration 110 took 3.65 seconds (mean sampled reward: -6337.94). Current reward after update: -578.83, Optimal reward -331.62
Iteration 111 took 4.00 seconds (mean sampled reward: -6287.37). Current reward after update: -515.64, Optimal reward -331.62
Iteration 112 took 3.74 seconds (mean sampled reward: -6570.19). Current reward after update: -520.95, Optimal reward -331.62
Iteration 113 took 3.74 seconds (mean sampled reward: -6837.32). Current reward after update: -578.61, Optimal reward -331.62
Iteration 114 took 3.82 seconds (mean sampled reward: -6970.78). Current reward after update: -676.91, Optimal reward -331.62
Iteration 115 took 3.66 seconds (mean sampled reward: -6935.37). Current reward after update: -586.33, Optimal reward -331.62
Iteration 116 took 3.63 seconds (mean sampled reward: -6468.30). Current reward after update: -609.89, Optimal reward -331.62
Iteration 117 took 3.59 seconds (mean sampled reward: -6745.36). Current reward after update: -699.49, Optimal reward -331.62
Iteration 118 took 3.60 seconds (mean sampled reward: -6614.36). Current reward after update: -630.08, Optimal reward -331.62
Iteration 119 took 3.49 seconds (mean sampled reward: -5684.92). Current reward after update: -547.88, Optimal reward -331.62
Iteration 120 took 3.49 seconds (mean sampled reward: -5699.53). Current reward after update: -567.52, Optimal reward -331.62
Iteration 121 took 3.60 seconds (mean sampled reward: -5910.76). Current reward after update: -511.09, Optimal reward -331.62
Iteration 122 took 3.65 seconds (mean sampled reward: -6348.63). Current reward after update: -526.22, Optimal reward -331.62
Iteration 123 took 3.61 seconds (mean sampled reward: -6448.02). Current reward after update: -562.59, Optimal reward -331.62
Iteration 124 took 3.49 seconds (mean sampled reward: -6417.62). Current reward after update: -607.86, Optimal reward -331.62
Iteration 125 took 3.55 seconds (mean sampled reward: -6093.55). Current reward after update: -581.42, Optimal reward -331.62
Iteration 126 took 3.44 seconds (mean sampled reward: -5621.73). Current reward after update: -594.70, Optimal reward -331.62
Iteration 127 took 3.47 seconds (mean sampled reward: -5650.82). Current reward after update: -595.30, Optimal reward -331.62
Iteration 128 took 3.46 seconds (mean sampled reward: -5849.79). Current reward after update: -497.68, Optimal reward -331.62
Iteration 129 took 3.59 seconds (mean sampled reward: -6012.29). Current reward after update: -531.43, Optimal reward -331.62
Iteration 130 took 3.53 seconds (mean sampled reward: -6017.97). Current reward after update: -513.38, Optimal reward -331.62
Iteration 131 took 3.49 seconds (mean sampled reward: -6119.21). Current reward after update: -515.94, Optimal reward -331.62
Iteration 132 took 3.56 seconds (mean sampled reward: -6167.94). Current reward after update: -495.49, Optimal reward -331.62
Iteration 133 took 3.61 seconds (mean sampled reward: -6184.26). Current reward after update: -433.75, Optimal reward -331.62
Iteration 134 took 3.61 seconds (mean sampled reward: -6241.02). Current reward after update: -422.28, Optimal reward -331.62
Iteration 135 took 3.47 seconds (mean sampled reward: -6139.67). Current reward after update: -301.52, Optimal reward -301.52
Iteration 136 took 3.57 seconds (mean sampled reward: -6353.96). Current reward after update: -404.52, Optimal reward -301.52
Iteration 137 took 3.60 seconds (mean sampled reward: -6214.63). Current reward after update: -361.72, Optimal reward -301.52
Iteration 138 took 3.64 seconds (mean sampled reward: -5875.36). Current reward after update: -460.08, Optimal reward -301.52
Iteration 139 took 3.61 seconds (mean sampled reward: -6042.24). Current reward after update: -564.24, Optimal reward -301.52
Iteration 140 took 3.48 seconds (mean sampled reward: -5894.65). Current reward after update: -609.54, Optimal reward -301.52
Iteration 141 took 3.58 seconds (mean sampled reward: -6093.29). Current reward after update: -477.27, Optimal reward -301.52
Iteration 142 took 3.53 seconds (mean sampled reward: -6180.52). Current reward after update: -507.81, Optimal reward -301.52
Iteration 143 took 3.49 seconds (mean sampled reward: -5866.41). Current reward after update: -522.24, Optimal reward -301.52
Iteration 144 took 3.60 seconds (mean sampled reward: -6211.18). Current reward after update: -800.71, Optimal reward -301.52
Iteration 145 took 3.58 seconds (mean sampled reward: -6158.56). Current reward after update: -518.47, Optimal reward -301.52
Iteration 146 took 3.63 seconds (mean sampled reward: -5951.47). Current reward after update: -516.16, Optimal reward -301.52
Iteration 147 took 3.63 seconds (mean sampled reward: -6091.79). Current reward after update: -522.83, Optimal reward -301.52
Iteration 148 took 3.65 seconds (mean sampled reward: -6083.47). Current reward after update: -592.34, Optimal reward -301.52
Iteration 149 took 3.70 seconds (mean sampled reward: -5555.09). Current reward after update: -461.69, Optimal reward -301.52
Iteration 150 took 3.70 seconds (mean sampled reward: -6249.66). Current reward after update: -478.41, Optimal reward -301.52
Iteration 151 took 3.52 seconds (mean sampled reward: -5243.58). Current reward after update: -431.13, Optimal reward -301.52
Iteration 152 took 3.62 seconds (mean sampled reward: -4892.07). Current reward after update: -409.92, Optimal reward -301.52
Iteration 153 took 3.72 seconds (mean sampled reward: -6293.17). Current reward after update: -401.47, Optimal reward -301.52
Iteration 154 took 3.74 seconds (mean sampled reward: -6287.02). Current reward after update: -473.00, Optimal reward -301.52
Iteration 155 took 3.76 seconds (mean sampled reward: -6255.54). Current reward after update: -384.14, Optimal reward -301.52
Iteration 156 took 3.91 seconds (mean sampled reward: -6694.62). Current reward after update: -493.09, Optimal reward -301.52
Iteration 157 took 3.70 seconds (mean sampled reward: -6567.09). Current reward after update: -516.12, Optimal reward -301.52
Iteration 158 took 3.83 seconds (mean sampled reward: -5825.71). Current reward after update: -431.16, Optimal reward -301.52
Iteration 159 took 3.75 seconds (mean sampled reward: -6385.18). Current reward after update: -425.28, Optimal reward -301.52
Iteration 160 took 3.76 seconds (mean sampled reward: -6754.60). Current reward after update: -565.41, Optimal reward -301.52
Iteration 161 took 3.71 seconds (mean sampled reward: -6399.13). Current reward after update: -441.84, Optimal reward -301.52
Iteration 162 took 3.85 seconds (mean sampled reward: -6851.93). Current reward after update: -593.01, Optimal reward -301.52
Iteration 163 took 3.75 seconds (mean sampled reward: -6387.16). Current reward after update: -560.01, Optimal reward -301.52
Iteration 164 took 3.65 seconds (mean sampled reward: -6766.27). Current reward after update: -660.70, Optimal reward -301.52
Iteration 165 took 3.68 seconds (mean sampled reward: -6132.80). Current reward after update: -315.40, Optimal reward -301.52
Iteration 166 took 3.71 seconds (mean sampled reward: -5801.57). Current reward after update: -374.63, Optimal reward -301.52
Iteration 167 took 3.81 seconds (mean sampled reward: -6764.66). Current reward after update: -348.96, Optimal reward -301.52
Iteration 168 took 4.01 seconds (mean sampled reward: -6100.24). Current reward after update: -286.59, Optimal reward -286.59
Iteration 169 took 3.66 seconds (mean sampled reward: -6212.93). Current reward after update: -405.74, Optimal reward -286.59
Iteration 170 took 3.70 seconds (mean sampled reward: -6083.44). Current reward after update: -355.89, Optimal reward -286.59
Iteration 171 took 3.98 seconds (mean sampled reward: -6317.57). Current reward after update: -384.97, Optimal reward -286.59
Iteration 172 took 3.80 seconds (mean sampled reward: -6648.97). Current reward after update: -270.00, Optimal reward -270.00
Iteration 173 took 4.03 seconds (mean sampled reward: -5900.45). Current reward after update: -374.81, Optimal reward -270.00
Iteration 174 took 3.86 seconds (mean sampled reward: -6531.76). Current reward after update: -393.56, Optimal reward -270.00
Iteration 175 took 3.62 seconds (mean sampled reward: -6029.05). Current reward after update: -397.15, Optimal reward -270.00
Iteration 176 took 3.67 seconds (mean sampled reward: -6421.80). Current reward after update: -416.08, Optimal reward -270.00
Iteration 177 took 3.60 seconds (mean sampled reward: -6016.38). Current reward after update: -433.16, Optimal reward -270.00
Iteration 178 took 3.47 seconds (mean sampled reward: -5559.40). Current reward after update: -527.17, Optimal reward -270.00
Iteration 179 took 3.45 seconds (mean sampled reward: -5815.38). Current reward after update: -455.54, Optimal reward -270.00
Iteration 180 took 3.46 seconds (mean sampled reward: -5249.24). Current reward after update: -466.84, Optimal reward -270.00
Iteration 181 took 3.66 seconds (mean sampled reward: -5229.76). Current reward after update: -383.75, Optimal reward -270.00
Iteration 182 took 3.61 seconds (mean sampled reward: -6112.59). Current reward after update: -531.33, Optimal reward -270.00
Iteration 183 took 3.77 seconds (mean sampled reward: -6058.67). Current reward after update: -302.40, Optimal reward -270.00
Iteration 184 took 3.70 seconds (mean sampled reward: -6339.29). Current reward after update: -416.54, Optimal reward -270.00
Iteration 185 took 3.54 seconds (mean sampled reward: -5623.29). Current reward after update: -375.38, Optimal reward -270.00
Iteration 186 took 3.91 seconds (mean sampled reward: -6371.16). Current reward after update: -374.12, Optimal reward -270.00
Iteration 187 took 3.83 seconds (mean sampled reward: -6500.84). Current reward after update: -421.91, Optimal reward -270.00
Iteration 188 took 4.04 seconds (mean sampled reward: -7123.45). Current reward after update: -558.13, Optimal reward -270.00
Iteration 189 took 4.01 seconds (mean sampled reward: -6351.17). Current reward after update: -517.87, Optimal reward -270.00
Iteration 190 took 3.97 seconds (mean sampled reward: -6620.80). Current reward after update: -358.23, Optimal reward -270.00
Iteration 191 took 3.69 seconds (mean sampled reward: -6483.81). Current reward after update: -691.84, Optimal reward -270.00
Iteration 192 took 3.84 seconds (mean sampled reward: -6680.66). Current reward after update: -365.04, Optimal reward -270.00
Iteration 193 took 3.58 seconds (mean sampled reward: -6867.85). Current reward after update: -350.44, Optimal reward -270.00
Iteration 194 took 3.66 seconds (mean sampled reward: -6847.20). Current reward after update: -566.13, Optimal reward -270.00
Iteration 195 took 3.65 seconds (mean sampled reward: -6783.06). Current reward after update: -719.19, Optimal reward -270.00
Iteration 196 took 3.58 seconds (mean sampled reward: -5687.42). Current reward after update: -892.48, Optimal reward -270.00
Iteration 197 took 3.60 seconds (mean sampled reward: -6172.09). Current reward after update: -492.91, Optimal reward -270.00
Iteration 198 took 3.53 seconds (mean sampled reward: -5692.36). Current reward after update: -295.42, Optimal reward -270.00
Iteration 199 took 3.48 seconds (mean sampled reward: -5219.24). Current reward after update: -409.77, Optimal reward -270.00
Iteration 200 took 3.57 seconds (mean sampled reward: -5517.40). Current reward after update: -283.73, Optimal reward -270.00
Iteration 201 took 3.47 seconds (mean sampled reward: -5330.33). Current reward after update: -382.68, Optimal reward -270.00
Iteration 202 took 3.47 seconds (mean sampled reward: -5244.11). Current reward after update: -268.85, Optimal reward -268.85
Iteration 203 took 3.50 seconds (mean sampled reward: -5234.92). Current reward after update: -442.42, Optimal reward -268.85
Iteration 204 took 3.46 seconds (mean sampled reward: -4966.02). Current reward after update: -342.56, Optimal reward -268.85
Iteration 205 took 3.53 seconds (mean sampled reward: -4882.82). Current reward after update: -341.16, Optimal reward -268.85
Iteration 206 took 3.47 seconds (mean sampled reward: -4001.89). Current reward after update: -305.08, Optimal reward -268.85
Iteration 207 took 3.50 seconds (mean sampled reward: -3557.83). Current reward after update: -358.14, Optimal reward -268.85
Iteration 208 took 3.50 seconds (mean sampled reward: -4497.97). Current reward after update: -257.28, Optimal reward -257.28
Iteration 209 took 3.57 seconds (mean sampled reward: -4532.84). Current reward after update: -443.92, Optimal reward -257.28
Iteration 210 took 3.57 seconds (mean sampled reward: -4812.33). Current reward after update: -328.80, Optimal reward -257.28
Iteration 211 took 3.55 seconds (mean sampled reward: -4672.96). Current reward after update: -333.43, Optimal reward -257.28
Iteration 212 took 3.91 seconds (mean sampled reward: -5179.22). Current reward after update: -326.60, Optimal reward -257.28
Iteration 213 took 3.82 seconds (mean sampled reward: -4454.87). Current reward after update: -283.19, Optimal reward -257.28
Iteration 214 took 3.58 seconds (mean sampled reward: -4202.44). Current reward after update: -357.33, Optimal reward -257.28
Iteration 215 took 3.58 seconds (mean sampled reward: -4431.98). Current reward after update: -363.09, Optimal reward -257.28
Iteration 216 took 3.49 seconds (mean sampled reward: -4093.10). Current reward after update: -317.82, Optimal reward -257.28
Iteration 217 took 3.60 seconds (mean sampled reward: -5231.02). Current reward after update: -378.23, Optimal reward -257.28
Iteration 218 took 3.55 seconds (mean sampled reward: -4373.56). Current reward after update: -392.99, Optimal reward -257.28
Iteration 219 took 3.59 seconds (mean sampled reward: -3949.93). Current reward after update: -301.65, Optimal reward -257.28
Iteration 220 took 3.82 seconds (mean sampled reward: -5429.41). Current reward after update: -355.32, Optimal reward -257.28
Iteration 221 took 3.87 seconds (mean sampled reward: -5947.44). Current reward after update: -315.22, Optimal reward -257.28
Iteration 222 took 3.68 seconds (mean sampled reward: -5274.41). Current reward after update: -290.47, Optimal reward -257.28
Iteration 223 took 3.78 seconds (mean sampled reward: -6332.42). Current reward after update: -371.54, Optimal reward -257.28
Iteration 224 took 3.73 seconds (mean sampled reward: -5467.05). Current reward after update: -407.85, Optimal reward -257.28
Iteration 225 took 3.56 seconds (mean sampled reward: -5562.04). Current reward after update: -286.12, Optimal reward -257.28
Iteration 226 took 3.65 seconds (mean sampled reward: -5913.40). Current reward after update: -353.57, Optimal reward -257.28
Iteration 227 took 3.56 seconds (mean sampled reward: -4518.22). Current reward after update: -281.81, Optimal reward -257.28
Iteration 228 took 3.41 seconds (mean sampled reward: -4474.06). Current reward after update: -293.30, Optimal reward -257.28
Iteration 229 took 3.49 seconds (mean sampled reward: -5059.74). Current reward after update: -298.37, Optimal reward -257.28
Iteration 230 took 3.59 seconds (mean sampled reward: -5329.77). Current reward after update: -278.92, Optimal reward -257.28
Iteration 231 took 3.54 seconds (mean sampled reward: -4841.38). Current reward after update: -361.10, Optimal reward -257.28
Iteration 232 took 3.59 seconds (mean sampled reward: -4606.01). Current reward after update: -301.45, Optimal reward -257.28
Iteration 233 took 3.48 seconds (mean sampled reward: -3549.60). Current reward after update: -324.54, Optimal reward -257.28
Iteration 234 took 3.58 seconds (mean sampled reward: -4032.69). Current reward after update: -311.86, Optimal reward -257.28
Iteration 235 took 3.74 seconds (mean sampled reward: -4758.36). Current reward after update: -239.44, Optimal reward -239.44
Iteration 236 took 3.67 seconds (mean sampled reward: -4355.92). Current reward after update: -376.22, Optimal reward -239.44
Iteration 237 took 3.98 seconds (mean sampled reward: -4368.82). Current reward after update: -248.13, Optimal reward -239.44
Iteration 238 took 3.88 seconds (mean sampled reward: -4144.11). Current reward after update: -344.36, Optimal reward -239.44
Iteration 239 took 3.74 seconds (mean sampled reward: -3810.79). Current reward after update: -306.13, Optimal reward -239.44
Iteration 240 took 3.80 seconds (mean sampled reward: -3672.18). Current reward after update: -280.69, Optimal reward -239.44
Iteration 241 took 3.79 seconds (mean sampled reward: -3731.71). Current reward after update: -292.19, Optimal reward -239.44
Iteration 242 took 3.69 seconds (mean sampled reward: -5127.79). Current reward after update: -255.46, Optimal reward -239.44
Iteration 243 took 3.53 seconds (mean sampled reward: -4260.90). Current reward after update: -300.84, Optimal reward -239.44
Iteration 244 took 3.52 seconds (mean sampled reward: -3660.35). Current reward after update: -327.62, Optimal reward -239.44
Iteration 245 took 3.50 seconds (mean sampled reward: -4157.92). Current reward after update: -293.85, Optimal reward -239.44
Iteration 246 took 3.60 seconds (mean sampled reward: -3677.48). Current reward after update: -281.52, Optimal reward -239.44
Iteration 247 took 3.54 seconds (mean sampled reward: -3312.00). Current reward after update: -293.96, Optimal reward -239.44
Iteration 248 took 3.60 seconds (mean sampled reward: -3614.50). Current reward after update: -302.34, Optimal reward -239.44
Iteration 249 took 3.65 seconds (mean sampled reward: -4085.19). Current reward after update: -327.16, Optimal reward -239.44
Iteration 250 took 3.53 seconds (mean sampled reward: -3544.41). Current reward after update: -344.52, Optimal reward -239.44
Iteration 251 took 3.68 seconds (mean sampled reward: -3835.52). Current reward after update: -349.17, Optimal reward -239.44
Iteration 252 took 3.67 seconds (mean sampled reward: -4034.16). Current reward after update: -316.13, Optimal reward -239.44
Iteration 253 took 3.81 seconds (mean sampled reward: -4689.56). Current reward after update: -334.13, Optimal reward -239.44
Iteration 254 took 3.62 seconds (mean sampled reward: -3602.56). Current reward after update: -325.15, Optimal reward -239.44
Iteration 255 took 3.61 seconds (mean sampled reward: -3619.36). Current reward after update: -287.18, Optimal reward -239.44
Iteration 256 took 3.85 seconds (mean sampled reward: -3354.25). Current reward after update: -333.53, Optimal reward -239.44
Iteration 257 took 3.69 seconds (mean sampled reward: -3192.39). Current reward after update: -501.04, Optimal reward -239.44
Iteration 258 took 3.62 seconds (mean sampled reward: -3518.08). Current reward after update: -310.97, Optimal reward -239.44
Iteration 259 took 3.71 seconds (mean sampled reward: -3329.02). Current reward after update: -261.37, Optimal reward -239.44
Iteration 260 took 3.65 seconds (mean sampled reward: -3156.33). Current reward after update: -271.44, Optimal reward -239.44
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
Iteration 261 took 3.55 seconds (mean sampled reward: -3473.96). Current reward after update: -313.02, Optimal reward -239.44
Iteration 262 took 3.55 seconds (mean sampled reward: -3780.31). Current reward after update: -304.04, Optimal reward -239.44
Iteration 263 took 3.58 seconds (mean sampled reward: -4018.24). Current reward after update: -276.41, Optimal reward -239.44
Iteration 264 took 3.55 seconds (mean sampled reward: -3460.83). Current reward after update: -369.33, Optimal reward -239.44
Iteration 265 took 3.62 seconds (mean sampled reward: -3621.74). Current reward after update: -321.99, Optimal reward -239.44
Iteration 266 took 3.61 seconds (mean sampled reward: -3730.02). Current reward after update: -396.94, Optimal reward -239.44
Iteration 267 took 3.54 seconds (mean sampled reward: -3668.53). Current reward after update: -388.27, Optimal reward -239.44
Iteration 268 took 3.52 seconds (mean sampled reward: -3421.49). Current reward after update: -334.55, Optimal reward -239.44
Iteration 269 took 3.97 seconds (mean sampled reward: -3383.76). Current reward after update: -339.17, Optimal reward -239.44
Iteration 270 took 3.54 seconds (mean sampled reward: -3089.82). Current reward after update: -282.02, Optimal reward -239.44
Iteration 271 took 3.52 seconds (mean sampled reward: -3404.64). Current reward after update: -317.80, Optimal reward -239.44
Iteration 272 took 3.59 seconds (mean sampled reward: -3803.64). Current reward after update: -328.02, Optimal reward -239.44
Iteration 273 took 3.60 seconds (mean sampled reward: -4129.20). Current reward after update: -358.22, Optimal reward -239.44
Iteration 274 took 3.72 seconds (mean sampled reward: -4609.97). Current reward after update: -333.04, Optimal reward -239.44
Iteration 275 took 3.81 seconds (mean sampled reward: -3694.97). Current reward after update: -381.96, Optimal reward -239.44
Iteration 276 took 3.71 seconds (mean sampled reward: -3218.83). Current reward after update: -324.16, Optimal reward -239.44
Iteration 277 took 3.61 seconds (mean sampled reward: -3783.69). Current reward after update: -312.35, Optimal reward -239.44
Iteration 278 took 3.69 seconds (mean sampled reward: -4620.96). Current reward after update: -301.24, Optimal reward -239.44
Iteration 279 took 3.64 seconds (mean sampled reward: -4235.12). Current reward after update: -274.98, Optimal reward -239.44
Iteration 280 took 3.58 seconds (mean sampled reward: -3624.34). Current reward after update: -368.02, Optimal reward -239.44
Iteration 281 took 3.52 seconds (mean sampled reward: -3779.47). Current reward after update: -330.46, Optimal reward -239.44
Iteration 282 took 3.53 seconds (mean sampled reward: -4534.97). Current reward after update: -230.19, Optimal reward -230.19
Iteration 283 took 3.80 seconds (mean sampled reward: -4026.12). Current reward after update: -291.44, Optimal reward -230.19
Iteration 284 took 3.82 seconds (mean sampled reward: -3194.13). Current reward after update: -308.09, Optimal reward -230.19
Iteration 285 took 3.90 seconds (mean sampled reward: -3440.25). Current reward after update: -213.94, Optimal reward -213.94
Iteration 286 took 3.43 seconds (mean sampled reward: -3104.91). Current reward after update: -225.28, Optimal reward -213.94
Iteration 287 took 3.68 seconds (mean sampled reward: -3309.38). Current reward after update: -1410.60, Optimal reward -213.94
Iteration 288 took 3.77 seconds (mean sampled reward: -3562.35). Current reward after update: -257.85, Optimal reward -213.94
Iteration 289 took 3.48 seconds (mean sampled reward: -3834.97). Current reward after update: -305.89, Optimal reward -213.94
Iteration 290 took 3.94 seconds (mean sampled reward: -3208.88). Current reward after update: -320.53, Optimal reward -213.94
Iteration 291 took 3.64 seconds (mean sampled reward: -3144.56). Current reward after update: -317.30, Optimal reward -213.94
Iteration 292 took 3.62 seconds (mean sampled reward: -3592.09). Current reward after update: -300.21, Optimal reward -213.94
Iteration 293 took 3.54 seconds (mean sampled reward: -3649.93). Current reward after update: -312.91, Optimal reward -213.94
Iteration 294 took 3.66 seconds (mean sampled reward: -3803.12). Current reward after update: -272.08, Optimal reward -213.94
Iteration 295 took 3.70 seconds (mean sampled reward: -3492.65). Current reward after update: -403.21, Optimal reward -213.94
Iteration 296 took 3.64 seconds (mean sampled reward: -3467.57). Current reward after update: -316.49, Optimal reward -213.94
Iteration 297 took 3.69 seconds (mean sampled reward: -3102.34). Current reward after update: -295.58, Optimal reward -213.94
Iteration 298 took 3.58 seconds (mean sampled reward: -3148.86). Current reward after update: -308.42, Optimal reward -213.94
Iteration 299 took 3.54 seconds (mean sampled reward: -3323.98). Current reward after update: -342.18, Optimal reward -213.94
Iteration 300 took 3.63 seconds (mean sampled reward: -4041.79). Current reward after update: -265.85, Optimal reward -213.94
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
Iteration 1 took 3.89 seconds (mean sampled reward: -7479.78). Current reward after update: -4533.49, Optimal reward -4533.49
Iteration 2 took 3.64 seconds (mean sampled reward: -7129.13). Current reward after update: -3420.06, Optimal reward -3420.06
Iteration 3 took 3.61 seconds (mean sampled reward: -6968.17). Current reward after update: -2835.28, Optimal reward -2835.28
Iteration 4 took 3.70 seconds (mean sampled reward: -6634.94). Current reward after update: -2084.82, Optimal reward -2084.82
Iteration 5 took 3.72 seconds (mean sampled reward: -6257.00). Current reward after update: -1585.97, Optimal reward -1585.97
Iteration 6 took 3.66 seconds (mean sampled reward: -6348.87). Current reward after update: -1263.72, Optimal reward -1263.72
Iteration 7 took 3.74 seconds (mean sampled reward: -6444.68). Current reward after update: -1256.64, Optimal reward -1256.64
Iteration 8 took 3.64 seconds (mean sampled reward: -6034.41). Current reward after update: -1146.66, Optimal reward -1146.66
Iteration 9 took 3.71 seconds (mean sampled reward: -6068.95). Current reward after update: -874.12, Optimal reward -874.12
Iteration 10 took 3.69 seconds (mean sampled reward: -6212.82). Current reward after update: -1059.28, Optimal reward -874.12
Iteration 11 took 3.87 seconds (mean sampled reward: -6244.82). Current reward after update: -825.41, Optimal reward -825.41
Iteration 12 took 3.87 seconds (mean sampled reward: -6858.74). Current reward after update: -720.42, Optimal reward -720.42
Iteration 13 took 3.77 seconds (mean sampled reward: -6723.51). Current reward after update: -593.62, Optimal reward -593.62
Iteration 14 took 3.83 seconds (mean sampled reward: -6789.79). Current reward after update: -921.37, Optimal reward -593.62
Iteration 15 took 4.06 seconds (mean sampled reward: -6884.38). Current reward after update: -958.38, Optimal reward -593.62
Iteration 16 took 3.85 seconds (mean sampled reward: -6615.28). Current reward after update: -910.67, Optimal reward -593.62
Iteration 17 took 3.93 seconds (mean sampled reward: -6380.10). Current reward after update: -799.21, Optimal reward -593.62
Iteration 18 took 3.92 seconds (mean sampled reward: -6469.33). Current reward after update: -914.58, Optimal reward -593.62
Iteration 19 took 4.24 seconds (mean sampled reward: -6401.94). Current reward after update: -1259.72, Optimal reward -593.62
Iteration 20 took 4.31 seconds (mean sampled reward: -6949.37). Current reward after update: -1034.42, Optimal reward -593.62
Iteration 21 took 4.01 seconds (mean sampled reward: -7144.39). Current reward after update: -1260.77, Optimal reward -593.62
Iteration 22 took 4.08 seconds (mean sampled reward: -6919.06). Current reward after update: -1195.21, Optimal reward -593.62
Iteration 23 took 3.94 seconds (mean sampled reward: -6178.54). Current reward after update: -1029.41, Optimal reward -593.62
Iteration 24 took 3.93 seconds (mean sampled reward: -6161.67). Current reward after update: -1097.46, Optimal reward -593.62
Iteration 25 took 3.96 seconds (mean sampled reward: -6450.90). Current reward after update: -875.65, Optimal reward -593.62
Iteration 26 took 4.10 seconds (mean sampled reward: -6500.16). Current reward after update: -802.21, Optimal reward -593.62
Iteration 27 took 4.05 seconds (mean sampled reward: -5514.50). Current reward after update: -714.34, Optimal reward -593.62
Iteration 28 took 3.89 seconds (mean sampled reward: -5791.96). Current reward after update: -765.53, Optimal reward -593.62
Iteration 29 took 4.36 seconds (mean sampled reward: -6648.56). Current reward after update: -974.40, Optimal reward -593.62
Iteration 30 took 3.96 seconds (mean sampled reward: -6828.61). Current reward after update: -591.08, Optimal reward -591.08
Iteration 31 took 4.09 seconds (mean sampled reward: -6409.72). Current reward after update: -689.36, Optimal reward -591.08
Iteration 32 took 3.91 seconds (mean sampled reward: -6209.61). Current reward after update: -836.11, Optimal reward -591.08
Iteration 33 took 3.88 seconds (mean sampled reward: -6090.61). Current reward after update: -842.72, Optimal reward -591.08
Iteration 34 took 3.80 seconds (mean sampled reward: -5830.59). Current reward after update: -778.88, Optimal reward -591.08
Iteration 35 took 3.89 seconds (mean sampled reward: -5978.17). Current reward after update: -524.85, Optimal reward -524.85
Iteration 36 took 3.94 seconds (mean sampled reward: -6083.59). Current reward after update: -733.10, Optimal reward -524.85
Iteration 37 took 3.69 seconds (mean sampled reward: -5642.95). Current reward after update: -806.93, Optimal reward -524.85
Iteration 38 took 3.94 seconds (mean sampled reward: -6217.02). Current reward after update: -654.21, Optimal reward -524.85
Iteration 39 took 3.74 seconds (mean sampled reward: -6198.05). Current reward after update: -860.12, Optimal reward -524.85
Iteration 40 took 3.75 seconds (mean sampled reward: -6347.22). Current reward after update: -635.99, Optimal reward -524.85
Iteration 41 took 4.08 seconds (mean sampled reward: -5625.00). Current reward after update: -647.50, Optimal reward -524.85
Iteration 42 took 3.97 seconds (mean sampled reward: -5660.74). Current reward after update: -1048.25, Optimal reward -524.85
Iteration 43 took 3.97 seconds (mean sampled reward: -6312.42). Current reward after update: -1191.20, Optimal reward -524.85
Iteration 44 took 4.00 seconds (mean sampled reward: -6291.42). Current reward after update: -819.88, Optimal reward -524.85
Iteration 45 took 3.93 seconds (mean sampled reward: -5699.50). Current reward after update: -882.80, Optimal reward -524.85
Iteration 46 took 3.94 seconds (mean sampled reward: -5861.51). Current reward after update: -705.04, Optimal reward -524.85
Iteration 47 took 4.47 seconds (mean sampled reward: -6297.49). Current reward after update: -735.29, Optimal reward -524.85
Iteration 48 took 4.15 seconds (mean sampled reward: -6236.59). Current reward after update: -920.23, Optimal reward -524.85
Iteration 49 took 4.04 seconds (mean sampled reward: -6249.39). Current reward after update: -874.61, Optimal reward -524.85
Iteration 50 took 3.81 seconds (mean sampled reward: -6616.89). Current reward after update: -835.74, Optimal reward -524.85
Iteration 51 took 3.80 seconds (mean sampled reward: -6734.77). Current reward after update: -1007.52, Optimal reward -524.85
Iteration 52 took 3.75 seconds (mean sampled reward: -6523.55). Current reward after update: -1170.83, Optimal reward -524.85
Iteration 53 took 3.68 seconds (mean sampled reward: -6553.24). Current reward after update: -1074.64, Optimal reward -524.85
Iteration 54 took 3.75 seconds (mean sampled reward: -5749.53). Current reward after update: -775.03, Optimal reward -524.85
Iteration 55 took 3.66 seconds (mean sampled reward: -5659.99). Current reward after update: -793.54, Optimal reward -524.85
Iteration 56 took 3.75 seconds (mean sampled reward: -5963.92). Current reward after update: -844.70, Optimal reward -524.85
Iteration 57 took 3.66 seconds (mean sampled reward: -5799.13). Current reward after update: -843.92, Optimal reward -524.85
Iteration 58 took 3.69 seconds (mean sampled reward: -5766.52). Current reward after update: -808.40, Optimal reward -524.85
Iteration 59 took 3.65 seconds (mean sampled reward: -5917.77). Current reward after update: -787.92, Optimal reward -524.85
Iteration 60 took 3.76 seconds (mean sampled reward: -6673.90). Current reward after update: -984.21, Optimal reward -524.85
Iteration 61 took 3.83 seconds (mean sampled reward: -6191.54). Current reward after update: -807.22, Optimal reward -524.85
Iteration 62 took 3.70 seconds (mean sampled reward: -6092.45). Current reward after update: -785.72, Optimal reward -524.85
Iteration 63 took 3.56 seconds (mean sampled reward: -5895.33). Current reward after update: -728.62, Optimal reward -524.85
Iteration 64 took 3.62 seconds (mean sampled reward: -6048.29). Current reward after update: -700.50, Optimal reward -524.85
Iteration 65 took 3.74 seconds (mean sampled reward: -6123.27). Current reward after update: -572.36, Optimal reward -524.85
Iteration 66 took 3.91 seconds (mean sampled reward: -6143.96). Current reward after update: -502.09, Optimal reward -502.09
Iteration 67 took 3.78 seconds (mean sampled reward: -6053.62). Current reward after update: -615.25, Optimal reward -502.09
Iteration 68 took 3.75 seconds (mean sampled reward: -5823.96). Current reward after update: -721.19, Optimal reward -502.09
Iteration 69 took 3.75 seconds (mean sampled reward: -6195.68). Current reward after update: -591.55, Optimal reward -502.09
Iteration 70 took 3.65 seconds (mean sampled reward: -6200.63). Current reward after update: -557.12, Optimal reward -502.09
Iteration 71 took 3.69 seconds (mean sampled reward: -5816.11). Current reward after update: -638.82, Optimal reward -502.09
Iteration 72 took 3.67 seconds (mean sampled reward: -5544.55). Current reward after update: -625.14, Optimal reward -502.09
Iteration 73 took 3.70 seconds (mean sampled reward: -5598.84). Current reward after update: -609.76, Optimal reward -502.09
Iteration 74 took 3.62 seconds (mean sampled reward: -5536.94). Current reward after update: -605.81, Optimal reward -502.09
Iteration 75 took 3.63 seconds (mean sampled reward: -5453.37). Current reward after update: -759.85, Optimal reward -502.09
Iteration 76 took 3.80 seconds (mean sampled reward: -5683.99). Current reward after update: -843.72, Optimal reward -502.09
Iteration 77 took 4.17 seconds (mean sampled reward: -6302.70). Current reward after update: -551.75, Optimal reward -502.09
Iteration 78 took 4.20 seconds (mean sampled reward: -6465.04). Current reward after update: -603.95, Optimal reward -502.09
Iteration 79 took 3.80 seconds (mean sampled reward: -6234.44). Current reward after update: -682.69, Optimal reward -502.09
Iteration 80 took 3.70 seconds (mean sampled reward: -6055.48). Current reward after update: -818.43, Optimal reward -502.09
Iteration 81 took 3.71 seconds (mean sampled reward: -6409.48). Current reward after update: -684.12, Optimal reward -502.09
Iteration 82 took 3.67 seconds (mean sampled reward: -6308.63). Current reward after update: -727.87, Optimal reward -502.09
Iteration 83 took 3.85 seconds (mean sampled reward: -6289.07). Current reward after update: -743.18, Optimal reward -502.09
Iteration 84 took 3.76 seconds (mean sampled reward: -6254.81). Current reward after update: -775.27, Optimal reward -502.09
Iteration 85 took 3.76 seconds (mean sampled reward: -6204.28). Current reward after update: -713.26, Optimal reward -502.09
Iteration 86 took 3.77 seconds (mean sampled reward: -5990.15). Current reward after update: -692.58, Optimal reward -502.09
Iteration 87 took 3.77 seconds (mean sampled reward: -6346.89). Current reward after update: -664.93, Optimal reward -502.09
Iteration 88 took 3.78 seconds (mean sampled reward: -6275.69). Current reward after update: -751.06, Optimal reward -502.09
Iteration 89 took 3.74 seconds (mean sampled reward: -6259.56). Current reward after update: -832.33, Optimal reward -502.09
Iteration 90 took 3.75 seconds (mean sampled reward: -6201.01). Current reward after update: -682.88, Optimal reward -502.09
Iteration 91 took 3.74 seconds (mean sampled reward: -6150.05). Current reward after update: -681.86, Optimal reward -502.09
Iteration 92 took 3.68 seconds (mean sampled reward: -5113.92). Current reward after update: -753.18, Optimal reward -502.09
Iteration 93 took 3.69 seconds (mean sampled reward: -5776.32). Current reward after update: -678.91, Optimal reward -502.09
Iteration 94 took 3.69 seconds (mean sampled reward: -5781.92). Current reward after update: -681.14, Optimal reward -502.09
Iteration 95 took 3.69 seconds (mean sampled reward: -5681.11). Current reward after update: -602.40, Optimal reward -502.09
Iteration 96 took 3.90 seconds (mean sampled reward: -6075.66). Current reward after update: -671.69, Optimal reward -502.09
Iteration 97 took 4.50 seconds (mean sampled reward: -5831.43). Current reward after update: -596.29, Optimal reward -502.09
Iteration 98 took 4.12 seconds (mean sampled reward: -6502.57). Current reward after update: -690.53, Optimal reward -502.09
Iteration 99 took 3.75 seconds (mean sampled reward: -6277.36). Current reward after update: -718.29, Optimal reward -502.09
Iteration 100 took 3.67 seconds (mean sampled reward: -6039.21). Current reward after update: -1041.39, Optimal reward -502.09
Iteration 101 took 3.80 seconds (mean sampled reward: -5366.91). Current reward after update: -779.12, Optimal reward -502.09
Iteration 102 took 3.60 seconds (mean sampled reward: -5450.22). Current reward after update: -877.75, Optimal reward -502.09
Iteration 103 took 3.69 seconds (mean sampled reward: -5753.25). Current reward after update: -871.21, Optimal reward -502.09
Iteration 104 took 3.91 seconds (mean sampled reward: -5657.07). Current reward after update: -714.97, Optimal reward -502.09
Iteration 105 took 3.97 seconds (mean sampled reward: -5654.89). Current reward after update: -574.87, Optimal reward -502.09
Iteration 106 took 4.05 seconds (mean sampled reward: -5597.22). Current reward after update: -581.60, Optimal reward -502.09
Iteration 107 took 4.07 seconds (mean sampled reward: -5796.02). Current reward after update: -473.84, Optimal reward -473.84
Iteration 108 took 3.66 seconds (mean sampled reward: -5875.59). Current reward after update: -371.16, Optimal reward -371.16
Iteration 109 took 4.05 seconds (mean sampled reward: -5219.69). Current reward after update: -521.53, Optimal reward -371.16
Iteration 110 took 3.80 seconds (mean sampled reward: -5484.67). Current reward after update: -490.48, Optimal reward -371.16
Iteration 111 took 4.13 seconds (mean sampled reward: -5509.58). Current reward after update: -615.06, Optimal reward -371.16
Iteration 112 took 4.12 seconds (mean sampled reward: -5424.05). Current reward after update: -450.29, Optimal reward -371.16
Iteration 113 took 3.85 seconds (mean sampled reward: -5706.54). Current reward after update: -734.79, Optimal reward -371.16
Iteration 114 took 3.80 seconds (mean sampled reward: -5646.13). Current reward after update: -594.41, Optimal reward -371.16
Iteration 115 took 3.95 seconds (mean sampled reward: -5453.85). Current reward after update: -435.02, Optimal reward -371.16
Iteration 116 took 3.75 seconds (mean sampled reward: -5293.51). Current reward after update: -319.02, Optimal reward -319.02
Iteration 117 took 3.83 seconds (mean sampled reward: -5883.20). Current reward after update: -340.05, Optimal reward -319.02
Iteration 118 took 3.75 seconds (mean sampled reward: -5350.18). Current reward after update: -371.45, Optimal reward -319.02
Iteration 119 took 3.88 seconds (mean sampled reward: -5449.29). Current reward after update: -328.17, Optimal reward -319.02
Iteration 120 took 3.83 seconds (mean sampled reward: -5802.15). Current reward after update: -337.51, Optimal reward -319.02
Iteration 121 took 3.84 seconds (mean sampled reward: -5493.76). Current reward after update: -336.74, Optimal reward -319.02
Iteration 122 took 3.79 seconds (mean sampled reward: -5873.39). Current reward after update: -272.18, Optimal reward -272.18
Iteration 123 took 3.68 seconds (mean sampled reward: -6061.97). Current reward after update: -567.29, Optimal reward -272.18
Iteration 124 took 3.77 seconds (mean sampled reward: -5575.32). Current reward after update: -366.74, Optimal reward -272.18
Iteration 125 took 3.80 seconds (mean sampled reward: -5600.78). Current reward after update: -314.78, Optimal reward -272.18
Iteration 126 took 3.77 seconds (mean sampled reward: -6126.65). Current reward after update: -466.06, Optimal reward -272.18
Iteration 127 took 3.73 seconds (mean sampled reward: -5837.38). Current reward after update: -596.09, Optimal reward -272.18
Iteration 128 took 3.78 seconds (mean sampled reward: -6305.06). Current reward after update: -132.83, Optimal reward -132.83
Iteration 129 took 3.66 seconds (mean sampled reward: -5590.54). Current reward after update: -374.05, Optimal reward -132.83
Iteration 130 took 3.78 seconds (mean sampled reward: -5440.71). Current reward after update: -1858.36, Optimal reward -132.83
Iteration 131 took 3.85 seconds (mean sampled reward: -5498.07). Current reward after update: -323.12, Optimal reward -132.83
Iteration 132 took 3.84 seconds (mean sampled reward: -5756.87). Current reward after update: -349.87, Optimal reward -132.83
Iteration 133 took 3.85 seconds (mean sampled reward: -5198.88). Current reward after update: -1317.73, Optimal reward -132.83
Iteration 134 took 3.85 seconds (mean sampled reward: -5225.84). Current reward after update: -256.47, Optimal reward -132.83
Iteration 135 took 3.85 seconds (mean sampled reward: -5283.56). Current reward after update: -323.08, Optimal reward -132.83
Iteration 136 took 3.87 seconds (mean sampled reward: -5471.11). Current reward after update: -325.21, Optimal reward -132.83
Iteration 137 took 3.85 seconds (mean sampled reward: -5525.45). Current reward after update: -320.97, Optimal reward -132.83
Iteration 138 took 3.75 seconds (mean sampled reward: -5627.13). Current reward after update: -274.83, Optimal reward -132.83
Iteration 139 took 4.03 seconds (mean sampled reward: -5473.31). Current reward after update: -372.33, Optimal reward -132.83
Iteration 140 took 4.09 seconds (mean sampled reward: -5583.79). Current reward after update: -370.21, Optimal reward -132.83
Iteration 141 took 3.97 seconds (mean sampled reward: -5318.73). Current reward after update: -360.89, Optimal reward -132.83
Iteration 142 took 4.20 seconds (mean sampled reward: -5524.25). Current reward after update: -299.62, Optimal reward -132.83
Iteration 143 took 3.79 seconds (mean sampled reward: -5563.67). Current reward after update: -431.51, Optimal reward -132.83
Iteration 144 took 3.86 seconds (mean sampled reward: -5743.99). Current reward after update: -194.61, Optimal reward -132.83
Iteration 145 took 3.82 seconds (mean sampled reward: -5647.60). Current reward after update: -336.18, Optimal reward -132.83
Iteration 146 took 3.75 seconds (mean sampled reward: -5931.39). Current reward after update: -296.10, Optimal reward -132.83
Iteration 147 took 3.78 seconds (mean sampled reward: -5579.69). Current reward after update: -215.88, Optimal reward -132.83
Iteration 148 took 3.72 seconds (mean sampled reward: -5833.04). Current reward after update: -391.88, Optimal reward -132.83
Iteration 149 took 4.02 seconds (mean sampled reward: -6017.02). Current reward after update: -480.82, Optimal reward -132.83
Iteration 150 took 4.20 seconds (mean sampled reward: -5824.36). Current reward after update: -435.79, Optimal reward -132.83
Iteration 151 took 3.75 seconds (mean sampled reward: -5815.70). Current reward after update: -590.78, Optimal reward -132.83
Iteration 152 took 3.78 seconds (mean sampled reward: -5768.54). Current reward after update: -396.10, Optimal reward -132.83
Iteration 153 took 3.81 seconds (mean sampled reward: -5945.57). Current reward after update: -510.30, Optimal reward -132.83
Iteration 154 took 3.79 seconds (mean sampled reward: -5355.59). Current reward after update: -443.84, Optimal reward -132.83
Iteration 155 took 3.81 seconds (mean sampled reward: -5839.46). Current reward after update: -438.45, Optimal reward -132.83
Iteration 156 took 3.85 seconds (mean sampled reward: -6072.60). Current reward after update: -279.24, Optimal reward -132.83
Iteration 157 took 3.80 seconds (mean sampled reward: -5742.06). Current reward after update: -281.40, Optimal reward -132.83
Iteration 158 took 3.76 seconds (mean sampled reward: -6475.14). Current reward after update: -407.63, Optimal reward -132.83
Iteration 159 took 3.72 seconds (mean sampled reward: -6190.62). Current reward after update: -549.04, Optimal reward -132.83
Iteration 160 took 3.74 seconds (mean sampled reward: -6698.54). Current reward after update: -513.87, Optimal reward -132.83
Iteration 161 took 3.72 seconds (mean sampled reward: -5856.79). Current reward after update: -580.61, Optimal reward -132.83
Iteration 162 took 3.73 seconds (mean sampled reward: -6347.73). Current reward after update: -574.75, Optimal reward -132.83
Iteration 163 took 3.77 seconds (mean sampled reward: -6052.49). Current reward after update: -1348.02, Optimal reward -132.83
Iteration 164 took 3.76 seconds (mean sampled reward: -5911.00). Current reward after update: -734.83, Optimal reward -132.83
Iteration 165 took 3.86 seconds (mean sampled reward: -5534.21). Current reward after update: -271.52, Optimal reward -132.83
Iteration 166 took 3.81 seconds (mean sampled reward: -5304.22). Current reward after update: -310.94, Optimal reward -132.83
Iteration 167 took 3.82 seconds (mean sampled reward: -5479.72). Current reward after update: -299.35, Optimal reward -132.83
Iteration 168 took 3.81 seconds (mean sampled reward: -5594.71). Current reward after update: -420.88, Optimal reward -132.83
Iteration 169 took 3.75 seconds (mean sampled reward: -6143.29). Current reward after update: -499.10, Optimal reward -132.83
Iteration 170 took 3.82 seconds (mean sampled reward: -5565.19). Current reward after update: -338.15, Optimal reward -132.83
Iteration 171 took 3.80 seconds (mean sampled reward: -5701.25). Current reward after update: -393.67, Optimal reward -132.83
Iteration 172 took 3.82 seconds (mean sampled reward: -5708.15). Current reward after update: -364.12, Optimal reward -132.83
Iteration 173 took 3.82 seconds (mean sampled reward: -6200.26). Current reward after update: -384.02, Optimal reward -132.83
Iteration 174 took 3.80 seconds (mean sampled reward: -5707.38). Current reward after update: -301.51, Optimal reward -132.83
Iteration 175 took 3.81 seconds (mean sampled reward: -5641.40). Current reward after update: -339.27, Optimal reward -132.83
Iteration 176 took 3.85 seconds (mean sampled reward: -5427.78). Current reward after update: -343.14, Optimal reward -132.83
Iteration 177 took 3.80 seconds (mean sampled reward: -5906.98). Current reward after update: -1587.48, Optimal reward -132.83
Iteration 178 took 3.82 seconds (mean sampled reward: -6194.32). Current reward after update: -373.35, Optimal reward -132.83
Iteration 179 took 3.79 seconds (mean sampled reward: -6275.69). Current reward after update: -426.65, Optimal reward -132.83
Iteration 180 took 3.77 seconds (mean sampled reward: -6146.96). Current reward after update: -429.10, Optimal reward -132.83
Iteration 181 took 3.80 seconds (mean sampled reward: -6671.38). Current reward after update: -438.21, Optimal reward -132.83
Iteration 182 took 3.77 seconds (mean sampled reward: -5678.01). Current reward after update: -220.14, Optimal reward -132.83
Iteration 183 took 3.75 seconds (mean sampled reward: -6096.72). Current reward after update: -242.47, Optimal reward -132.83
Iteration 184 took 3.92 seconds (mean sampled reward: -6180.50). Current reward after update: -220.88, Optimal reward -132.83
Iteration 185 took 3.80 seconds (mean sampled reward: -6547.82). Current reward after update: -696.17, Optimal reward -132.83
Iteration 186 took 3.77 seconds (mean sampled reward: -6498.04). Current reward after update: -846.57, Optimal reward -132.83
Iteration 187 took 3.89 seconds (mean sampled reward: -6451.52). Current reward after update: -639.82, Optimal reward -132.83
Iteration 188 took 4.07 seconds (mean sampled reward: -6530.84). Current reward after update: -532.44, Optimal reward -132.83
Iteration 189 took 4.39 seconds (mean sampled reward: -6938.11). Current reward after update: -568.69, Optimal reward -132.83
Iteration 190 took 4.05 seconds (mean sampled reward: -6816.51). Current reward after update: -781.66, Optimal reward -132.83
Iteration 191 took 3.95 seconds (mean sampled reward: -6460.34). Current reward after update: -541.86, Optimal reward -132.83
Iteration 192 took 3.89 seconds (mean sampled reward: -6035.47). Current reward after update: -869.42, Optimal reward -132.83
Iteration 193 took 3.77 seconds (mean sampled reward: -6533.17). Current reward after update: -510.86, Optimal reward -132.83
Iteration 194 took 3.81 seconds (mean sampled reward: -6334.54). Current reward after update: -468.89, Optimal reward -132.83
Iteration 195 took 3.89 seconds (mean sampled reward: -6558.36). Current reward after update: -420.91, Optimal reward -132.83
Iteration 196 took 3.77 seconds (mean sampled reward: -6832.62). Current reward after update: -1116.99, Optimal reward -132.83
Iteration 197 took 3.77 seconds (mean sampled reward: -6382.46). Current reward after update: -325.80, Optimal reward -132.83
Iteration 198 took 3.74 seconds (mean sampled reward: -6506.34). Current reward after update: -542.18, Optimal reward -132.83
Iteration 199 took 3.77 seconds (mean sampled reward: -6658.21). Current reward after update: -829.68, Optimal reward -132.83
Iteration 200 took 3.69 seconds (mean sampled reward: -6522.57). Current reward after update: -1157.75, Optimal reward -132.83
Iteration 201 took 3.71 seconds (mean sampled reward: -6433.80). Current reward after update: -854.01, Optimal reward -132.83
Iteration 202 took 3.77 seconds (mean sampled reward: -6934.76). Current reward after update: -1096.07, Optimal reward -132.83
Iteration 203 took 4.02 seconds (mean sampled reward: -6996.20). Current reward after update: -613.78, Optimal reward -132.83
Iteration 204 took 4.00 seconds (mean sampled reward: -7032.30). Current reward after update: -931.58, Optimal reward -132.83
Iteration 205 took 3.84 seconds (mean sampled reward: -6718.60). Current reward after update: -560.87, Optimal reward -132.83
Iteration 206 took 3.78 seconds (mean sampled reward: -6882.03). Current reward after update: -721.80, Optimal reward -132.83
Iteration 207 took 3.63 seconds (mean sampled reward: -6558.31). Current reward after update: -584.50, Optimal reward -132.83
Iteration 208 took 3.70 seconds (mean sampled reward: -6705.63). Current reward after update: -751.79, Optimal reward -132.83
Iteration 209 took 3.74 seconds (mean sampled reward: -6008.27). Current reward after update: -443.47, Optimal reward -132.83
Iteration 210 took 3.81 seconds (mean sampled reward: -6120.24). Current reward after update: -434.41, Optimal reward -132.83
Iteration 211 took 3.78 seconds (mean sampled reward: -6591.42). Current reward after update: -475.37, Optimal reward -132.83
Iteration 212 took 4.09 seconds (mean sampled reward: -5782.93). Current reward after update: -515.93, Optimal reward -132.83
Iteration 213 took 3.86 seconds (mean sampled reward: -6323.89). Current reward after update: -331.16, Optimal reward -132.83
Iteration 214 took 4.15 seconds (mean sampled reward: -6658.66). Current reward after update: -249.65, Optimal reward -132.83
Iteration 215 took 3.91 seconds (mean sampled reward: -6327.33). Current reward after update: -287.32, Optimal reward -132.83
Iteration 216 took 3.69 seconds (mean sampled reward: -6446.60). Current reward after update: -490.43, Optimal reward -132.83
Iteration 217 took 3.94 seconds (mean sampled reward: -6459.85). Current reward after update: -414.55, Optimal reward -132.83
Iteration 218 took 3.82 seconds (mean sampled reward: -6229.17). Current reward after update: -592.87, Optimal reward -132.83
Iteration 219 took 3.69 seconds (mean sampled reward: -5917.40). Current reward after update: -691.55, Optimal reward -132.83
Iteration 220 took 3.77 seconds (mean sampled reward: -6172.24). Current reward after update: -761.90, Optimal reward -132.83
Iteration 221 took 3.71 seconds (mean sampled reward: -6609.54). Current reward after update: -796.00, Optimal reward -132.83
Iteration 222 took 3.68 seconds (mean sampled reward: -6548.13). Current reward after update: -752.43, Optimal reward -132.83
Iteration 223 took 3.70 seconds (mean sampled reward: -6400.59). Current reward after update: -767.15, Optimal reward -132.83
Iteration 224 took 3.67 seconds (mean sampled reward: -6354.94). Current reward after update: -551.47, Optimal reward -132.83
Iteration 225 took 3.62 seconds (mean sampled reward: -6280.87). Current reward after update: -453.83, Optimal reward -132.83
Iteration 226 took 3.74 seconds (mean sampled reward: -6649.15). Current reward after update: -617.23, Optimal reward -132.83
Iteration 227 took 3.67 seconds (mean sampled reward: -6922.33). Current reward after update: -673.43, Optimal reward -132.83
Iteration 228 took 3.62 seconds (mean sampled reward: -7078.75). Current reward after update: -663.32, Optimal reward -132.83
Iteration 229 took 3.79 seconds (mean sampled reward: -6650.22). Current reward after update: -485.55, Optimal reward -132.83
Iteration 230 took 3.78 seconds (mean sampled reward: -6259.52). Current reward after update: -440.61, Optimal reward -132.83
Iteration 231 took 3.67 seconds (mean sampled reward: -5942.46). Current reward after update: -471.09, Optimal reward -132.83
Iteration 232 took 3.69 seconds (mean sampled reward: -6363.06). Current reward after update: -446.31, Optimal reward -132.83
Iteration 233 took 3.64 seconds (mean sampled reward: -6755.88). Current reward after update: -433.81, Optimal reward -132.83
Iteration 234 took 3.74 seconds (mean sampled reward: -6246.43). Current reward after update: -462.36, Optimal reward -132.83
Iteration 235 took 3.77 seconds (mean sampled reward: -6267.14). Current reward after update: -514.39, Optimal reward -132.83
Iteration 236 took 3.71 seconds (mean sampled reward: -6110.08). Current reward after update: -380.62, Optimal reward -132.83
Iteration 237 took 3.78 seconds (mean sampled reward: -6109.38). Current reward after update: -221.28, Optimal reward -132.83
Iteration 238 took 3.73 seconds (mean sampled reward: -5742.36). Current reward after update: -297.01, Optimal reward -132.83
Iteration 239 took 3.73 seconds (mean sampled reward: -5911.38). Current reward after update: -532.95, Optimal reward -132.83
Iteration 240 took 3.87 seconds (mean sampled reward: -5882.67). Current reward after update: -422.51, Optimal reward -132.83
Iteration 241 took 3.82 seconds (mean sampled reward: -6181.31). Current reward after update: -269.58, Optimal reward -132.83
Iteration 242 took 3.79 seconds (mean sampled reward: -5352.45). Current reward after update: -199.49, Optimal reward -132.83
Iteration 243 took 3.78 seconds (mean sampled reward: -5444.63). Current reward after update: -294.50, Optimal reward -132.83
Iteration 244 took 3.82 seconds (mean sampled reward: -6097.61). Current reward after update: -313.95, Optimal reward -132.83
Iteration 245 took 3.71 seconds (mean sampled reward: -6093.32). Current reward after update: -371.34, Optimal reward -132.83
Iteration 246 took 3.73 seconds (mean sampled reward: -6553.04). Current reward after update: -460.16, Optimal reward -132.83
Iteration 247 took 3.77 seconds (mean sampled reward: -6400.98). Current reward after update: -185.15, Optimal reward -132.83
Iteration 248 took 3.77 seconds (mean sampled reward: -6410.12). Current reward after update: -188.96, Optimal reward -132.83
Iteration 249 took 3.74 seconds (mean sampled reward: -6613.71). Current reward after update: -218.70, Optimal reward -132.83
Iteration 250 took 3.71 seconds (mean sampled reward: -5930.92). Current reward after update: -440.49, Optimal reward -132.83
Iteration 251 took 3.79 seconds (mean sampled reward: -5409.13). Current reward after update: -287.22, Optimal reward -132.83
Iteration 252 took 3.82 seconds (mean sampled reward: -6029.35). Current reward after update: -340.24, Optimal reward -132.83
Iteration 253 took 3.80 seconds (mean sampled reward: -5721.80). Current reward after update: -314.49, Optimal reward -132.83
Iteration 254 took 3.73 seconds (mean sampled reward: -5899.55). Current reward after update: -308.69, Optimal reward -132.83
Iteration 255 took 3.74 seconds (mean sampled reward: -5286.55). Current reward after update: -320.04, Optimal reward -132.83
Iteration 256 took 3.82 seconds (mean sampled reward: -5764.62). Current reward after update: -410.06, Optimal reward -132.83
Iteration 257 took 3.81 seconds (mean sampled reward: -5650.85). Current reward after update: -1203.48, Optimal reward -132.83
Iteration 258 took 3.78 seconds (mean sampled reward: -5901.09). Current reward after update: -369.96, Optimal reward -132.83
Iteration 259 took 3.79 seconds (mean sampled reward: -5769.09). Current reward after update: -479.84, Optimal reward -132.83
Iteration 260 took 3.79 seconds (mean sampled reward: -5765.50). Current reward after update: -448.60, Optimal reward -132.83
Iteration 261 took 3.79 seconds (mean sampled reward: -6207.31). Current reward after update: -508.99, Optimal reward -132.83
Iteration 262 took 3.77 seconds (mean sampled reward: -6039.93). Current reward after update: -365.57, Optimal reward -132.83
Iteration 263 took 3.78 seconds (mean sampled reward: -5828.45). Current reward after update: -374.95, Optimal reward -132.83
Iteration 264 took 3.88 seconds (mean sampled reward: -5465.66). Current reward after update: -465.62, Optimal reward -132.83
Iteration 265 took 3.79 seconds (mean sampled reward: -5356.00). Current reward after update: -464.05, Optimal reward -132.83
Iteration 266 took 3.79 seconds (mean sampled reward: -5559.73). Current reward after update: -438.84, Optimal reward -132.83
Iteration 267 took 3.83 seconds (mean sampled reward: -5680.40). Current reward after update: -520.17, Optimal reward -132.83
Iteration 268 took 3.85 seconds (mean sampled reward: -6058.38). Current reward after update: -303.03, Optimal reward -132.83
Iteration 269 took 3.80 seconds (mean sampled reward: -5560.14). Current reward after update: -455.64, Optimal reward -132.83
Iteration 270 took 3.80 seconds (mean sampled reward: -5725.71). Current reward after update: -533.79, Optimal reward -132.83
Iteration 271 took 3.87 seconds (mean sampled reward: -5442.61). Current reward after update: -497.96, Optimal reward -132.83
Iteration 272 took 3.81 seconds (mean sampled reward: -5975.12). Current reward after update: -373.34, Optimal reward -132.83
Iteration 273 took 3.83 seconds (mean sampled reward: -5553.45). Current reward after update: -333.69, Optimal reward -132.83
Iteration 274 took 3.83 seconds (mean sampled reward: -5594.44). Current reward after update: -356.54, Optimal reward -132.83
Iteration 275 took 3.82 seconds (mean sampled reward: -5915.28). Current reward after update: -483.56, Optimal reward -132.83
Iteration 276 took 3.82 seconds (mean sampled reward: -6016.21). Current reward after update: -371.12, Optimal reward -132.83
Iteration 277 took 3.83 seconds (mean sampled reward: -5685.28). Current reward after update: -418.54, Optimal reward -132.83
Iteration 278 took 3.85 seconds (mean sampled reward: -5709.99). Current reward after update: -416.52, Optimal reward -132.83
Iteration 279 took 3.84 seconds (mean sampled reward: -6006.31). Current reward after update: -393.46, Optimal reward -132.83
Iteration 280 took 3.83 seconds (mean sampled reward: -5837.76). Current reward after update: -383.71, Optimal reward -132.83
Iteration 281 took 3.81 seconds (mean sampled reward: -6370.42). Current reward after update: -416.58, Optimal reward -132.83
Iteration 282 took 3.80 seconds (mean sampled reward: -5326.37). Current reward after update: -556.04, Optimal reward -132.83
Iteration 283 took 3.84 seconds (mean sampled reward: -5437.71). Current reward after update: -582.18, Optimal reward -132.83
Iteration 284 took 3.84 seconds (mean sampled reward: -5508.78). Current reward after update: -450.58, Optimal reward -132.83
Iteration 285 took 3.81 seconds (mean sampled reward: -5535.22). Current reward after update: -505.93, Optimal reward -132.83
Iteration 286 took 3.85 seconds (mean sampled reward: -5798.09). Current reward after update: -497.41, Optimal reward -132.83
Iteration 287 took 3.84 seconds (mean sampled reward: -6064.14). Current reward after update: -649.61, Optimal reward -132.83
Iteration 288 took 3.78 seconds (mean sampled reward: -5820.62). Current reward after update: -472.22, Optimal reward -132.83
Iteration 289 took 3.73 seconds (mean sampled reward: -5566.10). Current reward after update: -362.28, Optimal reward -132.83
Iteration 290 took 3.75 seconds (mean sampled reward: -5609.91). Current reward after update: -330.01, Optimal reward -132.83
Iteration 291 took 3.77 seconds (mean sampled reward: -5553.24). Current reward after update: -438.38, Optimal reward -132.83
Iteration 292 took 3.78 seconds (mean sampled reward: -5323.90). Current reward after update: -492.84, Optimal reward -132.83
Iteration 293 took 3.79 seconds (mean sampled reward: -5312.52). Current reward after update: -441.67, Optimal reward -132.83
Iteration 294 took 3.76 seconds (mean sampled reward: -5475.49). Current reward after update: -481.92, Optimal reward -132.83
Iteration 295 took 3.83 seconds (mean sampled reward: -5538.60). Current reward after update: -388.82, Optimal reward -132.83
Iteration 296 took 3.83 seconds (mean sampled reward: -5757.37). Current reward after update: -314.32, Optimal reward -132.83
Iteration 297 took 3.86 seconds (mean sampled reward: -5949.28). Current reward after update: -443.24, Optimal reward -132.83
Iteration 298 took 3.81 seconds (mean sampled reward: -5936.01). Current reward after update: -352.65, Optimal reward -132.83
Iteration 299 took 3.80 seconds (mean sampled reward: -6084.44). Current reward after update: -495.66, Optimal reward -132.83
Iteration 300 took 3.79 seconds (mean sampled reward: -6612.70). Current reward after update: -663.53, Optimal reward -132.83
Iteration 1 took 3.91 seconds (mean sampled reward: -7466.33). Current reward after update: -4299.63, Optimal reward -4299.63
Iteration 2 took 3.80 seconds (mean sampled reward: -7006.79). Current reward after update: -2181.55, Optimal reward -2181.55
Iteration 3 took 3.70 seconds (mean sampled reward: -6201.27). Current reward after update: -2143.46, Optimal reward -2143.46
Iteration 4 took 3.78 seconds (mean sampled reward: -6442.82). Current reward after update: -1775.30, Optimal reward -1775.30
Iteration 5 took 3.98 seconds (mean sampled reward: -6388.62). Current reward after update: -1250.67, Optimal reward -1250.67
Iteration 6 took 3.82 seconds (mean sampled reward: -5474.60). Current reward after update: -1215.28, Optimal reward -1215.28
Iteration 7 took 3.89 seconds (mean sampled reward: -6028.75). Current reward after update: -937.22, Optimal reward -937.22
Iteration 8 took 3.77 seconds (mean sampled reward: -6347.34). Current reward after update: -1489.94, Optimal reward -937.22
Iteration 9 took 3.76 seconds (mean sampled reward: -6213.95). Current reward after update: -1002.52, Optimal reward -937.22
Iteration 10 took 3.74 seconds (mean sampled reward: -6540.75). Current reward after update: -889.58, Optimal reward -889.58
Iteration 11 took 3.67 seconds (mean sampled reward: -6123.64). Current reward after update: -869.73, Optimal reward -869.73
Iteration 12 took 3.52 seconds (mean sampled reward: -4992.67). Current reward after update: -945.43, Optimal reward -869.73
Iteration 13 took 3.55 seconds (mean sampled reward: -5450.02). Current reward after update: -800.71, Optimal reward -800.71
Iteration 14 took 3.61 seconds (mean sampled reward: -5437.51). Current reward after update: -1171.07, Optimal reward -800.71
Iteration 15 took 4.01 seconds (mean sampled reward: -5339.18). Current reward after update: -643.32, Optimal reward -643.32
Iteration 16 took 3.67 seconds (mean sampled reward: -5857.78). Current reward after update: -703.97, Optimal reward -643.32
Iteration 17 took 3.83 seconds (mean sampled reward: -6168.58). Current reward after update: -849.04, Optimal reward -643.32
Iteration 18 took 3.76 seconds (mean sampled reward: -6324.96). Current reward after update: -766.91, Optimal reward -643.32
Iteration 19 took 3.77 seconds (mean sampled reward: -6175.67). Current reward after update: -575.21, Optimal reward -575.21
Iteration 20 took 3.68 seconds (mean sampled reward: -5918.17). Current reward after update: -514.12, Optimal reward -514.12
Iteration 21 took 3.60 seconds (mean sampled reward: -4355.14). Current reward after update: -439.82, Optimal reward -439.82
Iteration 22 took 3.80 seconds (mean sampled reward: -4883.14). Current reward after update: -488.84, Optimal reward -439.82
Iteration 23 took 3.78 seconds (mean sampled reward: -4605.05). Current reward after update: -616.72, Optimal reward -439.82
Iteration 24 took 3.87 seconds (mean sampled reward: -4413.02). Current reward after update: -678.81, Optimal reward -439.82
Iteration 25 took 3.71 seconds (mean sampled reward: -4207.00). Current reward after update: -453.10, Optimal reward -439.82
Iteration 26 took 3.83 seconds (mean sampled reward: -4160.74). Current reward after update: -565.72, Optimal reward -439.82
Iteration 27 took 3.84 seconds (mean sampled reward: -3810.39). Current reward after update: -418.00, Optimal reward -418.00
Iteration 28 took 3.68 seconds (mean sampled reward: -4966.38). Current reward after update: -618.97, Optimal reward -418.00
Iteration 29 took 3.90 seconds (mean sampled reward: -5624.06). Current reward after update: -779.77, Optimal reward -418.00
Iteration 30 took 3.96 seconds (mean sampled reward: -6157.30). Current reward after update: -849.41, Optimal reward -418.00
Iteration 31 took 3.93 seconds (mean sampled reward: -5428.65). Current reward after update: -469.10, Optimal reward -418.00
Iteration 32 took 4.22 seconds (mean sampled reward: -6168.26). Current reward after update: -631.47, Optimal reward -418.00
Iteration 33 took 3.76 seconds (mean sampled reward: -5257.03). Current reward after update: -579.88, Optimal reward -418.00
Iteration 34 took 3.62 seconds (mean sampled reward: -4872.26). Current reward after update: -708.83, Optimal reward -418.00
Iteration 35 took 3.67 seconds (mean sampled reward: -4401.31). Current reward after update: -649.81, Optimal reward -418.00
Iteration 36 took 3.75 seconds (mean sampled reward: -3874.85). Current reward after update: -435.24, Optimal reward -418.00
Iteration 37 took 3.67 seconds (mean sampled reward: -4099.08). Current reward after update: -660.28, Optimal reward -418.00
Iteration 38 took 3.91 seconds (mean sampled reward: -5411.12). Current reward after update: -418.06, Optimal reward -418.00
Iteration 39 took 3.85 seconds (mean sampled reward: -5371.36). Current reward after update: -396.60, Optimal reward -396.60
Iteration 40 took 3.75 seconds (mean sampled reward: -4450.79). Current reward after update: -432.22, Optimal reward -396.60
Iteration 41 took 3.71 seconds (mean sampled reward: -3963.38). Current reward after update: -357.58, Optimal reward -357.58
Iteration 42 took 4.08 seconds (mean sampled reward: -4257.80). Current reward after update: -413.39, Optimal reward -357.58
Iteration 43 took 4.24 seconds (mean sampled reward: -5417.27). Current reward after update: -327.95, Optimal reward -327.95
Iteration 44 took 4.28 seconds (mean sampled reward: -5259.76). Current reward after update: -437.38, Optimal reward -327.95
Iteration 45 took 3.93 seconds (mean sampled reward: -5458.45). Current reward after update: -362.33, Optimal reward -327.95
Iteration 46 took 3.96 seconds (mean sampled reward: -5305.79). Current reward after update: -427.26, Optimal reward -327.95
Iteration 47 took 3.86 seconds (mean sampled reward: -5710.04). Current reward after update: -484.13, Optimal reward -327.95
Iteration 48 took 3.80 seconds (mean sampled reward: -5327.00). Current reward after update: -411.30, Optimal reward -327.95
Iteration 49 took 3.91 seconds (mean sampled reward: -5947.34). Current reward after update: -556.98, Optimal reward -327.95
Iteration 50 took 3.92 seconds (mean sampled reward: -5547.21). Current reward after update: -477.15, Optimal reward -327.95
Iteration 51 took 3.80 seconds (mean sampled reward: -4655.07). Current reward after update: -293.07, Optimal reward -293.07
Iteration 52 took 4.03 seconds (mean sampled reward: -5483.64). Current reward after update: -419.36, Optimal reward -293.07
Iteration 53 took 4.02 seconds (mean sampled reward: -5722.34). Current reward after update: -465.28, Optimal reward -293.07
Iteration 54 took 3.95 seconds (mean sampled reward: -5226.04). Current reward after update: -330.19, Optimal reward -293.07
Iteration 55 took 3.95 seconds (mean sampled reward: -5088.51). Current reward after update: -384.31, Optimal reward -293.07
Iteration 56 took 3.94 seconds (mean sampled reward: -4692.36). Current reward after update: -373.80, Optimal reward -293.07
Iteration 57 took 4.24 seconds (mean sampled reward: -5127.42). Current reward after update: -410.85, Optimal reward -293.07
Iteration 58 took 4.24 seconds (mean sampled reward: -4499.76). Current reward after update: -374.44, Optimal reward -293.07
Iteration 59 took 3.73 seconds (mean sampled reward: -4784.41). Current reward after update: -419.27, Optimal reward -293.07
Iteration 60 took 3.87 seconds (mean sampled reward: -4997.17). Current reward after update: -333.50, Optimal reward -293.07
Iteration 61 took 3.95 seconds (mean sampled reward: -4573.94). Current reward after update: -426.08, Optimal reward -293.07
Iteration 62 took 3.92 seconds (mean sampled reward: -4494.36). Current reward after update: -311.92, Optimal reward -293.07
Iteration 63 took 3.91 seconds (mean sampled reward: -5027.41). Current reward after update: -344.99, Optimal reward -293.07
Iteration 64 took 3.79 seconds (mean sampled reward: -4862.71). Current reward after update: -357.24, Optimal reward -293.07
Iteration 65 took 3.77 seconds (mean sampled reward: -4911.65). Current reward after update: -273.54, Optimal reward -273.54
Iteration 66 took 3.76 seconds (mean sampled reward: -5226.87). Current reward after update: -288.56, Optimal reward -273.54
Iteration 67 took 3.93 seconds (mean sampled reward: -5555.84). Current reward after update: -319.73, Optimal reward -273.54
Iteration 68 took 4.08 seconds (mean sampled reward: -5329.85). Current reward after update: -169.57, Optimal reward -169.57
Iteration 69 took 4.06 seconds (mean sampled reward: -4984.95). Current reward after update: -275.21, Optimal reward -169.57
Iteration 70 took 3.68 seconds (mean sampled reward: -4767.29). Current reward after update: -275.55, Optimal reward -169.57
Iteration 71 took 3.74 seconds (mean sampled reward: -5556.35). Current reward after update: -259.10, Optimal reward -169.57
Iteration 72 took 3.63 seconds (mean sampled reward: -4526.95). Current reward after update: -490.31, Optimal reward -169.57
Iteration 73 took 3.66 seconds (mean sampled reward: -4877.57). Current reward after update: -290.90, Optimal reward -169.57
Iteration 74 took 3.70 seconds (mean sampled reward: -4682.60). Current reward after update: -245.74, Optimal reward -169.57
Iteration 75 took 3.60 seconds (mean sampled reward: -3664.49). Current reward after update: -257.61, Optimal reward -169.57
Iteration 76 took 3.60 seconds (mean sampled reward: -4626.86). Current reward after update: -311.68, Optimal reward -169.57
Iteration 77 took 3.78 seconds (mean sampled reward: -5216.96). Current reward after update: -364.31, Optimal reward -169.57
Iteration 78 took 4.16 seconds (mean sampled reward: -4491.40). Current reward after update: -327.46, Optimal reward -169.57
Iteration 79 took 3.78 seconds (mean sampled reward: -5007.56). Current reward after update: -313.36, Optimal reward -169.57
Iteration 80 took 3.95 seconds (mean sampled reward: -4104.34). Current reward after update: -230.73, Optimal reward -169.57
Iteration 81 took 4.08 seconds (mean sampled reward: -5199.66). Current reward after update: -242.94, Optimal reward -169.57
Iteration 82 took 4.26 seconds (mean sampled reward: -5248.06). Current reward after update: -279.41, Optimal reward -169.57
Iteration 83 took 3.77 seconds (mean sampled reward: -4197.89). Current reward after update: -323.51, Optimal reward -169.57
Iteration 84 took 4.21 seconds (mean sampled reward: -5529.89). Current reward after update: -336.40, Optimal reward -169.57
Iteration 85 took 4.12 seconds (mean sampled reward: -5726.90). Current reward after update: -289.70, Optimal reward -169.57
Iteration 86 took 4.01 seconds (mean sampled reward: -5014.59). Current reward after update: -360.59, Optimal reward -169.57
Iteration 87 took 4.02 seconds (mean sampled reward: -5036.34). Current reward after update: -247.29, Optimal reward -169.57
Iteration 88 took 3.93 seconds (mean sampled reward: -4654.18). Current reward after update: -398.97, Optimal reward -169.57
Iteration 89 took 3.82 seconds (mean sampled reward: -4496.47). Current reward after update: -270.55, Optimal reward -169.57
Iteration 90 took 3.91 seconds (mean sampled reward: -4951.43). Current reward after update: -339.55, Optimal reward -169.57
Iteration 91 took 3.79 seconds (mean sampled reward: -4356.04). Current reward after update: -216.41, Optimal reward -169.57
Iteration 92 took 3.90 seconds (mean sampled reward: -4891.64). Current reward after update: -260.66, Optimal reward -169.57
Iteration 93 took 3.55 seconds (mean sampled reward: -4970.16). Current reward after update: -377.31, Optimal reward -169.57
Iteration 94 took 3.65 seconds (mean sampled reward: -5190.46). Current reward after update: -412.94, Optimal reward -169.57
Iteration 95 took 3.54 seconds (mean sampled reward: -5031.83). Current reward after update: -464.38, Optimal reward -169.57
Iteration 96 took 3.65 seconds (mean sampled reward: -5151.80). Current reward after update: -308.87, Optimal reward -169.57
Iteration 97 took 3.72 seconds (mean sampled reward: -5196.79). Current reward after update: -414.06, Optimal reward -169.57
Iteration 98 took 3.58 seconds (mean sampled reward: -4774.52). Current reward after update: -361.18, Optimal reward -169.57
Iteration 99 took 3.58 seconds (mean sampled reward: -4258.04). Current reward after update: -269.34, Optimal reward -169.57
Iteration 100 took 3.78 seconds (mean sampled reward: -3734.96). Current reward after update: -280.26, Optimal reward -169.57
Iteration 101 took 3.60 seconds (mean sampled reward: -5410.67). Current reward after update: -300.26, Optimal reward -169.57
Iteration 102 took 3.81 seconds (mean sampled reward: -4252.51). Current reward after update: -454.21, Optimal reward -169.57
Iteration 103 took 3.83 seconds (mean sampled reward: -4565.59). Current reward after update: -283.89, Optimal reward -169.57
Iteration 104 took 3.87 seconds (mean sampled reward: -4365.69). Current reward after update: -333.47, Optimal reward -169.57
Iteration 105 took 4.01 seconds (mean sampled reward: -4897.78). Current reward after update: -389.50, Optimal reward -169.57
Iteration 106 took 3.93 seconds (mean sampled reward: -4606.34). Current reward after update: -470.78, Optimal reward -169.57
Iteration 107 took 3.85 seconds (mean sampled reward: -4351.97). Current reward after update: -379.47, Optimal reward -169.57
Iteration 108 took 3.71 seconds (mean sampled reward: -3408.45). Current reward after update: -378.87, Optimal reward -169.57
Iteration 109 took 3.64 seconds (mean sampled reward: -3478.44). Current reward after update: -423.41, Optimal reward -169.57
Iteration 110 took 3.66 seconds (mean sampled reward: -3762.14). Current reward after update: -448.53, Optimal reward -169.57
Iteration 111 took 3.89 seconds (mean sampled reward: -4433.97). Current reward after update: -352.67, Optimal reward -169.57
Iteration 112 took 4.08 seconds (mean sampled reward: -4976.29). Current reward after update: -388.73, Optimal reward -169.57
Iteration 113 took 3.92 seconds (mean sampled reward: -4314.44). Current reward after update: -439.04, Optimal reward -169.57
Iteration 114 took 4.00 seconds (mean sampled reward: -4565.22). Current reward after update: -484.20, Optimal reward -169.57
Iteration 115 took 3.83 seconds (mean sampled reward: -5019.82). Current reward after update: -614.91, Optimal reward -169.57
Iteration 116 took 3.76 seconds (mean sampled reward: -4752.46). Current reward after update: -451.04, Optimal reward -169.57
Iteration 117 took 3.78 seconds (mean sampled reward: -4983.77). Current reward after update: -510.11, Optimal reward -169.57
Iteration 118 took 3.90 seconds (mean sampled reward: -5399.27). Current reward after update: -421.84, Optimal reward -169.57
Iteration 119 took 3.83 seconds (mean sampled reward: -5127.42). Current reward after update: -397.60, Optimal reward -169.57
Iteration 120 took 3.80 seconds (mean sampled reward: -5169.31). Current reward after update: -454.35, Optimal reward -169.57
Iteration 121 took 3.87 seconds (mean sampled reward: -4700.12). Current reward after update: -431.83, Optimal reward -169.57
Iteration 122 took 3.93 seconds (mean sampled reward: -3821.38). Current reward after update: -415.84, Optimal reward -169.57
Iteration 123 took 3.80 seconds (mean sampled reward: -3889.02). Current reward after update: -506.85, Optimal reward -169.57
Iteration 124 took 4.12 seconds (mean sampled reward: -4090.80). Current reward after update: -421.62, Optimal reward -169.57
Iteration 125 took 3.96 seconds (mean sampled reward: -4158.96). Current reward after update: -469.55, Optimal reward -169.57
Iteration 126 took 3.90 seconds (mean sampled reward: -4384.10). Current reward after update: -478.04, Optimal reward -169.57
Iteration 127 took 3.86 seconds (mean sampled reward: -3869.26). Current reward after update: -467.43, Optimal reward -169.57
Iteration 128 took 3.88 seconds (mean sampled reward: -3687.47). Current reward after update: -421.22, Optimal reward -169.57
Iteration 129 took 3.80 seconds (mean sampled reward: -4140.85). Current reward after update: -437.21, Optimal reward -169.57
Iteration 130 took 3.93 seconds (mean sampled reward: -3715.18). Current reward after update: -389.52, Optimal reward -169.57
Iteration 131 took 3.95 seconds (mean sampled reward: -4225.91). Current reward after update: -433.20, Optimal reward -169.57
Iteration 132 took 3.84 seconds (mean sampled reward: -4182.02). Current reward after update: -381.04, Optimal reward -169.57
Iteration 133 took 3.84 seconds (mean sampled reward: -4602.83). Current reward after update: -418.26, Optimal reward -169.57
Iteration 134 took 3.92 seconds (mean sampled reward: -4770.59). Current reward after update: -342.90, Optimal reward -169.57
Iteration 135 took 3.79 seconds (mean sampled reward: -5398.69). Current reward after update: -379.45, Optimal reward -169.57
Iteration 136 took 3.80 seconds (mean sampled reward: -5522.30). Current reward after update: -420.49, Optimal reward -169.57
Iteration 137 took 3.85 seconds (mean sampled reward: -5582.89). Current reward after update: -540.57, Optimal reward -169.57
Iteration 138 took 3.86 seconds (mean sampled reward: -4699.14). Current reward after update: -395.53, Optimal reward -169.57
Iteration 139 took 3.90 seconds (mean sampled reward: -5341.84). Current reward after update: -425.20, Optimal reward -169.57
Iteration 140 took 3.82 seconds (mean sampled reward: -4503.81). Current reward after update: -458.83, Optimal reward -169.57
Iteration 141 took 3.96 seconds (mean sampled reward: -4276.39). Current reward after update: -435.74, Optimal reward -169.57
Iteration 142 took 3.85 seconds (mean sampled reward: -4097.19). Current reward after update: -739.56, Optimal reward -169.57
Iteration 143 took 3.79 seconds (mean sampled reward: -3799.36). Current reward after update: -473.60, Optimal reward -169.57
Iteration 144 took 3.87 seconds (mean sampled reward: -4413.53). Current reward after update: -471.37, Optimal reward -169.57
Iteration 145 took 3.84 seconds (mean sampled reward: -4797.25). Current reward after update: -539.13, Optimal reward -169.57
Iteration 146 took 4.07 seconds (mean sampled reward: -5818.60). Current reward after update: -456.00, Optimal reward -169.57
Iteration 147 took 4.01 seconds (mean sampled reward: -5794.74). Current reward after update: -434.65, Optimal reward -169.57
Iteration 148 took 4.06 seconds (mean sampled reward: -5764.70). Current reward after update: -626.70, Optimal reward -169.57
Iteration 149 took 3.75 seconds (mean sampled reward: -5278.61). Current reward after update: -453.03, Optimal reward -169.57
Iteration 150 took 3.72 seconds (mean sampled reward: -4813.62). Current reward after update: -470.06, Optimal reward -169.57
Iteration 151 took 3.93 seconds (mean sampled reward: -5243.10). Current reward after update: -487.00, Optimal reward -169.57
Iteration 152 took 3.73 seconds (mean sampled reward: -5019.46). Current reward after update: -494.77, Optimal reward -169.57
Iteration 153 took 3.66 seconds (mean sampled reward: -4487.72). Current reward after update: -461.42, Optimal reward -169.57
Iteration 154 took 3.75 seconds (mean sampled reward: -5361.50). Current reward after update: -440.13, Optimal reward -169.57
Iteration 155 took 3.87 seconds (mean sampled reward: -4541.41). Current reward after update: -409.14, Optimal reward -169.57
Iteration 156 took 3.83 seconds (mean sampled reward: -4392.85). Current reward after update: -384.65, Optimal reward -169.57
Iteration 157 took 3.97 seconds (mean sampled reward: -5789.44). Current reward after update: -369.61, Optimal reward -169.57
Iteration 158 took 3.86 seconds (mean sampled reward: -5235.09). Current reward after update: -318.96, Optimal reward -169.57
Iteration 159 took 3.77 seconds (mean sampled reward: -5039.83). Current reward after update: -471.95, Optimal reward -169.57
Iteration 160 took 3.80 seconds (mean sampled reward: -4621.34). Current reward after update: -385.09, Optimal reward -169.57
Iteration 161 took 3.92 seconds (mean sampled reward: -4950.44). Current reward after update: -407.38, Optimal reward -169.57
Iteration 162 took 3.91 seconds (mean sampled reward: -5112.02). Current reward after update: -382.92, Optimal reward -169.57
Iteration 163 took 4.05 seconds (mean sampled reward: -5568.23). Current reward after update: -366.80, Optimal reward -169.57
Iteration 164 took 3.94 seconds (mean sampled reward: -5022.53). Current reward after update: -406.15, Optimal reward -169.57
Iteration 165 took 3.87 seconds (mean sampled reward: -4827.16). Current reward after update: -374.93, Optimal reward -169.57
Iteration 166 took 3.86 seconds (mean sampled reward: -5036.85). Current reward after update: -351.77, Optimal reward -169.57
Iteration 167 took 3.83 seconds (mean sampled reward: -6018.13). Current reward after update: -671.66, Optimal reward -169.57
Iteration 168 took 3.68 seconds (mean sampled reward: -5208.32). Current reward after update: -352.77, Optimal reward -169.57
Iteration 169 took 3.61 seconds (mean sampled reward: -5017.68). Current reward after update: -331.09, Optimal reward -169.57
Iteration 170 took 3.74 seconds (mean sampled reward: -5389.99). Current reward after update: -345.93, Optimal reward -169.57
Iteration 171 took 3.67 seconds (mean sampled reward: -5443.02). Current reward after update: -394.60, Optimal reward -169.57
Iteration 172 took 3.73 seconds (mean sampled reward: -5782.44). Current reward after update: -477.13, Optimal reward -169.57
Iteration 173 took 3.80 seconds (mean sampled reward: -5641.24). Current reward after update: -415.70, Optimal reward -169.57
Iteration 174 took 3.79 seconds (mean sampled reward: -4270.79). Current reward after update: -442.96, Optimal reward -169.57
Iteration 175 took 3.83 seconds (mean sampled reward: -4746.90). Current reward after update: -532.63, Optimal reward -169.57
Iteration 176 took 3.80 seconds (mean sampled reward: -4801.48). Current reward after update: -467.17, Optimal reward -169.57
Iteration 177 took 3.83 seconds (mean sampled reward: -5350.24). Current reward after update: -524.01, Optimal reward -169.57
Iteration 178 took 3.71 seconds (mean sampled reward: -5880.01). Current reward after update: -448.90, Optimal reward -169.57
Iteration 179 took 3.86 seconds (mean sampled reward: -4936.64). Current reward after update: -442.29, Optimal reward -169.57
Iteration 180 took 3.84 seconds (mean sampled reward: -5514.88). Current reward after update: -475.14, Optimal reward -169.57
Iteration 181 took 3.85 seconds (mean sampled reward: -5432.65). Current reward after update: -440.33, Optimal reward -169.57
Iteration 182 took 3.95 seconds (mean sampled reward: -5567.64). Current reward after update: -396.57, Optimal reward -169.57
Iteration 183 took 3.97 seconds (mean sampled reward: -5752.83). Current reward after update: -649.67, Optimal reward -169.57
Iteration 184 took 3.93 seconds (mean sampled reward: -4891.00). Current reward after update: -526.72, Optimal reward -169.57
Iteration 185 took 3.96 seconds (mean sampled reward: -4382.22). Current reward after update: -478.46, Optimal reward -169.57
Iteration 186 took 3.84 seconds (mean sampled reward: -4834.23). Current reward after update: -452.84, Optimal reward -169.57
Iteration 187 took 3.78 seconds (mean sampled reward: -4424.64). Current reward after update: -437.82, Optimal reward -169.57
Iteration 188 took 3.87 seconds (mean sampled reward: -4722.34). Current reward after update: -494.63, Optimal reward -169.57
Iteration 189 took 4.07 seconds (mean sampled reward: -4606.39). Current reward after update: -529.84, Optimal reward -169.57
Iteration 190 took 4.07 seconds (mean sampled reward: -4658.24). Current reward after update: -453.32, Optimal reward -169.57
Iteration 191 took 4.14 seconds (mean sampled reward: -4748.97). Current reward after update: -445.28, Optimal reward -169.57
Iteration 192 took 4.15 seconds (mean sampled reward: -5366.63). Current reward after update: -493.52, Optimal reward -169.57
Iteration 193 took 4.13 seconds (mean sampled reward: -5210.43). Current reward after update: -387.14, Optimal reward -169.57
Iteration 194 took 3.99 seconds (mean sampled reward: -5370.80). Current reward after update: -512.80, Optimal reward -169.57
Iteration 195 took 3.97 seconds (mean sampled reward: -5837.18). Current reward after update: -461.58, Optimal reward -169.57
Iteration 196 took 3.98 seconds (mean sampled reward: -5781.22). Current reward after update: -514.17, Optimal reward -169.57
Iteration 197 took 4.08 seconds (mean sampled reward: -5335.13). Current reward after update: -410.00, Optimal reward -169.57
Iteration 198 took 3.97 seconds (mean sampled reward: -4602.27). Current reward after update: -426.27, Optimal reward -169.57
Iteration 199 took 3.89 seconds (mean sampled reward: -5071.67). Current reward after update: -525.96, Optimal reward -169.57
Iteration 200 took 4.01 seconds (mean sampled reward: -5641.49). Current reward after update: -539.83, Optimal reward -169.57
Iteration 201 took 4.04 seconds (mean sampled reward: -5622.57). Current reward after update: -648.39, Optimal reward -169.57
Iteration 202 took 4.28 seconds (mean sampled reward: -5285.08). Current reward after update: -474.50, Optimal reward -169.57
Iteration 203 took 3.94 seconds (mean sampled reward: -5182.82). Current reward after update: -428.62, Optimal reward -169.57
Iteration 204 took 4.12 seconds (mean sampled reward: -5261.74). Current reward after update: -1464.58, Optimal reward -169.57
Iteration 205 took 4.14 seconds (mean sampled reward: -4682.88). Current reward after update: -491.54, Optimal reward -169.57
Iteration 206 took 4.01 seconds (mean sampled reward: -4903.31). Current reward after update: -440.89, Optimal reward -169.57
Iteration 207 took 3.91 seconds (mean sampled reward: -5122.54). Current reward after update: -376.79, Optimal reward -169.57
Iteration 208 took 3.64 seconds (mean sampled reward: -5180.12). Current reward after update: -503.44, Optimal reward -169.57
Iteration 209 took 3.65 seconds (mean sampled reward: -5170.56). Current reward after update: -469.94, Optimal reward -169.57
Iteration 210 took 3.83 seconds (mean sampled reward: -5030.74). Current reward after update: -334.54, Optimal reward -169.57
Iteration 211 took 3.81 seconds (mean sampled reward: -4897.51). Current reward after update: -399.49, Optimal reward -169.57
Iteration 212 took 4.07 seconds (mean sampled reward: -4568.94). Current reward after update: -326.26, Optimal reward -169.57
Iteration 213 took 4.20 seconds (mean sampled reward: -4360.90). Current reward after update: -308.20, Optimal reward -169.57
Iteration 214 took 3.85 seconds (mean sampled reward: -4528.49). Current reward after update: -387.93, Optimal reward -169.57
Iteration 215 took 3.74 seconds (mean sampled reward: -5475.44). Current reward after update: -257.35, Optimal reward -169.57
Iteration 216 took 3.71 seconds (mean sampled reward: -5438.23). Current reward after update: -394.28, Optimal reward -169.57
Iteration 217 took 3.98 seconds (mean sampled reward: -5304.95). Current reward after update: -323.93, Optimal reward -169.57
Iteration 218 took 3.76 seconds (mean sampled reward: -4817.07). Current reward after update: -310.90, Optimal reward -169.57
Iteration 219 took 3.75 seconds (mean sampled reward: -4351.88). Current reward after update: -325.36, Optimal reward -169.57
Iteration 220 took 3.96 seconds (mean sampled reward: -4709.06). Current reward after update: -396.50, Optimal reward -169.57
Iteration 221 took 4.08 seconds (mean sampled reward: -6108.39). Current reward after update: -463.02, Optimal reward -169.57
Iteration 222 took 4.08 seconds (mean sampled reward: -5257.67). Current reward after update: -499.08, Optimal reward -169.57
Iteration 223 took 3.86 seconds (mean sampled reward: -5181.50). Current reward after update: -472.89, Optimal reward -169.57
Iteration 224 took 3.81 seconds (mean sampled reward: -5517.55). Current reward after update: -354.54, Optimal reward -169.57
Iteration 225 took 3.73 seconds (mean sampled reward: -4948.56). Current reward after update: -513.50, Optimal reward -169.57
Iteration 226 took 3.78 seconds (mean sampled reward: -5365.67). Current reward after update: -575.13, Optimal reward -169.57
Iteration 227 took 3.90 seconds (mean sampled reward: -5623.17). Current reward after update: -511.40, Optimal reward -169.57
Iteration 228 took 3.72 seconds (mean sampled reward: -5449.64). Current reward after update: -566.70, Optimal reward -169.57
Iteration 229 took 3.80 seconds (mean sampled reward: -5780.49). Current reward after update: -543.39, Optimal reward -169.57
Iteration 230 took 4.02 seconds (mean sampled reward: -5577.98). Current reward after update: -587.21, Optimal reward -169.57
Iteration 231 took 3.85 seconds (mean sampled reward: -5443.68). Current reward after update: -480.32, Optimal reward -169.57
Iteration 232 took 4.08 seconds (mean sampled reward: -5263.61). Current reward after update: -479.78, Optimal reward -169.57
Iteration 233 took 3.85 seconds (mean sampled reward: -3971.41). Current reward after update: -435.18, Optimal reward -169.57
Iteration 234 took 4.30 seconds (mean sampled reward: -4956.19). Current reward after update: -516.66, Optimal reward -169.57
Iteration 235 took 4.01 seconds (mean sampled reward: -5554.40). Current reward after update: -515.44, Optimal reward -169.57
Iteration 236 took 4.04 seconds (mean sampled reward: -5032.64). Current reward after update: -445.35, Optimal reward -169.57
Iteration 237 took 3.95 seconds (mean sampled reward: -5186.58). Current reward after update: -860.26, Optimal reward -169.57
Iteration 238 took 3.75 seconds (mean sampled reward: -5094.75). Current reward after update: -412.42, Optimal reward -169.57
Iteration 239 took 3.89 seconds (mean sampled reward: -5936.08). Current reward after update: -435.19, Optimal reward -169.57
Iteration 240 took 3.87 seconds (mean sampled reward: -5637.48). Current reward after update: -536.72, Optimal reward -169.57
Iteration 241 took 3.88 seconds (mean sampled reward: -5727.80). Current reward after update: -600.21, Optimal reward -169.57
Iteration 242 took 3.93 seconds (mean sampled reward: -5777.35). Current reward after update: -594.65, Optimal reward -169.57
Iteration 243 took 3.92 seconds (mean sampled reward: -5889.43). Current reward after update: -664.12, Optimal reward -169.57
Iteration 244 took 3.82 seconds (mean sampled reward: -4764.67). Current reward after update: -501.43, Optimal reward -169.57
Iteration 245 took 3.87 seconds (mean sampled reward: -4580.22). Current reward after update: -543.27, Optimal reward -169.57
Iteration 246 took 3.87 seconds (mean sampled reward: -4512.86). Current reward after update: -691.22, Optimal reward -169.57
Iteration 247 took 3.75 seconds (mean sampled reward: -4589.41). Current reward after update: -616.80, Optimal reward -169.57
Iteration 248 took 3.98 seconds (mean sampled reward: -5010.27). Current reward after update: -743.50, Optimal reward -169.57
Iteration 249 took 3.81 seconds (mean sampled reward: -3558.87). Current reward after update: -609.71, Optimal reward -169.57
Iteration 250 took 4.02 seconds (mean sampled reward: -4343.53). Current reward after update: -645.14, Optimal reward -169.57
Iteration 251 took 3.95 seconds (mean sampled reward: -4782.12). Current reward after update: -682.66, Optimal reward -169.57
Iteration 252 took 3.95 seconds (mean sampled reward: -4852.08). Current reward after update: -710.24, Optimal reward -169.57
Iteration 253 took 3.97 seconds (mean sampled reward: -4404.03). Current reward after update: -514.87, Optimal reward -169.57
Iteration 254 took 3.73 seconds (mean sampled reward: -3956.32). Current reward after update: -589.70, Optimal reward -169.57
Iteration 255 took 3.94 seconds (mean sampled reward: -4117.98). Current reward after update: -532.16, Optimal reward -169.57
Iteration 256 took 3.91 seconds (mean sampled reward: -4755.70). Current reward after update: -670.22, Optimal reward -169.57
Iteration 257 took 3.91 seconds (mean sampled reward: -5134.43). Current reward after update: -2458.78, Optimal reward -169.57
Iteration 258 took 3.84 seconds (mean sampled reward: -4855.82). Current reward after update: -589.01, Optimal reward -169.57
Iteration 259 took 3.91 seconds (mean sampled reward: -5082.77). Current reward after update: -495.97, Optimal reward -169.57
Iteration 260 took 3.72 seconds (mean sampled reward: -4047.75). Current reward after update: -565.06, Optimal reward -169.57
Iteration 261 took 3.85 seconds (mean sampled reward: -4501.48). Current reward after update: -633.76, Optimal reward -169.57
Iteration 262 took 3.81 seconds (mean sampled reward: -4547.14). Current reward after update: -653.50, Optimal reward -169.57
Iteration 263 took 3.87 seconds (mean sampled reward: -4600.78). Current reward after update: -602.34, Optimal reward -169.57
Iteration 264 took 3.61 seconds (mean sampled reward: -3354.28). Current reward after update: -585.73, Optimal reward -169.57
Iteration 265 took 3.73 seconds (mean sampled reward: -3062.39). Current reward after update: -507.12, Optimal reward -169.57
Iteration 266 took 3.84 seconds (mean sampled reward: -3556.16). Current reward after update: -519.86, Optimal reward -169.57
Iteration 267 took 3.74 seconds (mean sampled reward: -4366.11). Current reward after update: -681.42, Optimal reward -169.57
Iteration 268 took 3.63 seconds (mean sampled reward: -3357.78). Current reward after update: -543.03, Optimal reward -169.57
Iteration 269 took 3.80 seconds (mean sampled reward: -4068.09). Current reward after update: -581.55, Optimal reward -169.57
Iteration 270 took 4.04 seconds (mean sampled reward: -6152.94). Current reward after update: -507.56, Optimal reward -169.57
Iteration 271 took 4.06 seconds (mean sampled reward: -6193.41). Current reward after update: -574.88, Optimal reward -169.57
Iteration 272 took 3.99 seconds (mean sampled reward: -6171.85). Current reward after update: -640.80, Optimal reward -169.57
Iteration 273 took 3.94 seconds (mean sampled reward: -6151.54). Current reward after update: -485.72, Optimal reward -169.57
Iteration 274 took 4.06 seconds (mean sampled reward: -6409.82). Current reward after update: -686.60, Optimal reward -169.57
Iteration 275 took 4.04 seconds (mean sampled reward: -6204.16). Current reward after update: -669.28, Optimal reward -169.57
Iteration 276 took 4.08 seconds (mean sampled reward: -6125.44). Current reward after update: -577.02, Optimal reward -169.57
Iteration 277 took 3.97 seconds (mean sampled reward: -6216.11). Current reward after update: -668.19, Optimal reward -169.57
Iteration 278 took 3.80 seconds (mean sampled reward: -6567.89). Current reward after update: -829.07, Optimal reward -169.57
Iteration 279 took 3.94 seconds (mean sampled reward: -6757.24). Current reward after update: -1509.26, Optimal reward -169.57
Iteration 280 took 4.03 seconds (mean sampled reward: -6617.28). Current reward after update: -642.93, Optimal reward -169.57
Iteration 281 took 3.92 seconds (mean sampled reward: -6185.62). Current reward after update: -641.47, Optimal reward -169.57
Iteration 282 took 3.99 seconds (mean sampled reward: -6482.72). Current reward after update: -729.89, Optimal reward -169.57
Iteration 283 took 3.94 seconds (mean sampled reward: -6359.81). Current reward after update: -517.60, Optimal reward -169.57
Iteration 284 took 3.92 seconds (mean sampled reward: -5534.22). Current reward after update: -570.62, Optimal reward -169.57
Iteration 285 took 3.87 seconds (mean sampled reward: -5833.49). Current reward after update: -581.81, Optimal reward -169.57
Iteration 286 took 3.90 seconds (mean sampled reward: -6257.96). Current reward after update: -610.08, Optimal reward -169.57
Iteration 287 took 4.00 seconds (mean sampled reward: -6486.17). Current reward after update: -6973.03, Optimal reward -169.57
Iteration 288 took 3.93 seconds (mean sampled reward: -6031.51). Current reward after update: -618.50, Optimal reward -169.57
Iteration 289 took 3.83 seconds (mean sampled reward: -6172.82). Current reward after update: -602.99, Optimal reward -169.57
Iteration 290 took 3.79 seconds (mean sampled reward: -5772.28). Current reward after update: -617.50, Optimal reward -169.57
Iteration 291 took 3.89 seconds (mean sampled reward: -5736.37). Current reward after update: -469.20, Optimal reward -169.57
Iteration 292 took 3.96 seconds (mean sampled reward: -6109.74). Current reward after update: -544.04, Optimal reward -169.57
Iteration 293 took 3.99 seconds (mean sampled reward: -6277.89). Current reward after update: -649.16, Optimal reward -169.57
Iteration 294 took 3.79 seconds (mean sampled reward: -5544.31). Current reward after update: -752.98, Optimal reward -169.57
Iteration 295 took 3.83 seconds (mean sampled reward: -5663.66). Current reward after update: -690.79, Optimal reward -169.57
Iteration 296 took 3.87 seconds (mean sampled reward: -6436.08). Current reward after update: -769.78, Optimal reward -169.57
Iteration 297 took 3.58 seconds (mean sampled reward: -5856.38). Current reward after update: -742.45, Optimal reward -169.57
Iteration 298 took 3.37 seconds (mean sampled reward: -3732.70). Current reward after update: -765.61, Optimal reward -169.57
Iteration 299 took 3.35 seconds (mean sampled reward: -4089.57). Current reward after update: -675.65, Optimal reward -169.57
Iteration 300 took 3.40 seconds (mean sampled reward: -4592.41). Current reward after update: -775.52, Optimal reward -169.57
Iteration 1 took 3.92 seconds (mean sampled reward: -7501.24). Current reward after update: -4974.15, Optimal reward -4974.15
Iteration 2 took 3.68 seconds (mean sampled reward: -6603.40). Current reward after update: -3228.59, Optimal reward -3228.59
Iteration 3 took 3.64 seconds (mean sampled reward: -6695.44). Current reward after update: -2335.14, Optimal reward -2335.14
Iteration 4 took 3.84 seconds (mean sampled reward: -6581.72). Current reward after update: -1862.69, Optimal reward -1862.69
Iteration 5 took 3.81 seconds (mean sampled reward: -6615.34). Current reward after update: -1869.20, Optimal reward -1862.69
Iteration 6 took 3.76 seconds (mean sampled reward: -5129.91). Current reward after update: -1616.48, Optimal reward -1616.48
Iteration 7 took 3.72 seconds (mean sampled reward: -5696.77). Current reward after update: -1846.51, Optimal reward -1616.48
Iteration 8 took 3.70 seconds (mean sampled reward: -5496.66). Current reward after update: -1416.42, Optimal reward -1416.42
Iteration 9 took 3.79 seconds (mean sampled reward: -5826.89). Current reward after update: -1422.54, Optimal reward -1416.42
Iteration 10 took 3.88 seconds (mean sampled reward: -6432.55). Current reward after update: -1090.61, Optimal reward -1090.61
Iteration 11 took 3.81 seconds (mean sampled reward: -6228.13). Current reward after update: -1107.02, Optimal reward -1090.61
Iteration 12 took 3.69 seconds (mean sampled reward: -6037.29). Current reward after update: -1214.15, Optimal reward -1090.61
Iteration 13 took 3.71 seconds (mean sampled reward: -6027.49). Current reward after update: -1147.36, Optimal reward -1090.61
Iteration 14 took 3.86 seconds (mean sampled reward: -5638.56). Current reward after update: -1133.83, Optimal reward -1090.61
Iteration 15 took 3.83 seconds (mean sampled reward: -5970.32). Current reward after update: -1356.86, Optimal reward -1090.61
Iteration 16 took 3.83 seconds (mean sampled reward: -6086.51). Current reward after update: -1275.65, Optimal reward -1090.61
Iteration 17 took 3.81 seconds (mean sampled reward: -6017.76). Current reward after update: -1519.55, Optimal reward -1090.61
Iteration 18 took 3.74 seconds (mean sampled reward: -5572.09). Current reward after update: -1512.40, Optimal reward -1090.61
Iteration 19 took 3.86 seconds (mean sampled reward: -6087.37). Current reward after update: -1337.21, Optimal reward -1090.61
Iteration 20 took 3.86 seconds (mean sampled reward: -5950.96). Current reward after update: -1084.66, Optimal reward -1084.66
Iteration 21 took 3.86 seconds (mean sampled reward: -5867.49). Current reward after update: -999.60, Optimal reward -999.60
Iteration 22 took 3.83 seconds (mean sampled reward: -6228.49). Current reward after update: -1407.62, Optimal reward -999.60
Iteration 23 took 3.78 seconds (mean sampled reward: -5611.45). Current reward after update: -1548.05, Optimal reward -999.60
Iteration 24 took 3.85 seconds (mean sampled reward: -6228.80). Current reward after update: -1231.11, Optimal reward -999.60
Iteration 25 took 3.75 seconds (mean sampled reward: -6126.52). Current reward after update: -1415.94, Optimal reward -999.60
Iteration 26 took 3.64 seconds (mean sampled reward: -6421.82). Current reward after update: -1420.90, Optimal reward -999.60
Iteration 27 took 3.72 seconds (mean sampled reward: -6753.16). Current reward after update: -1438.82, Optimal reward -999.60
Iteration 28 took 3.74 seconds (mean sampled reward: -6205.45). Current reward after update: -1675.12, Optimal reward -999.60
Iteration 29 took 3.71 seconds (mean sampled reward: -6191.00). Current reward after update: -1451.30, Optimal reward -999.60
Iteration 30 took 3.83 seconds (mean sampled reward: -6366.43). Current reward after update: -1388.97, Optimal reward -999.60
Iteration 31 took 3.68 seconds (mean sampled reward: -6085.02). Current reward after update: -1241.84, Optimal reward -999.60
Iteration 32 took 3.69 seconds (mean sampled reward: -5898.36). Current reward after update: -1212.98, Optimal reward -999.60
Iteration 33 took 3.70 seconds (mean sampled reward: -6373.51). Current reward after update: -1361.78, Optimal reward -999.60
Iteration 34 took 3.83 seconds (mean sampled reward: -6094.93). Current reward after update: -1193.30, Optimal reward -999.60
Iteration 35 took 3.80 seconds (mean sampled reward: -6794.76). Current reward after update: -1132.41, Optimal reward -999.60
Iteration 36 took 3.89 seconds (mean sampled reward: -6486.54). Current reward after update: -1024.47, Optimal reward -999.60
Iteration 37 took 3.80 seconds (mean sampled reward: -6500.86). Current reward after update: -1234.80, Optimal reward -999.60
Iteration 38 took 3.83 seconds (mean sampled reward: -6267.74). Current reward after update: -1054.99, Optimal reward -999.60
Iteration 39 took 3.81 seconds (mean sampled reward: -5739.70). Current reward after update: -923.00, Optimal reward -923.00
Iteration 40 took 3.82 seconds (mean sampled reward: -5836.16). Current reward after update: -1138.90, Optimal reward -923.00
Iteration 41 took 3.82 seconds (mean sampled reward: -6297.45). Current reward after update: -945.85, Optimal reward -923.00
Iteration 42 took 3.85 seconds (mean sampled reward: -6182.02). Current reward after update: -1039.34, Optimal reward -923.00
Iteration 43 took 3.86 seconds (mean sampled reward: -6450.50). Current reward after update: -1117.67, Optimal reward -923.00
Iteration 44 took 3.79 seconds (mean sampled reward: -6078.36). Current reward after update: -1218.34, Optimal reward -923.00
Iteration 45 took 3.75 seconds (mean sampled reward: -6003.33). Current reward after update: -1185.94, Optimal reward -923.00
Iteration 46 took 3.92 seconds (mean sampled reward: -6140.17). Current reward after update: -1404.17, Optimal reward -923.00
Iteration 47 took 3.66 seconds (mean sampled reward: -5602.02). Current reward after update: -1201.92, Optimal reward -923.00
Iteration 48 took 3.62 seconds (mean sampled reward: -5193.22). Current reward after update: -1358.06, Optimal reward -923.00
Iteration 49 took 3.63 seconds (mean sampled reward: -5143.53). Current reward after update: -1127.70, Optimal reward -923.00
Iteration 50 took 3.65 seconds (mean sampled reward: -5031.60). Current reward after update: -1256.41, Optimal reward -923.00
Iteration 51 took 3.79 seconds (mean sampled reward: -5282.92). Current reward after update: -1076.89, Optimal reward -923.00
Iteration 52 took 3.87 seconds (mean sampled reward: -5284.18). Current reward after update: -1075.88, Optimal reward -923.00
Iteration 53 took 3.73 seconds (mean sampled reward: -4761.60). Current reward after update: -1035.69, Optimal reward -923.00
Iteration 54 took 3.63 seconds (mean sampled reward: -4378.45). Current reward after update: -989.26, Optimal reward -923.00
Iteration 55 took 3.73 seconds (mean sampled reward: -4598.48). Current reward after update: -1013.99, Optimal reward -923.00
Iteration 56 took 3.84 seconds (mean sampled reward: -4809.32). Current reward after update: -983.66, Optimal reward -923.00
Iteration 57 took 3.79 seconds (mean sampled reward: -4611.03). Current reward after update: -924.11, Optimal reward -923.00
Iteration 58 took 4.15 seconds (mean sampled reward: -5756.91). Current reward after update: -894.33, Optimal reward -894.33
Iteration 59 took 4.00 seconds (mean sampled reward: -5854.32). Current reward after update: -1044.62, Optimal reward -894.33
Iteration 60 took 3.85 seconds (mean sampled reward: -5717.78). Current reward after update: -1039.18, Optimal reward -894.33
Iteration 61 took 3.76 seconds (mean sampled reward: -5483.80). Current reward after update: -1109.77, Optimal reward -894.33
Iteration 62 took 3.83 seconds (mean sampled reward: -6066.10). Current reward after update: -1073.01, Optimal reward -894.33
Iteration 63 took 3.80 seconds (mean sampled reward: -5491.10). Current reward after update: -943.63, Optimal reward -894.33
Iteration 64 took 3.62 seconds (mean sampled reward: -5183.23). Current reward after update: -793.66, Optimal reward -793.66
Iteration 65 took 3.71 seconds (mean sampled reward: -5577.61). Current reward after update: -861.39, Optimal reward -793.66
Iteration 66 took 3.64 seconds (mean sampled reward: -5400.04). Current reward after update: -885.57, Optimal reward -793.66
Iteration 67 took 3.52 seconds (mean sampled reward: -4658.29). Current reward after update: -853.21, Optimal reward -793.66
Iteration 68 took 3.62 seconds (mean sampled reward: -5055.48). Current reward after update: -753.23, Optimal reward -753.23
Iteration 69 took 3.67 seconds (mean sampled reward: -5331.94). Current reward after update: -711.28, Optimal reward -711.28
Iteration 70 took 3.62 seconds (mean sampled reward: -5341.11). Current reward after update: -607.55, Optimal reward -607.55
Iteration 71 took 3.69 seconds (mean sampled reward: -5888.42). Current reward after update: -638.77, Optimal reward -607.55
Iteration 72 took 3.66 seconds (mean sampled reward: -6328.53). Current reward after update: -661.91, Optimal reward -607.55
Iteration 73 took 3.67 seconds (mean sampled reward: -5691.81). Current reward after update: -707.13, Optimal reward -607.55
Iteration 74 took 3.58 seconds (mean sampled reward: -5725.97). Current reward after update: -678.15, Optimal reward -607.55
Iteration 75 took 3.67 seconds (mean sampled reward: -5993.18). Current reward after update: -589.76, Optimal reward -589.76
Iteration 76 took 3.65 seconds (mean sampled reward: -5636.82). Current reward after update: -918.70, Optimal reward -589.76
Iteration 77 took 3.64 seconds (mean sampled reward: -5328.00). Current reward after update: -632.33, Optimal reward -589.76
Iteration 78 took 3.59 seconds (mean sampled reward: -5432.73). Current reward after update: -682.48, Optimal reward -589.76
Iteration 79 took 3.63 seconds (mean sampled reward: -5352.03). Current reward after update: -564.81, Optimal reward -564.81
Iteration 80 took 3.60 seconds (mean sampled reward: -5984.33). Current reward after update: -619.16, Optimal reward -564.81
Iteration 81 took 3.65 seconds (mean sampled reward: -5912.34). Current reward after update: -590.25, Optimal reward -564.81
Iteration 82 took 3.68 seconds (mean sampled reward: -5482.63). Current reward after update: -541.60, Optimal reward -541.60
Iteration 83 took 3.56 seconds (mean sampled reward: -5642.52). Current reward after update: -658.92, Optimal reward -541.60
Iteration 84 took 3.56 seconds (mean sampled reward: -5452.93). Current reward after update: -537.50, Optimal reward -537.50
Iteration 85 took 3.59 seconds (mean sampled reward: -5710.30). Current reward after update: -554.38, Optimal reward -537.50
Iteration 86 took 3.65 seconds (mean sampled reward: -5784.40). Current reward after update: -1012.92, Optimal reward -537.50
Iteration 87 took 3.64 seconds (mean sampled reward: -5753.31). Current reward after update: -726.86, Optimal reward -537.50
Iteration 88 took 3.64 seconds (mean sampled reward: -5705.33). Current reward after update: -546.62, Optimal reward -537.50
Iteration 89 took 3.65 seconds (mean sampled reward: -6047.13). Current reward after update: -542.98, Optimal reward -537.50
Iteration 90 took 3.60 seconds (mean sampled reward: -6052.46). Current reward after update: -694.73, Optimal reward -537.50
Iteration 91 took 3.64 seconds (mean sampled reward: -5744.10). Current reward after update: -660.76, Optimal reward -537.50
Iteration 92 took 3.62 seconds (mean sampled reward: -5897.15). Current reward after update: -721.05, Optimal reward -537.50
Iteration 93 took 3.48 seconds (mean sampled reward: -4705.45). Current reward after update: -758.60, Optimal reward -537.50
Iteration 94 took 3.60 seconds (mean sampled reward: -4991.04). Current reward after update: -930.96, Optimal reward -537.50
Iteration 95 took 3.66 seconds (mean sampled reward: -5839.61). Current reward after update: -718.53, Optimal reward -537.50
Iteration 96 took 3.71 seconds (mean sampled reward: -5699.41). Current reward after update: -901.87, Optimal reward -537.50
Iteration 97 took 3.57 seconds (mean sampled reward: -5137.00). Current reward after update: -794.95, Optimal reward -537.50
Iteration 98 took 3.68 seconds (mean sampled reward: -5444.23). Current reward after update: -692.08, Optimal reward -537.50
Iteration 99 took 3.66 seconds (mean sampled reward: -5582.38). Current reward after update: -913.62, Optimal reward -537.50
Iteration 100 took 3.63 seconds (mean sampled reward: -5357.59). Current reward after update: -880.12, Optimal reward -537.50
Iteration 101 took 3.62 seconds (mean sampled reward: -5631.70). Current reward after update: -1094.66, Optimal reward -537.50
Iteration 102 took 3.65 seconds (mean sampled reward: -5276.02). Current reward after update: -1400.32, Optimal reward -537.50
Iteration 103 took 3.60 seconds (mean sampled reward: -5595.50). Current reward after update: -809.65, Optimal reward -537.50
Iteration 104 took 3.50 seconds (mean sampled reward: -5024.50). Current reward after update: -583.37, Optimal reward -537.50
Iteration 105 took 3.66 seconds (mean sampled reward: -5790.25). Current reward after update: -675.19, Optimal reward -537.50
Iteration 106 took 3.69 seconds (mean sampled reward: -5651.95). Current reward after update: -559.38, Optimal reward -537.50
Iteration 107 took 3.95 seconds (mean sampled reward: -5913.59). Current reward after update: -587.90, Optimal reward -537.50
Iteration 108 took 3.80 seconds (mean sampled reward: -5734.15). Current reward after update: -743.30, Optimal reward -537.50
Iteration 109 took 3.60 seconds (mean sampled reward: -6154.81). Current reward after update: -735.52, Optimal reward -537.50
Iteration 110 took 3.63 seconds (mean sampled reward: -5751.42). Current reward after update: -591.24, Optimal reward -537.50
Iteration 111 took 3.68 seconds (mean sampled reward: -5851.10). Current reward after update: -551.07, Optimal reward -537.50
Iteration 112 took 3.71 seconds (mean sampled reward: -5874.76). Current reward after update: -597.81, Optimal reward -537.50
Iteration 113 took 3.54 seconds (mean sampled reward: -5112.98). Current reward after update: -587.52, Optimal reward -537.50
Iteration 114 took 3.71 seconds (mean sampled reward: -5639.55). Current reward after update: -559.90, Optimal reward -537.50
Iteration 115 took 3.55 seconds (mean sampled reward: -4778.92). Current reward after update: -522.91, Optimal reward -522.91
Iteration 116 took 3.63 seconds (mean sampled reward: -5718.72). Current reward after update: -494.14, Optimal reward -494.14
Iteration 117 took 3.70 seconds (mean sampled reward: -5291.11). Current reward after update: -520.71, Optimal reward -494.14
Iteration 118 took 3.73 seconds (mean sampled reward: -5757.71). Current reward after update: -575.14, Optimal reward -494.14
Iteration 119 took 3.67 seconds (mean sampled reward: -5434.68). Current reward after update: -643.58, Optimal reward -494.14
Iteration 120 took 3.63 seconds (mean sampled reward: -5227.67). Current reward after update: -579.74, Optimal reward -494.14
Iteration 121 took 3.68 seconds (mean sampled reward: -5365.47). Current reward after update: -435.74, Optimal reward -435.74
Iteration 122 took 3.73 seconds (mean sampled reward: -5605.31). Current reward after update: -528.11, Optimal reward -435.74
Iteration 123 took 3.73 seconds (mean sampled reward: -5608.71). Current reward after update: -486.56, Optimal reward -435.74
Iteration 124 took 3.63 seconds (mean sampled reward: -5680.58). Current reward after update: -481.69, Optimal reward -435.74
Iteration 125 took 3.73 seconds (mean sampled reward: -5747.67). Current reward after update: -677.63, Optimal reward -435.74
Iteration 126 took 3.69 seconds (mean sampled reward: -5326.87). Current reward after update: -632.69, Optimal reward -435.74
Iteration 127 took 3.76 seconds (mean sampled reward: -5752.87). Current reward after update: -558.69, Optimal reward -435.74
Iteration 128 took 3.76 seconds (mean sampled reward: -5337.90). Current reward after update: -374.09, Optimal reward -374.09
Iteration 129 took 3.79 seconds (mean sampled reward: -6002.43). Current reward after update: -438.98, Optimal reward -374.09
Iteration 130 took 3.69 seconds (mean sampled reward: -5790.17). Current reward after update: -829.85, Optimal reward -374.09
Iteration 131 took 3.73 seconds (mean sampled reward: -5546.47). Current reward after update: -662.61, Optimal reward -374.09
Iteration 132 took 3.66 seconds (mean sampled reward: -5426.93). Current reward after update: -470.52, Optimal reward -374.09
Iteration 133 took 3.74 seconds (mean sampled reward: -5922.26). Current reward after update: -425.08, Optimal reward -374.09
Iteration 134 took 3.90 seconds (mean sampled reward: -5478.49). Current reward after update: -615.65, Optimal reward -374.09
Iteration 135 took 3.99 seconds (mean sampled reward: -5457.81). Current reward after update: -572.82, Optimal reward -374.09
Iteration 136 took 3.69 seconds (mean sampled reward: -5303.40). Current reward after update: -782.20, Optimal reward -374.09
Iteration 137 took 3.98 seconds (mean sampled reward: -5365.47). Current reward after update: -445.31, Optimal reward -374.09
Iteration 138 took 3.77 seconds (mean sampled reward: -5739.92). Current reward after update: -511.64, Optimal reward -374.09
Iteration 139 took 3.86 seconds (mean sampled reward: -5799.76). Current reward after update: -379.52, Optimal reward -374.09
Iteration 140 took 3.80 seconds (mean sampled reward: -5622.16). Current reward after update: -514.88, Optimal reward -374.09
Iteration 141 took 3.93 seconds (mean sampled reward: -5616.73). Current reward after update: -522.15, Optimal reward -374.09
Iteration 142 took 3.88 seconds (mean sampled reward: -5744.28). Current reward after update: -579.92, Optimal reward -374.09
Iteration 143 took 3.70 seconds (mean sampled reward: -6042.29). Current reward after update: -542.58, Optimal reward -374.09
Iteration 144 took 3.64 seconds (mean sampled reward: -5849.41). Current reward after update: -559.63, Optimal reward -374.09
Iteration 145 took 3.75 seconds (mean sampled reward: -5543.08). Current reward after update: -591.92, Optimal reward -374.09
Iteration 146 took 3.68 seconds (mean sampled reward: -5534.29). Current reward after update: -421.00, Optimal reward -374.09
Iteration 147 took 3.77 seconds (mean sampled reward: -5759.26). Current reward after update: -610.77, Optimal reward -374.09
Iteration 148 took 3.64 seconds (mean sampled reward: -5261.31). Current reward after update: -585.85, Optimal reward -374.09
Iteration 149 took 3.67 seconds (mean sampled reward: -5499.16). Current reward after update: -595.64, Optimal reward -374.09
Iteration 150 took 3.69 seconds (mean sampled reward: -5669.00). Current reward after update: -378.76, Optimal reward -374.09
Iteration 151 took 3.73 seconds (mean sampled reward: -6104.02). Current reward after update: -526.61, Optimal reward -374.09
Iteration 152 took 3.49 seconds (mean sampled reward: -5894.93). Current reward after update: -589.73, Optimal reward -374.09
Iteration 153 took 3.44 seconds (mean sampled reward: -5387.45). Current reward after update: -584.02, Optimal reward -374.09
Iteration 154 took 3.53 seconds (mean sampled reward: -5254.35). Current reward after update: -622.36, Optimal reward -374.09
Iteration 155 took 3.56 seconds (mean sampled reward: -5778.11). Current reward after update: -838.22, Optimal reward -374.09
Iteration 156 took 3.67 seconds (mean sampled reward: -5718.61). Current reward after update: -738.75, Optimal reward -374.09
Iteration 157 took 3.62 seconds (mean sampled reward: -5705.71). Current reward after update: -650.72, Optimal reward -374.09
Iteration 158 took 3.61 seconds (mean sampled reward: -5334.11). Current reward after update: -525.04, Optimal reward -374.09
Iteration 159 took 3.69 seconds (mean sampled reward: -5965.75). Current reward after update: -466.77, Optimal reward -374.09
Iteration 160 took 3.63 seconds (mean sampled reward: -5907.18). Current reward after update: -418.07, Optimal reward -374.09
Iteration 161 took 3.70 seconds (mean sampled reward: -5697.95). Current reward after update: -647.63, Optimal reward -374.09
Iteration 162 took 3.61 seconds (mean sampled reward: -5424.57). Current reward after update: -755.06, Optimal reward -374.09
Iteration 163 took 3.77 seconds (mean sampled reward: -5891.00). Current reward after update: -437.20, Optimal reward -374.09
Iteration 164 took 3.70 seconds (mean sampled reward: -5455.62). Current reward after update: -552.27, Optimal reward -374.09
Iteration 165 took 3.61 seconds (mean sampled reward: -5618.58). Current reward after update: -909.02, Optimal reward -374.09
Iteration 166 took 3.63 seconds (mean sampled reward: -5960.18). Current reward after update: -908.84, Optimal reward -374.09
Iteration 167 took 3.57 seconds (mean sampled reward: -5287.46). Current reward after update: -544.69, Optimal reward -374.09
Iteration 168 took 3.55 seconds (mean sampled reward: -5717.86). Current reward after update: -616.70, Optimal reward -374.09
Iteration 169 took 3.59 seconds (mean sampled reward: -5345.28). Current reward after update: -504.06, Optimal reward -374.09
Iteration 170 took 3.46 seconds (mean sampled reward: -5875.56). Current reward after update: -726.25, Optimal reward -374.09
Iteration 171 took 3.47 seconds (mean sampled reward: -5640.74). Current reward after update: -534.64, Optimal reward -374.09
Iteration 172 took 3.35 seconds (mean sampled reward: -5459.14). Current reward after update: -594.14, Optimal reward -374.09
Iteration 173 took 3.35 seconds (mean sampled reward: -5391.15). Current reward after update: -619.02, Optimal reward -374.09
Iteration 174 took 3.47 seconds (mean sampled reward: -5700.59). Current reward after update: -639.25, Optimal reward -374.09
Iteration 175 took 3.40 seconds (mean sampled reward: -5842.19). Current reward after update: -686.26, Optimal reward -374.09
Iteration 176 took 3.43 seconds (mean sampled reward: -5523.45). Current reward after update: -601.98, Optimal reward -374.09
Iteration 177 took 3.51 seconds (mean sampled reward: -5451.27). Current reward after update: -842.74, Optimal reward -374.09
Iteration 178 took 3.52 seconds (mean sampled reward: -5434.07). Current reward after update: -553.20, Optimal reward -374.09
Iteration 179 took 3.43 seconds (mean sampled reward: -5770.67). Current reward after update: -634.50, Optimal reward -374.09
Iteration 180 took 3.46 seconds (mean sampled reward: -5530.07). Current reward after update: -496.60, Optimal reward -374.09
Iteration 181 took 3.41 seconds (mean sampled reward: -5453.99). Current reward after update: -587.02, Optimal reward -374.09
Iteration 182 took 3.41 seconds (mean sampled reward: -5420.91). Current reward after update: -660.20, Optimal reward -374.09
Iteration 183 took 3.41 seconds (mean sampled reward: -5457.97). Current reward after update: -602.86, Optimal reward -374.09
Iteration 184 took 3.49 seconds (mean sampled reward: -5246.35). Current reward after update: -680.43, Optimal reward -374.09
Iteration 185 took 3.49 seconds (mean sampled reward: -5266.76). Current reward after update: -714.94, Optimal reward -374.09
Iteration 186 took 3.52 seconds (mean sampled reward: -5221.58). Current reward after update: -807.20, Optimal reward -374.09
Iteration 187 took 3.55 seconds (mean sampled reward: -5057.67). Current reward after update: -654.10, Optimal reward -374.09
Iteration 188 took 3.86 seconds (mean sampled reward: -5482.36). Current reward after update: -798.91, Optimal reward -374.09
Iteration 189 took 3.72 seconds (mean sampled reward: -5474.26). Current reward after update: -1902.93, Optimal reward -374.09
Iteration 190 took 3.83 seconds (mean sampled reward: -5421.02). Current reward after update: -653.98, Optimal reward -374.09
Iteration 191 took 3.74 seconds (mean sampled reward: -6110.53). Current reward after update: -740.70, Optimal reward -374.09
Iteration 192 took 3.71 seconds (mean sampled reward: -5660.58). Current reward after update: -660.05, Optimal reward -374.09
Iteration 193 took 3.64 seconds (mean sampled reward: -5185.94). Current reward after update: -572.47, Optimal reward -374.09
Iteration 194 took 3.82 seconds (mean sampled reward: -5910.79). Current reward after update: -646.70, Optimal reward -374.09
Iteration 195 took 3.61 seconds (mean sampled reward: -5849.81). Current reward after update: -651.31, Optimal reward -374.09
Iteration 196 took 3.81 seconds (mean sampled reward: -5547.30). Current reward after update: -519.77, Optimal reward -374.09
Iteration 197 took 3.45 seconds (mean sampled reward: -5184.36). Current reward after update: -670.56, Optimal reward -374.09
Iteration 198 took 3.44 seconds (mean sampled reward: -5360.38). Current reward after update: -525.50, Optimal reward -374.09
Iteration 199 took 3.46 seconds (mean sampled reward: -5540.71). Current reward after update: -647.92, Optimal reward -374.09
Iteration 200 took 3.47 seconds (mean sampled reward: -5425.63). Current reward after update: -558.73, Optimal reward -374.09
Iteration 201 took 3.45 seconds (mean sampled reward: -5334.29). Current reward after update: -753.26, Optimal reward -374.09
Iteration 202 took 3.53 seconds (mean sampled reward: -5131.67). Current reward after update: -684.34, Optimal reward -374.09
Iteration 203 took 3.54 seconds (mean sampled reward: -5364.26). Current reward after update: -508.26, Optimal reward -374.09
Iteration 204 took 3.53 seconds (mean sampled reward: -5421.40). Current reward after update: -693.50, Optimal reward -374.09
Iteration 205 took 3.48 seconds (mean sampled reward: -5389.36). Current reward after update: -630.64, Optimal reward -374.09
Iteration 206 took 3.49 seconds (mean sampled reward: -5592.59). Current reward after update: -645.56, Optimal reward -374.09
Iteration 207 took 3.43 seconds (mean sampled reward: -5872.16). Current reward after update: -753.27, Optimal reward -374.09
Iteration 208 took 3.46 seconds (mean sampled reward: -5781.64). Current reward after update: -903.56, Optimal reward -374.09
Iteration 209 took 3.49 seconds (mean sampled reward: -5671.81). Current reward after update: -885.75, Optimal reward -374.09
Iteration 210 took 3.51 seconds (mean sampled reward: -5883.08). Current reward after update: -1128.57, Optimal reward -374.09
Iteration 211 took 3.48 seconds (mean sampled reward: -5427.89). Current reward after update: -779.88, Optimal reward -374.09
Iteration 212 took 3.75 seconds (mean sampled reward: -5566.74). Current reward after update: -772.14, Optimal reward -374.09
Iteration 213 took 3.71 seconds (mean sampled reward: -5405.65). Current reward after update: -857.34, Optimal reward -374.09
Iteration 214 took 3.54 seconds (mean sampled reward: -5396.93). Current reward after update: -687.48, Optimal reward -374.09
Iteration 215 took 3.49 seconds (mean sampled reward: -5763.41). Current reward after update: -833.63, Optimal reward -374.09
Iteration 216 took 3.53 seconds (mean sampled reward: -5702.05). Current reward after update: -643.02, Optimal reward -374.09
Iteration 217 took 3.65 seconds (mean sampled reward: -5475.11). Current reward after update: -693.15, Optimal reward -374.09
Iteration 218 took 3.58 seconds (mean sampled reward: -5846.77). Current reward after update: -927.09, Optimal reward -374.09
Iteration 219 took 3.58 seconds (mean sampled reward: -5367.91). Current reward after update: -1289.31, Optimal reward -374.09
Iteration 220 took 3.54 seconds (mean sampled reward: -5582.84). Current reward after update: -1068.17, Optimal reward -374.09
Iteration 221 took 3.59 seconds (mean sampled reward: -5436.90). Current reward after update: -886.75, Optimal reward -374.09
Iteration 222 took 3.62 seconds (mean sampled reward: -5428.91). Current reward after update: -847.66, Optimal reward -374.09
Iteration 223 took 3.68 seconds (mean sampled reward: -5468.06). Current reward after update: -759.46, Optimal reward -374.09
Iteration 224 took 3.53 seconds (mean sampled reward: -5898.61). Current reward after update: -711.49, Optimal reward -374.09
Iteration 225 took 3.51 seconds (mean sampled reward: -5458.25). Current reward after update: -744.00, Optimal reward -374.09
Iteration 226 took 3.50 seconds (mean sampled reward: -5407.98). Current reward after update: -956.96, Optimal reward -374.09
Iteration 227 took 3.52 seconds (mean sampled reward: -5532.50). Current reward after update: -944.78, Optimal reward -374.09
Iteration 228 took 3.49 seconds (mean sampled reward: -5184.30). Current reward after update: -844.15, Optimal reward -374.09
Iteration 229 took 3.71 seconds (mean sampled reward: -5409.88). Current reward after update: -719.31, Optimal reward -374.09
Iteration 230 took 3.53 seconds (mean sampled reward: -5509.36). Current reward after update: -842.85, Optimal reward -374.09
Iteration 231 took 3.47 seconds (mean sampled reward: -5836.55). Current reward after update: -746.66, Optimal reward -374.09
Iteration 232 took 3.58 seconds (mean sampled reward: -5440.00). Current reward after update: -689.59, Optimal reward -374.09
Iteration 233 took 3.55 seconds (mean sampled reward: -5543.17). Current reward after update: -707.81, Optimal reward -374.09
Iteration 234 took 3.59 seconds (mean sampled reward: -5703.02). Current reward after update: -610.76, Optimal reward -374.09
Iteration 235 took 3.63 seconds (mean sampled reward: -5341.56). Current reward after update: -958.69, Optimal reward -374.09
Iteration 236 took 3.51 seconds (mean sampled reward: -5274.04). Current reward after update: -813.81, Optimal reward -374.09
Iteration 237 took 3.62 seconds (mean sampled reward: -5548.12). Current reward after update: -672.87, Optimal reward -374.09
Iteration 238 took 3.67 seconds (mean sampled reward: -5663.82). Current reward after update: -734.81, Optimal reward -374.09
Iteration 239 took 3.58 seconds (mean sampled reward: -5578.56). Current reward after update: -645.55, Optimal reward -374.09
Iteration 240 took 3.43 seconds (mean sampled reward: -5650.91). Current reward after update: -723.95, Optimal reward -374.09
Iteration 241 took 3.43 seconds (mean sampled reward: -5750.78). Current reward after update: -479.00, Optimal reward -374.09
Iteration 242 took 3.50 seconds (mean sampled reward: -5457.22). Current reward after update: -670.78, Optimal reward -374.09
Iteration 243 took 3.50 seconds (mean sampled reward: -5602.59). Current reward after update: -565.99, Optimal reward -374.09
Iteration 244 took 3.43 seconds (mean sampled reward: -5716.30). Current reward after update: -638.66, Optimal reward -374.09
Iteration 245 took 3.53 seconds (mean sampled reward: -5518.65). Current reward after update: -837.74, Optimal reward -374.09
Iteration 246 took 3.47 seconds (mean sampled reward: -5526.53). Current reward after update: -610.75, Optimal reward -374.09
Iteration 247 took 3.58 seconds (mean sampled reward: -5604.54). Current reward after update: -472.32, Optimal reward -374.09
Iteration 248 took 3.58 seconds (mean sampled reward: -5734.90). Current reward after update: -586.94, Optimal reward -374.09
Iteration 249 took 3.57 seconds (mean sampled reward: -5917.26). Current reward after update: -754.17, Optimal reward -374.09
Iteration 250 took 3.54 seconds (mean sampled reward: -5925.35). Current reward after update: -668.91, Optimal reward -374.09
Iteration 251 took 3.58 seconds (mean sampled reward: -5602.22). Current reward after update: -727.17, Optimal reward -374.09
Iteration 252 took 3.58 seconds (mean sampled reward: -5955.96). Current reward after update: -828.34, Optimal reward -374.09
Iteration 253 took 3.53 seconds (mean sampled reward: -5889.77). Current reward after update: -722.09, Optimal reward -374.09
Iteration 254 took 3.47 seconds (mean sampled reward: -6121.02). Current reward after update: -874.24, Optimal reward -374.09
Iteration 255 took 3.47 seconds (mean sampled reward: -5440.03). Current reward after update: -863.15, Optimal reward -374.09
Iteration 256 took 3.50 seconds (mean sampled reward: -5716.70). Current reward after update: -904.73, Optimal reward -374.09
Iteration 257 took 3.44 seconds (mean sampled reward: -5731.85). Current reward after update: -791.49, Optimal reward -374.09
Iteration 258 took 3.44 seconds (mean sampled reward: -5583.53). Current reward after update: -646.41, Optimal reward -374.09
Iteration 259 took 3.42 seconds (mean sampled reward: -5675.54). Current reward after update: -747.27, Optimal reward -374.09
Iteration 260 took 3.47 seconds (mean sampled reward: -5668.36). Current reward after update: -408.28, Optimal reward -374.09
Iteration 261 took 3.42 seconds (mean sampled reward: -5795.35). Current reward after update: -688.80, Optimal reward -374.09
Iteration 262 took 3.45 seconds (mean sampled reward: -5517.06). Current reward after update: -683.36, Optimal reward -374.09
Iteration 263 took 3.45 seconds (mean sampled reward: -5723.30). Current reward after update: -545.65, Optimal reward -374.09
Iteration 264 took 3.40 seconds (mean sampled reward: -5871.16). Current reward after update: -925.12, Optimal reward -374.09
Iteration 265 took 3.43 seconds (mean sampled reward: -6164.39). Current reward after update: -892.59, Optimal reward -374.09
Iteration 266 took 3.36 seconds (mean sampled reward: -5944.28). Current reward after update: -791.56, Optimal reward -374.09
Iteration 267 took 3.39 seconds (mean sampled reward: -6082.64). Current reward after update: -775.84, Optimal reward -374.09
Iteration 268 took 3.39 seconds (mean sampled reward: -6218.87). Current reward after update: -649.08, Optimal reward -374.09
Iteration 269 took 3.40 seconds (mean sampled reward: -6029.95). Current reward after update: -778.69, Optimal reward -374.09
Iteration 270 took 3.40 seconds (mean sampled reward: -6185.05). Current reward after update: -600.30, Optimal reward -374.09
Iteration 271 took 3.40 seconds (mean sampled reward: -5981.81). Current reward after update: -735.28, Optimal reward -374.09
Iteration 272 took 3.50 seconds (mean sampled reward: -6132.43). Current reward after update: -940.98, Optimal reward -374.09
Iteration 273 took 3.44 seconds (mean sampled reward: -5923.76). Current reward after update: -683.26, Optimal reward -374.09
Iteration 274 took 3.46 seconds (mean sampled reward: -6144.63). Current reward after update: -816.37, Optimal reward -374.09
Iteration 275 took 3.43 seconds (mean sampled reward: -6006.09). Current reward after update: -760.77, Optimal reward -374.09
Iteration 276 took 3.38 seconds (mean sampled reward: -5899.49). Current reward after update: -832.96, Optimal reward -374.09
Iteration 277 took 3.40 seconds (mean sampled reward: -5757.37). Current reward after update: -699.04, Optimal reward -374.09
Iteration 278 took 3.39 seconds (mean sampled reward: -6065.56). Current reward after update: -750.42, Optimal reward -374.09
Iteration 279 took 3.31 seconds (mean sampled reward: -5593.21). Current reward after update: -861.32, Optimal reward -374.09
Iteration 280 took 3.34 seconds (mean sampled reward: -5550.85). Current reward after update: -837.77, Optimal reward -374.09
Iteration 281 took 3.36 seconds (mean sampled reward: -5861.51). Current reward after update: -811.30, Optimal reward -374.09
Iteration 282 took 3.39 seconds (mean sampled reward: -5798.72). Current reward after update: -792.43, Optimal reward -374.09
Iteration 283 took 3.44 seconds (mean sampled reward: -5993.50). Current reward after update: -635.22, Optimal reward -374.09
Iteration 284 took 3.40 seconds (mean sampled reward: -6097.08). Current reward after update: -752.68, Optimal reward -374.09
Iteration 285 took 3.44 seconds (mean sampled reward: -5869.29). Current reward after update: -801.77, Optimal reward -374.09
Iteration 286 took 3.43 seconds (mean sampled reward: -6081.79). Current reward after update: -962.21, Optimal reward -374.09
Iteration 287 took 3.45 seconds (mean sampled reward: -5877.85). Current reward after update: -819.57, Optimal reward -374.09
Iteration 288 took 3.36 seconds (mean sampled reward: -6075.58). Current reward after update: -672.06, Optimal reward -374.09
Iteration 289 took 3.42 seconds (mean sampled reward: -5905.80). Current reward after update: -696.31, Optimal reward -374.09
Iteration 290 took 3.44 seconds (mean sampled reward: -5909.14). Current reward after update: -663.22, Optimal reward -374.09
Iteration 291 took 3.45 seconds (mean sampled reward: -6082.31). Current reward after update: -805.32, Optimal reward -374.09
Iteration 292 took 3.46 seconds (mean sampled reward: -5901.40). Current reward after update: -659.24, Optimal reward -374.09
Iteration 293 took 3.46 seconds (mean sampled reward: -5902.74). Current reward after update: -923.34, Optimal reward -374.09
Iteration 294 took 3.40 seconds (mean sampled reward: -5870.30). Current reward after update: -687.08, Optimal reward -374.09
Iteration 295 took 3.42 seconds (mean sampled reward: -5919.72). Current reward after update: -593.53, Optimal reward -374.09
Iteration 296 took 3.43 seconds (mean sampled reward: -6118.90). Current reward after update: -792.25, Optimal reward -374.09
Iteration 297 took 3.38 seconds (mean sampled reward: -5829.49). Current reward after update: -701.82, Optimal reward -374.09
Iteration 298 took 3.42 seconds (mean sampled reward: -6527.27). Current reward after update: -473.62, Optimal reward -374.09
Iteration 299 took 3.46 seconds (mean sampled reward: -6404.92). Current reward after update: -1040.76, Optimal reward -374.09
Iteration 300 took 3.43 seconds (mean sampled reward: -6150.46). Current reward after update: -681.26, Optimal reward -374.09
Iteration 1 took 3.85 seconds (mean sampled reward: -7503.73). Current reward after update: -4207.34, Optimal reward -4207.34
Iteration 2 took 3.68 seconds (mean sampled reward: -6845.75). Current reward after update: -2216.46, Optimal reward -2216.46
Iteration 3 took 3.66 seconds (mean sampled reward: -6104.54). Current reward after update: -2148.02, Optimal reward -2148.02
Iteration 4 took 3.58 seconds (mean sampled reward: -5575.72). Current reward after update: -1341.06, Optimal reward -1341.06
Iteration 5 took 3.57 seconds (mean sampled reward: -4877.17). Current reward after update: -1490.22, Optimal reward -1341.06
Iteration 6 took 3.66 seconds (mean sampled reward: -5454.77). Current reward after update: -848.73, Optimal reward -848.73
Iteration 7 took 3.53 seconds (mean sampled reward: -5620.76). Current reward after update: -1160.24, Optimal reward -848.73
Iteration 8 took 3.58 seconds (mean sampled reward: -5405.52). Current reward after update: -787.68, Optimal reward -787.68
Iteration 9 took 3.66 seconds (mean sampled reward: -5200.35). Current reward after update: -698.50, Optimal reward -698.50
Iteration 10 took 3.68 seconds (mean sampled reward: -5529.29). Current reward after update: -978.44, Optimal reward -698.50
Iteration 11 took 3.56 seconds (mean sampled reward: -3605.98). Current reward after update: -691.73, Optimal reward -691.73
Iteration 12 took 3.50 seconds (mean sampled reward: -4100.54). Current reward after update: -557.77, Optimal reward -557.77
Iteration 13 took 3.85 seconds (mean sampled reward: -4381.63). Current reward after update: -685.05, Optimal reward -557.77
Iteration 14 took 3.70 seconds (mean sampled reward: -4538.87). Current reward after update: -754.84, Optimal reward -557.77
Iteration 15 took 3.66 seconds (mean sampled reward: -3216.27). Current reward after update: -745.95, Optimal reward -557.77
Iteration 16 took 3.47 seconds (mean sampled reward: -3335.79). Current reward after update: -384.51, Optimal reward -384.51
Iteration 17 took 3.78 seconds (mean sampled reward: -4230.64). Current reward after update: -498.69, Optimal reward -384.51
Iteration 18 took 3.45 seconds (mean sampled reward: -3170.06). Current reward after update: -400.02, Optimal reward -384.51
Iteration 19 took 3.42 seconds (mean sampled reward: -2724.71). Current reward after update: -444.75, Optimal reward -384.51
Iteration 20 took 3.52 seconds (mean sampled reward: -4079.30). Current reward after update: -487.29, Optimal reward -384.51
Iteration 21 took 3.54 seconds (mean sampled reward: -4204.44). Current reward after update: -582.98, Optimal reward -384.51
Iteration 22 took 3.48 seconds (mean sampled reward: -4137.69). Current reward after update: -542.09, Optimal reward -384.51
Iteration 23 took 3.45 seconds (mean sampled reward: -3650.11). Current reward after update: -648.73, Optimal reward -384.51
Iteration 24 took 3.50 seconds (mean sampled reward: -3864.22). Current reward after update: -596.72, Optimal reward -384.51
Iteration 25 took 3.59 seconds (mean sampled reward: -4184.39). Current reward after update: -831.73, Optimal reward -384.51
Iteration 26 took 3.62 seconds (mean sampled reward: -4307.49). Current reward after update: -610.05, Optimal reward -384.51
Iteration 27 took 3.67 seconds (mean sampled reward: -4786.58). Current reward after update: -485.51, Optimal reward -384.51
Iteration 28 took 3.89 seconds (mean sampled reward: -5532.95). Current reward after update: -437.00, Optimal reward -384.51
Iteration 29 took 3.73 seconds (mean sampled reward: -4868.98). Current reward after update: -576.70, Optimal reward -384.51
Iteration 30 took 3.97 seconds (mean sampled reward: -5381.70). Current reward after update: -544.31, Optimal reward -384.51
Iteration 31 took 3.73 seconds (mean sampled reward: -5705.81). Current reward after update: -609.04, Optimal reward -384.51
Iteration 32 took 3.61 seconds (mean sampled reward: -5217.80). Current reward after update: -818.14, Optimal reward -384.51
Iteration 33 took 3.96 seconds (mean sampled reward: -5273.46). Current reward after update: -578.64, Optimal reward -384.51
Iteration 34 took 3.58 seconds (mean sampled reward: -4296.59). Current reward after update: -494.69, Optimal reward -384.51
Iteration 35 took 3.71 seconds (mean sampled reward: -3068.48). Current reward after update: -517.40, Optimal reward -384.51
Iteration 36 took 3.41 seconds (mean sampled reward: -3838.31). Current reward after update: -724.28, Optimal reward -384.51
Iteration 37 took 3.68 seconds (mean sampled reward: -4783.66). Current reward after update: -566.73, Optimal reward -384.51
Iteration 38 took 3.48 seconds (mean sampled reward: -4403.30). Current reward after update: -676.30, Optimal reward -384.51
Iteration 39 took 3.43 seconds (mean sampled reward: -3982.55). Current reward after update: -788.14, Optimal reward -384.51
Iteration 40 took 3.39 seconds (mean sampled reward: -4106.35). Current reward after update: -727.33, Optimal reward -384.51
Iteration 41 took 3.42 seconds (mean sampled reward: -4515.85). Current reward after update: -736.43, Optimal reward -384.51
Iteration 42 took 3.49 seconds (mean sampled reward: -4713.99). Current reward after update: -838.04, Optimal reward -384.51
Iteration 43 took 3.57 seconds (mean sampled reward: -4577.87). Current reward after update: -763.21, Optimal reward -384.51
Iteration 44 took 3.39 seconds (mean sampled reward: -4124.12). Current reward after update: -664.20, Optimal reward -384.51
Iteration 45 took 3.54 seconds (mean sampled reward: -4402.35). Current reward after update: -859.40, Optimal reward -384.51
Iteration 46 took 3.47 seconds (mean sampled reward: -4297.84). Current reward after update: -574.45, Optimal reward -384.51
Iteration 47 took 3.51 seconds (mean sampled reward: -3238.26). Current reward after update: -682.39, Optimal reward -384.51
Iteration 48 took 3.31 seconds (mean sampled reward: -3032.28). Current reward after update: -574.93, Optimal reward -384.51
Iteration 49 took 3.36 seconds (mean sampled reward: -2884.71). Current reward after update: -604.29, Optimal reward -384.51
Iteration 50 took 3.33 seconds (mean sampled reward: -2837.76). Current reward after update: -634.02, Optimal reward -384.51
Iteration 51 took 3.35 seconds (mean sampled reward: -3191.50). Current reward after update: -905.60, Optimal reward -384.51
Iteration 52 took 3.45 seconds (mean sampled reward: -4719.28). Current reward after update: -1005.41, Optimal reward -384.51
Iteration 53 took 3.54 seconds (mean sampled reward: -4118.48). Current reward after update: -761.52, Optimal reward -384.51
Iteration 54 took 3.47 seconds (mean sampled reward: -3351.25). Current reward after update: -838.01, Optimal reward -384.51
Iteration 55 took 3.57 seconds (mean sampled reward: -3139.35). Current reward after update: -525.42, Optimal reward -384.51
Iteration 56 took 3.71 seconds (mean sampled reward: -4108.56). Current reward after update: -658.22, Optimal reward -384.51
Iteration 57 took 3.45 seconds (mean sampled reward: -4421.28). Current reward after update: -688.74, Optimal reward -384.51
Iteration 58 took 3.50 seconds (mean sampled reward: -3918.23). Current reward after update: -770.82, Optimal reward -384.51
Iteration 59 took 3.43 seconds (mean sampled reward: -3198.10). Current reward after update: -638.13, Optimal reward -384.51
Iteration 60 took 3.46 seconds (mean sampled reward: -3249.58). Current reward after update: -735.04, Optimal reward -384.51
Iteration 61 took 3.43 seconds (mean sampled reward: -3685.60). Current reward after update: -671.31, Optimal reward -384.51
Iteration 62 took 3.32 seconds (mean sampled reward: -3189.20). Current reward after update: -643.20, Optimal reward -384.51
Iteration 63 took 3.30 seconds (mean sampled reward: -3348.26). Current reward after update: -563.35, Optimal reward -384.51
Iteration 64 took 3.36 seconds (mean sampled reward: -3399.55). Current reward after update: -556.35, Optimal reward -384.51
Iteration 65 took 3.42 seconds (mean sampled reward: -3633.24). Current reward after update: -665.28, Optimal reward -384.51
Iteration 66 took 3.31 seconds (mean sampled reward: -3496.69). Current reward after update: -543.79, Optimal reward -384.51
Iteration 67 took 3.45 seconds (mean sampled reward: -3754.60). Current reward after update: -573.65, Optimal reward -384.51
Iteration 68 took 3.35 seconds (mean sampled reward: -3318.83). Current reward after update: -590.20, Optimal reward -384.51
Iteration 69 took 3.51 seconds (mean sampled reward: -4474.00). Current reward after update: -555.28, Optimal reward -384.51
Iteration 70 took 3.45 seconds (mean sampled reward: -3896.30). Current reward after update: -773.83, Optimal reward -384.51
Iteration 71 took 3.44 seconds (mean sampled reward: -4178.81). Current reward after update: -795.65, Optimal reward -384.51
Iteration 72 took 3.52 seconds (mean sampled reward: -5134.56). Current reward after update: -730.78, Optimal reward -384.51
Iteration 73 took 3.57 seconds (mean sampled reward: -5481.92). Current reward after update: -1073.17, Optimal reward -384.51
Iteration 74 took 3.55 seconds (mean sampled reward: -4423.00). Current reward after update: -498.14, Optimal reward -384.51
Iteration 75 took 3.53 seconds (mean sampled reward: -4112.99). Current reward after update: -780.65, Optimal reward -384.51
Iteration 76 took 3.50 seconds (mean sampled reward: -3771.65). Current reward after update: -743.80, Optimal reward -384.51
Iteration 77 took 3.58 seconds (mean sampled reward: -4872.81). Current reward after update: -726.96, Optimal reward -384.51
Iteration 78 took 3.50 seconds (mean sampled reward: -4979.06). Current reward after update: -688.05, Optimal reward -384.51
Iteration 79 took 3.50 seconds (mean sampled reward: -4113.83). Current reward after update: -638.22, Optimal reward -384.51
Iteration 80 took 3.53 seconds (mean sampled reward: -3932.14). Current reward after update: -664.57, Optimal reward -384.51
Iteration 81 took 3.50 seconds (mean sampled reward: -5375.58). Current reward after update: -749.73, Optimal reward -384.51
Iteration 82 took 3.58 seconds (mean sampled reward: -5650.08). Current reward after update: -634.91, Optimal reward -384.51
Iteration 83 took 3.56 seconds (mean sampled reward: -5089.92). Current reward after update: -732.87, Optimal reward -384.51
Iteration 84 took 3.63 seconds (mean sampled reward: -6026.64). Current reward after update: -835.26, Optimal reward -384.51
Iteration 85 took 3.51 seconds (mean sampled reward: -4937.03). Current reward after update: -762.50, Optimal reward -384.51
Iteration 86 took 3.46 seconds (mean sampled reward: -3865.94). Current reward after update: -701.87, Optimal reward -384.51
Iteration 87 took 3.48 seconds (mean sampled reward: -3980.96). Current reward after update: -627.94, Optimal reward -384.51
Iteration 88 took 3.40 seconds (mean sampled reward: -3832.46). Current reward after update: -667.57, Optimal reward -384.51
Iteration 89 took 3.43 seconds (mean sampled reward: -3085.59). Current reward after update: -626.74, Optimal reward -384.51
Iteration 90 took 3.49 seconds (mean sampled reward: -3197.63). Current reward after update: -607.85, Optimal reward -384.51
Iteration 91 took 3.51 seconds (mean sampled reward: -3179.60). Current reward after update: -589.66, Optimal reward -384.51
Iteration 92 took 3.52 seconds (mean sampled reward: -3505.30). Current reward after update: -594.02, Optimal reward -384.51
Iteration 93 took 3.54 seconds (mean sampled reward: -4280.31). Current reward after update: -701.94, Optimal reward -384.51
Iteration 94 took 3.55 seconds (mean sampled reward: -4577.85). Current reward after update: -672.50, Optimal reward -384.51
Iteration 95 took 3.57 seconds (mean sampled reward: -5138.01). Current reward after update: -747.87, Optimal reward -384.51
Iteration 96 took 3.57 seconds (mean sampled reward: -4390.69). Current reward after update: -628.17, Optimal reward -384.51
Iteration 97 took 3.53 seconds (mean sampled reward: -3793.03). Current reward after update: -635.13, Optimal reward -384.51
Iteration 98 took 3.58 seconds (mean sampled reward: -3981.81). Current reward after update: -669.06, Optimal reward -384.51
Iteration 99 took 3.57 seconds (mean sampled reward: -3677.45). Current reward after update: -672.49, Optimal reward -384.51
Iteration 100 took 3.50 seconds (mean sampled reward: -3227.47). Current reward after update: -731.47, Optimal reward -384.51
Iteration 101 took 3.56 seconds (mean sampled reward: -4188.10). Current reward after update: -825.83, Optimal reward -384.51
Iteration 102 took 3.61 seconds (mean sampled reward: -4140.73). Current reward after update: -960.38, Optimal reward -384.51
Iteration 103 took 3.55 seconds (mean sampled reward: -3886.41). Current reward after update: -661.89, Optimal reward -384.51
Iteration 104 took 3.54 seconds (mean sampled reward: -3880.01). Current reward after update: -660.62, Optimal reward -384.51
Iteration 105 took 3.72 seconds (mean sampled reward: -4808.78). Current reward after update: -666.23, Optimal reward -384.51
Iteration 106 took 3.73 seconds (mean sampled reward: -4664.03). Current reward after update: -659.08, Optimal reward -384.51
Iteration 107 took 3.59 seconds (mean sampled reward: -3758.13). Current reward after update: -782.29, Optimal reward -384.51
Iteration 108 took 3.79 seconds (mean sampled reward: -4596.96). Current reward after update: -717.18, Optimal reward -384.51
Iteration 109 took 3.88 seconds (mean sampled reward: -5645.44). Current reward after update: -1043.57, Optimal reward -384.51
Iteration 110 took 3.64 seconds (mean sampled reward: -5284.23). Current reward after update: -956.09, Optimal reward -384.51
Iteration 111 took 3.63 seconds (mean sampled reward: -4860.32). Current reward after update: -886.50, Optimal reward -384.51
Iteration 112 took 3.63 seconds (mean sampled reward: -4497.53). Current reward after update: -851.80, Optimal reward -384.51
Iteration 113 took 3.70 seconds (mean sampled reward: -4775.21). Current reward after update: -587.08, Optimal reward -384.51
Iteration 114 took 3.63 seconds (mean sampled reward: -5146.21). Current reward after update: -733.96, Optimal reward -384.51
Iteration 115 took 3.67 seconds (mean sampled reward: -5903.59). Current reward after update: -774.66, Optimal reward -384.51
Iteration 116 took 3.55 seconds (mean sampled reward: -4084.53). Current reward after update: -745.68, Optimal reward -384.51
Iteration 117 took 3.67 seconds (mean sampled reward: -4846.87). Current reward after update: -890.29, Optimal reward -384.51
Iteration 118 took 3.45 seconds (mean sampled reward: -3899.05). Current reward after update: -723.04, Optimal reward -384.51
Iteration 119 took 3.49 seconds (mean sampled reward: -3527.17). Current reward after update: -806.05, Optimal reward -384.51
Iteration 120 took 3.52 seconds (mean sampled reward: -4189.51). Current reward after update: -534.96, Optimal reward -384.51
Iteration 121 took 3.57 seconds (mean sampled reward: -4389.81). Current reward after update: -726.08, Optimal reward -384.51
Iteration 122 took 3.58 seconds (mean sampled reward: -5578.32). Current reward after update: -661.29, Optimal reward -384.51
Iteration 123 took 3.61 seconds (mean sampled reward: -5137.35). Current reward after update: -654.65, Optimal reward -384.51
Iteration 124 took 3.57 seconds (mean sampled reward: -5270.42). Current reward after update: -641.65, Optimal reward -384.51
Iteration 125 took 3.65 seconds (mean sampled reward: -5674.50). Current reward after update: -973.61, Optimal reward -384.51
Iteration 126 took 3.61 seconds (mean sampled reward: -5402.50). Current reward after update: -839.82, Optimal reward -384.51
Iteration 127 took 3.67 seconds (mean sampled reward: -5584.81). Current reward after update: -550.90, Optimal reward -384.51
Iteration 128 took 3.63 seconds (mean sampled reward: -5118.74). Current reward after update: -653.48, Optimal reward -384.51
Iteration 129 took 3.59 seconds (mean sampled reward: -4429.92). Current reward after update: -593.69, Optimal reward -384.51
Iteration 130 took 3.64 seconds (mean sampled reward: -4808.81). Current reward after update: -697.67, Optimal reward -384.51
Iteration 131 took 3.70 seconds (mean sampled reward: -5086.45). Current reward after update: -622.28, Optimal reward -384.51
Iteration 132 took 3.66 seconds (mean sampled reward: -5346.25). Current reward after update: -789.14, Optimal reward -384.51
Iteration 133 took 3.61 seconds (mean sampled reward: -4492.70). Current reward after update: -631.30, Optimal reward -384.51
Iteration 134 took 3.64 seconds (mean sampled reward: -4590.98). Current reward after update: -640.97, Optimal reward -384.51
Iteration 135 took 3.65 seconds (mean sampled reward: -4792.84). Current reward after update: -647.53, Optimal reward -384.51
Iteration 136 took 3.55 seconds (mean sampled reward: -4866.99). Current reward after update: -585.16, Optimal reward -384.51
Iteration 137 took 3.56 seconds (mean sampled reward: -4726.11). Current reward after update: -689.30, Optimal reward -384.51
Iteration 138 took 3.54 seconds (mean sampled reward: -3223.98). Current reward after update: -595.86, Optimal reward -384.51
Iteration 139 took 3.50 seconds (mean sampled reward: -2965.64). Current reward after update: -618.91, Optimal reward -384.51
Iteration 140 took 3.59 seconds (mean sampled reward: -3021.78). Current reward after update: -537.55, Optimal reward -384.51
Iteration 141 took 3.49 seconds (mean sampled reward: -3114.13). Current reward after update: -644.01, Optimal reward -384.51
Iteration 142 took 3.57 seconds (mean sampled reward: -3502.99). Current reward after update: -547.57, Optimal reward -384.51
Iteration 143 took 3.59 seconds (mean sampled reward: -3772.30). Current reward after update: -627.69, Optimal reward -384.51
Iteration 144 took 3.51 seconds (mean sampled reward: -3265.51). Current reward after update: -713.55, Optimal reward -384.51
Iteration 145 took 3.54 seconds (mean sampled reward: -3096.09). Current reward after update: -617.82, Optimal reward -384.51
Iteration 146 took 3.56 seconds (mean sampled reward: -3728.92). Current reward after update: -621.89, Optimal reward -384.51
Iteration 147 took 3.51 seconds (mean sampled reward: -3320.79). Current reward after update: -539.55, Optimal reward -384.51
Iteration 148 took 3.51 seconds (mean sampled reward: -3499.73). Current reward after update: -655.91, Optimal reward -384.51
Iteration 149 took 3.57 seconds (mean sampled reward: -3641.32). Current reward after update: -720.70, Optimal reward -384.51
Iteration 150 took 3.54 seconds (mean sampled reward: -3402.69). Current reward after update: -464.62, Optimal reward -384.51
Iteration 151 took 3.49 seconds (mean sampled reward: -3329.81). Current reward after update: -514.86, Optimal reward -384.51
Iteration 152 took 3.57 seconds (mean sampled reward: -3477.21). Current reward after update: -667.48, Optimal reward -384.51
Iteration 153 took 3.48 seconds (mean sampled reward: -3519.00). Current reward after update: -615.51, Optimal reward -384.51
Iteration 154 took 3.44 seconds (mean sampled reward: -3679.81). Current reward after update: -643.55, Optimal reward -384.51
Iteration 155 took 3.58 seconds (mean sampled reward: -3668.21). Current reward after update: -612.13, Optimal reward -384.51
Iteration 156 took 3.61 seconds (mean sampled reward: -4924.75). Current reward after update: -662.97, Optimal reward -384.51
Iteration 157 took 3.60 seconds (mean sampled reward: -4778.95). Current reward after update: -727.17, Optimal reward -384.51
Iteration 158 took 3.73 seconds (mean sampled reward: -4376.56). Current reward after update: -619.08, Optimal reward -384.51
Iteration 159 took 3.59 seconds (mean sampled reward: -4360.82). Current reward after update: -734.21, Optimal reward -384.51
Iteration 160 took 3.68 seconds (mean sampled reward: -4575.26). Current reward after update: -589.90, Optimal reward -384.51
Iteration 161 took 3.60 seconds (mean sampled reward: -3930.88). Current reward after update: -778.53, Optimal reward -384.51
Iteration 162 took 3.67 seconds (mean sampled reward: -5074.83). Current reward after update: -647.69, Optimal reward -384.51
Iteration 163 took 3.62 seconds (mean sampled reward: -4572.21). Current reward after update: -548.77, Optimal reward -384.51
Iteration 164 took 3.63 seconds (mean sampled reward: -5108.43). Current reward after update: -719.30, Optimal reward -384.51
Iteration 165 took 3.66 seconds (mean sampled reward: -4991.18). Current reward after update: -724.32, Optimal reward -384.51
Iteration 166 took 3.62 seconds (mean sampled reward: -4565.22). Current reward after update: -810.55, Optimal reward -384.51
Iteration 167 took 3.66 seconds (mean sampled reward: -4773.01). Current reward after update: -815.37, Optimal reward -384.51
Iteration 168 took 3.69 seconds (mean sampled reward: -5369.97). Current reward after update: -758.73, Optimal reward -384.51
Iteration 169 took 3.63 seconds (mean sampled reward: -5243.22). Current reward after update: -583.90, Optimal reward -384.51
Iteration 170 took 3.64 seconds (mean sampled reward: -4999.13). Current reward after update: -700.67, Optimal reward -384.51
Iteration 171 took 3.64 seconds (mean sampled reward: -5200.53). Current reward after update: -854.63, Optimal reward -384.51
Iteration 172 took 3.72 seconds (mean sampled reward: -6230.29). Current reward after update: -870.69, Optimal reward -384.51
Iteration 173 took 3.70 seconds (mean sampled reward: -5716.54). Current reward after update: -544.03, Optimal reward -384.51
Iteration 174 took 3.65 seconds (mean sampled reward: -5403.01). Current reward after update: -634.20, Optimal reward -384.51
Iteration 175 took 3.68 seconds (mean sampled reward: -4355.20). Current reward after update: -692.41, Optimal reward -384.51
Iteration 176 took 3.65 seconds (mean sampled reward: -5324.54). Current reward after update: -651.99, Optimal reward -384.51
Iteration 177 took 3.62 seconds (mean sampled reward: -4701.87). Current reward after update: -708.73, Optimal reward -384.51
Iteration 178 took 3.64 seconds (mean sampled reward: -4795.58). Current reward after update: -681.99, Optimal reward -384.51
Iteration 179 took 3.66 seconds (mean sampled reward: -4427.97). Current reward after update: -645.55, Optimal reward -384.51
Iteration 180 took 3.65 seconds (mean sampled reward: -5002.30). Current reward after update: -781.03, Optimal reward -384.51
Iteration 181 took 3.64 seconds (mean sampled reward: -4703.00). Current reward after update: -606.91, Optimal reward -384.51
Iteration 182 took 3.63 seconds (mean sampled reward: -4303.83). Current reward after update: -601.95, Optimal reward -384.51
Iteration 183 took 3.58 seconds (mean sampled reward: -3410.94). Current reward after update: -544.99, Optimal reward -384.51
Iteration 184 took 3.56 seconds (mean sampled reward: -3706.15). Current reward after update: -505.76, Optimal reward -384.51
Iteration 185 took 3.53 seconds (mean sampled reward: -2860.05). Current reward after update: -537.83, Optimal reward -384.51
Iteration 186 took 3.51 seconds (mean sampled reward: -2938.59). Current reward after update: -486.10, Optimal reward -384.51
Iteration 187 took 3.37 seconds (mean sampled reward: -2923.68). Current reward after update: -479.36, Optimal reward -384.51
Iteration 188 took 3.42 seconds (mean sampled reward: -2839.30). Current reward after update: -564.97, Optimal reward -384.51
Iteration 189 took 3.29 seconds (mean sampled reward: -2800.06). Current reward after update: -459.15, Optimal reward -384.51
Iteration 190 took 3.33 seconds (mean sampled reward: -2895.04). Current reward after update: -525.17, Optimal reward -384.51
Iteration 191 took 3.32 seconds (mean sampled reward: -2920.74). Current reward after update: -495.03, Optimal reward -384.51
Iteration 192 took 3.34 seconds (mean sampled reward: -2899.21). Current reward after update: -443.43, Optimal reward -384.51
Iteration 193 took 3.46 seconds (mean sampled reward: -3153.02). Current reward after update: -561.89, Optimal reward -384.51
Iteration 194 took 3.54 seconds (mean sampled reward: -3516.23). Current reward after update: -545.86, Optimal reward -384.51
Iteration 195 took 3.56 seconds (mean sampled reward: -3770.30). Current reward after update: -597.21, Optimal reward -384.51
Iteration 196 took 3.55 seconds (mean sampled reward: -3574.32). Current reward after update: -534.64, Optimal reward -384.51
Iteration 197 took 3.54 seconds (mean sampled reward: -3606.45). Current reward after update: -640.04, Optimal reward -384.51
Iteration 198 took 3.70 seconds (mean sampled reward: -4686.89). Current reward after update: -548.01, Optimal reward -384.51
Iteration 199 took 3.70 seconds (mean sampled reward: -5336.25). Current reward after update: -733.34, Optimal reward -384.51
Iteration 200 took 3.70 seconds (mean sampled reward: -4517.47). Current reward after update: -561.64, Optimal reward -384.51
Iteration 201 took 3.59 seconds (mean sampled reward: -5000.51). Current reward after update: -557.68, Optimal reward -384.51
Iteration 202 took 3.47 seconds (mean sampled reward: -3718.10). Current reward after update: -602.76, Optimal reward -384.51
Iteration 203 took 3.66 seconds (mean sampled reward: -3591.47). Current reward after update: -461.28, Optimal reward -384.51
Iteration 204 took 3.61 seconds (mean sampled reward: -3668.48). Current reward after update: -644.92, Optimal reward -384.51
Iteration 205 took 3.62 seconds (mean sampled reward: -3966.20). Current reward after update: -588.31, Optimal reward -384.51
Iteration 206 took 3.58 seconds (mean sampled reward: -3679.85). Current reward after update: -493.60, Optimal reward -384.51
Iteration 207 took 3.55 seconds (mean sampled reward: -3291.45). Current reward after update: -473.11, Optimal reward -384.51
Iteration 208 took 3.60 seconds (mean sampled reward: -4010.91). Current reward after update: -554.35, Optimal reward -384.51
Iteration 209 took 3.60 seconds (mean sampled reward: -3707.61). Current reward after update: -640.30, Optimal reward -384.51
Iteration 210 took 3.60 seconds (mean sampled reward: -3905.35). Current reward after update: -471.66, Optimal reward -384.51
Iteration 211 took 3.57 seconds (mean sampled reward: -3277.96). Current reward after update: -531.71, Optimal reward -384.51
Iteration 212 took 3.92 seconds (mean sampled reward: -3677.68). Current reward after update: -692.67, Optimal reward -384.51
Iteration 213 took 3.66 seconds (mean sampled reward: -3471.95). Current reward after update: -559.05, Optimal reward -384.51
Iteration 214 took 4.02 seconds (mean sampled reward: -3598.54). Current reward after update: -606.19, Optimal reward -384.51
Iteration 215 took 3.53 seconds (mean sampled reward: -3063.08). Current reward after update: -575.62, Optimal reward -384.51
Iteration 216 took 3.55 seconds (mean sampled reward: -3333.26). Current reward after update: -583.14, Optimal reward -384.51
Iteration 217 took 3.59 seconds (mean sampled reward: -3303.09). Current reward after update: -592.24, Optimal reward -384.51
Iteration 218 took 3.52 seconds (mean sampled reward: -3038.11). Current reward after update: -604.13, Optimal reward -384.51
Iteration 219 took 3.58 seconds (mean sampled reward: -3975.43). Current reward after update: -585.26, Optimal reward -384.51
Iteration 220 took 3.57 seconds (mean sampled reward: -3592.98). Current reward after update: -542.35, Optimal reward -384.51
Iteration 221 took 3.54 seconds (mean sampled reward: -3655.25). Current reward after update: -620.06, Optimal reward -384.51
Iteration 222 took 3.67 seconds (mean sampled reward: -2931.09). Current reward after update: -561.84, Optimal reward -384.51
Iteration 223 took 3.74 seconds (mean sampled reward: -3257.41). Current reward after update: -550.62, Optimal reward -384.51
Iteration 224 took 3.60 seconds (mean sampled reward: -3240.35). Current reward after update: -572.31, Optimal reward -384.51
Iteration 225 took 3.76 seconds (mean sampled reward: -4318.04). Current reward after update: -562.42, Optimal reward -384.51
Iteration 226 took 3.58 seconds (mean sampled reward: -4978.55). Current reward after update: -665.07, Optimal reward -384.51
Iteration 227 took 3.79 seconds (mean sampled reward: -5884.42). Current reward after update: -557.11, Optimal reward -384.51
Iteration 228 took 3.64 seconds (mean sampled reward: -3705.87). Current reward after update: -460.13, Optimal reward -384.51
Iteration 229 took 3.58 seconds (mean sampled reward: -3476.49). Current reward after update: -548.53, Optimal reward -384.51
Iteration 230 took 3.72 seconds (mean sampled reward: -3938.77). Current reward after update: -495.07, Optimal reward -384.51
Iteration 231 took 3.60 seconds (mean sampled reward: -4909.87). Current reward after update: -516.09, Optimal reward -384.51
Iteration 232 took 3.78 seconds (mean sampled reward: -3888.81). Current reward after update: -351.99, Optimal reward -351.99
Iteration 233 took 3.66 seconds (mean sampled reward: -3695.15). Current reward after update: -575.19, Optimal reward -351.99
Iteration 234 took 3.54 seconds (mean sampled reward: -3475.55). Current reward after update: -1132.94, Optimal reward -351.99
Iteration 235 took 3.71 seconds (mean sampled reward: -4714.62). Current reward after update: -745.92, Optimal reward -351.99
Iteration 236 took 3.58 seconds (mean sampled reward: -3391.27). Current reward after update: -644.63, Optimal reward -351.99
Iteration 237 took 3.54 seconds (mean sampled reward: -3634.72). Current reward after update: -508.19, Optimal reward -351.99
Iteration 238 took 3.53 seconds (mean sampled reward: -3251.97). Current reward after update: -601.59, Optimal reward -351.99
Iteration 239 took 3.51 seconds (mean sampled reward: -3435.79). Current reward after update: -564.67, Optimal reward -351.99
Iteration 240 took 3.64 seconds (mean sampled reward: -3896.96). Current reward after update: -624.69, Optimal reward -351.99
Iteration 241 took 3.57 seconds (mean sampled reward: -2968.67). Current reward after update: -606.22, Optimal reward -351.99
Iteration 242 took 3.64 seconds (mean sampled reward: -5379.37). Current reward after update: -7034.09, Optimal reward -351.99
Iteration 243 took 3.65 seconds (mean sampled reward: -5276.08). Current reward after update: -590.07, Optimal reward -351.99
Iteration 244 took 3.69 seconds (mean sampled reward: -5844.52). Current reward after update: -608.42, Optimal reward -351.99
Iteration 245 took 3.74 seconds (mean sampled reward: -5717.79). Current reward after update: -569.50, Optimal reward -351.99
Iteration 246 took 3.57 seconds (mean sampled reward: -4084.67). Current reward after update: -475.40, Optimal reward -351.99
Iteration 247 took 3.74 seconds (mean sampled reward: -4578.15). Current reward after update: -601.16, Optimal reward -351.99
Iteration 248 took 3.52 seconds (mean sampled reward: -3730.88). Current reward after update: -463.61, Optimal reward -351.99
Iteration 249 took 3.50 seconds (mean sampled reward: -2653.69). Current reward after update: -544.22, Optimal reward -351.99
Iteration 250 took 3.57 seconds (mean sampled reward: -2510.59). Current reward after update: -584.16, Optimal reward -351.99
Iteration 251 took 3.50 seconds (mean sampled reward: -2706.59). Current reward after update: -456.74, Optimal reward -351.99
Iteration 252 took 3.49 seconds (mean sampled reward: -2942.06). Current reward after update: -368.25, Optimal reward -351.99
Iteration 253 took 3.58 seconds (mean sampled reward: -2527.11). Current reward after update: -546.15, Optimal reward -351.99
Iteration 254 took 3.32 seconds (mean sampled reward: -2143.32). Current reward after update: -554.69, Optimal reward -351.99
Iteration 255 took 3.36 seconds (mean sampled reward: -2445.62). Current reward after update: -623.99, Optimal reward -351.99
Iteration 256 took 3.32 seconds (mean sampled reward: -2853.79). Current reward after update: -574.02, Optimal reward -351.99
Iteration 257 took 3.32 seconds (mean sampled reward: -2619.80). Current reward after update: -476.22, Optimal reward -351.99
Iteration 258 took 3.30 seconds (mean sampled reward: -2451.41). Current reward after update: -527.63, Optimal reward -351.99
Iteration 259 took 3.36 seconds (mean sampled reward: -2481.40). Current reward after update: -549.84, Optimal reward -351.99
Iteration 260 took 3.40 seconds (mean sampled reward: -2685.95). Current reward after update: -587.87, Optimal reward -351.99
Iteration 261 took 3.47 seconds (mean sampled reward: -2638.61). Current reward after update: -613.75, Optimal reward -351.99
Iteration 262 took 3.37 seconds (mean sampled reward: -2407.51). Current reward after update: -565.65, Optimal reward -351.99
Iteration 263 took 3.47 seconds (mean sampled reward: -2885.43). Current reward after update: -812.43, Optimal reward -351.99
Iteration 264 took 3.45 seconds (mean sampled reward: -3326.58). Current reward after update: -1087.00, Optimal reward -351.99
Iteration 265 took 3.51 seconds (mean sampled reward: -2811.07). Current reward after update: -620.00, Optimal reward -351.99
Iteration 266 took 3.60 seconds (mean sampled reward: -5250.90). Current reward after update: -625.32, Optimal reward -351.99
Iteration 267 took 3.67 seconds (mean sampled reward: -6290.22). Current reward after update: -619.24, Optimal reward -351.99
Iteration 268 took 3.54 seconds (mean sampled reward: -5090.15). Current reward after update: -715.45, Optimal reward -351.99
Iteration 269 took 3.47 seconds (mean sampled reward: -4765.45). Current reward after update: -548.11, Optimal reward -351.99
Iteration 270 took 3.56 seconds (mean sampled reward: -4537.65). Current reward after update: -542.21, Optimal reward -351.99
Iteration 271 took 3.50 seconds (mean sampled reward: -3931.54). Current reward after update: -655.62, Optimal reward -351.99
Iteration 272 took 3.36 seconds (mean sampled reward: -3229.85). Current reward after update: -513.42, Optimal reward -351.99
Iteration 273 took 3.56 seconds (mean sampled reward: -4781.30). Current reward after update: -631.79, Optimal reward -351.99
Iteration 274 took 3.64 seconds (mean sampled reward: -4676.79). Current reward after update: -591.55, Optimal reward -351.99
Iteration 275 took 3.61 seconds (mean sampled reward: -5188.69). Current reward after update: -522.62, Optimal reward -351.99
Iteration 276 took 3.39 seconds (mean sampled reward: -2955.27). Current reward after update: -497.45, Optimal reward -351.99
Iteration 277 took 3.38 seconds (mean sampled reward: -3337.83). Current reward after update: -415.33, Optimal reward -351.99
Iteration 278 took 3.39 seconds (mean sampled reward: -3252.87). Current reward after update: -512.14, Optimal reward -351.99
Iteration 279 took 3.46 seconds (mean sampled reward: -3413.46). Current reward after update: -651.98, Optimal reward -351.99
Iteration 280 took 3.50 seconds (mean sampled reward: -4218.17). Current reward after update: -502.73, Optimal reward -351.99
Iteration 281 took 3.47 seconds (mean sampled reward: -3836.81). Current reward after update: -492.32, Optimal reward -351.99
Iteration 282 took 3.37 seconds (mean sampled reward: -2998.18). Current reward after update: -566.40, Optimal reward -351.99
Iteration 283 took 3.50 seconds (mean sampled reward: -4222.63). Current reward after update: -488.32, Optimal reward -351.99
Iteration 284 took 3.49 seconds (mean sampled reward: -4235.83). Current reward after update: -568.93, Optimal reward -351.99
Iteration 285 took 3.34 seconds (mean sampled reward: -3608.50). Current reward after update: -479.43, Optimal reward -351.99
Iteration 286 took 3.41 seconds (mean sampled reward: -3493.24). Current reward after update: -6693.24, Optimal reward -351.99
Iteration 287 took 3.43 seconds (mean sampled reward: -3874.33). Current reward after update: -544.44, Optimal reward -351.99
Iteration 288 took 3.35 seconds (mean sampled reward: -3753.00). Current reward after update: -634.21, Optimal reward -351.99
Iteration 289 took 3.37 seconds (mean sampled reward: -3412.56). Current reward after update: -703.32, Optimal reward -351.99
Iteration 290 took 3.42 seconds (mean sampled reward: -4381.83). Current reward after update: -614.11, Optimal reward -351.99
Iteration 291 took 3.28 seconds (mean sampled reward: -2916.77). Current reward after update: -515.31, Optimal reward -351.99
Iteration 292 took 3.30 seconds (mean sampled reward: -3107.00). Current reward after update: -457.54, Optimal reward -351.99
Iteration 293 took 3.23 seconds (mean sampled reward: -2521.78). Current reward after update: -398.63, Optimal reward -351.99
Iteration 294 took 3.26 seconds (mean sampled reward: -2670.81). Current reward after update: -489.09, Optimal reward -351.99
Iteration 295 took 3.39 seconds (mean sampled reward: -3699.81). Current reward after update: -417.57, Optimal reward -351.99
Iteration 296 took 3.28 seconds (mean sampled reward: -3073.04). Current reward after update: -462.17, Optimal reward -351.99
Iteration 297 took 3.50 seconds (mean sampled reward: -4294.48). Current reward after update: -532.00, Optimal reward -351.99
Iteration 298 took 3.67 seconds (mean sampled reward: -5757.26). Current reward after update: -531.73, Optimal reward -351.99
Iteration 299 took 3.45 seconds (mean sampled reward: -4129.08). Current reward after update: -1413.14, Optimal reward -351.99
Iteration 300 took 3.63 seconds (mean sampled reward: -5186.54). Current reward after update: -371.24, Optimal reward -351.99
Max force: 50 Sigma: 0.8 mean rewards: -248.483288129404, best rewards:-132.825701400092

argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
pybullet build time: Jan 28 2022 20:17:22
Iteration 1 took 3.93 seconds (mean sampled reward: -7478.71). Current reward after update: -4292.29, Optimal reward -4292.29
Iteration 2 took 3.68 seconds (mean sampled reward: -6637.26). Current reward after update: -4015.29, Optimal reward -4015.29
Iteration 3 took 3.77 seconds (mean sampled reward: -6640.89). Current reward after update: -3924.91, Optimal reward -3924.91
Iteration 4 took 3.78 seconds (mean sampled reward: -5934.88). Current reward after update: -2876.79, Optimal reward -2876.79
Iteration 5 took 3.79 seconds (mean sampled reward: -5614.94). Current reward after update: -2439.53, Optimal reward -2439.53
Iteration 6 took 3.71 seconds (mean sampled reward: -5622.58). Current reward after update: -2588.08, Optimal reward -2439.53
Iteration 7 took 3.96 seconds (mean sampled reward: -5737.22). Current reward after update: -2098.79, Optimal reward -2098.79
Iteration 8 took 3.69 seconds (mean sampled reward: -5882.72). Current reward after update: -1453.67, Optimal reward -1453.67
Iteration 9 took 3.81 seconds (mean sampled reward: -5862.07). Current reward after update: -1461.93, Optimal reward -1453.67
Iteration 10 took 3.85 seconds (mean sampled reward: -5677.50). Current reward after update: -1155.01, Optimal reward -1155.01
Iteration 11 took 3.70 seconds (mean sampled reward: -4839.63). Current reward after update: -1255.01, Optimal reward -1155.01
Iteration 12 took 3.77 seconds (mean sampled reward: -4601.11). Current reward after update: -1089.28, Optimal reward -1089.28
Iteration 13 took 3.84 seconds (mean sampled reward: -5013.55). Current reward after update: -954.64, Optimal reward -954.64
Iteration 14 took 3.75 seconds (mean sampled reward: -5356.86). Current reward after update: -879.52, Optimal reward -879.52
Iteration 15 took 3.69 seconds (mean sampled reward: -5199.50). Current reward after update: -936.24, Optimal reward -879.52
Iteration 16 took 3.71 seconds (mean sampled reward: -5050.50). Current reward after update: -832.55, Optimal reward -832.55
Iteration 17 took 3.71 seconds (mean sampled reward: -4470.20). Current reward after update: -844.95, Optimal reward -832.55
Iteration 18 took 3.69 seconds (mean sampled reward: -3345.34). Current reward after update: -838.94, Optimal reward -832.55
Iteration 19 took 3.72 seconds (mean sampled reward: -3927.54). Current reward after update: -764.70, Optimal reward -764.70
Iteration 20 took 3.87 seconds (mean sampled reward: -4872.05). Current reward after update: -785.76, Optimal reward -764.70
Iteration 21 took 3.69 seconds (mean sampled reward: -5681.94). Current reward after update: -958.74, Optimal reward -764.70
Iteration 22 took 3.87 seconds (mean sampled reward: -5871.73). Current reward after update: -737.66, Optimal reward -737.66
Iteration 23 took 3.67 seconds (mean sampled reward: -5149.86). Current reward after update: -1052.31, Optimal reward -737.66
Iteration 24 took 3.52 seconds (mean sampled reward: -5238.60). Current reward after update: -1022.17, Optimal reward -737.66
Iteration 25 took 3.54 seconds (mean sampled reward: -4541.26). Current reward after update: -985.44, Optimal reward -737.66
Iteration 26 took 3.58 seconds (mean sampled reward: -4704.40). Current reward after update: -784.42, Optimal reward -737.66
Iteration 27 took 3.68 seconds (mean sampled reward: -5584.20). Current reward after update: -1081.98, Optimal reward -737.66
Iteration 28 took 3.78 seconds (mean sampled reward: -5230.03). Current reward after update: -765.39, Optimal reward -737.66
Iteration 29 took 3.73 seconds (mean sampled reward: -5304.70). Current reward after update: -865.55, Optimal reward -737.66
Iteration 30 took 4.03 seconds (mean sampled reward: -4717.52). Current reward after update: -852.61, Optimal reward -737.66
Iteration 31 took 3.63 seconds (mean sampled reward: -4977.92). Current reward after update: -886.78, Optimal reward -737.66
Iteration 32 took 3.49 seconds (mean sampled reward: -4990.57). Current reward after update: -903.88, Optimal reward -737.66
Iteration 33 took 3.50 seconds (mean sampled reward: -5236.36). Current reward after update: -931.23, Optimal reward -737.66
Iteration 34 took 3.68 seconds (mean sampled reward: -5461.47). Current reward after update: -686.78, Optimal reward -686.78
Iteration 35 took 3.50 seconds (mean sampled reward: -4156.99). Current reward after update: -653.91, Optimal reward -653.91
Iteration 36 took 3.71 seconds (mean sampled reward: -4344.23). Current reward after update: -687.28, Optimal reward -653.91
Iteration 37 took 3.71 seconds (mean sampled reward: -4488.41). Current reward after update: -729.18, Optimal reward -653.91
Iteration 38 took 3.57 seconds (mean sampled reward: -4617.58). Current reward after update: -914.66, Optimal reward -653.91
Iteration 39 took 3.75 seconds (mean sampled reward: -4249.20). Current reward after update: -796.98, Optimal reward -653.91
Iteration 40 took 3.64 seconds (mean sampled reward: -3255.08). Current reward after update: -707.45, Optimal reward -653.91
Iteration 41 took 3.56 seconds (mean sampled reward: -3402.50). Current reward after update: -628.81, Optimal reward -628.81
Iteration 42 took 3.54 seconds (mean sampled reward: -3004.19). Current reward after update: -625.78, Optimal reward -625.78
Iteration 43 took 3.89 seconds (mean sampled reward: -3203.00). Current reward after update: -688.37, Optimal reward -625.78
Iteration 44 took 3.98 seconds (mean sampled reward: -2958.77). Current reward after update: -664.88, Optimal reward -625.78
Iteration 45 took 3.48 seconds (mean sampled reward: -3360.87). Current reward after update: -629.07, Optimal reward -625.78
Iteration 46 took 3.52 seconds (mean sampled reward: -3063.75). Current reward after update: -733.10, Optimal reward -625.78
Iteration 47 took 3.93 seconds (mean sampled reward: -2978.73). Current reward after update: -559.59, Optimal reward -559.59
Iteration 48 took 4.04 seconds (mean sampled reward: -4958.55). Current reward after update: -742.37, Optimal reward -559.59
Iteration 49 took 3.97 seconds (mean sampled reward: -4105.50). Current reward after update: -753.73, Optimal reward -559.59
Iteration 50 took 3.51 seconds (mean sampled reward: -3455.61). Current reward after update: -717.37, Optimal reward -559.59
Iteration 51 took 3.43 seconds (mean sampled reward: -3771.59). Current reward after update: -674.11, Optimal reward -559.59
Iteration 52 took 3.55 seconds (mean sampled reward: -3750.05). Current reward after update: -557.73, Optimal reward -557.73
Iteration 53 took 3.65 seconds (mean sampled reward: -3012.92). Current reward after update: -780.79, Optimal reward -557.73
Iteration 54 took 3.65 seconds (mean sampled reward: -3243.29). Current reward after update: -665.59, Optimal reward -557.73
Iteration 55 took 3.89 seconds (mean sampled reward: -2652.94). Current reward after update: -638.14, Optimal reward -557.73
Iteration 56 took 3.59 seconds (mean sampled reward: -3028.79). Current reward after update: -645.53, Optimal reward -557.73
Iteration 57 took 3.95 seconds (mean sampled reward: -2789.99). Current reward after update: -613.72, Optimal reward -557.73
Iteration 58 took 3.91 seconds (mean sampled reward: -2917.54). Current reward after update: -580.26, Optimal reward -557.73
Iteration 59 took 3.90 seconds (mean sampled reward: -3245.19). Current reward after update: -692.14, Optimal reward -557.73
Iteration 60 took 3.79 seconds (mean sampled reward: -3298.23). Current reward after update: -680.90, Optimal reward -557.73
Iteration 61 took 3.79 seconds (mean sampled reward: -3290.39). Current reward after update: -570.45, Optimal reward -557.73
Iteration 62 took 3.76 seconds (mean sampled reward: -3511.65). Current reward after update: -611.90, Optimal reward -557.73
Iteration 63 took 3.63 seconds (mean sampled reward: -3223.59). Current reward after update: -587.99, Optimal reward -557.73
Iteration 64 took 3.51 seconds (mean sampled reward: -3297.18). Current reward after update: -653.47, Optimal reward -557.73
Iteration 65 took 3.49 seconds (mean sampled reward: -3246.89). Current reward after update: -727.69, Optimal reward -557.73
Iteration 66 took 3.53 seconds (mean sampled reward: -3191.60). Current reward after update: -658.86, Optimal reward -557.73
Iteration 67 took 3.51 seconds (mean sampled reward: -3148.79). Current reward after update: -586.42, Optimal reward -557.73
Iteration 68 took 3.45 seconds (mean sampled reward: -2945.91). Current reward after update: -544.91, Optimal reward -544.91
Iteration 69 took 3.50 seconds (mean sampled reward: -3256.54). Current reward after update: -523.87, Optimal reward -523.87
Iteration 70 took 3.36 seconds (mean sampled reward: -3088.45). Current reward after update: -643.78, Optimal reward -523.87
Iteration 71 took 3.41 seconds (mean sampled reward: -3289.63). Current reward after update: -593.62, Optimal reward -523.87
Iteration 72 took 3.43 seconds (mean sampled reward: -3886.10). Current reward after update: -655.30, Optimal reward -523.87
Iteration 73 took 3.63 seconds (mean sampled reward: -3062.92). Current reward after update: -555.60, Optimal reward -523.87
Iteration 74 took 3.71 seconds (mean sampled reward: -3104.97). Current reward after update: -470.05, Optimal reward -470.05
Iteration 75 took 3.77 seconds (mean sampled reward: -3243.45). Current reward after update: -529.14, Optimal reward -470.05
Iteration 76 took 3.67 seconds (mean sampled reward: -2907.28). Current reward after update: -533.25, Optimal reward -470.05
Iteration 77 took 3.64 seconds (mean sampled reward: -3939.30). Current reward after update: -566.15, Optimal reward -470.05
Iteration 78 took 3.65 seconds (mean sampled reward: -3701.29). Current reward after update: -440.40, Optimal reward -440.40
Iteration 79 took 3.53 seconds (mean sampled reward: -3156.22). Current reward after update: -628.27, Optimal reward -440.40
Iteration 80 took 3.66 seconds (mean sampled reward: -2828.57). Current reward after update: -559.62, Optimal reward -440.40
Iteration 81 took 3.74 seconds (mean sampled reward: -2648.55). Current reward after update: -587.18, Optimal reward -440.40
Iteration 82 took 3.64 seconds (mean sampled reward: -2774.67). Current reward after update: -598.61, Optimal reward -440.40
Iteration 83 took 3.80 seconds (mean sampled reward: -3256.13). Current reward after update: -557.80, Optimal reward -440.40
Iteration 84 took 3.59 seconds (mean sampled reward: -4144.43). Current reward after update: -584.63, Optimal reward -440.40
Iteration 85 took 3.68 seconds (mean sampled reward: -3962.95). Current reward after update: -559.66, Optimal reward -440.40
Iteration 86 took 3.45 seconds (mean sampled reward: -2624.68). Current reward after update: -507.74, Optimal reward -440.40
Iteration 87 took 3.45 seconds (mean sampled reward: -3416.64). Current reward after update: -548.34, Optimal reward -440.40
Iteration 88 took 3.71 seconds (mean sampled reward: -3335.54). Current reward after update: -518.27, Optimal reward -440.40
Iteration 89 took 3.65 seconds (mean sampled reward: -3190.90). Current reward after update: -567.33, Optimal reward -440.40
Iteration 90 took 3.59 seconds (mean sampled reward: -4158.61). Current reward after update: -618.26, Optimal reward -440.40
Iteration 91 took 3.75 seconds (mean sampled reward: -4031.81). Current reward after update: -729.13, Optimal reward -440.40
Iteration 92 took 3.65 seconds (mean sampled reward: -3570.53). Current reward after update: -455.03, Optimal reward -440.40
Iteration 93 took 3.50 seconds (mean sampled reward: -3440.72). Current reward after update: -543.23, Optimal reward -440.40
Iteration 94 took 3.45 seconds (mean sampled reward: -3167.26). Current reward after update: -492.79, Optimal reward -440.40
Iteration 95 took 3.44 seconds (mean sampled reward: -3128.06). Current reward after update: -962.89, Optimal reward -440.40
Iteration 96 took 3.51 seconds (mean sampled reward: -2902.10). Current reward after update: -540.12, Optimal reward -440.40
Iteration 97 took 3.55 seconds (mean sampled reward: -2920.06). Current reward after update: -453.16, Optimal reward -440.40
Iteration 98 took 3.67 seconds (mean sampled reward: -3058.28). Current reward after update: -420.11, Optimal reward -420.11
Iteration 99 took 3.56 seconds (mean sampled reward: -3138.46). Current reward after update: -873.30, Optimal reward -420.11
Iteration 100 took 3.90 seconds (mean sampled reward: -3080.75). Current reward after update: -484.05, Optimal reward -420.11
Iteration 101 took 3.75 seconds (mean sampled reward: -3167.28). Current reward after update: -491.76, Optimal reward -420.11
Iteration 102 took 4.03 seconds (mean sampled reward: -3452.51). Current reward after update: -555.49, Optimal reward -420.11
Iteration 103 took 3.91 seconds (mean sampled reward: -3692.42). Current reward after update: -574.07, Optimal reward -420.11
Iteration 104 took 3.52 seconds (mean sampled reward: -3025.64). Current reward after update: -478.66, Optimal reward -420.11
Iteration 105 took 3.58 seconds (mean sampled reward: -2860.02). Current reward after update: -688.70, Optimal reward -420.11
Iteration 106 took 3.63 seconds (mean sampled reward: -2601.21). Current reward after update: -573.73, Optimal reward -420.11
Iteration 107 took 3.71 seconds (mean sampled reward: -2753.97). Current reward after update: -555.43, Optimal reward -420.11
Iteration 108 took 3.59 seconds (mean sampled reward: -2750.79). Current reward after update: -558.83, Optimal reward -420.11
Iteration 109 took 3.47 seconds (mean sampled reward: -3055.22). Current reward after update: -518.67, Optimal reward -420.11
Iteration 110 took 3.56 seconds (mean sampled reward: -2690.91). Current reward after update: -463.19, Optimal reward -420.11
Iteration 111 took 3.65 seconds (mean sampled reward: -4133.66). Current reward after update: -580.05, Optimal reward -420.11
Iteration 112 took 3.80 seconds (mean sampled reward: -3907.66). Current reward after update: -491.07, Optimal reward -420.11
Iteration 113 took 3.64 seconds (mean sampled reward: -4336.75). Current reward after update: -541.22, Optimal reward -420.11
Iteration 114 took 3.68 seconds (mean sampled reward: -4564.62). Current reward after update: -461.28, Optimal reward -420.11
Iteration 115 took 3.77 seconds (mean sampled reward: -3580.35). Current reward after update: -427.91, Optimal reward -420.11
Iteration 116 took 3.53 seconds (mean sampled reward: -3354.26). Current reward after update: -484.55, Optimal reward -420.11
Iteration 117 took 3.50 seconds (mean sampled reward: -2977.32). Current reward after update: -506.36, Optimal reward -420.11
Iteration 118 took 3.57 seconds (mean sampled reward: -3476.41). Current reward after update: -456.83, Optimal reward -420.11
Iteration 119 took 3.73 seconds (mean sampled reward: -4783.89). Current reward after update: -554.18, Optimal reward -420.11
Iteration 120 took 3.70 seconds (mean sampled reward: -4198.21). Current reward after update: -570.31, Optimal reward -420.11
Iteration 121 took 3.60 seconds (mean sampled reward: -3036.48). Current reward after update: -480.83, Optimal reward -420.11
Iteration 122 took 3.66 seconds (mean sampled reward: -3945.68). Current reward after update: -1363.59, Optimal reward -420.11
Iteration 123 took 3.58 seconds (mean sampled reward: -3162.77). Current reward after update: -371.35, Optimal reward -371.35
Iteration 124 took 3.67 seconds (mean sampled reward: -3866.46). Current reward after update: -514.99, Optimal reward -371.35
Iteration 125 took 3.71 seconds (mean sampled reward: -5010.54). Current reward after update: -459.75, Optimal reward -371.35
Iteration 126 took 4.04 seconds (mean sampled reward: -4866.43). Current reward after update: -597.81, Optimal reward -371.35
Iteration 127 took 4.03 seconds (mean sampled reward: -4713.37). Current reward after update: -726.11, Optimal reward -371.35
Iteration 128 took 3.87 seconds (mean sampled reward: -3383.80). Current reward after update: -567.01, Optimal reward -371.35
Iteration 129 took 3.62 seconds (mean sampled reward: -3938.21). Current reward after update: -492.65, Optimal reward -371.35
Iteration 130 took 3.69 seconds (mean sampled reward: -3946.56). Current reward after update: -492.37, Optimal reward -371.35
Iteration 131 took 3.60 seconds (mean sampled reward: -3746.74). Current reward after update: -6238.03, Optimal reward -371.35
Iteration 132 took 3.66 seconds (mean sampled reward: -4255.73). Current reward after update: -542.51, Optimal reward -371.35
Iteration 133 took 3.65 seconds (mean sampled reward: -4325.02). Current reward after update: -385.98, Optimal reward -371.35
Iteration 134 took 3.77 seconds (mean sampled reward: -5413.86). Current reward after update: -498.85, Optimal reward -371.35
Iteration 135 took 3.78 seconds (mean sampled reward: -4747.97). Current reward after update: -533.75, Optimal reward -371.35
Iteration 136 took 3.78 seconds (mean sampled reward: -4149.86). Current reward after update: -384.99, Optimal reward -371.35
Iteration 137 took 3.64 seconds (mean sampled reward: -3576.81). Current reward after update: -442.24, Optimal reward -371.35
Iteration 138 took 3.68 seconds (mean sampled reward: -4339.05). Current reward after update: -480.60, Optimal reward -371.35
Iteration 139 took 3.74 seconds (mean sampled reward: -5422.22). Current reward after update: -474.54, Optimal reward -371.35
Iteration 140 took 3.87 seconds (mean sampled reward: -5056.02). Current reward after update: -454.18, Optimal reward -371.35
Iteration 141 took 3.85 seconds (mean sampled reward: -5211.36). Current reward after update: -453.03, Optimal reward -371.35
Iteration 142 took 3.72 seconds (mean sampled reward: -5521.37). Current reward after update: -480.03, Optimal reward -371.35
Iteration 143 took 3.93 seconds (mean sampled reward: -5455.74). Current reward after update: -454.64, Optimal reward -371.35
Iteration 144 took 3.69 seconds (mean sampled reward: -5252.24). Current reward after update: -425.04, Optimal reward -371.35
Iteration 145 took 3.62 seconds (mean sampled reward: -5311.05). Current reward after update: -631.21, Optimal reward -371.35
Iteration 146 took 3.57 seconds (mean sampled reward: -4722.11). Current reward after update: -575.07, Optimal reward -371.35
Iteration 147 took 3.84 seconds (mean sampled reward: -5779.44). Current reward after update: -612.10, Optimal reward -371.35
Iteration 148 took 4.08 seconds (mean sampled reward: -5640.27). Current reward after update: -403.71, Optimal reward -371.35
Iteration 149 took 3.96 seconds (mean sampled reward: -5324.66). Current reward after update: -1310.81, Optimal reward -371.35
Iteration 150 took 3.70 seconds (mean sampled reward: -5439.30). Current reward after update: -515.06, Optimal reward -371.35
Iteration 151 took 3.83 seconds (mean sampled reward: -5835.14). Current reward after update: -573.37, Optimal reward -371.35
Iteration 152 took 3.68 seconds (mean sampled reward: -4690.01). Current reward after update: -492.94, Optimal reward -371.35
Iteration 153 took 3.69 seconds (mean sampled reward: -5018.09). Current reward after update: -563.62, Optimal reward -371.35
Iteration 154 took 3.77 seconds (mean sampled reward: -4696.42). Current reward after update: -518.69, Optimal reward -371.35
Iteration 155 took 3.65 seconds (mean sampled reward: -4289.85). Current reward after update: -486.69, Optimal reward -371.35
Iteration 156 took 3.70 seconds (mean sampled reward: -4188.06). Current reward after update: -492.28, Optimal reward -371.35
Iteration 157 took 3.56 seconds (mean sampled reward: -3705.72). Current reward after update: -552.29, Optimal reward -371.35
Iteration 158 took 3.63 seconds (mean sampled reward: -3820.31). Current reward after update: -489.40, Optimal reward -371.35
Iteration 159 took 3.66 seconds (mean sampled reward: -4203.51). Current reward after update: -515.91, Optimal reward -371.35
Iteration 160 took 3.56 seconds (mean sampled reward: -2978.64). Current reward after update: -538.23, Optimal reward -371.35
Iteration 161 took 3.49 seconds (mean sampled reward: -3033.03). Current reward after update: -515.82, Optimal reward -371.35
Iteration 162 took 3.58 seconds (mean sampled reward: -3047.95). Current reward after update: -422.54, Optimal reward -371.35
Iteration 163 took 3.59 seconds (mean sampled reward: -2632.73). Current reward after update: -527.22, Optimal reward -371.35
Iteration 164 took 3.63 seconds (mean sampled reward: -2364.32). Current reward after update: -535.63, Optimal reward -371.35
Iteration 165 took 3.57 seconds (mean sampled reward: -2843.38). Current reward after update: -437.85, Optimal reward -371.35
Iteration 166 took 3.53 seconds (mean sampled reward: -2486.27). Current reward after update: -465.60, Optimal reward -371.35
Iteration 167 took 3.62 seconds (mean sampled reward: -3287.96). Current reward after update: -455.63, Optimal reward -371.35
Iteration 168 took 3.86 seconds (mean sampled reward: -4340.27). Current reward after update: -495.26, Optimal reward -371.35
Iteration 169 took 3.70 seconds (mean sampled reward: -4911.15). Current reward after update: -425.16, Optimal reward -371.35
Iteration 170 took 3.62 seconds (mean sampled reward: -3293.77). Current reward after update: -439.30, Optimal reward -371.35
Iteration 171 took 3.61 seconds (mean sampled reward: -3235.46). Current reward after update: -473.03, Optimal reward -371.35
Iteration 172 took 3.60 seconds (mean sampled reward: -3453.78). Current reward after update: -517.86, Optimal reward -371.35
Iteration 173 took 3.64 seconds (mean sampled reward: -3185.43). Current reward after update: -523.73, Optimal reward -371.35
Iteration 174 took 3.67 seconds (mean sampled reward: -3922.38). Current reward after update: -447.63, Optimal reward -371.35
Iteration 175 took 3.68 seconds (mean sampled reward: -3727.02). Current reward after update: -438.05, Optimal reward -371.35
Iteration 176 took 3.61 seconds (mean sampled reward: -3243.56). Current reward after update: -433.87, Optimal reward -371.35
Iteration 177 took 3.55 seconds (mean sampled reward: -2757.59). Current reward after update: -555.30, Optimal reward -371.35
Iteration 178 took 3.59 seconds (mean sampled reward: -3071.33). Current reward after update: -428.10, Optimal reward -371.35
Iteration 179 took 3.60 seconds (mean sampled reward: -2801.73). Current reward after update: -441.96, Optimal reward -371.35
Iteration 180 took 3.70 seconds (mean sampled reward: -2308.05). Current reward after update: -456.74, Optimal reward -371.35
Iteration 181 took 3.53 seconds (mean sampled reward: -2438.08). Current reward after update: -457.91, Optimal reward -371.35
Iteration 182 took 3.68 seconds (mean sampled reward: -2087.81). Current reward after update: -397.51, Optimal reward -371.35
Iteration 183 took 3.57 seconds (mean sampled reward: -2261.55). Current reward after update: -406.44, Optimal reward -371.35
Iteration 184 took 3.69 seconds (mean sampled reward: -3301.09). Current reward after update: -321.76, Optimal reward -321.76
Iteration 185 took 3.68 seconds (mean sampled reward: -3565.83). Current reward after update: -344.74, Optimal reward -321.76
Iteration 186 took 3.71 seconds (mean sampled reward: -3735.73). Current reward after update: -389.82, Optimal reward -321.76
Iteration 187 took 3.71 seconds (mean sampled reward: -3632.46). Current reward after update: -373.58, Optimal reward -321.76
Iteration 188 took 3.77 seconds (mean sampled reward: -5119.43). Current reward after update: -446.61, Optimal reward -321.76
Iteration 189 took 3.89 seconds (mean sampled reward: -4912.99). Current reward after update: -387.78, Optimal reward -321.76
Iteration 190 took 3.98 seconds (mean sampled reward: -3610.85). Current reward after update: -427.87, Optimal reward -321.76
Iteration 191 took 3.69 seconds (mean sampled reward: -3138.68). Current reward after update: -462.95, Optimal reward -321.76
Iteration 192 took 3.58 seconds (mean sampled reward: -2734.35). Current reward after update: -346.67, Optimal reward -321.76
Iteration 193 took 3.56 seconds (mean sampled reward: -3273.58). Current reward after update: -428.25, Optimal reward -321.76
Iteration 194 took 3.67 seconds (mean sampled reward: -3803.27). Current reward after update: -457.88, Optimal reward -321.76
Iteration 195 took 3.57 seconds (mean sampled reward: -2699.59). Current reward after update: -464.55, Optimal reward -321.76
Iteration 196 took 3.65 seconds (mean sampled reward: -2930.54). Current reward after update: -504.70, Optimal reward -321.76
Iteration 197 took 3.70 seconds (mean sampled reward: -3693.66). Current reward after update: -577.31, Optimal reward -321.76
Iteration 198 took 3.83 seconds (mean sampled reward: -2872.56). Current reward after update: -443.09, Optimal reward -321.76
Iteration 199 took 3.72 seconds (mean sampled reward: -2638.36). Current reward after update: -486.13, Optimal reward -321.76
Iteration 200 took 3.73 seconds (mean sampled reward: -2904.21). Current reward after update: -524.20, Optimal reward -321.76
Iteration 201 took 3.81 seconds (mean sampled reward: -2093.20). Current reward after update: -460.18, Optimal reward -321.76
Iteration 202 took 3.78 seconds (mean sampled reward: -2068.59). Current reward after update: -451.15, Optimal reward -321.76
Iteration 203 took 3.84 seconds (mean sampled reward: -2180.36). Current reward after update: -465.79, Optimal reward -321.76
Iteration 204 took 3.99 seconds (mean sampled reward: -2197.85). Current reward after update: -492.21, Optimal reward -321.76
Iteration 205 took 3.73 seconds (mean sampled reward: -2217.34). Current reward after update: -540.03, Optimal reward -321.76
Iteration 206 took 3.86 seconds (mean sampled reward: -2346.35). Current reward after update: -457.56, Optimal reward -321.76
Iteration 207 took 4.01 seconds (mean sampled reward: -2048.06). Current reward after update: -656.92, Optimal reward -321.76
Iteration 208 took 3.93 seconds (mean sampled reward: -2431.69). Current reward after update: -486.00, Optimal reward -321.76
Iteration 209 took 3.94 seconds (mean sampled reward: -3391.63). Current reward after update: -484.56, Optimal reward -321.76
Iteration 210 took 3.96 seconds (mean sampled reward: -2183.55). Current reward after update: -519.00, Optimal reward -321.76
Iteration 211 took 4.17 seconds (mean sampled reward: -2278.78). Current reward after update: -459.41, Optimal reward -321.76
Iteration 212 took 3.97 seconds (mean sampled reward: -2464.26). Current reward after update: -471.01, Optimal reward -321.76
Iteration 213 took 4.14 seconds (mean sampled reward: -2124.50). Current reward after update: -394.67, Optimal reward -321.76
Iteration 214 took 3.75 seconds (mean sampled reward: -2464.40). Current reward after update: -504.50, Optimal reward -321.76
Iteration 215 took 3.67 seconds (mean sampled reward: -2460.17). Current reward after update: -537.71, Optimal reward -321.76
Iteration 216 took 3.72 seconds (mean sampled reward: -2459.36). Current reward after update: -575.98, Optimal reward -321.76
Iteration 217 took 3.48 seconds (mean sampled reward: -2941.97). Current reward after update: -452.81, Optimal reward -321.76
Iteration 218 took 3.62 seconds (mean sampled reward: -2322.82). Current reward after update: -398.84, Optimal reward -321.76
Iteration 219 took 3.61 seconds (mean sampled reward: -2422.88). Current reward after update: -659.57, Optimal reward -321.76
Iteration 220 took 3.79 seconds (mean sampled reward: -2367.86). Current reward after update: -359.75, Optimal reward -321.76
Iteration 221 took 3.56 seconds (mean sampled reward: -2543.01). Current reward after update: -451.69, Optimal reward -321.76
Iteration 222 took 3.81 seconds (mean sampled reward: -2631.29). Current reward after update: -473.18, Optimal reward -321.76
Iteration 223 took 3.98 seconds (mean sampled reward: -2498.18). Current reward after update: -427.74, Optimal reward -321.76
Iteration 224 took 3.87 seconds (mean sampled reward: -2373.95). Current reward after update: -464.35, Optimal reward -321.76
Iteration 225 took 3.90 seconds (mean sampled reward: -2435.58). Current reward after update: -445.18, Optimal reward -321.76
Iteration 226 took 3.78 seconds (mean sampled reward: -2464.81). Current reward after update: -417.29, Optimal reward -321.76
Iteration 227 took 3.65 seconds (mean sampled reward: -2486.67). Current reward after update: -500.25, Optimal reward -321.76
Iteration 228 took 3.81 seconds (mean sampled reward: -2384.96). Current reward after update: -515.49, Optimal reward -321.76
Iteration 229 took 3.84 seconds (mean sampled reward: -2578.80). Current reward after update: -595.94, Optimal reward -321.76
Iteration 230 took 3.69 seconds (mean sampled reward: -2502.23). Current reward after update: -351.76, Optimal reward -321.76
Iteration 231 took 3.82 seconds (mean sampled reward: -2132.06). Current reward after update: -328.16, Optimal reward -321.76
Iteration 232 took 3.94 seconds (mean sampled reward: -2211.14). Current reward after update: -477.47, Optimal reward -321.76
Iteration 233 took 3.98 seconds (mean sampled reward: -2162.58). Current reward after update: -521.29, Optimal reward -321.76
Iteration 234 took 3.62 seconds (mean sampled reward: -2334.21). Current reward after update: -429.08, Optimal reward -321.76
Iteration 235 took 3.82 seconds (mean sampled reward: -2389.85). Current reward after update: -448.56, Optimal reward -321.76
Iteration 236 took 3.60 seconds (mean sampled reward: -2475.65). Current reward after update: -589.25, Optimal reward -321.76
Iteration 237 took 3.71 seconds (mean sampled reward: -2530.82). Current reward after update: -704.84, Optimal reward -321.76
Iteration 238 took 3.63 seconds (mean sampled reward: -2471.48). Current reward after update: -545.00, Optimal reward -321.76
Iteration 239 took 3.82 seconds (mean sampled reward: -2610.48). Current reward after update: -699.82, Optimal reward -321.76
Iteration 240 took 3.88 seconds (mean sampled reward: -2734.88). Current reward after update: -607.18, Optimal reward -321.76
Iteration 241 took 3.61 seconds (mean sampled reward: -3131.84). Current reward after update: -512.58, Optimal reward -321.76
Iteration 242 took 3.55 seconds (mean sampled reward: -2901.48). Current reward after update: -777.19, Optimal reward -321.76
Iteration 243 took 3.63 seconds (mean sampled reward: -3129.27). Current reward after update: -971.27, Optimal reward -321.76
Iteration 244 took 3.69 seconds (mean sampled reward: -3252.68). Current reward after update: -690.12, Optimal reward -321.76
Iteration 245 took 3.61 seconds (mean sampled reward: -3254.55). Current reward after update: -493.59, Optimal reward -321.76
Iteration 246 took 3.77 seconds (mean sampled reward: -4398.73). Current reward after update: -743.50, Optimal reward -321.76
Iteration 247 took 3.57 seconds (mean sampled reward: -2736.31). Current reward after update: -677.12, Optimal reward -321.76
Iteration 248 took 3.57 seconds (mean sampled reward: -2947.84). Current reward after update: -602.38, Optimal reward -321.76
Iteration 249 took 3.57 seconds (mean sampled reward: -2896.13). Current reward after update: -515.03, Optimal reward -321.76
Iteration 250 took 3.56 seconds (mean sampled reward: -2949.03). Current reward after update: -497.47, Optimal reward -321.76
Iteration 251 took 3.55 seconds (mean sampled reward: -2906.44). Current reward after update: -665.44, Optimal reward -321.76
Iteration 252 took 3.77 seconds (mean sampled reward: -3195.68). Current reward after update: -565.27, Optimal reward -321.76
Iteration 253 took 3.81 seconds (mean sampled reward: -3054.30). Current reward after update: -505.29, Optimal reward -321.76
Iteration 254 took 4.01 seconds (mean sampled reward: -2875.31). Current reward after update: -427.64, Optimal reward -321.76
Iteration 255 took 3.75 seconds (mean sampled reward: -2809.86). Current reward after update: -476.11, Optimal reward -321.76
Iteration 256 took 4.08 seconds (mean sampled reward: -3316.37). Current reward after update: -2407.43, Optimal reward -321.76
Iteration 257 took 3.54 seconds (mean sampled reward: -2833.89). Current reward after update: -441.37, Optimal reward -321.76
Iteration 258 took 3.70 seconds (mean sampled reward: -2864.39). Current reward after update: -513.21, Optimal reward -321.76
Iteration 259 took 3.84 seconds (mean sampled reward: -2889.21). Current reward after update: -691.84, Optimal reward -321.76
Iteration 260 took 3.58 seconds (mean sampled reward: -2804.51). Current reward after update: -494.51, Optimal reward -321.76
Iteration 261 took 3.82 seconds (mean sampled reward: -2914.55). Current reward after update: -533.78, Optimal reward -321.76
Iteration 262 took 3.89 seconds (mean sampled reward: -2777.98). Current reward after update: -531.98, Optimal reward -321.76
Iteration 263 took 3.68 seconds (mean sampled reward: -2948.76). Current reward after update: -552.72, Optimal reward -321.76
Iteration 264 took 3.55 seconds (mean sampled reward: -3148.45). Current reward after update: -530.05, Optimal reward -321.76
Iteration 265 took 3.58 seconds (mean sampled reward: -3495.11). Current reward after update: -463.19, Optimal reward -321.76
Iteration 266 took 3.60 seconds (mean sampled reward: -3244.86). Current reward after update: -456.14, Optimal reward -321.76
Iteration 267 took 3.61 seconds (mean sampled reward: -3288.48). Current reward after update: -874.24, Optimal reward -321.76
Iteration 268 took 3.53 seconds (mean sampled reward: -2900.67). Current reward after update: -520.57, Optimal reward -321.76
Iteration 269 took 3.57 seconds (mean sampled reward: -3300.43). Current reward after update: -1125.26, Optimal reward -321.76
Iteration 270 took 3.62 seconds (mean sampled reward: -3651.06). Current reward after update: -426.22, Optimal reward -321.76
Iteration 271 took 3.56 seconds (mean sampled reward: -2846.42). Current reward after update: -430.88, Optimal reward -321.76
Iteration 272 took 3.61 seconds (mean sampled reward: -3191.33). Current reward after update: -467.74, Optimal reward -321.76
Iteration 273 took 3.66 seconds (mean sampled reward: -3261.95). Current reward after update: -527.18, Optimal reward -321.76
Iteration 274 took 3.60 seconds (mean sampled reward: -3120.67). Current reward after update: -511.93, Optimal reward -321.76
Iteration 275 took 3.65 seconds (mean sampled reward: -3881.36). Current reward after update: -482.05, Optimal reward -321.76
Iteration 276 took 3.52 seconds (mean sampled reward: -2989.09). Current reward after update: -474.01, Optimal reward -321.76
Iteration 277 took 3.59 seconds (mean sampled reward: -3358.85). Current reward after update: -477.12, Optimal reward -321.76
Iteration 278 took 3.69 seconds (mean sampled reward: -4342.58). Current reward after update: -508.18, Optimal reward -321.76
Iteration 279 took 3.64 seconds (mean sampled reward: -3624.55). Current reward after update: -500.04, Optimal reward -321.76
Iteration 280 took 3.51 seconds (mean sampled reward: -2931.78). Current reward after update: -488.98, Optimal reward -321.76
Iteration 281 took 3.52 seconds (mean sampled reward: -3021.65). Current reward after update: -470.23, Optimal reward -321.76
Iteration 282 took 3.54 seconds (mean sampled reward: -2670.36). Current reward after update: -1160.32, Optimal reward -321.76
Iteration 283 took 3.47 seconds (mean sampled reward: -2681.05). Current reward after update: -419.40, Optimal reward -321.76
Iteration 284 took 3.49 seconds (mean sampled reward: -2626.72). Current reward after update: -407.92, Optimal reward -321.76
Iteration 285 took 3.47 seconds (mean sampled reward: -2573.99). Current reward after update: -425.49, Optimal reward -321.76
Iteration 286 took 3.48 seconds (mean sampled reward: -2677.63). Current reward after update: -455.07, Optimal reward -321.76
Iteration 287 took 3.48 seconds (mean sampled reward: -2600.44). Current reward after update: -385.39, Optimal reward -321.76
Iteration 288 took 3.45 seconds (mean sampled reward: -2692.55). Current reward after update: -330.39, Optimal reward -321.76
Iteration 289 took 3.47 seconds (mean sampled reward: -2629.09). Current reward after update: -348.93, Optimal reward -321.76
Iteration 290 took 3.48 seconds (mean sampled reward: -2657.48). Current reward after update: -472.45, Optimal reward -321.76
Iteration 291 took 3.45 seconds (mean sampled reward: -2694.60). Current reward after update: -336.97, Optimal reward -321.76
Iteration 292 took 3.52 seconds (mean sampled reward: -2672.55). Current reward after update: -331.76, Optimal reward -321.76
Iteration 293 took 3.50 seconds (mean sampled reward: -2646.84). Current reward after update: -373.53, Optimal reward -321.76
Iteration 294 took 3.53 seconds (mean sampled reward: -2716.58). Current reward after update: -357.09, Optimal reward -321.76
Iteration 295 took 3.53 seconds (mean sampled reward: -2637.57). Current reward after update: -372.29, Optimal reward -321.76
Iteration 296 took 3.49 seconds (mean sampled reward: -2624.06). Current reward after update: -1099.10, Optimal reward -321.76
Iteration 297 took 3.51 seconds (mean sampled reward: -2613.23). Current reward after update: -373.61, Optimal reward -321.76
Iteration 298 took 3.49 seconds (mean sampled reward: -2661.80). Current reward after update: -296.43, Optimal reward -296.43
Iteration 299 took 3.39 seconds (mean sampled reward: -2631.53). Current reward after update: -293.77, Optimal reward -293.77
Iteration 300 took 3.45 seconds (mean sampled reward: -2770.13). Current reward after update: -339.48, Optimal reward -293.77
Iteration 301 took 3.44 seconds (mean sampled reward: -2710.52). Current reward after update: -311.27, Optimal reward -293.77
Iteration 302 took 3.45 seconds (mean sampled reward: -2755.40). Current reward after update: -387.37, Optimal reward -293.77
Iteration 303 took 3.48 seconds (mean sampled reward: -2864.26). Current reward after update: -652.48, Optimal reward -293.77
Iteration 304 took 3.45 seconds (mean sampled reward: -3025.60). Current reward after update: -421.60, Optimal reward -293.77
Iteration 305 took 3.47 seconds (mean sampled reward: -2835.28). Current reward after update: -276.68, Optimal reward -276.68
Iteration 306 took 3.47 seconds (mean sampled reward: -2958.53). Current reward after update: -346.79, Optimal reward -276.68
Iteration 307 took 3.46 seconds (mean sampled reward: -2920.74). Current reward after update: -383.98, Optimal reward -276.68
Iteration 308 took 3.54 seconds (mean sampled reward: -3275.79). Current reward after update: -323.03, Optimal reward -276.68
Iteration 309 took 3.48 seconds (mean sampled reward: -3195.76). Current reward after update: -247.27, Optimal reward -247.27
Iteration 310 took 3.43 seconds (mean sampled reward: -2990.77). Current reward after update: -244.57, Optimal reward -244.57
Iteration 311 took 3.44 seconds (mean sampled reward: -2743.82). Current reward after update: -362.35, Optimal reward -244.57
Iteration 312 took 3.43 seconds (mean sampled reward: -2724.07). Current reward after update: -348.63, Optimal reward -244.57
Iteration 313 took 3.44 seconds (mean sampled reward: -2784.27). Current reward after update: -305.95, Optimal reward -244.57
Iteration 314 took 3.38 seconds (mean sampled reward: -2866.62). Current reward after update: -242.09, Optimal reward -242.09
Iteration 315 took 3.42 seconds (mean sampled reward: -2713.97). Current reward after update: -290.73, Optimal reward -242.09
Iteration 316 took 3.46 seconds (mean sampled reward: -2623.68). Current reward after update: -301.30, Optimal reward -242.09
Iteration 317 took 3.43 seconds (mean sampled reward: -2722.57). Current reward after update: -328.74, Optimal reward -242.09
Iteration 318 took 3.43 seconds (mean sampled reward: -2695.08). Current reward after update: -268.98, Optimal reward -242.09
Iteration 319 took 3.42 seconds (mean sampled reward: -2902.96). Current reward after update: -389.19, Optimal reward -242.09
Iteration 320 took 3.43 seconds (mean sampled reward: -2912.27). Current reward after update: -343.63, Optimal reward -242.09
Iteration 321 took 3.49 seconds (mean sampled reward: -2782.19). Current reward after update: -231.12, Optimal reward -231.12
Iteration 322 took 3.44 seconds (mean sampled reward: -2775.45). Current reward after update: -275.72, Optimal reward -231.12
Iteration 323 took 3.46 seconds (mean sampled reward: -2864.87). Current reward after update: -291.26, Optimal reward -231.12
Iteration 324 took 3.45 seconds (mean sampled reward: -2743.76). Current reward after update: -320.63, Optimal reward -231.12
Iteration 325 took 3.40 seconds (mean sampled reward: -2637.28). Current reward after update: -303.08, Optimal reward -231.12
Iteration 326 took 3.40 seconds (mean sampled reward: -2732.33). Current reward after update: -358.99, Optimal reward -231.12
Iteration 327 took 3.46 seconds (mean sampled reward: -2668.84). Current reward after update: -372.19, Optimal reward -231.12
Iteration 328 took 3.44 seconds (mean sampled reward: -2846.81). Current reward after update: -266.84, Optimal reward -231.12
Iteration 329 took 3.44 seconds (mean sampled reward: -2779.56). Current reward after update: -288.24, Optimal reward -231.12
Iteration 330 took 3.40 seconds (mean sampled reward: -2745.31). Current reward after update: -282.38, Optimal reward -231.12
Iteration 331 took 3.40 seconds (mean sampled reward: -2635.46). Current reward after update: -385.62, Optimal reward -231.12
Iteration 332 took 3.42 seconds (mean sampled reward: -2739.73). Current reward after update: -274.70, Optimal reward -231.12
Iteration 333 took 3.44 seconds (mean sampled reward: -2850.60). Current reward after update: -266.52, Optimal reward -231.12
Iteration 334 took 3.45 seconds (mean sampled reward: -2862.28). Current reward after update: -375.41, Optimal reward -231.12
Iteration 335 took 3.46 seconds (mean sampled reward: -2943.36). Current reward after update: -354.20, Optimal reward -231.12
Iteration 336 took 3.48 seconds (mean sampled reward: -2758.02). Current reward after update: -296.58, Optimal reward -231.12
Iteration 337 took 3.45 seconds (mean sampled reward: -2833.84). Current reward after update: -259.77, Optimal reward -231.12
Iteration 338 took 3.60 seconds (mean sampled reward: -4394.27). Current reward after update: -258.59, Optimal reward -231.12
Iteration 339 took 3.48 seconds (mean sampled reward: -3303.09). Current reward after update: -371.02, Optimal reward -231.12
Iteration 340 took 3.45 seconds (mean sampled reward: -2804.98). Current reward after update: -238.04, Optimal reward -231.12
Iteration 341 took 3.41 seconds (mean sampled reward: -2975.95). Current reward after update: -290.53, Optimal reward -231.12
Iteration 342 took 3.48 seconds (mean sampled reward: -3685.47). Current reward after update: -229.70, Optimal reward -229.70
Iteration 343 took 3.47 seconds (mean sampled reward: -3435.06). Current reward after update: -231.91, Optimal reward -229.70
Iteration 344 took 3.42 seconds (mean sampled reward: -2727.30). Current reward after update: -326.32, Optimal reward -229.70
Iteration 345 took 3.42 seconds (mean sampled reward: -3050.70). Current reward after update: -362.32, Optimal reward -229.70
Iteration 346 took 3.45 seconds (mean sampled reward: -3042.03). Current reward after update: -261.49, Optimal reward -229.70
Iteration 347 took 3.54 seconds (mean sampled reward: -3378.22). Current reward after update: -292.00, Optimal reward -229.70
Iteration 348 took 3.50 seconds (mean sampled reward: -3220.52). Current reward after update: -307.46, Optimal reward -229.70
Iteration 349 took 3.48 seconds (mean sampled reward: -2886.29). Current reward after update: -235.15, Optimal reward -229.70
Iteration 350 took 3.46 seconds (mean sampled reward: -2666.22). Current reward after update: -276.47, Optimal reward -229.70
Iteration 351 took 3.41 seconds (mean sampled reward: -2744.97). Current reward after update: -294.98, Optimal reward -229.70
Iteration 352 took 3.40 seconds (mean sampled reward: -2719.44). Current reward after update: -300.46, Optimal reward -229.70
Iteration 353 took 3.42 seconds (mean sampled reward: -2635.10). Current reward after update: -252.72, Optimal reward -229.70
Iteration 354 took 3.53 seconds (mean sampled reward: -3630.45). Current reward after update: -368.22, Optimal reward -229.70
Iteration 355 took 3.45 seconds (mean sampled reward: -2862.54). Current reward after update: -262.72, Optimal reward -229.70
Iteration 356 took 3.48 seconds (mean sampled reward: -3508.50). Current reward after update: -311.13, Optimal reward -229.70
Iteration 357 took 3.51 seconds (mean sampled reward: -3131.64). Current reward after update: -333.05, Optimal reward -229.70
Iteration 358 took 3.50 seconds (mean sampled reward: -2478.70). Current reward after update: -292.03, Optimal reward -229.70
Iteration 359 took 3.50 seconds (mean sampled reward: -2586.46). Current reward after update: -289.82, Optimal reward -229.70
Iteration 360 took 3.46 seconds (mean sampled reward: -2278.19). Current reward after update: -346.02, Optimal reward -229.70
Iteration 361 took 3.49 seconds (mean sampled reward: -2491.17). Current reward after update: -870.34, Optimal reward -229.70
Iteration 362 took 3.47 seconds (mean sampled reward: -2745.14). Current reward after update: -275.01, Optimal reward -229.70
Iteration 363 took 3.45 seconds (mean sampled reward: -2432.46). Current reward after update: -314.91, Optimal reward -229.70
Iteration 364 took 3.41 seconds (mean sampled reward: -2200.99). Current reward after update: -344.18, Optimal reward -229.70
Iteration 365 took 3.44 seconds (mean sampled reward: -2146.09). Current reward after update: -333.87, Optimal reward -229.70
Iteration 366 took 3.49 seconds (mean sampled reward: -2218.34). Current reward after update: -344.68, Optimal reward -229.70
Iteration 367 took 3.43 seconds (mean sampled reward: -2109.76). Current reward after update: -331.82, Optimal reward -229.70
Iteration 368 took 3.47 seconds (mean sampled reward: -2177.60). Current reward after update: -365.35, Optimal reward -229.70
Iteration 369 took 3.45 seconds (mean sampled reward: -2280.84). Current reward after update: -367.62, Optimal reward -229.70
Iteration 370 took 3.42 seconds (mean sampled reward: -2761.65). Current reward after update: -388.98, Optimal reward -229.70
Iteration 371 took 3.43 seconds (mean sampled reward: -2405.51). Current reward after update: -616.76, Optimal reward -229.70
Iteration 372 took 3.43 seconds (mean sampled reward: -2650.71). Current reward after update: -351.24, Optimal reward -229.70
Iteration 373 took 3.45 seconds (mean sampled reward: -2393.19). Current reward after update: -305.22, Optimal reward -229.70
Iteration 374 took 3.43 seconds (mean sampled reward: -2176.21). Current reward after update: -300.75, Optimal reward -229.70
Iteration 375 took 3.43 seconds (mean sampled reward: -2108.27). Current reward after update: -333.51, Optimal reward -229.70
Iteration 376 took 3.41 seconds (mean sampled reward: -2194.16). Current reward after update: -246.66, Optimal reward -229.70
Iteration 377 took 3.48 seconds (mean sampled reward: -2176.68). Current reward after update: -327.97, Optimal reward -229.70
Iteration 378 took 3.48 seconds (mean sampled reward: -2202.90). Current reward after update: -311.84, Optimal reward -229.70
Iteration 379 took 3.44 seconds (mean sampled reward: -2284.79). Current reward after update: -318.77, Optimal reward -229.70
Iteration 380 took 3.42 seconds (mean sampled reward: -2477.57). Current reward after update: -318.93, Optimal reward -229.70
Iteration 381 took 3.40 seconds (mean sampled reward: -2491.87). Current reward after update: -269.59, Optimal reward -229.70
Iteration 382 took 3.41 seconds (mean sampled reward: -2392.49). Current reward after update: -281.60, Optimal reward -229.70
Iteration 383 took 3.36 seconds (mean sampled reward: -2540.17). Current reward after update: -267.61, Optimal reward -229.70
Iteration 384 took 3.36 seconds (mean sampled reward: -2715.30). Current reward after update: -338.98, Optimal reward -229.70
Iteration 385 took 3.41 seconds (mean sampled reward: -2443.77). Current reward after update: -265.65, Optimal reward -229.70
Iteration 386 took 3.40 seconds (mean sampled reward: -2560.65). Current reward after update: -232.10, Optimal reward -229.70
Iteration 387 took 3.36 seconds (mean sampled reward: -3092.85). Current reward after update: -269.63, Optimal reward -229.70
Iteration 388 took 3.41 seconds (mean sampled reward: -2452.27). Current reward after update: -280.98, Optimal reward -229.70
Iteration 389 took 3.34 seconds (mean sampled reward: -2952.00). Current reward after update: -275.86, Optimal reward -229.70
Iteration 390 took 3.38 seconds (mean sampled reward: -2932.73). Current reward after update: -332.09, Optimal reward -229.70
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
Iteration 391 took 3.44 seconds (mean sampled reward: -2735.19). Current reward after update: -507.86, Optimal reward -229.70
Iteration 392 took 3.44 seconds (mean sampled reward: -2867.08). Current reward after update: -266.07, Optimal reward -229.70
Iteration 393 took 3.38 seconds (mean sampled reward: -3016.59). Current reward after update: -266.39, Optimal reward -229.70
Iteration 394 took 3.47 seconds (mean sampled reward: -2744.06). Current reward after update: -292.11, Optimal reward -229.70
Iteration 395 took 3.47 seconds (mean sampled reward: -3196.47). Current reward after update: -301.50, Optimal reward -229.70
Iteration 396 took 3.44 seconds (mean sampled reward: -3252.56). Current reward after update: -327.20, Optimal reward -229.70
Iteration 397 took 3.33 seconds (mean sampled reward: -3362.33). Current reward after update: -292.55, Optimal reward -229.70
Iteration 398 took 3.40 seconds (mean sampled reward: -3141.07). Current reward after update: -295.06, Optimal reward -229.70
Iteration 399 took 3.50 seconds (mean sampled reward: -2561.00). Current reward after update: -298.95, Optimal reward -229.70
Iteration 400 took 3.41 seconds (mean sampled reward: -2571.97). Current reward after update: -295.06, Optimal reward -229.70
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
Iteration 1 took 3.90 seconds (mean sampled reward: -7493.19). Current reward after update: -4753.47, Optimal reward -4753.47
Iteration 2 took 3.75 seconds (mean sampled reward: -7190.85). Current reward after update: -3795.51, Optimal reward -3795.51
Iteration 3 took 3.63 seconds (mean sampled reward: -6579.87). Current reward after update: -3312.13, Optimal reward -3312.13
Iteration 4 took 3.63 seconds (mean sampled reward: -6820.48). Current reward after update: -3095.50, Optimal reward -3095.50
Iteration 5 took 3.82 seconds (mean sampled reward: -6461.30). Current reward after update: -2856.35, Optimal reward -2856.35
Iteration 6 took 3.92 seconds (mean sampled reward: -6429.42). Current reward after update: -2742.64, Optimal reward -2742.64
Iteration 7 took 3.63 seconds (mean sampled reward: -5769.79). Current reward after update: -2512.56, Optimal reward -2512.56
Iteration 8 took 3.80 seconds (mean sampled reward: -5666.76). Current reward after update: -2084.28, Optimal reward -2084.28
Iteration 9 took 3.85 seconds (mean sampled reward: -4335.68). Current reward after update: -2034.15, Optimal reward -2034.15
Iteration 10 took 3.74 seconds (mean sampled reward: -5377.29). Current reward after update: -1777.14, Optimal reward -1777.14
Iteration 11 took 3.65 seconds (mean sampled reward: -5360.60). Current reward after update: -1726.00, Optimal reward -1726.00
Iteration 12 took 3.73 seconds (mean sampled reward: -6292.06). Current reward after update: -1626.22, Optimal reward -1626.22
Iteration 13 took 3.67 seconds (mean sampled reward: -6805.54). Current reward after update: -1675.68, Optimal reward -1626.22
Iteration 14 took 3.70 seconds (mean sampled reward: -7192.35). Current reward after update: -1272.87, Optimal reward -1272.87
Iteration 15 took 3.65 seconds (mean sampled reward: -5637.66). Current reward after update: -1324.40, Optimal reward -1272.87
Iteration 16 took 3.72 seconds (mean sampled reward: -6342.57). Current reward after update: -1562.35, Optimal reward -1272.87
Iteration 17 took 3.75 seconds (mean sampled reward: -6761.86). Current reward after update: -1327.82, Optimal reward -1272.87
Iteration 18 took 3.65 seconds (mean sampled reward: -5252.24). Current reward after update: -939.37, Optimal reward -939.37
Iteration 19 took 3.65 seconds (mean sampled reward: -6011.26). Current reward after update: -954.81, Optimal reward -939.37
Iteration 20 took 3.65 seconds (mean sampled reward: -5829.22). Current reward after update: -977.55, Optimal reward -939.37
Iteration 21 took 3.71 seconds (mean sampled reward: -5715.15). Current reward after update: -886.44, Optimal reward -886.44
Iteration 22 took 3.67 seconds (mean sampled reward: -5551.76). Current reward after update: -804.25, Optimal reward -804.25
Iteration 23 took 3.81 seconds (mean sampled reward: -6121.00). Current reward after update: -771.67, Optimal reward -771.67
Iteration 24 took 3.69 seconds (mean sampled reward: -5761.00). Current reward after update: -897.64, Optimal reward -771.67
Iteration 25 took 3.68 seconds (mean sampled reward: -5499.45). Current reward after update: -822.21, Optimal reward -771.67
Iteration 26 took 3.68 seconds (mean sampled reward: -5121.02). Current reward after update: -849.70, Optimal reward -771.67
Iteration 27 took 3.73 seconds (mean sampled reward: -5310.15). Current reward after update: -766.12, Optimal reward -766.12
Iteration 28 took 3.88 seconds (mean sampled reward: -5870.71). Current reward after update: -935.95, Optimal reward -766.12
Iteration 29 took 3.69 seconds (mean sampled reward: -5455.73). Current reward after update: -827.07, Optimal reward -766.12
Iteration 30 took 3.70 seconds (mean sampled reward: -5951.07). Current reward after update: -802.93, Optimal reward -766.12
Iteration 31 took 3.78 seconds (mean sampled reward: -6429.71). Current reward after update: -798.32, Optimal reward -766.12
Iteration 32 took 3.69 seconds (mean sampled reward: -5322.35). Current reward after update: -638.21, Optimal reward -638.21
Iteration 33 took 3.76 seconds (mean sampled reward: -5901.89). Current reward after update: -871.00, Optimal reward -638.21
Iteration 34 took 3.71 seconds (mean sampled reward: -6068.32). Current reward after update: -945.62, Optimal reward -638.21
Iteration 35 took 3.70 seconds (mean sampled reward: -6296.73). Current reward after update: -724.10, Optimal reward -638.21
Iteration 36 took 3.57 seconds (mean sampled reward: -4929.54). Current reward after update: -785.30, Optimal reward -638.21
Iteration 37 took 3.65 seconds (mean sampled reward: -4455.77). Current reward after update: -756.06, Optimal reward -638.21
Iteration 38 took 3.70 seconds (mean sampled reward: -4617.26). Current reward after update: -715.24, Optimal reward -638.21
Iteration 39 took 3.63 seconds (mean sampled reward: -4221.62). Current reward after update: -363.01, Optimal reward -363.01
Iteration 40 took 3.78 seconds (mean sampled reward: -5132.91). Current reward after update: -390.94, Optimal reward -363.01
Iteration 41 took 3.69 seconds (mean sampled reward: -4502.42). Current reward after update: -352.51, Optimal reward -352.51
Iteration 42 took 3.80 seconds (mean sampled reward: -5241.03). Current reward after update: -259.95, Optimal reward -259.95
Iteration 43 took 3.85 seconds (mean sampled reward: -5789.33). Current reward after update: -408.69, Optimal reward -259.95
Iteration 44 took 3.84 seconds (mean sampled reward: -6181.14). Current reward after update: -429.47, Optimal reward -259.95
Iteration 45 took 3.91 seconds (mean sampled reward: -6328.65). Current reward after update: -454.34, Optimal reward -259.95
Iteration 46 took 3.93 seconds (mean sampled reward: -6182.61). Current reward after update: -449.42, Optimal reward -259.95
Iteration 47 took 3.91 seconds (mean sampled reward: -6255.92). Current reward after update: -312.17, Optimal reward -259.95
Iteration 48 took 3.78 seconds (mean sampled reward: -5023.77). Current reward after update: -313.28, Optimal reward -259.95
Iteration 49 took 3.82 seconds (mean sampled reward: -5965.59). Current reward after update: -374.48, Optimal reward -259.95
Iteration 50 took 3.71 seconds (mean sampled reward: -4980.00). Current reward after update: -374.18, Optimal reward -259.95
Iteration 51 took 3.72 seconds (mean sampled reward: -4496.87). Current reward after update: -296.30, Optimal reward -259.95
Iteration 52 took 3.73 seconds (mean sampled reward: -5001.69). Current reward after update: -303.32, Optimal reward -259.95
Iteration 53 took 3.84 seconds (mean sampled reward: -5239.66). Current reward after update: -387.29, Optimal reward -259.95
Iteration 54 took 3.80 seconds (mean sampled reward: -5652.55). Current reward after update: -357.13, Optimal reward -259.95
Iteration 55 took 4.07 seconds (mean sampled reward: -5619.68). Current reward after update: -373.63, Optimal reward -259.95
Iteration 56 took 3.77 seconds (mean sampled reward: -5098.18). Current reward after update: -355.91, Optimal reward -259.95
Iteration 57 took 3.72 seconds (mean sampled reward: -4104.25). Current reward after update: -346.58, Optimal reward -259.95
Iteration 58 took 3.76 seconds (mean sampled reward: -3629.93). Current reward after update: -238.90, Optimal reward -238.90
Iteration 59 took 3.67 seconds (mean sampled reward: -3475.33). Current reward after update: -234.37, Optimal reward -234.37
Iteration 60 took 3.72 seconds (mean sampled reward: -4308.00). Current reward after update: -361.81, Optimal reward -234.37
Iteration 61 took 3.68 seconds (mean sampled reward: -4924.63). Current reward after update: -347.85, Optimal reward -234.37
Iteration 62 took 3.80 seconds (mean sampled reward: -5686.54). Current reward after update: -338.56, Optimal reward -234.37
Iteration 63 took 3.69 seconds (mean sampled reward: -4883.37). Current reward after update: -411.52, Optimal reward -234.37
Iteration 64 took 3.71 seconds (mean sampled reward: -3199.72). Current reward after update: -324.06, Optimal reward -234.37
Iteration 65 took 3.63 seconds (mean sampled reward: -3486.58). Current reward after update: -303.67, Optimal reward -234.37
Iteration 66 took 3.64 seconds (mean sampled reward: -5277.30). Current reward after update: -264.18, Optimal reward -234.37
Iteration 67 took 3.75 seconds (mean sampled reward: -4386.24). Current reward after update: -264.28, Optimal reward -234.37
Iteration 68 took 3.82 seconds (mean sampled reward: -5506.86). Current reward after update: -288.09, Optimal reward -234.37
Iteration 69 took 3.74 seconds (mean sampled reward: -5513.83). Current reward after update: -347.78, Optimal reward -234.37
Iteration 70 took 3.71 seconds (mean sampled reward: -3596.05). Current reward after update: -338.32, Optimal reward -234.37
Iteration 71 took 3.68 seconds (mean sampled reward: -3424.09). Current reward after update: -268.29, Optimal reward -234.37
Iteration 72 took 3.85 seconds (mean sampled reward: -4281.74). Current reward after update: -342.79, Optimal reward -234.37
Iteration 73 took 3.81 seconds (mean sampled reward: -5351.75). Current reward after update: -409.04, Optimal reward -234.37
Iteration 74 took 3.88 seconds (mean sampled reward: -5233.59). Current reward after update: -440.40, Optimal reward -234.37
Iteration 75 took 3.83 seconds (mean sampled reward: -4915.43). Current reward after update: -403.00, Optimal reward -234.37
Iteration 76 took 3.80 seconds (mean sampled reward: -5138.85). Current reward after update: -466.99, Optimal reward -234.37
Iteration 77 took 3.76 seconds (mean sampled reward: -4322.43). Current reward after update: -400.58, Optimal reward -234.37
Iteration 78 took 3.82 seconds (mean sampled reward: -4542.50). Current reward after update: -354.36, Optimal reward -234.37
Iteration 79 took 3.82 seconds (mean sampled reward: -5078.96). Current reward after update: -413.09, Optimal reward -234.37
Iteration 80 took 3.81 seconds (mean sampled reward: -3661.89). Current reward after update: -361.20, Optimal reward -234.37
Iteration 81 took 3.77 seconds (mean sampled reward: -4434.33). Current reward after update: -333.47, Optimal reward -234.37
Iteration 82 took 3.81 seconds (mean sampled reward: -3923.29). Current reward after update: -328.08, Optimal reward -234.37
Iteration 83 took 3.72 seconds (mean sampled reward: -4626.48). Current reward after update: -402.09, Optimal reward -234.37
Iteration 84 took 3.74 seconds (mean sampled reward: -3898.28). Current reward after update: -343.12, Optimal reward -234.37
Iteration 85 took 3.87 seconds (mean sampled reward: -5234.31). Current reward after update: -283.35, Optimal reward -234.37
Iteration 86 took 3.79 seconds (mean sampled reward: -4428.13). Current reward after update: -326.13, Optimal reward -234.37
Iteration 87 took 3.80 seconds (mean sampled reward: -4154.90). Current reward after update: -257.62, Optimal reward -234.37
Iteration 88 took 3.71 seconds (mean sampled reward: -4113.48). Current reward after update: -310.53, Optimal reward -234.37
Iteration 89 took 3.79 seconds (mean sampled reward: -3263.14). Current reward after update: -259.93, Optimal reward -234.37
Iteration 90 took 3.84 seconds (mean sampled reward: -4538.69). Current reward after update: -274.82, Optimal reward -234.37
Iteration 91 took 3.77 seconds (mean sampled reward: -4632.24). Current reward after update: -291.66, Optimal reward -234.37
Iteration 92 took 3.80 seconds (mean sampled reward: -3923.04). Current reward after update: -303.45, Optimal reward -234.37
Iteration 93 took 3.80 seconds (mean sampled reward: -3935.80). Current reward after update: -281.45, Optimal reward -234.37
Iteration 94 took 3.69 seconds (mean sampled reward: -3419.88). Current reward after update: -290.55, Optimal reward -234.37
Iteration 95 took 3.69 seconds (mean sampled reward: -3385.89). Current reward after update: -214.82, Optimal reward -214.82
Iteration 96 took 3.69 seconds (mean sampled reward: -3175.39). Current reward after update: -227.67, Optimal reward -214.82
Iteration 97 took 3.77 seconds (mean sampled reward: -3690.89). Current reward after update: -300.99, Optimal reward -214.82
Iteration 98 took 3.67 seconds (mean sampled reward: -3606.97). Current reward after update: -344.95, Optimal reward -214.82
Iteration 99 took 3.70 seconds (mean sampled reward: -5045.76). Current reward after update: -330.81, Optimal reward -214.82
Iteration 100 took 3.71 seconds (mean sampled reward: -4391.76). Current reward after update: -337.51, Optimal reward -214.82
Iteration 101 took 3.72 seconds (mean sampled reward: -3403.23). Current reward after update: -328.71, Optimal reward -214.82
Iteration 102 took 3.64 seconds (mean sampled reward: -3531.37). Current reward after update: -307.82, Optimal reward -214.82
Iteration 103 took 3.72 seconds (mean sampled reward: -4616.28). Current reward after update: -284.30, Optimal reward -214.82
Iteration 104 took 3.67 seconds (mean sampled reward: -4184.65). Current reward after update: -291.95, Optimal reward -214.82
Iteration 105 took 3.75 seconds (mean sampled reward: -3732.59). Current reward after update: -377.17, Optimal reward -214.82
Iteration 106 took 3.77 seconds (mean sampled reward: -3426.56). Current reward after update: -287.29, Optimal reward -214.82
Iteration 107 took 3.72 seconds (mean sampled reward: -3666.93). Current reward after update: -314.58, Optimal reward -214.82
Iteration 108 took 3.59 seconds (mean sampled reward: -4116.52). Current reward after update: -287.32, Optimal reward -214.82
Iteration 109 took 3.58 seconds (mean sampled reward: -4438.35). Current reward after update: -334.02, Optimal reward -214.82
Iteration 110 took 3.87 seconds (mean sampled reward: -5089.98). Current reward after update: -320.13, Optimal reward -214.82
Iteration 111 took 3.55 seconds (mean sampled reward: -4389.41). Current reward after update: -448.33, Optimal reward -214.82
Iteration 112 took 3.60 seconds (mean sampled reward: -4610.94). Current reward after update: -381.67, Optimal reward -214.82
Iteration 113 took 3.88 seconds (mean sampled reward: -4681.97). Current reward after update: -244.99, Optimal reward -214.82
Iteration 114 took 3.74 seconds (mean sampled reward: -4251.01). Current reward after update: -262.02, Optimal reward -214.82
Iteration 115 took 3.60 seconds (mean sampled reward: -4018.40). Current reward after update: -272.72, Optimal reward -214.82
Iteration 116 took 3.75 seconds (mean sampled reward: -3670.85). Current reward after update: -290.58, Optimal reward -214.82
Iteration 117 took 3.65 seconds (mean sampled reward: -3341.83). Current reward after update: -262.45, Optimal reward -214.82
Iteration 118 took 3.71 seconds (mean sampled reward: -3315.36). Current reward after update: -232.64, Optimal reward -214.82
Iteration 119 took 3.73 seconds (mean sampled reward: -3420.79). Current reward after update: -248.15, Optimal reward -214.82
Iteration 120 took 3.71 seconds (mean sampled reward: -3745.36). Current reward after update: -245.53, Optimal reward -214.82
Iteration 121 took 3.76 seconds (mean sampled reward: -3948.00). Current reward after update: -296.85, Optimal reward -214.82
Iteration 122 took 3.78 seconds (mean sampled reward: -4782.03). Current reward after update: -319.61, Optimal reward -214.82
Iteration 123 took 3.69 seconds (mean sampled reward: -3991.58). Current reward after update: -280.24, Optimal reward -214.82
Iteration 124 took 3.74 seconds (mean sampled reward: -4473.54). Current reward after update: -254.25, Optimal reward -214.82
Iteration 125 took 3.76 seconds (mean sampled reward: -4478.46). Current reward after update: -280.34, Optimal reward -214.82
Iteration 126 took 3.69 seconds (mean sampled reward: -3657.79). Current reward after update: -249.38, Optimal reward -214.82
Iteration 127 took 3.77 seconds (mean sampled reward: -3523.56). Current reward after update: -224.92, Optimal reward -214.82
Iteration 128 took 3.74 seconds (mean sampled reward: -3437.16). Current reward after update: -231.81, Optimal reward -214.82
Iteration 129 took 3.75 seconds (mean sampled reward: -3529.85). Current reward after update: -219.43, Optimal reward -214.82
Iteration 130 took 3.73 seconds (mean sampled reward: -4076.96). Current reward after update: -202.28, Optimal reward -202.28
Iteration 131 took 3.71 seconds (mean sampled reward: -3687.58). Current reward after update: -214.85, Optimal reward -202.28
Iteration 132 took 3.72 seconds (mean sampled reward: -4372.22). Current reward after update: -6610.37, Optimal reward -202.28
Iteration 133 took 3.78 seconds (mean sampled reward: -5424.31). Current reward after update: -225.58, Optimal reward -202.28
Iteration 134 took 3.75 seconds (mean sampled reward: -5089.63). Current reward after update: -206.13, Optimal reward -202.28
Iteration 135 took 3.75 seconds (mean sampled reward: -4988.45). Current reward after update: -230.00, Optimal reward -202.28
Iteration 136 took 3.82 seconds (mean sampled reward: -3599.28). Current reward after update: -198.31, Optimal reward -198.31
Iteration 137 took 3.77 seconds (mean sampled reward: -4140.55). Current reward after update: -230.67, Optimal reward -198.31
Iteration 138 took 3.73 seconds (mean sampled reward: -3815.11). Current reward after update: -212.02, Optimal reward -198.31
Iteration 139 took 3.73 seconds (mean sampled reward: -3883.89). Current reward after update: -231.48, Optimal reward -198.31
Iteration 140 took 3.78 seconds (mean sampled reward: -3638.11). Current reward after update: -253.52, Optimal reward -198.31
Iteration 141 took 3.72 seconds (mean sampled reward: -2177.51). Current reward after update: -249.71, Optimal reward -198.31
Iteration 142 took 3.87 seconds (mean sampled reward: -3669.72). Current reward after update: -249.02, Optimal reward -198.31
Iteration 143 took 3.75 seconds (mean sampled reward: -3879.56). Current reward after update: -242.36, Optimal reward -198.31
Iteration 144 took 3.88 seconds (mean sampled reward: -3783.08). Current reward after update: -204.04, Optimal reward -198.31
Iteration 145 took 3.79 seconds (mean sampled reward: -3879.53). Current reward after update: -210.15, Optimal reward -198.31
Iteration 146 took 3.76 seconds (mean sampled reward: -3902.09). Current reward after update: -223.19, Optimal reward -198.31
Iteration 147 took 3.69 seconds (mean sampled reward: -3082.11). Current reward after update: -208.64, Optimal reward -198.31
Iteration 148 took 3.69 seconds (mean sampled reward: -3864.59). Current reward after update: -215.86, Optimal reward -198.31
Iteration 149 took 3.71 seconds (mean sampled reward: -4014.24). Current reward after update: -195.96, Optimal reward -195.96
Iteration 150 took 3.64 seconds (mean sampled reward: -3148.94). Current reward after update: -188.81, Optimal reward -188.81
Iteration 151 took 3.71 seconds (mean sampled reward: -4291.55). Current reward after update: -206.14, Optimal reward -188.81
Iteration 152 took 3.65 seconds (mean sampled reward: -3333.69). Current reward after update: -228.11, Optimal reward -188.81
Iteration 153 took 3.71 seconds (mean sampled reward: -2921.25). Current reward after update: -193.73, Optimal reward -188.81
Iteration 154 took 3.71 seconds (mean sampled reward: -4219.09). Current reward after update: -214.14, Optimal reward -188.81
Iteration 155 took 3.73 seconds (mean sampled reward: -3446.52). Current reward after update: -207.61, Optimal reward -188.81
Iteration 156 took 3.66 seconds (mean sampled reward: -3171.49). Current reward after update: -196.02, Optimal reward -188.81
Iteration 157 took 3.67 seconds (mean sampled reward: -2987.25). Current reward after update: -191.09, Optimal reward -188.81
Iteration 158 took 3.71 seconds (mean sampled reward: -3023.36). Current reward after update: -221.45, Optimal reward -188.81
Iteration 159 took 3.71 seconds (mean sampled reward: -3990.33). Current reward after update: -235.71, Optimal reward -188.81
Iteration 160 took 3.78 seconds (mean sampled reward: -3872.68). Current reward after update: -235.97, Optimal reward -188.81
Iteration 161 took 3.69 seconds (mean sampled reward: -3999.37). Current reward after update: -214.28, Optimal reward -188.81
Iteration 162 took 3.71 seconds (mean sampled reward: -3447.69). Current reward after update: -221.90, Optimal reward -188.81
Iteration 163 took 3.63 seconds (mean sampled reward: -2981.06). Current reward after update: -209.73, Optimal reward -188.81
Iteration 164 took 3.63 seconds (mean sampled reward: -3621.76). Current reward after update: -188.07, Optimal reward -188.07
Iteration 165 took 3.62 seconds (mean sampled reward: -2818.62). Current reward after update: -183.17, Optimal reward -183.17
Iteration 166 took 3.75 seconds (mean sampled reward: -4470.91). Current reward after update: -250.07, Optimal reward -183.17
Iteration 167 took 3.70 seconds (mean sampled reward: -3392.07). Current reward after update: -236.54, Optimal reward -183.17
Iteration 168 took 3.66 seconds (mean sampled reward: -2768.94). Current reward after update: -231.27, Optimal reward -183.17
Iteration 169 took 3.57 seconds (mean sampled reward: -2857.36). Current reward after update: -229.07, Optimal reward -183.17
Iteration 170 took 3.63 seconds (mean sampled reward: -3223.22). Current reward after update: -241.42, Optimal reward -183.17
Iteration 171 took 3.59 seconds (mean sampled reward: -2303.64). Current reward after update: -294.27, Optimal reward -183.17
Iteration 172 took 3.54 seconds (mean sampled reward: -2352.05). Current reward after update: -299.22, Optimal reward -183.17
Iteration 173 took 3.64 seconds (mean sampled reward: -2462.52). Current reward after update: -279.04, Optimal reward -183.17
Iteration 174 took 3.64 seconds (mean sampled reward: -2491.32). Current reward after update: -290.16, Optimal reward -183.17
Iteration 175 took 3.66 seconds (mean sampled reward: -2755.93). Current reward after update: -274.28, Optimal reward -183.17
Iteration 176 took 3.68 seconds (mean sampled reward: -3015.24). Current reward after update: -258.37, Optimal reward -183.17
Iteration 177 took 3.63 seconds (mean sampled reward: -2112.30). Current reward after update: -234.36, Optimal reward -183.17
Iteration 178 took 3.64 seconds (mean sampled reward: -2583.96). Current reward after update: -207.96, Optimal reward -183.17
Iteration 179 took 3.64 seconds (mean sampled reward: -2644.82). Current reward after update: -208.61, Optimal reward -183.17
Iteration 180 took 3.60 seconds (mean sampled reward: -2881.69). Current reward after update: -197.57, Optimal reward -183.17
Iteration 181 took 3.63 seconds (mean sampled reward: -3119.90). Current reward after update: -248.99, Optimal reward -183.17
Iteration 182 took 3.61 seconds (mean sampled reward: -2650.95). Current reward after update: -238.51, Optimal reward -183.17
Iteration 183 took 3.57 seconds (mean sampled reward: -2786.92). Current reward after update: -276.78, Optimal reward -183.17
Iteration 184 took 3.56 seconds (mean sampled reward: -2194.45). Current reward after update: -258.03, Optimal reward -183.17
Iteration 185 took 3.60 seconds (mean sampled reward: -2660.41). Current reward after update: -192.06, Optimal reward -183.17
Iteration 186 took 3.55 seconds (mean sampled reward: -2748.37). Current reward after update: -217.05, Optimal reward -183.17
Iteration 187 took 3.51 seconds (mean sampled reward: -2335.92). Current reward after update: -210.48, Optimal reward -183.17
Iteration 188 took 3.64 seconds (mean sampled reward: -3651.83). Current reward after update: -271.66, Optimal reward -183.17
Iteration 189 took 3.51 seconds (mean sampled reward: -3312.29). Current reward after update: -228.47, Optimal reward -183.17
Iteration 190 took 3.52 seconds (mean sampled reward: -3748.05). Current reward after update: -144.50, Optimal reward -144.50
Iteration 191 took 3.60 seconds (mean sampled reward: -3026.77). Current reward after update: -125.04, Optimal reward -125.04
Iteration 192 took 3.53 seconds (mean sampled reward: -3553.10). Current reward after update: -144.76, Optimal reward -125.04
Iteration 193 took 3.51 seconds (mean sampled reward: -3227.20). Current reward after update: -159.24, Optimal reward -125.04
Iteration 194 took 3.56 seconds (mean sampled reward: -2782.67). Current reward after update: -158.79, Optimal reward -125.04
Iteration 195 took 3.57 seconds (mean sampled reward: -2110.53). Current reward after update: -169.19, Optimal reward -125.04
Iteration 196 took 3.57 seconds (mean sampled reward: -2788.07). Current reward after update: -148.02, Optimal reward -125.04
Iteration 197 took 3.64 seconds (mean sampled reward: -2673.18). Current reward after update: -155.11, Optimal reward -125.04
Iteration 198 took 3.56 seconds (mean sampled reward: -2780.43). Current reward after update: -167.22, Optimal reward -125.04
Iteration 199 took 3.52 seconds (mean sampled reward: -3758.75). Current reward after update: -150.25, Optimal reward -125.04
Iteration 200 took 3.51 seconds (mean sampled reward: -3630.42). Current reward after update: -204.69, Optimal reward -125.04
Iteration 201 took 3.56 seconds (mean sampled reward: -3235.99). Current reward after update: -159.10, Optimal reward -125.04
Iteration 202 took 3.55 seconds (mean sampled reward: -2840.28). Current reward after update: -162.41, Optimal reward -125.04
Iteration 203 took 3.54 seconds (mean sampled reward: -2404.56). Current reward after update: -143.03, Optimal reward -125.04
Iteration 204 took 3.50 seconds (mean sampled reward: -2592.08). Current reward after update: -149.49, Optimal reward -125.04
Iteration 205 took 3.55 seconds (mean sampled reward: -3176.15). Current reward after update: -154.24, Optimal reward -125.04
Iteration 206 took 3.59 seconds (mean sampled reward: -2731.51). Current reward after update: -178.66, Optimal reward -125.04
Iteration 207 took 3.69 seconds (mean sampled reward: -2696.17). Current reward after update: -176.56, Optimal reward -125.04
Iteration 208 took 3.55 seconds (mean sampled reward: -2035.25). Current reward after update: -182.01, Optimal reward -125.04
Iteration 209 took 3.70 seconds (mean sampled reward: -2604.08). Current reward after update: -127.09, Optimal reward -125.04
Iteration 210 took 3.71 seconds (mean sampled reward: -3112.23). Current reward after update: -216.00, Optimal reward -125.04
Iteration 211 took 3.66 seconds (mean sampled reward: -3088.62). Current reward after update: -143.03, Optimal reward -125.04
Iteration 212 took 3.93 seconds (mean sampled reward: -2537.19). Current reward after update: -189.51, Optimal reward -125.04
Iteration 213 took 3.83 seconds (mean sampled reward: -1853.11). Current reward after update: -168.88, Optimal reward -125.04
Iteration 214 took 3.67 seconds (mean sampled reward: -1829.32). Current reward after update: -156.90, Optimal reward -125.04
Iteration 215 took 3.60 seconds (mean sampled reward: -1952.24). Current reward after update: -149.69, Optimal reward -125.04
Iteration 216 took 3.52 seconds (mean sampled reward: -2015.32). Current reward after update: -179.92, Optimal reward -125.04
Iteration 217 took 3.58 seconds (mean sampled reward: -2149.18). Current reward after update: -164.82, Optimal reward -125.04
Iteration 218 took 3.69 seconds (mean sampled reward: -2692.21). Current reward after update: -213.34, Optimal reward -125.04
Iteration 219 took 3.66 seconds (mean sampled reward: -2518.86). Current reward after update: -166.42, Optimal reward -125.04
Iteration 220 took 3.63 seconds (mean sampled reward: -2519.07). Current reward after update: -152.31, Optimal reward -125.04
Iteration 221 took 3.61 seconds (mean sampled reward: -2457.85). Current reward after update: -213.36, Optimal reward -125.04
Iteration 222 took 3.58 seconds (mean sampled reward: -1587.59). Current reward after update: -164.25, Optimal reward -125.04
Iteration 223 took 3.60 seconds (mean sampled reward: -1493.34). Current reward after update: -148.04, Optimal reward -125.04
Iteration 224 took 3.68 seconds (mean sampled reward: -1944.21). Current reward after update: -210.00, Optimal reward -125.04
Iteration 225 took 3.69 seconds (mean sampled reward: -1761.31). Current reward after update: -200.56, Optimal reward -125.04
Iteration 226 took 3.69 seconds (mean sampled reward: -2399.98). Current reward after update: -216.24, Optimal reward -125.04
Iteration 227 took 3.67 seconds (mean sampled reward: -2591.88). Current reward after update: -190.46, Optimal reward -125.04
Iteration 228 took 3.64 seconds (mean sampled reward: -2775.91). Current reward after update: -219.93, Optimal reward -125.04
Iteration 229 took 3.60 seconds (mean sampled reward: -1860.72). Current reward after update: -236.43, Optimal reward -125.04
Iteration 230 took 3.79 seconds (mean sampled reward: -2330.69). Current reward after update: -178.47, Optimal reward -125.04
Iteration 231 took 3.62 seconds (mean sampled reward: -1555.42). Current reward after update: -183.72, Optimal reward -125.04
Iteration 232 took 3.62 seconds (mean sampled reward: -1491.95). Current reward after update: -241.73, Optimal reward -125.04
Iteration 233 took 3.57 seconds (mean sampled reward: -1678.21). Current reward after update: -182.04, Optimal reward -125.04
Iteration 234 took 3.61 seconds (mean sampled reward: -1756.18). Current reward after update: -214.46, Optimal reward -125.04
Iteration 235 took 3.65 seconds (mean sampled reward: -1580.08). Current reward after update: -202.60, Optimal reward -125.04
Iteration 236 took 3.63 seconds (mean sampled reward: -1930.44). Current reward after update: -192.47, Optimal reward -125.04
Iteration 237 took 3.61 seconds (mean sampled reward: -1788.49). Current reward after update: -204.58, Optimal reward -125.04
Iteration 238 took 3.62 seconds (mean sampled reward: -1819.26). Current reward after update: -207.46, Optimal reward -125.04
Iteration 239 took 3.60 seconds (mean sampled reward: -1585.51). Current reward after update: -200.83, Optimal reward -125.04
Iteration 240 took 3.60 seconds (mean sampled reward: -1741.20). Current reward after update: -204.27, Optimal reward -125.04
Iteration 241 took 3.64 seconds (mean sampled reward: -1723.71). Current reward after update: -200.42, Optimal reward -125.04
Iteration 242 took 3.62 seconds (mean sampled reward: -1416.16). Current reward after update: -141.14, Optimal reward -125.04
Iteration 243 took 3.64 seconds (mean sampled reward: -1434.47). Current reward after update: -148.96, Optimal reward -125.04
Iteration 244 took 3.61 seconds (mean sampled reward: -1297.77). Current reward after update: -173.08, Optimal reward -125.04
Iteration 245 took 3.65 seconds (mean sampled reward: -1403.07). Current reward after update: -174.10, Optimal reward -125.04
Iteration 246 took 3.72 seconds (mean sampled reward: -2703.24). Current reward after update: -904.16, Optimal reward -125.04
Iteration 247 took 3.66 seconds (mean sampled reward: -1516.01). Current reward after update: -183.33, Optimal reward -125.04
Iteration 248 took 3.68 seconds (mean sampled reward: -1401.60). Current reward after update: -141.15, Optimal reward -125.04
Iteration 249 took 3.70 seconds (mean sampled reward: -1329.06). Current reward after update: -141.35, Optimal reward -125.04
Iteration 250 took 3.63 seconds (mean sampled reward: -1678.33). Current reward after update: -149.01, Optimal reward -125.04
Iteration 251 took 3.67 seconds (mean sampled reward: -2357.03). Current reward after update: -175.72, Optimal reward -125.04
Iteration 252 took 3.62 seconds (mean sampled reward: -1182.51). Current reward after update: -158.71, Optimal reward -125.04
Iteration 253 took 3.64 seconds (mean sampled reward: -1266.39). Current reward after update: -150.58, Optimal reward -125.04
Iteration 254 took 3.64 seconds (mean sampled reward: -1560.38). Current reward after update: -134.62, Optimal reward -125.04
Iteration 255 took 3.78 seconds (mean sampled reward: -4017.40). Current reward after update: -144.57, Optimal reward -125.04
Iteration 256 took 3.78 seconds (mean sampled reward: -3784.98). Current reward after update: -166.72, Optimal reward -125.04
Iteration 257 took 3.66 seconds (mean sampled reward: -1518.26). Current reward after update: -165.06, Optimal reward -125.04
Iteration 258 took 3.69 seconds (mean sampled reward: -1545.45). Current reward after update: -181.91, Optimal reward -125.04
Iteration 259 took 3.78 seconds (mean sampled reward: -3427.37). Current reward after update: -164.45, Optimal reward -125.04
Iteration 260 took 3.68 seconds (mean sampled reward: -2451.96). Current reward after update: -187.78, Optimal reward -125.04
Iteration 261 took 3.63 seconds (mean sampled reward: -2108.95). Current reward after update: -175.41, Optimal reward -125.04
Iteration 262 took 3.62 seconds (mean sampled reward: -1719.24). Current reward after update: -156.71, Optimal reward -125.04
Iteration 263 took 3.68 seconds (mean sampled reward: -1523.44). Current reward after update: -140.50, Optimal reward -125.04
Iteration 264 took 3.67 seconds (mean sampled reward: -1378.22). Current reward after update: -148.13, Optimal reward -125.04
Iteration 265 took 3.70 seconds (mean sampled reward: -1786.90). Current reward after update: -194.37, Optimal reward -125.04
Iteration 266 took 3.71 seconds (mean sampled reward: -2036.67). Current reward after update: -121.19, Optimal reward -121.19
Iteration 267 took 3.72 seconds (mean sampled reward: -3775.49). Current reward after update: -166.82, Optimal reward -121.19
Iteration 268 took 3.75 seconds (mean sampled reward: -2990.87). Current reward after update: -182.39, Optimal reward -121.19
Iteration 269 took 3.71 seconds (mean sampled reward: -2629.66). Current reward after update: -165.36, Optimal reward -121.19
Iteration 270 took 3.74 seconds (mean sampled reward: -3366.74). Current reward after update: -137.55, Optimal reward -121.19
Iteration 271 took 3.70 seconds (mean sampled reward: -3105.70). Current reward after update: -141.36, Optimal reward -121.19
Iteration 272 took 3.69 seconds (mean sampled reward: -2673.12). Current reward after update: -157.55, Optimal reward -121.19
Iteration 273 took 3.67 seconds (mean sampled reward: -2663.59). Current reward after update: -250.59, Optimal reward -121.19
Iteration 274 took 3.75 seconds (mean sampled reward: -1463.21). Current reward after update: -136.34, Optimal reward -121.19
Iteration 275 took 3.66 seconds (mean sampled reward: -1391.13). Current reward after update: -148.15, Optimal reward -121.19
Iteration 276 took 3.71 seconds (mean sampled reward: -1652.25). Current reward after update: -146.62, Optimal reward -121.19
Iteration 277 took 3.70 seconds (mean sampled reward: -2020.56). Current reward after update: -164.03, Optimal reward -121.19
Iteration 278 took 3.65 seconds (mean sampled reward: -1882.08). Current reward after update: -162.15, Optimal reward -121.19
Iteration 279 took 3.73 seconds (mean sampled reward: -2337.73). Current reward after update: -173.77, Optimal reward -121.19
Iteration 280 took 3.69 seconds (mean sampled reward: -3004.17). Current reward after update: -174.34, Optimal reward -121.19
Iteration 281 took 3.74 seconds (mean sampled reward: -4456.65). Current reward after update: -167.95, Optimal reward -121.19
Iteration 282 took 3.82 seconds (mean sampled reward: -4947.20). Current reward after update: -129.54, Optimal reward -121.19
Iteration 283 took 3.72 seconds (mean sampled reward: -3962.30). Current reward after update: -161.57, Optimal reward -121.19
Iteration 284 took 3.77 seconds (mean sampled reward: -4410.57). Current reward after update: -149.63, Optimal reward -121.19
Iteration 285 took 3.77 seconds (mean sampled reward: -5038.50). Current reward after update: -111.75, Optimal reward -111.75
Iteration 286 took 3.82 seconds (mean sampled reward: -5085.58). Current reward after update: -182.81, Optimal reward -111.75
Iteration 287 took 3.67 seconds (mean sampled reward: -6128.13). Current reward after update: -229.75, Optimal reward -111.75
Iteration 288 took 3.70 seconds (mean sampled reward: -5015.81). Current reward after update: -132.43, Optimal reward -111.75
Iteration 289 took 3.62 seconds (mean sampled reward: -4541.90). Current reward after update: -155.98, Optimal reward -111.75
Iteration 290 took 3.67 seconds (mean sampled reward: -3092.58). Current reward after update: -120.57, Optimal reward -111.75
Iteration 291 took 3.66 seconds (mean sampled reward: -3678.96). Current reward after update: -172.91, Optimal reward -111.75
Iteration 292 took 3.73 seconds (mean sampled reward: -4737.39). Current reward after update: -138.75, Optimal reward -111.75
Iteration 293 took 3.65 seconds (mean sampled reward: -3932.37). Current reward after update: -135.17, Optimal reward -111.75
Iteration 294 took 3.67 seconds (mean sampled reward: -2870.74). Current reward after update: -140.73, Optimal reward -111.75
Iteration 295 took 3.74 seconds (mean sampled reward: -3589.41). Current reward after update: -129.03, Optimal reward -111.75
Iteration 296 took 3.73 seconds (mean sampled reward: -4265.63). Current reward after update: -162.24, Optimal reward -111.75
Iteration 297 took 3.80 seconds (mean sampled reward: -4721.98). Current reward after update: -143.75, Optimal reward -111.75
Iteration 298 took 3.72 seconds (mean sampled reward: -3943.02). Current reward after update: -186.42, Optimal reward -111.75
Iteration 299 took 3.74 seconds (mean sampled reward: -3966.78). Current reward after update: -171.11, Optimal reward -111.75
Iteration 300 took 3.75 seconds (mean sampled reward: -4142.35). Current reward after update: -115.01, Optimal reward -111.75
Iteration 301 took 3.81 seconds (mean sampled reward: -5525.45). Current reward after update: -142.70, Optimal reward -111.75
Iteration 302 took 3.80 seconds (mean sampled reward: -5349.87). Current reward after update: -215.51, Optimal reward -111.75
Iteration 303 took 3.78 seconds (mean sampled reward: -5730.36). Current reward after update: -196.52, Optimal reward -111.75
Iteration 304 took 3.60 seconds (mean sampled reward: -4196.63). Current reward after update: -146.38, Optimal reward -111.75
Iteration 305 took 3.77 seconds (mean sampled reward: -4737.14). Current reward after update: -181.02, Optimal reward -111.75
Iteration 306 took 3.73 seconds (mean sampled reward: -5294.17). Current reward after update: -128.52, Optimal reward -111.75
Iteration 307 took 3.68 seconds (mean sampled reward: -4364.58). Current reward after update: -125.30, Optimal reward -111.75
Iteration 308 took 3.71 seconds (mean sampled reward: -4241.67). Current reward after update: -157.46, Optimal reward -111.75
Iteration 309 took 3.70 seconds (mean sampled reward: -4181.65). Current reward after update: -165.12, Optimal reward -111.75
Iteration 310 took 3.60 seconds (mean sampled reward: -3014.90). Current reward after update: -161.18, Optimal reward -111.75
Iteration 311 took 3.68 seconds (mean sampled reward: -2762.51). Current reward after update: -128.04, Optimal reward -111.75
Iteration 312 took 3.63 seconds (mean sampled reward: -2506.65). Current reward after update: -194.30, Optimal reward -111.75
Iteration 313 took 3.61 seconds (mean sampled reward: -2704.48). Current reward after update: -161.42, Optimal reward -111.75
Iteration 314 took 3.63 seconds (mean sampled reward: -3237.22). Current reward after update: -200.38, Optimal reward -111.75
Iteration 315 took 3.63 seconds (mean sampled reward: -3136.28). Current reward after update: -180.03, Optimal reward -111.75
Iteration 316 took 3.64 seconds (mean sampled reward: -2626.38). Current reward after update: -152.18, Optimal reward -111.75
Iteration 317 took 3.79 seconds (mean sampled reward: -4044.25). Current reward after update: -194.97, Optimal reward -111.75
Iteration 318 took 3.78 seconds (mean sampled reward: -3730.93). Current reward after update: -133.07, Optimal reward -111.75
Iteration 319 took 3.77 seconds (mean sampled reward: -2745.19). Current reward after update: -144.44, Optimal reward -111.75
Iteration 320 took 3.72 seconds (mean sampled reward: -1721.32). Current reward after update: -161.32, Optimal reward -111.75
Iteration 321 took 3.75 seconds (mean sampled reward: -3008.72). Current reward after update: -135.04, Optimal reward -111.75
Iteration 322 took 3.77 seconds (mean sampled reward: -3254.31). Current reward after update: -152.72, Optimal reward -111.75
Iteration 323 took 3.72 seconds (mean sampled reward: -1427.89). Current reward after update: -136.25, Optimal reward -111.75
Iteration 324 took 3.74 seconds (mean sampled reward: -3167.51). Current reward after update: -144.30, Optimal reward -111.75
Iteration 325 took 3.73 seconds (mean sampled reward: -3916.53). Current reward after update: -176.56, Optimal reward -111.75
Iteration 326 took 3.69 seconds (mean sampled reward: -2729.96). Current reward after update: -155.92, Optimal reward -111.75
Iteration 327 took 3.71 seconds (mean sampled reward: -3329.91). Current reward after update: -121.33, Optimal reward -111.75
Iteration 328 took 3.73 seconds (mean sampled reward: -2507.44). Current reward after update: -111.69, Optimal reward -111.69
Iteration 329 took 3.73 seconds (mean sampled reward: -4086.68). Current reward after update: -121.51, Optimal reward -111.69
Iteration 330 took 3.71 seconds (mean sampled reward: -4658.23). Current reward after update: -135.29, Optimal reward -111.69
Iteration 331 took 3.69 seconds (mean sampled reward: -2947.03). Current reward after update: -142.09, Optimal reward -111.69
Iteration 332 took 3.66 seconds (mean sampled reward: -3394.18). Current reward after update: -109.33, Optimal reward -109.33
Iteration 333 took 3.70 seconds (mean sampled reward: -4817.53). Current reward after update: -149.30, Optimal reward -109.33
Iteration 334 took 3.69 seconds (mean sampled reward: -3901.55). Current reward after update: -137.96, Optimal reward -109.33
Iteration 335 took 3.72 seconds (mean sampled reward: -4434.77). Current reward after update: -135.08, Optimal reward -109.33
Iteration 336 took 3.76 seconds (mean sampled reward: -4474.05). Current reward after update: -158.95, Optimal reward -109.33
Iteration 337 took 3.74 seconds (mean sampled reward: -3325.29). Current reward after update: -139.49, Optimal reward -109.33
Iteration 338 took 3.74 seconds (mean sampled reward: -2954.14). Current reward after update: -131.83, Optimal reward -109.33
Iteration 339 took 3.76 seconds (mean sampled reward: -4066.79). Current reward after update: -146.02, Optimal reward -109.33
Iteration 340 took 3.67 seconds (mean sampled reward: -3458.74). Current reward after update: -141.16, Optimal reward -109.33
Iteration 341 took 3.59 seconds (mean sampled reward: -2409.25). Current reward after update: -157.93, Optimal reward -109.33
Iteration 342 took 3.74 seconds (mean sampled reward: -4525.07). Current reward after update: -130.85, Optimal reward -109.33
Iteration 343 took 3.75 seconds (mean sampled reward: -4604.68). Current reward after update: -161.22, Optimal reward -109.33
Iteration 344 took 3.71 seconds (mean sampled reward: -4577.67). Current reward after update: -144.73, Optimal reward -109.33
Iteration 345 took 3.75 seconds (mean sampled reward: -4506.35). Current reward after update: -139.53, Optimal reward -109.33
Iteration 346 took 3.72 seconds (mean sampled reward: -4317.30). Current reward after update: -138.16, Optimal reward -109.33
Iteration 347 took 3.72 seconds (mean sampled reward: -2530.90). Current reward after update: -144.71, Optimal reward -109.33
Iteration 348 took 3.71 seconds (mean sampled reward: -3147.73). Current reward after update: -144.89, Optimal reward -109.33
Iteration 349 took 3.73 seconds (mean sampled reward: -3622.29). Current reward after update: -154.46, Optimal reward -109.33
Iteration 350 took 3.67 seconds (mean sampled reward: -1594.65). Current reward after update: -152.00, Optimal reward -109.33
Iteration 351 took 3.74 seconds (mean sampled reward: -2477.02). Current reward after update: -149.02, Optimal reward -109.33
Iteration 352 took 3.67 seconds (mean sampled reward: -1787.22). Current reward after update: -152.62, Optimal reward -109.33
Iteration 353 took 3.64 seconds (mean sampled reward: -2421.85). Current reward after update: -153.96, Optimal reward -109.33
Iteration 354 took 3.65 seconds (mean sampled reward: -3106.15). Current reward after update: -146.49, Optimal reward -109.33
Iteration 355 took 3.56 seconds (mean sampled reward: -3337.34). Current reward after update: -158.28, Optimal reward -109.33
Iteration 356 took 3.57 seconds (mean sampled reward: -2889.90). Current reward after update: -184.30, Optimal reward -109.33
Iteration 357 took 3.66 seconds (mean sampled reward: -1250.85). Current reward after update: -125.63, Optimal reward -109.33
Iteration 358 took 3.74 seconds (mean sampled reward: -3363.77). Current reward after update: -151.44, Optimal reward -109.33
Iteration 359 took 3.69 seconds (mean sampled reward: -2573.81). Current reward after update: -142.90, Optimal reward -109.33
Iteration 360 took 3.66 seconds (mean sampled reward: -1775.18). Current reward after update: -133.74, Optimal reward -109.33
Iteration 361 took 3.73 seconds (mean sampled reward: -2541.81). Current reward after update: -160.72, Optimal reward -109.33
Iteration 362 took 3.67 seconds (mean sampled reward: -1297.12). Current reward after update: -152.43, Optimal reward -109.33
Iteration 363 took 3.72 seconds (mean sampled reward: -3175.65). Current reward after update: -146.60, Optimal reward -109.33
Iteration 364 took 3.70 seconds (mean sampled reward: -2650.69). Current reward after update: -160.69, Optimal reward -109.33
Iteration 365 took 3.71 seconds (mean sampled reward: -2407.07). Current reward after update: -159.14, Optimal reward -109.33
Iteration 366 took 3.64 seconds (mean sampled reward: -1771.28). Current reward after update: -155.23, Optimal reward -109.33
Iteration 367 took 3.58 seconds (mean sampled reward: -1425.31). Current reward after update: -132.44, Optimal reward -109.33
Iteration 368 took 3.57 seconds (mean sampled reward: -1076.42). Current reward after update: -99.37, Optimal reward -99.37
Iteration 369 took 3.60 seconds (mean sampled reward: -1755.29). Current reward after update: -108.06, Optimal reward -99.37
Iteration 370 took 3.62 seconds (mean sampled reward: -1637.98). Current reward after update: -100.81, Optimal reward -99.37
Iteration 371 took 3.58 seconds (mean sampled reward: -2017.96). Current reward after update: -115.73, Optimal reward -99.37
Iteration 372 took 3.62 seconds (mean sampled reward: -1942.90). Current reward after update: -116.63, Optimal reward -99.37
Iteration 373 took 3.70 seconds (mean sampled reward: -2287.50). Current reward after update: -125.45, Optimal reward -99.37
Iteration 374 took 3.59 seconds (mean sampled reward: -1512.68). Current reward after update: -117.04, Optimal reward -99.37
Iteration 375 took 3.55 seconds (mean sampled reward: -1514.76). Current reward after update: -115.12, Optimal reward -99.37
Iteration 376 took 3.60 seconds (mean sampled reward: -1804.42). Current reward after update: -129.55, Optimal reward -99.37
Iteration 377 took 3.54 seconds (mean sampled reward: -1574.58). Current reward after update: -111.86, Optimal reward -99.37
Iteration 378 took 3.61 seconds (mean sampled reward: -2057.39). Current reward after update: -134.21, Optimal reward -99.37
Iteration 379 took 3.59 seconds (mean sampled reward: -1376.26). Current reward after update: -112.60, Optimal reward -99.37
Iteration 380 took 3.53 seconds (mean sampled reward: -1262.28). Current reward after update: -128.65, Optimal reward -99.37
Iteration 381 took 3.58 seconds (mean sampled reward: -1124.91). Current reward after update: -107.45, Optimal reward -99.37
Iteration 382 took 3.57 seconds (mean sampled reward: -1121.31). Current reward after update: -125.36, Optimal reward -99.37
Iteration 383 took 3.58 seconds (mean sampled reward: -1161.64). Current reward after update: -115.35, Optimal reward -99.37
Iteration 384 took 3.62 seconds (mean sampled reward: -1396.25). Current reward after update: -164.35, Optimal reward -99.37
Iteration 385 took 3.61 seconds (mean sampled reward: -1237.80). Current reward after update: -111.66, Optimal reward -99.37
Iteration 386 took 3.61 seconds (mean sampled reward: -1393.87). Current reward after update: -107.49, Optimal reward -99.37
Iteration 387 took 3.53 seconds (mean sampled reward: -1199.67). Current reward after update: -122.16, Optimal reward -99.37
Iteration 388 took 3.58 seconds (mean sampled reward: -1231.96). Current reward after update: -119.81, Optimal reward -99.37
Iteration 389 took 3.58 seconds (mean sampled reward: -1257.92). Current reward after update: -147.81, Optimal reward -99.37
Iteration 390 took 3.59 seconds (mean sampled reward: -1327.11). Current reward after update: -137.54, Optimal reward -99.37
Iteration 391 took 3.60 seconds (mean sampled reward: -1684.41). Current reward after update: -132.00, Optimal reward -99.37
Iteration 392 took 3.63 seconds (mean sampled reward: -2115.83). Current reward after update: -159.03, Optimal reward -99.37
Iteration 393 took 3.56 seconds (mean sampled reward: -1528.05). Current reward after update: -101.29, Optimal reward -99.37
Iteration 394 took 3.60 seconds (mean sampled reward: -1840.93). Current reward after update: -146.82, Optimal reward -99.37
Iteration 395 took 3.64 seconds (mean sampled reward: -2422.28). Current reward after update: -123.69, Optimal reward -99.37
Iteration 396 took 3.60 seconds (mean sampled reward: -1801.83). Current reward after update: -196.29, Optimal reward -99.37
Iteration 397 took 3.62 seconds (mean sampled reward: -2154.74). Current reward after update: -106.09, Optimal reward -99.37
Iteration 398 took 3.61 seconds (mean sampled reward: -1617.56). Current reward after update: -151.41, Optimal reward -99.37
Iteration 399 took 3.63 seconds (mean sampled reward: -2183.83). Current reward after update: -139.68, Optimal reward -99.37
Iteration 400 took 3.61 seconds (mean sampled reward: -2947.17). Current reward after update: -145.30, Optimal reward -99.37
Iteration 1 took 3.90 seconds (mean sampled reward: -7506.76). Current reward after update: -3838.98, Optimal reward -3838.98
Iteration 2 took 3.77 seconds (mean sampled reward: -7075.02). Current reward after update: -3011.91, Optimal reward -3011.91
Iteration 3 took 4.04 seconds (mean sampled reward: -6604.73). Current reward after update: -2861.70, Optimal reward -2861.70
Iteration 4 took 3.88 seconds (mean sampled reward: -6638.14). Current reward after update: -2345.04, Optimal reward -2345.04
Iteration 5 took 3.90 seconds (mean sampled reward: -5797.07). Current reward after update: -1538.70, Optimal reward -1538.70
Iteration 6 took 3.55 seconds (mean sampled reward: -5938.49). Current reward after update: -1293.28, Optimal reward -1293.28
Iteration 7 took 3.69 seconds (mean sampled reward: -6009.81). Current reward after update: -1034.79, Optimal reward -1034.79
Iteration 8 took 3.70 seconds (mean sampled reward: -6254.64). Current reward after update: -1028.69, Optimal reward -1028.69
Iteration 9 took 3.78 seconds (mean sampled reward: -5667.88). Current reward after update: -1221.95, Optimal reward -1028.69
Iteration 10 took 3.49 seconds (mean sampled reward: -5557.80). Current reward after update: -911.66, Optimal reward -911.66
Iteration 11 took 3.48 seconds (mean sampled reward: -5492.07). Current reward after update: -942.06, Optimal reward -911.66
Iteration 12 took 3.52 seconds (mean sampled reward: -6049.56). Current reward after update: -1452.14, Optimal reward -911.66
Iteration 13 took 3.52 seconds (mean sampled reward: -5429.85). Current reward after update: -1004.51, Optimal reward -911.66
Iteration 14 took 3.39 seconds (mean sampled reward: -5250.59). Current reward after update: -829.85, Optimal reward -829.85
Iteration 15 took 3.51 seconds (mean sampled reward: -5554.49). Current reward after update: -1110.24, Optimal reward -829.85
Iteration 16 took 3.56 seconds (mean sampled reward: -5917.67). Current reward after update: -922.68, Optimal reward -829.85
Iteration 17 took 3.35 seconds (mean sampled reward: -4453.68). Current reward after update: -899.32, Optimal reward -829.85
Iteration 18 took 3.45 seconds (mean sampled reward: -5417.67). Current reward after update: -912.56, Optimal reward -829.85
Iteration 19 took 3.46 seconds (mean sampled reward: -5139.06). Current reward after update: -925.26, Optimal reward -829.85
Iteration 20 took 3.58 seconds (mean sampled reward: -5912.97). Current reward after update: -841.76, Optimal reward -829.85
Iteration 21 took 3.64 seconds (mean sampled reward: -6217.93). Current reward after update: -858.35, Optimal reward -829.85
Iteration 22 took 3.35 seconds (mean sampled reward: -5819.25). Current reward after update: -953.77, Optimal reward -829.85
Iteration 23 took 3.42 seconds (mean sampled reward: -5916.41). Current reward after update: -870.84, Optimal reward -829.85
Iteration 24 took 3.28 seconds (mean sampled reward: -5487.78). Current reward after update: -1038.81, Optimal reward -829.85
Iteration 25 took 3.38 seconds (mean sampled reward: -6134.02). Current reward after update: -870.95, Optimal reward -829.85
Iteration 26 took 3.52 seconds (mean sampled reward: -6068.23). Current reward after update: -827.16, Optimal reward -827.16
Iteration 27 took 3.41 seconds (mean sampled reward: -5188.71). Current reward after update: -924.70, Optimal reward -827.16
Iteration 28 took 3.41 seconds (mean sampled reward: -4182.20). Current reward after update: -821.42, Optimal reward -821.42
Iteration 29 took 3.48 seconds (mean sampled reward: -4618.61). Current reward after update: -920.11, Optimal reward -821.42
Iteration 30 took 3.42 seconds (mean sampled reward: -4957.12). Current reward after update: -909.72, Optimal reward -821.42
Iteration 31 took 3.36 seconds (mean sampled reward: -5354.55). Current reward after update: -856.97, Optimal reward -821.42
Iteration 32 took 3.40 seconds (mean sampled reward: -5268.57). Current reward after update: -1012.20, Optimal reward -821.42
Iteration 33 took 3.58 seconds (mean sampled reward: -5481.50). Current reward after update: -1080.42, Optimal reward -821.42
Iteration 34 took 3.42 seconds (mean sampled reward: -5370.39). Current reward after update: -1149.03, Optimal reward -821.42
Iteration 35 took 3.56 seconds (mean sampled reward: -5896.49). Current reward after update: -1127.26, Optimal reward -821.42
Iteration 36 took 3.56 seconds (mean sampled reward: -6444.13). Current reward after update: -1204.99, Optimal reward -821.42
Iteration 37 took 3.51 seconds (mean sampled reward: -6425.99). Current reward after update: -1206.98, Optimal reward -821.42
Iteration 38 took 3.35 seconds (mean sampled reward: -5832.37). Current reward after update: -999.62, Optimal reward -821.42
Iteration 39 took 3.49 seconds (mean sampled reward: -5667.65). Current reward after update: -967.60, Optimal reward -821.42
Iteration 40 took 3.49 seconds (mean sampled reward: -5931.24). Current reward after update: -1077.86, Optimal reward -821.42
Iteration 41 took 3.43 seconds (mean sampled reward: -6395.42). Current reward after update: -865.31, Optimal reward -821.42
Iteration 42 took 3.51 seconds (mean sampled reward: -6018.71). Current reward after update: -982.85, Optimal reward -821.42
Iteration 43 took 3.45 seconds (mean sampled reward: -6084.19). Current reward after update: -714.63, Optimal reward -714.63
Iteration 44 took 3.37 seconds (mean sampled reward: -5579.01). Current reward after update: -910.91, Optimal reward -714.63
Iteration 45 took 3.25 seconds (mean sampled reward: -4868.75). Current reward after update: -925.33, Optimal reward -714.63
Iteration 46 took 3.32 seconds (mean sampled reward: -5715.02). Current reward after update: -959.79, Optimal reward -714.63
Iteration 47 took 3.30 seconds (mean sampled reward: -5180.86). Current reward after update: -912.88, Optimal reward -714.63
Iteration 48 took 3.46 seconds (mean sampled reward: -5993.06). Current reward after update: -900.83, Optimal reward -714.63
Iteration 49 took 3.47 seconds (mean sampled reward: -6164.63). Current reward after update: -1013.08, Optimal reward -714.63
Iteration 50 took 3.39 seconds (mean sampled reward: -5889.09). Current reward after update: -1203.31, Optimal reward -714.63
Iteration 51 took 3.37 seconds (mean sampled reward: -5110.38). Current reward after update: -894.46, Optimal reward -714.63
Iteration 52 took 3.43 seconds (mean sampled reward: -5479.34). Current reward after update: -1009.28, Optimal reward -714.63
Iteration 53 took 3.46 seconds (mean sampled reward: -5718.24). Current reward after update: -1062.37, Optimal reward -714.63
Iteration 54 took 3.54 seconds (mean sampled reward: -6180.75). Current reward after update: -966.56, Optimal reward -714.63
Iteration 55 took 3.63 seconds (mean sampled reward: -6579.85). Current reward after update: -959.81, Optimal reward -714.63
Iteration 56 took 3.35 seconds (mean sampled reward: -6372.39). Current reward after update: -903.24, Optimal reward -714.63
Iteration 57 took 3.28 seconds (mean sampled reward: -5699.09). Current reward after update: -1091.66, Optimal reward -714.63
Iteration 58 took 3.25 seconds (mean sampled reward: -5433.69). Current reward after update: -1001.55, Optimal reward -714.63
Iteration 59 took 3.39 seconds (mean sampled reward: -5814.89). Current reward after update: -789.40, Optimal reward -714.63
Iteration 60 took 3.35 seconds (mean sampled reward: -6205.76). Current reward after update: -824.84, Optimal reward -714.63
Iteration 61 took 3.32 seconds (mean sampled reward: -5857.33). Current reward after update: -841.23, Optimal reward -714.63
Iteration 62 took 3.27 seconds (mean sampled reward: -5767.17). Current reward after update: -790.89, Optimal reward -714.63
Iteration 63 took 3.31 seconds (mean sampled reward: -5969.64). Current reward after update: -836.91, Optimal reward -714.63
Iteration 64 took 3.23 seconds (mean sampled reward: -6053.50). Current reward after update: -895.21, Optimal reward -714.63
Iteration 65 took 3.26 seconds (mean sampled reward: -5562.98). Current reward after update: -1392.51, Optimal reward -714.63
Iteration 66 took 3.25 seconds (mean sampled reward: -5823.15). Current reward after update: -847.71, Optimal reward -714.63
Iteration 67 took 3.29 seconds (mean sampled reward: -5347.41). Current reward after update: -782.12, Optimal reward -714.63
Iteration 68 took 3.31 seconds (mean sampled reward: -5014.87). Current reward after update: -753.62, Optimal reward -714.63
Iteration 69 took 3.31 seconds (mean sampled reward: -5478.82). Current reward after update: -841.15, Optimal reward -714.63
Iteration 70 took 3.22 seconds (mean sampled reward: -5567.21). Current reward after update: -823.74, Optimal reward -714.63
Iteration 71 took 3.30 seconds (mean sampled reward: -5479.84). Current reward after update: -710.88, Optimal reward -710.88
Iteration 72 took 3.26 seconds (mean sampled reward: -5733.50). Current reward after update: -700.12, Optimal reward -700.12
Iteration 73 took 3.28 seconds (mean sampled reward: -5982.83). Current reward after update: -848.47, Optimal reward -700.12
Iteration 74 took 3.30 seconds (mean sampled reward: -6054.63). Current reward after update: -755.44, Optimal reward -700.12
Iteration 75 took 3.23 seconds (mean sampled reward: -5270.85). Current reward after update: -835.96, Optimal reward -700.12
Iteration 76 took 3.21 seconds (mean sampled reward: -4426.66). Current reward after update: -757.60, Optimal reward -700.12
Iteration 77 took 3.37 seconds (mean sampled reward: -5124.06). Current reward after update: -1035.14, Optimal reward -700.12
Iteration 78 took 3.28 seconds (mean sampled reward: -4633.42). Current reward after update: -759.10, Optimal reward -700.12
Iteration 79 took 3.31 seconds (mean sampled reward: -4321.59). Current reward after update: -820.30, Optimal reward -700.12
Iteration 80 took 3.24 seconds (mean sampled reward: -4601.71). Current reward after update: -769.69, Optimal reward -700.12
Iteration 81 took 3.29 seconds (mean sampled reward: -4752.46). Current reward after update: -817.35, Optimal reward -700.12
Iteration 82 took 3.30 seconds (mean sampled reward: -4725.24). Current reward after update: -771.41, Optimal reward -700.12
Iteration 83 took 3.35 seconds (mean sampled reward: -4858.74). Current reward after update: -830.19, Optimal reward -700.12
Iteration 84 took 3.25 seconds (mean sampled reward: -4163.80). Current reward after update: -770.14, Optimal reward -700.12
Iteration 85 took 3.35 seconds (mean sampled reward: -4467.03). Current reward after update: -858.49, Optimal reward -700.12
Iteration 86 took 3.24 seconds (mean sampled reward: -4284.01). Current reward after update: -828.55, Optimal reward -700.12
Iteration 87 took 3.35 seconds (mean sampled reward: -4991.46). Current reward after update: -777.10, Optimal reward -700.12
Iteration 88 took 3.23 seconds (mean sampled reward: -4670.08). Current reward after update: -709.36, Optimal reward -700.12
Iteration 89 took 3.40 seconds (mean sampled reward: -5322.47). Current reward after update: -710.75, Optimal reward -700.12
Iteration 90 took 3.26 seconds (mean sampled reward: -4973.65). Current reward after update: -734.37, Optimal reward -700.12
Iteration 91 took 3.42 seconds (mean sampled reward: -5335.67). Current reward after update: -865.24, Optimal reward -700.12
Iteration 92 took 3.31 seconds (mean sampled reward: -5032.48). Current reward after update: -813.98, Optimal reward -700.12
Iteration 93 took 3.25 seconds (mean sampled reward: -4900.69). Current reward after update: -718.68, Optimal reward -700.12
Iteration 94 took 3.23 seconds (mean sampled reward: -5082.39). Current reward after update: -744.03, Optimal reward -700.12
Iteration 95 took 3.30 seconds (mean sampled reward: -5036.39). Current reward after update: -786.67, Optimal reward -700.12
Iteration 96 took 3.20 seconds (mean sampled reward: -4776.20). Current reward after update: -742.64, Optimal reward -700.12
Iteration 97 took 3.27 seconds (mean sampled reward: -5062.41). Current reward after update: -783.89, Optimal reward -700.12
Iteration 98 took 3.23 seconds (mean sampled reward: -5460.01). Current reward after update: -787.77, Optimal reward -700.12
Iteration 99 took 3.24 seconds (mean sampled reward: -5502.11). Current reward after update: -827.02, Optimal reward -700.12
Iteration 100 took 3.27 seconds (mean sampled reward: -4965.38). Current reward after update: -760.29, Optimal reward -700.12
Iteration 101 took 3.30 seconds (mean sampled reward: -5067.02). Current reward after update: -819.57, Optimal reward -700.12
Iteration 102 took 3.28 seconds (mean sampled reward: -5091.89). Current reward after update: -850.51, Optimal reward -700.12
Iteration 103 took 3.27 seconds (mean sampled reward: -4717.26). Current reward after update: -810.41, Optimal reward -700.12
Iteration 104 took 3.17 seconds (mean sampled reward: -4042.17). Current reward after update: -885.09, Optimal reward -700.12
Iteration 105 took 3.12 seconds (mean sampled reward: -4016.77). Current reward after update: -836.14, Optimal reward -700.12
Iteration 106 took 3.28 seconds (mean sampled reward: -4229.11). Current reward after update: -829.31, Optimal reward -700.12
Iteration 107 took 3.41 seconds (mean sampled reward: -4010.42). Current reward after update: -718.62, Optimal reward -700.12
Iteration 108 took 3.52 seconds (mean sampled reward: -5249.23). Current reward after update: -808.56, Optimal reward -700.12
Iteration 109 took 3.48 seconds (mean sampled reward: -5023.32). Current reward after update: -807.68, Optimal reward -700.12
Iteration 110 took 3.18 seconds (mean sampled reward: -4585.77). Current reward after update: -744.85, Optimal reward -700.12
Iteration 111 took 3.21 seconds (mean sampled reward: -4128.97). Current reward after update: -788.12, Optimal reward -700.12
Iteration 112 took 3.27 seconds (mean sampled reward: -4471.42). Current reward after update: -847.77, Optimal reward -700.12
Iteration 113 took 3.16 seconds (mean sampled reward: -4390.47). Current reward after update: -775.36, Optimal reward -700.12
Iteration 114 took 3.43 seconds (mean sampled reward: -4888.91). Current reward after update: -791.21, Optimal reward -700.12
Iteration 115 took 3.23 seconds (mean sampled reward: -3626.96). Current reward after update: -750.93, Optimal reward -700.12
Iteration 116 took 3.38 seconds (mean sampled reward: -3989.08). Current reward after update: -851.49, Optimal reward -700.12
Iteration 117 took 3.52 seconds (mean sampled reward: -5041.34). Current reward after update: -785.47, Optimal reward -700.12
Iteration 118 took 3.37 seconds (mean sampled reward: -4317.36). Current reward after update: -829.25, Optimal reward -700.12
Iteration 119 took 3.34 seconds (mean sampled reward: -4394.73). Current reward after update: -773.60, Optimal reward -700.12
Iteration 120 took 3.49 seconds (mean sampled reward: -4984.03). Current reward after update: -788.27, Optimal reward -700.12
Iteration 121 took 3.63 seconds (mean sampled reward: -5099.76). Current reward after update: -992.05, Optimal reward -700.12
Iteration 122 took 3.52 seconds (mean sampled reward: -5898.00). Current reward after update: -917.22, Optimal reward -700.12
Iteration 123 took 3.29 seconds (mean sampled reward: -5979.74). Current reward after update: -740.72, Optimal reward -700.12
Iteration 124 took 3.24 seconds (mean sampled reward: -5744.06). Current reward after update: -695.27, Optimal reward -695.27
Iteration 125 took 3.42 seconds (mean sampled reward: -5584.33). Current reward after update: -715.32, Optimal reward -695.27
Iteration 126 took 3.28 seconds (mean sampled reward: -5479.12). Current reward after update: -653.87, Optimal reward -653.87
Iteration 127 took 3.24 seconds (mean sampled reward: -4493.74). Current reward after update: -712.50, Optimal reward -653.87
Iteration 128 took 3.27 seconds (mean sampled reward: -4986.40). Current reward after update: -790.82, Optimal reward -653.87
Iteration 129 took 3.21 seconds (mean sampled reward: -4782.27). Current reward after update: -680.65, Optimal reward -653.87
Iteration 130 took 3.42 seconds (mean sampled reward: -5448.16). Current reward after update: -870.17, Optimal reward -653.87
Iteration 131 took 3.35 seconds (mean sampled reward: -4993.41). Current reward after update: -772.72, Optimal reward -653.87
Iteration 132 took 3.32 seconds (mean sampled reward: -4684.89). Current reward after update: -784.26, Optimal reward -653.87
Iteration 133 took 3.22 seconds (mean sampled reward: -4695.85). Current reward after update: -787.44, Optimal reward -653.87
Iteration 134 took 3.21 seconds (mean sampled reward: -4613.46). Current reward after update: -749.95, Optimal reward -653.87
Iteration 135 took 3.29 seconds (mean sampled reward: -5373.76). Current reward after update: -750.78, Optimal reward -653.87
Iteration 136 took 3.26 seconds (mean sampled reward: -5293.78). Current reward after update: -799.86, Optimal reward -653.87
Iteration 137 took 3.23 seconds (mean sampled reward: -4975.24). Current reward after update: -721.61, Optimal reward -653.87
Iteration 138 took 3.21 seconds (mean sampled reward: -4610.54). Current reward after update: -674.57, Optimal reward -653.87
Iteration 139 took 3.20 seconds (mean sampled reward: -4445.91). Current reward after update: -659.93, Optimal reward -653.87
Iteration 140 took 3.20 seconds (mean sampled reward: -4622.39). Current reward after update: -609.31, Optimal reward -609.31
Iteration 141 took 3.35 seconds (mean sampled reward: -5618.82). Current reward after update: -642.83, Optimal reward -609.31
Iteration 142 took 3.28 seconds (mean sampled reward: -5557.51). Current reward after update: -644.01, Optimal reward -609.31
Iteration 143 took 3.32 seconds (mean sampled reward: -5081.69). Current reward after update: -669.80, Optimal reward -609.31
Iteration 144 took 3.32 seconds (mean sampled reward: -5192.70). Current reward after update: -645.16, Optimal reward -609.31
Iteration 145 took 3.26 seconds (mean sampled reward: -4489.35). Current reward after update: -619.86, Optimal reward -609.31
Iteration 146 took 3.21 seconds (mean sampled reward: -4359.55). Current reward after update: -698.56, Optimal reward -609.31
Iteration 147 took 3.23 seconds (mean sampled reward: -4633.27). Current reward after update: -589.85, Optimal reward -589.85
Iteration 148 took 3.20 seconds (mean sampled reward: -4315.51). Current reward after update: -627.08, Optimal reward -589.85
Iteration 149 took 3.23 seconds (mean sampled reward: -4369.56). Current reward after update: -655.65, Optimal reward -589.85
Iteration 150 took 3.26 seconds (mean sampled reward: -4231.31). Current reward after update: -574.74, Optimal reward -574.74
Iteration 151 took 3.26 seconds (mean sampled reward: -5137.46). Current reward after update: -624.52, Optimal reward -574.74
Iteration 152 took 3.26 seconds (mean sampled reward: -4993.80). Current reward after update: -624.33, Optimal reward -574.74
Iteration 153 took 3.22 seconds (mean sampled reward: -4961.51). Current reward after update: -634.81, Optimal reward -574.74
Iteration 154 took 3.28 seconds (mean sampled reward: -4607.04). Current reward after update: -650.74, Optimal reward -574.74
Iteration 155 took 3.14 seconds (mean sampled reward: -4320.22). Current reward after update: -628.10, Optimal reward -574.74
Iteration 156 took 3.18 seconds (mean sampled reward: -4897.28). Current reward after update: -609.73, Optimal reward -574.74
Iteration 157 took 3.30 seconds (mean sampled reward: -4903.59). Current reward after update: -627.37, Optimal reward -574.74
Iteration 158 took 3.19 seconds (mean sampled reward: -4719.60). Current reward after update: -580.55, Optimal reward -574.74
Iteration 159 took 3.14 seconds (mean sampled reward: -4379.73). Current reward after update: -608.41, Optimal reward -574.74
Iteration 160 took 3.12 seconds (mean sampled reward: -4089.21). Current reward after update: -673.46, Optimal reward -574.74
Iteration 161 took 3.12 seconds (mean sampled reward: -4386.06). Current reward after update: -615.73, Optimal reward -574.74
Iteration 162 took 3.15 seconds (mean sampled reward: -4139.59). Current reward after update: -627.69, Optimal reward -574.74
Iteration 163 took 3.35 seconds (mean sampled reward: -4694.44). Current reward after update: -619.69, Optimal reward -574.74
Iteration 164 took 3.36 seconds (mean sampled reward: -4781.82). Current reward after update: -675.58, Optimal reward -574.74
Iteration 165 took 3.41 seconds (mean sampled reward: -5369.06). Current reward after update: -796.09, Optimal reward -574.74
Iteration 166 took 3.37 seconds (mean sampled reward: -5384.43). Current reward after update: -855.68, Optimal reward -574.74
Iteration 167 took 3.26 seconds (mean sampled reward: -5762.88). Current reward after update: -1912.85, Optimal reward -574.74
Iteration 168 took 3.28 seconds (mean sampled reward: -5376.65). Current reward after update: -709.50, Optimal reward -574.74
Iteration 169 took 3.26 seconds (mean sampled reward: -5559.46). Current reward after update: -671.52, Optimal reward -574.74
Iteration 170 took 3.27 seconds (mean sampled reward: -6274.83). Current reward after update: -832.05, Optimal reward -574.74
Iteration 171 took 3.26 seconds (mean sampled reward: -5395.15). Current reward after update: -649.25, Optimal reward -574.74
Iteration 172 took 3.34 seconds (mean sampled reward: -5269.25). Current reward after update: -1719.63, Optimal reward -574.74
Iteration 173 took 3.39 seconds (mean sampled reward: -4784.86). Current reward after update: -654.99, Optimal reward -574.74
Iteration 174 took 3.34 seconds (mean sampled reward: -4663.16). Current reward after update: -629.64, Optimal reward -574.74
Iteration 175 took 3.43 seconds (mean sampled reward: -4701.19). Current reward after update: -656.61, Optimal reward -574.74
Iteration 176 took 3.47 seconds (mean sampled reward: -5607.89). Current reward after update: -656.58, Optimal reward -574.74
Iteration 177 took 3.41 seconds (mean sampled reward: -5154.12). Current reward after update: -650.10, Optimal reward -574.74
Iteration 178 took 3.43 seconds (mean sampled reward: -5114.57). Current reward after update: -632.21, Optimal reward -574.74
Iteration 179 took 3.45 seconds (mean sampled reward: -5507.80). Current reward after update: -712.79, Optimal reward -574.74
Iteration 180 took 3.37 seconds (mean sampled reward: -4877.53). Current reward after update: -680.80, Optimal reward -574.74
Iteration 181 took 3.36 seconds (mean sampled reward: -5125.30). Current reward after update: -608.29, Optimal reward -574.74
Iteration 182 took 3.41 seconds (mean sampled reward: -4854.86). Current reward after update: -616.53, Optimal reward -574.74
Iteration 183 took 3.35 seconds (mean sampled reward: -4631.57). Current reward after update: -632.44, Optimal reward -574.74
Iteration 184 took 3.29 seconds (mean sampled reward: -4596.52). Current reward after update: -611.53, Optimal reward -574.74
Iteration 185 took 3.33 seconds (mean sampled reward: -5106.13). Current reward after update: -633.99, Optimal reward -574.74
Iteration 186 took 3.30 seconds (mean sampled reward: -4505.70). Current reward after update: -620.11, Optimal reward -574.74
Iteration 187 took 3.21 seconds (mean sampled reward: -4271.45). Current reward after update: -608.67, Optimal reward -574.74
Iteration 188 took 3.29 seconds (mean sampled reward: -5558.78). Current reward after update: -632.24, Optimal reward -574.74
Iteration 189 took 3.37 seconds (mean sampled reward: -6216.17). Current reward after update: -544.23, Optimal reward -544.23
Iteration 190 took 3.28 seconds (mean sampled reward: -6371.45). Current reward after update: -826.15, Optimal reward -544.23
Iteration 191 took 3.25 seconds (mean sampled reward: -5855.80). Current reward after update: -635.58, Optimal reward -544.23
Iteration 192 took 3.24 seconds (mean sampled reward: -5183.27). Current reward after update: -542.85, Optimal reward -542.85
Iteration 193 took 3.34 seconds (mean sampled reward: -5858.73). Current reward after update: -601.48, Optimal reward -542.85
Iteration 194 took 3.26 seconds (mean sampled reward: -6108.75). Current reward after update: -554.87, Optimal reward -542.85
Iteration 195 took 3.35 seconds (mean sampled reward: -6361.17). Current reward after update: -520.35, Optimal reward -520.35
Iteration 196 took 3.34 seconds (mean sampled reward: -6548.71). Current reward after update: -889.64, Optimal reward -520.35
Iteration 197 took 3.30 seconds (mean sampled reward: -6644.89). Current reward after update: -668.06, Optimal reward -520.35
Iteration 198 took 3.29 seconds (mean sampled reward: -6023.44). Current reward after update: -584.99, Optimal reward -520.35
Iteration 199 took 3.39 seconds (mean sampled reward: -6146.68). Current reward after update: -614.64, Optimal reward -520.35
Iteration 200 took 3.35 seconds (mean sampled reward: -6217.82). Current reward after update: -563.22, Optimal reward -520.35
Iteration 201 took 3.29 seconds (mean sampled reward: -6138.07). Current reward after update: -651.44, Optimal reward -520.35
Iteration 202 took 3.25 seconds (mean sampled reward: -6424.71). Current reward after update: -658.68, Optimal reward -520.35
Iteration 203 took 3.22 seconds (mean sampled reward: -5977.09). Current reward after update: -531.22, Optimal reward -520.35
Iteration 204 took 3.31 seconds (mean sampled reward: -6156.98). Current reward after update: -772.45, Optimal reward -520.35
Iteration 205 took 3.23 seconds (mean sampled reward: -6054.80). Current reward after update: -613.78, Optimal reward -520.35
Iteration 206 took 3.30 seconds (mean sampled reward: -5837.25). Current reward after update: -642.71, Optimal reward -520.35
Iteration 207 took 3.34 seconds (mean sampled reward: -5836.77). Current reward after update: -769.60, Optimal reward -520.35
Iteration 208 took 3.30 seconds (mean sampled reward: -5590.06). Current reward after update: -613.61, Optimal reward -520.35
Iteration 209 took 3.24 seconds (mean sampled reward: -5632.70). Current reward after update: -581.80, Optimal reward -520.35
Iteration 210 took 3.38 seconds (mean sampled reward: -5741.48). Current reward after update: -609.01, Optimal reward -520.35
Iteration 211 took 3.25 seconds (mean sampled reward: -5813.25). Current reward after update: -624.87, Optimal reward -520.35
Iteration 212 took 3.65 seconds (mean sampled reward: -5711.79). Current reward after update: -743.19, Optimal reward -520.35
Iteration 213 took 3.44 seconds (mean sampled reward: -5926.92). Current reward after update: -676.59, Optimal reward -520.35
Iteration 214 took 3.30 seconds (mean sampled reward: -6023.58). Current reward after update: -652.60, Optimal reward -520.35
Iteration 215 took 3.42 seconds (mean sampled reward: -5628.97). Current reward after update: -646.00, Optimal reward -520.35
Iteration 216 took 3.35 seconds (mean sampled reward: -5637.90). Current reward after update: -557.88, Optimal reward -520.35
Iteration 217 took 3.32 seconds (mean sampled reward: -5803.39). Current reward after update: -694.28, Optimal reward -520.35
Iteration 218 took 3.32 seconds (mean sampled reward: -5615.57). Current reward after update: -710.77, Optimal reward -520.35
Iteration 219 took 3.30 seconds (mean sampled reward: -5687.75). Current reward after update: -646.72, Optimal reward -520.35
Iteration 220 took 3.25 seconds (mean sampled reward: -5484.17). Current reward after update: -622.50, Optimal reward -520.35
Iteration 221 took 3.27 seconds (mean sampled reward: -5231.67). Current reward after update: -601.05, Optimal reward -520.35
Iteration 222 took 3.19 seconds (mean sampled reward: -5787.41). Current reward after update: -670.54, Optimal reward -520.35
Iteration 223 took 3.12 seconds (mean sampled reward: -5744.81). Current reward after update: -646.44, Optimal reward -520.35
Iteration 224 took 3.16 seconds (mean sampled reward: -5674.44). Current reward after update: -587.94, Optimal reward -520.35
Iteration 225 took 3.18 seconds (mean sampled reward: -5773.60). Current reward after update: -660.98, Optimal reward -520.35
Iteration 226 took 3.27 seconds (mean sampled reward: -5638.24). Current reward after update: -548.13, Optimal reward -520.35
Iteration 227 took 3.28 seconds (mean sampled reward: -5804.93). Current reward after update: -669.21, Optimal reward -520.35
Iteration 228 took 3.27 seconds (mean sampled reward: -6749.38). Current reward after update: -666.96, Optimal reward -520.35
Iteration 229 took 3.33 seconds (mean sampled reward: -5922.48). Current reward after update: -631.90, Optimal reward -520.35
Iteration 230 took 3.31 seconds (mean sampled reward: -5725.39). Current reward after update: -561.70, Optimal reward -520.35
Iteration 231 took 3.32 seconds (mean sampled reward: -5764.04). Current reward after update: -591.03, Optimal reward -520.35
Iteration 232 took 3.33 seconds (mean sampled reward: -5723.08). Current reward after update: -609.47, Optimal reward -520.35
Iteration 233 took 3.44 seconds (mean sampled reward: -6057.07). Current reward after update: -848.05, Optimal reward -520.35
Iteration 234 took 3.32 seconds (mean sampled reward: -5851.68). Current reward after update: -684.30, Optimal reward -520.35
Iteration 235 took 3.40 seconds (mean sampled reward: -5587.22). Current reward after update: -664.86, Optimal reward -520.35
Iteration 236 took 3.28 seconds (mean sampled reward: -5693.85). Current reward after update: -699.84, Optimal reward -520.35
Iteration 237 took 3.25 seconds (mean sampled reward: -5630.52). Current reward after update: -644.02, Optimal reward -520.35
Iteration 238 took 3.28 seconds (mean sampled reward: -5409.37). Current reward after update: -607.54, Optimal reward -520.35
Iteration 239 took 3.26 seconds (mean sampled reward: -5577.13). Current reward after update: -652.01, Optimal reward -520.35
Iteration 240 took 3.30 seconds (mean sampled reward: -5139.83). Current reward after update: -723.52, Optimal reward -520.35
Iteration 241 took 3.31 seconds (mean sampled reward: -5652.98). Current reward after update: -824.79, Optimal reward -520.35
Iteration 242 took 3.21 seconds (mean sampled reward: -4727.05). Current reward after update: -628.32, Optimal reward -520.35
Iteration 243 took 3.31 seconds (mean sampled reward: -6175.23). Current reward after update: -748.41, Optimal reward -520.35
Iteration 244 took 3.25 seconds (mean sampled reward: -4828.23). Current reward after update: -657.50, Optimal reward -520.35
Iteration 245 took 3.27 seconds (mean sampled reward: -3936.58). Current reward after update: -2530.10, Optimal reward -520.35
Iteration 246 took 3.35 seconds (mean sampled reward: -4674.06). Current reward after update: -505.53, Optimal reward -505.53
Iteration 247 took 3.31 seconds (mean sampled reward: -4545.79). Current reward after update: -490.18, Optimal reward -490.18
Iteration 248 took 3.38 seconds (mean sampled reward: -4301.15). Current reward after update: -548.02, Optimal reward -490.18
Iteration 249 took 3.44 seconds (mean sampled reward: -4153.04). Current reward after update: -517.58, Optimal reward -490.18
Iteration 250 took 3.43 seconds (mean sampled reward: -4098.73). Current reward after update: -522.02, Optimal reward -490.18
Iteration 251 took 3.43 seconds (mean sampled reward: -4172.08). Current reward after update: -662.89, Optimal reward -490.18
Iteration 252 took 3.46 seconds (mean sampled reward: -4238.53). Current reward after update: -583.51, Optimal reward -490.18
Iteration 253 took 3.48 seconds (mean sampled reward: -4379.63). Current reward after update: -478.18, Optimal reward -478.18
Iteration 254 took 3.53 seconds (mean sampled reward: -4271.16). Current reward after update: -614.04, Optimal reward -478.18
Iteration 255 took 3.48 seconds (mean sampled reward: -4345.99). Current reward after update: -599.28, Optimal reward -478.18
Iteration 256 took 3.38 seconds (mean sampled reward: -4195.99). Current reward after update: -499.30, Optimal reward -478.18
Iteration 257 took 3.51 seconds (mean sampled reward: -4227.13). Current reward after update: -623.97, Optimal reward -478.18
Iteration 258 took 3.51 seconds (mean sampled reward: -4332.42). Current reward after update: -552.05, Optimal reward -478.18
Iteration 259 took 3.48 seconds (mean sampled reward: -4353.66). Current reward after update: -513.31, Optimal reward -478.18
Iteration 260 took 3.51 seconds (mean sampled reward: -4646.21). Current reward after update: -504.58, Optimal reward -478.18
Iteration 261 took 3.42 seconds (mean sampled reward: -5032.11). Current reward after update: -786.15, Optimal reward -478.18
Iteration 262 took 3.32 seconds (mean sampled reward: -4605.68). Current reward after update: -460.36, Optimal reward -460.36
Iteration 263 took 3.47 seconds (mean sampled reward: -4134.00). Current reward after update: -485.19, Optimal reward -460.36
Iteration 264 took 3.56 seconds (mean sampled reward: -4469.64). Current reward after update: -447.66, Optimal reward -447.66
Iteration 265 took 3.47 seconds (mean sampled reward: -4187.21). Current reward after update: -473.18, Optimal reward -447.66
Iteration 266 took 3.59 seconds (mean sampled reward: -4720.13). Current reward after update: -518.33, Optimal reward -447.66
Iteration 267 took 3.55 seconds (mean sampled reward: -4679.10). Current reward after update: -507.84, Optimal reward -447.66
Iteration 268 took 3.38 seconds (mean sampled reward: -4011.97). Current reward after update: -542.66, Optimal reward -447.66
Iteration 269 took 3.54 seconds (mean sampled reward: -4751.77). Current reward after update: -553.77, Optimal reward -447.66
Iteration 270 took 3.48 seconds (mean sampled reward: -4644.22). Current reward after update: -507.68, Optimal reward -447.66
Iteration 271 took 3.38 seconds (mean sampled reward: -4252.67). Current reward after update: -544.59, Optimal reward -447.66
Iteration 272 took 3.29 seconds (mean sampled reward: -3953.35). Current reward after update: -471.51, Optimal reward -447.66
Iteration 273 took 3.51 seconds (mean sampled reward: -5000.42). Current reward after update: -518.71, Optimal reward -447.66
Iteration 274 took 3.51 seconds (mean sampled reward: -5213.33). Current reward after update: -489.25, Optimal reward -447.66
Iteration 275 took 3.50 seconds (mean sampled reward: -5173.80). Current reward after update: -535.32, Optimal reward -447.66
Iteration 276 took 3.64 seconds (mean sampled reward: -5447.68). Current reward after update: -892.05, Optimal reward -447.66
Iteration 277 took 3.60 seconds (mean sampled reward: -4722.36). Current reward after update: -799.47, Optimal reward -447.66
Iteration 278 took 3.61 seconds (mean sampled reward: -5413.80). Current reward after update: -514.32, Optimal reward -447.66
Iteration 279 took 3.57 seconds (mean sampled reward: -5601.26). Current reward after update: -467.67, Optimal reward -447.66
Iteration 280 took 3.49 seconds (mean sampled reward: -6105.75). Current reward after update: -451.25, Optimal reward -447.66
Iteration 281 took 3.66 seconds (mean sampled reward: -5581.53). Current reward after update: -495.63, Optimal reward -447.66
Iteration 282 took 3.65 seconds (mean sampled reward: -5525.35). Current reward after update: -439.48, Optimal reward -439.48
Iteration 283 took 3.60 seconds (mean sampled reward: -5115.24). Current reward after update: -619.38, Optimal reward -439.48
Iteration 284 took 3.61 seconds (mean sampled reward: -5652.71). Current reward after update: -876.36, Optimal reward -439.48
Iteration 285 took 3.44 seconds (mean sampled reward: -4753.79). Current reward after update: -658.02, Optimal reward -439.48
Iteration 286 took 3.45 seconds (mean sampled reward: -4350.24). Current reward after update: -483.04, Optimal reward -439.48
Iteration 287 took 3.57 seconds (mean sampled reward: -4715.54). Current reward after update: -543.42, Optimal reward -439.48
Iteration 288 took 3.38 seconds (mean sampled reward: -4117.84). Current reward after update: -439.86, Optimal reward -439.48
Iteration 289 took 3.39 seconds (mean sampled reward: -3805.57). Current reward after update: -527.34, Optimal reward -439.48
Iteration 290 took 3.36 seconds (mean sampled reward: -4435.66). Current reward after update: -581.09, Optimal reward -439.48
Iteration 291 took 3.41 seconds (mean sampled reward: -4082.12). Current reward after update: -501.03, Optimal reward -439.48
Iteration 292 took 3.44 seconds (mean sampled reward: -4731.89). Current reward after update: -483.00, Optimal reward -439.48
Iteration 293 took 3.47 seconds (mean sampled reward: -4516.78). Current reward after update: -529.48, Optimal reward -439.48
Iteration 294 took 3.44 seconds (mean sampled reward: -4580.69). Current reward after update: -631.03, Optimal reward -439.48
Iteration 295 took 3.42 seconds (mean sampled reward: -4043.68). Current reward after update: -539.59, Optimal reward -439.48
Iteration 296 took 3.39 seconds (mean sampled reward: -4003.52). Current reward after update: -503.54, Optimal reward -439.48
Iteration 297 took 3.49 seconds (mean sampled reward: -3755.48). Current reward after update: -586.73, Optimal reward -439.48
Iteration 298 took 3.40 seconds (mean sampled reward: -4104.77). Current reward after update: -504.96, Optimal reward -439.48
Iteration 299 took 3.58 seconds (mean sampled reward: -5087.51). Current reward after update: -646.56, Optimal reward -439.48
Iteration 300 took 3.40 seconds (mean sampled reward: -4851.60). Current reward after update: -787.27, Optimal reward -439.48
Iteration 301 took 3.51 seconds (mean sampled reward: -4925.29). Current reward after update: -596.99, Optimal reward -439.48
Iteration 302 took 3.61 seconds (mean sampled reward: -4781.31). Current reward after update: -632.74, Optimal reward -439.48
Iteration 303 took 3.52 seconds (mean sampled reward: -4016.27). Current reward after update: -581.58, Optimal reward -439.48
Iteration 304 took 3.42 seconds (mean sampled reward: -4597.81). Current reward after update: -620.36, Optimal reward -439.48
Iteration 305 took 3.43 seconds (mean sampled reward: -4738.90). Current reward after update: -571.37, Optimal reward -439.48
Iteration 306 took 3.43 seconds (mean sampled reward: -4264.51). Current reward after update: -597.75, Optimal reward -439.48
Iteration 307 took 3.46 seconds (mean sampled reward: -4101.14). Current reward after update: -593.67, Optimal reward -439.48
Iteration 308 took 3.37 seconds (mean sampled reward: -4036.27). Current reward after update: -632.55, Optimal reward -439.48
Iteration 309 took 3.23 seconds (mean sampled reward: -3582.91). Current reward after update: -614.95, Optimal reward -439.48
Iteration 310 took 3.34 seconds (mean sampled reward: -3509.69). Current reward after update: -535.33, Optimal reward -439.48
Iteration 311 took 3.37 seconds (mean sampled reward: -3676.25). Current reward after update: -5258.51, Optimal reward -439.48
Iteration 312 took 3.41 seconds (mean sampled reward: -4036.20). Current reward after update: -582.31, Optimal reward -439.48
Iteration 313 took 3.40 seconds (mean sampled reward: -3599.81). Current reward after update: -574.59, Optimal reward -439.48
Iteration 314 took 3.38 seconds (mean sampled reward: -3860.69). Current reward after update: -2242.70, Optimal reward -439.48
Iteration 315 took 3.36 seconds (mean sampled reward: -3385.46). Current reward after update: -588.51, Optimal reward -439.48
Iteration 316 took 3.34 seconds (mean sampled reward: -3486.25). Current reward after update: -562.83, Optimal reward -439.48
Iteration 317 took 3.40 seconds (mean sampled reward: -3419.52). Current reward after update: -541.63, Optimal reward -439.48
Iteration 318 took 3.38 seconds (mean sampled reward: -3462.94). Current reward after update: -525.17, Optimal reward -439.48
Iteration 319 took 3.42 seconds (mean sampled reward: -3619.06). Current reward after update: -558.17, Optimal reward -439.48
Iteration 320 took 3.39 seconds (mean sampled reward: -3288.19). Current reward after update: -591.09, Optimal reward -439.48
Iteration 321 took 3.35 seconds (mean sampled reward: -3238.75). Current reward after update: -574.07, Optimal reward -439.48
Iteration 322 took 3.46 seconds (mean sampled reward: -4396.87). Current reward after update: -618.62, Optimal reward -439.48
Iteration 323 took 3.38 seconds (mean sampled reward: -4197.57). Current reward after update: -519.90, Optimal reward -439.48
Iteration 324 took 3.48 seconds (mean sampled reward: -4006.12). Current reward after update: -561.03, Optimal reward -439.48
Iteration 325 took 3.40 seconds (mean sampled reward: -4438.58). Current reward after update: -688.69, Optimal reward -439.48
Iteration 326 took 3.56 seconds (mean sampled reward: -4959.93). Current reward after update: -654.39, Optimal reward -439.48
Iteration 327 took 3.31 seconds (mean sampled reward: -4170.25). Current reward after update: -638.14, Optimal reward -439.48
Iteration 328 took 3.44 seconds (mean sampled reward: -5251.79). Current reward after update: -578.13, Optimal reward -439.48
Iteration 329 took 3.26 seconds (mean sampled reward: -4604.26). Current reward after update: -666.98, Optimal reward -439.48
Iteration 330 took 3.25 seconds (mean sampled reward: -4147.80). Current reward after update: -673.06, Optimal reward -439.48
Iteration 331 took 3.34 seconds (mean sampled reward: -3720.79). Current reward after update: -592.51, Optimal reward -439.48
Iteration 332 took 3.31 seconds (mean sampled reward: -4073.10). Current reward after update: -596.97, Optimal reward -439.48
Iteration 333 took 3.38 seconds (mean sampled reward: -4590.08). Current reward after update: -648.63, Optimal reward -439.48
Iteration 334 took 3.39 seconds (mean sampled reward: -4567.62). Current reward after update: -2684.98, Optimal reward -439.48
Iteration 335 took 3.39 seconds (mean sampled reward: -3621.60). Current reward after update: -534.47, Optimal reward -439.48
Iteration 336 took 3.49 seconds (mean sampled reward: -4675.68). Current reward after update: -670.65, Optimal reward -439.48
Iteration 337 took 3.30 seconds (mean sampled reward: -3641.54). Current reward after update: -596.57, Optimal reward -439.48
Iteration 338 took 3.47 seconds (mean sampled reward: -4109.02). Current reward after update: -585.19, Optimal reward -439.48
Iteration 339 took 3.48 seconds (mean sampled reward: -3655.73). Current reward after update: -624.98, Optimal reward -439.48
Iteration 340 took 3.36 seconds (mean sampled reward: -3561.38). Current reward after update: -570.66, Optimal reward -439.48
Iteration 341 took 3.29 seconds (mean sampled reward: -3614.27). Current reward after update: -647.22, Optimal reward -439.48
Iteration 342 took 3.33 seconds (mean sampled reward: -3773.51). Current reward after update: -572.64, Optimal reward -439.48
Iteration 343 took 3.42 seconds (mean sampled reward: -4174.25). Current reward after update: -643.12, Optimal reward -439.48
Iteration 344 took 3.49 seconds (mean sampled reward: -4677.78). Current reward after update: -585.65, Optimal reward -439.48
Iteration 345 took 3.48 seconds (mean sampled reward: -4490.37). Current reward after update: -686.29, Optimal reward -439.48
Iteration 346 took 3.29 seconds (mean sampled reward: -3427.64). Current reward after update: -644.72, Optimal reward -439.48
Iteration 347 took 3.32 seconds (mean sampled reward: -3395.59). Current reward after update: -626.49, Optimal reward -439.48
Iteration 348 took 3.35 seconds (mean sampled reward: -3595.70). Current reward after update: -672.46, Optimal reward -439.48
Iteration 349 took 3.26 seconds (mean sampled reward: -3398.45). Current reward after update: -553.48, Optimal reward -439.48
Iteration 350 took 3.31 seconds (mean sampled reward: -3587.70). Current reward after update: -573.14, Optimal reward -439.48
Iteration 351 took 3.29 seconds (mean sampled reward: -3571.55). Current reward after update: -567.76, Optimal reward -439.48
Iteration 352 took 3.31 seconds (mean sampled reward: -3591.72). Current reward after update: -522.39, Optimal reward -439.48
Iteration 353 took 3.34 seconds (mean sampled reward: -3874.33). Current reward after update: -582.81, Optimal reward -439.48
Iteration 354 took 3.28 seconds (mean sampled reward: -3526.77). Current reward after update: -625.55, Optimal reward -439.48
Iteration 355 took 3.32 seconds (mean sampled reward: -3507.32). Current reward after update: -595.91, Optimal reward -439.48
Iteration 356 took 3.18 seconds (mean sampled reward: -3815.83). Current reward after update: -572.21, Optimal reward -439.48
Iteration 357 took 3.16 seconds (mean sampled reward: -3754.30). Current reward after update: -576.07, Optimal reward -439.48
Iteration 358 took 3.28 seconds (mean sampled reward: -3415.29). Current reward after update: -582.31, Optimal reward -439.48
Iteration 359 took 3.22 seconds (mean sampled reward: -3249.02). Current reward after update: -572.53, Optimal reward -439.48
Iteration 360 took 3.19 seconds (mean sampled reward: -3356.99). Current reward after update: -558.82, Optimal reward -439.48
Iteration 361 took 3.18 seconds (mean sampled reward: -3406.76). Current reward after update: -625.13, Optimal reward -439.48
Iteration 362 took 3.22 seconds (mean sampled reward: -3784.90). Current reward after update: -650.84, Optimal reward -439.48
Iteration 363 took 3.24 seconds (mean sampled reward: -3629.22). Current reward after update: -580.39, Optimal reward -439.48
Iteration 364 took 3.23 seconds (mean sampled reward: -3297.86). Current reward after update: -597.01, Optimal reward -439.48
Iteration 365 took 3.23 seconds (mean sampled reward: -3253.13). Current reward after update: -627.26, Optimal reward -439.48
Iteration 366 took 3.30 seconds (mean sampled reward: -3240.00). Current reward after update: -600.60, Optimal reward -439.48
Iteration 367 took 3.24 seconds (mean sampled reward: -3066.87). Current reward after update: -567.35, Optimal reward -439.48
Iteration 368 took 3.29 seconds (mean sampled reward: -3256.80). Current reward after update: -578.19, Optimal reward -439.48
Iteration 369 took 3.33 seconds (mean sampled reward: -3111.91). Current reward after update: -660.72, Optimal reward -439.48
Iteration 370 took 3.30 seconds (mean sampled reward: -3065.95). Current reward after update: -608.74, Optimal reward -439.48
Iteration 371 took 3.30 seconds (mean sampled reward: -3455.36). Current reward after update: -600.74, Optimal reward -439.48
Iteration 372 took 3.32 seconds (mean sampled reward: -3553.16). Current reward after update: -605.76, Optimal reward -439.48
Iteration 373 took 3.30 seconds (mean sampled reward: -3515.08). Current reward after update: -574.57, Optimal reward -439.48
Iteration 374 took 3.28 seconds (mean sampled reward: -3303.64). Current reward after update: -557.25, Optimal reward -439.48
Iteration 375 took 3.25 seconds (mean sampled reward: -3881.85). Current reward after update: -1504.20, Optimal reward -439.48
Iteration 376 took 3.45 seconds (mean sampled reward: -3436.08). Current reward after update: -623.29, Optimal reward -439.48
Iteration 377 took 3.41 seconds (mean sampled reward: -3547.80). Current reward after update: -634.57, Optimal reward -439.48
Iteration 378 took 3.34 seconds (mean sampled reward: -3319.57). Current reward after update: -676.73, Optimal reward -439.48
Iteration 379 took 3.40 seconds (mean sampled reward: -3407.72). Current reward after update: -688.71, Optimal reward -439.48
Iteration 380 took 3.40 seconds (mean sampled reward: -3585.90). Current reward after update: -640.32, Optimal reward -439.48
Iteration 381 took 3.35 seconds (mean sampled reward: -3441.87). Current reward after update: -671.93, Optimal reward -439.48
Iteration 382 took 3.39 seconds (mean sampled reward: -3404.66). Current reward after update: -595.89, Optimal reward -439.48
Iteration 383 took 3.31 seconds (mean sampled reward: -2874.24). Current reward after update: -563.58, Optimal reward -439.48
Iteration 384 took 3.41 seconds (mean sampled reward: -2898.08). Current reward after update: -599.36, Optimal reward -439.48
Iteration 385 took 3.45 seconds (mean sampled reward: -3162.79). Current reward after update: -531.04, Optimal reward -439.48
Iteration 386 took 3.39 seconds (mean sampled reward: -3519.74). Current reward after update: -553.14, Optimal reward -439.48
Iteration 387 took 3.57 seconds (mean sampled reward: -4011.91). Current reward after update: -632.89, Optimal reward -439.48
Iteration 388 took 3.43 seconds (mean sampled reward: -3815.50). Current reward after update: -562.95, Optimal reward -439.48
Iteration 389 took 3.47 seconds (mean sampled reward: -3868.11). Current reward after update: -655.72, Optimal reward -439.48
Iteration 390 took 3.67 seconds (mean sampled reward: -4516.70). Current reward after update: -691.31, Optimal reward -439.48
Iteration 391 took 3.60 seconds (mean sampled reward: -4690.11). Current reward after update: -846.02, Optimal reward -439.48
Iteration 392 took 3.57 seconds (mean sampled reward: -4004.09). Current reward after update: -744.70, Optimal reward -439.48
Iteration 393 took 3.60 seconds (mean sampled reward: -4296.50). Current reward after update: -687.93, Optimal reward -439.48
Iteration 394 took 3.55 seconds (mean sampled reward: -4321.36). Current reward after update: -810.88, Optimal reward -439.48
Iteration 395 took 3.58 seconds (mean sampled reward: -4117.37). Current reward after update: -656.42, Optimal reward -439.48
Iteration 396 took 3.42 seconds (mean sampled reward: -3743.15). Current reward after update: -617.71, Optimal reward -439.48
Iteration 397 took 3.64 seconds (mean sampled reward: -4345.61). Current reward after update: -777.69, Optimal reward -439.48
Iteration 398 took 3.63 seconds (mean sampled reward: -4434.19). Current reward after update: -643.57, Optimal reward -439.48
Iteration 399 took 3.80 seconds (mean sampled reward: -5108.85). Current reward after update: -895.86, Optimal reward -439.48
Iteration 400 took 3.54 seconds (mean sampled reward: -4075.49). Current reward after update: -521.19, Optimal reward -439.48
Iteration 1 took 3.95 seconds (mean sampled reward: -7491.04). Current reward after update: -3270.01, Optimal reward -3270.01
Iteration 2 took 3.76 seconds (mean sampled reward: -6778.97). Current reward after update: -3147.31, Optimal reward -3147.31
Iteration 3 took 4.02 seconds (mean sampled reward: -6746.89). Current reward after update: -2559.95, Optimal reward -2559.95
Iteration 4 took 3.73 seconds (mean sampled reward: -6265.55). Current reward after update: -2080.75, Optimal reward -2080.75
Iteration 5 took 3.75 seconds (mean sampled reward: -5967.30). Current reward after update: -1377.92, Optimal reward -1377.92
Iteration 6 took 3.84 seconds (mean sampled reward: -6817.42). Current reward after update: -1297.83, Optimal reward -1297.83
Iteration 7 took 4.05 seconds (mean sampled reward: -6854.66). Current reward after update: -2101.76, Optimal reward -1297.83
Iteration 8 took 3.79 seconds (mean sampled reward: -6349.92). Current reward after update: -1511.14, Optimal reward -1297.83
Iteration 9 took 4.14 seconds (mean sampled reward: -6223.96). Current reward after update: -948.46, Optimal reward -948.46
Iteration 10 took 3.80 seconds (mean sampled reward: -6352.35). Current reward after update: -937.85, Optimal reward -937.85
Iteration 11 took 3.68 seconds (mean sampled reward: -6548.24). Current reward after update: -566.84, Optimal reward -566.84
Iteration 12 took 3.69 seconds (mean sampled reward: -6069.49). Current reward after update: -510.38, Optimal reward -510.38
Iteration 13 took 3.59 seconds (mean sampled reward: -6130.91). Current reward after update: -679.20, Optimal reward -510.38
Iteration 14 took 3.46 seconds (mean sampled reward: -5880.02). Current reward after update: -636.92, Optimal reward -510.38
Iteration 15 took 3.64 seconds (mean sampled reward: -5750.04). Current reward after update: -688.42, Optimal reward -510.38
Iteration 16 took 3.51 seconds (mean sampled reward: -5882.90). Current reward after update: -761.75, Optimal reward -510.38
Iteration 17 took 3.97 seconds (mean sampled reward: -6116.27). Current reward after update: -649.76, Optimal reward -510.38
Iteration 18 took 3.57 seconds (mean sampled reward: -6175.37). Current reward after update: -757.89, Optimal reward -510.38
Iteration 19 took 3.96 seconds (mean sampled reward: -6209.80). Current reward after update: -638.63, Optimal reward -510.38
Iteration 20 took 3.83 seconds (mean sampled reward: -5884.03). Current reward after update: -1011.79, Optimal reward -510.38
Iteration 21 took 3.75 seconds (mean sampled reward: -5574.70). Current reward after update: -814.66, Optimal reward -510.38
Iteration 22 took 3.56 seconds (mean sampled reward: -6026.38). Current reward after update: -928.22, Optimal reward -510.38
Iteration 23 took 3.66 seconds (mean sampled reward: -6605.58). Current reward after update: -931.50, Optimal reward -510.38
Iteration 24 took 3.69 seconds (mean sampled reward: -5976.94). Current reward after update: -976.06, Optimal reward -510.38
Iteration 25 took 3.51 seconds (mean sampled reward: -5788.84). Current reward after update: -667.40, Optimal reward -510.38
Iteration 26 took 3.58 seconds (mean sampled reward: -5460.51). Current reward after update: -822.76, Optimal reward -510.38
Iteration 27 took 3.63 seconds (mean sampled reward: -5470.36). Current reward after update: -754.98, Optimal reward -510.38
Iteration 28 took 3.54 seconds (mean sampled reward: -4890.37). Current reward after update: -612.53, Optimal reward -510.38
Iteration 29 took 3.85 seconds (mean sampled reward: -5342.68). Current reward after update: -691.48, Optimal reward -510.38
Iteration 30 took 3.49 seconds (mean sampled reward: -6267.84). Current reward after update: -693.37, Optimal reward -510.38
Iteration 31 took 3.47 seconds (mean sampled reward: -6302.24). Current reward after update: -667.99, Optimal reward -510.38
Iteration 32 took 3.40 seconds (mean sampled reward: -5501.66). Current reward after update: -797.94, Optimal reward -510.38
Iteration 33 took 3.38 seconds (mean sampled reward: -5414.13). Current reward after update: -651.69, Optimal reward -510.38
Iteration 34 took 3.67 seconds (mean sampled reward: -6564.43). Current reward after update: -596.32, Optimal reward -510.38
Iteration 35 took 3.50 seconds (mean sampled reward: -5827.92). Current reward after update: -629.69, Optimal reward -510.38
Iteration 36 took 3.77 seconds (mean sampled reward: -6645.40). Current reward after update: -563.83, Optimal reward -510.38
Iteration 37 took 3.55 seconds (mean sampled reward: -6529.69). Current reward after update: -843.84, Optimal reward -510.38
Iteration 38 took 3.54 seconds (mean sampled reward: -5603.04). Current reward after update: -600.08, Optimal reward -510.38
Iteration 39 took 3.54 seconds (mean sampled reward: -5787.91). Current reward after update: -684.61, Optimal reward -510.38
Iteration 40 took 3.64 seconds (mean sampled reward: -6427.78). Current reward after update: -757.02, Optimal reward -510.38
Iteration 41 took 3.67 seconds (mean sampled reward: -5575.53). Current reward after update: -583.09, Optimal reward -510.38
Iteration 42 took 3.58 seconds (mean sampled reward: -5770.77). Current reward after update: -804.84, Optimal reward -510.38
Iteration 43 took 3.53 seconds (mean sampled reward: -5641.77). Current reward after update: -592.83, Optimal reward -510.38
Iteration 44 took 3.44 seconds (mean sampled reward: -5724.76). Current reward after update: -885.86, Optimal reward -510.38
Iteration 45 took 3.31 seconds (mean sampled reward: -5370.28). Current reward after update: -803.90, Optimal reward -510.38
Iteration 46 took 3.48 seconds (mean sampled reward: -5977.51). Current reward after update: -925.87, Optimal reward -510.38
Iteration 47 took 3.38 seconds (mean sampled reward: -6136.59). Current reward after update: -781.70, Optimal reward -510.38
Iteration 48 took 3.45 seconds (mean sampled reward: -6824.58). Current reward after update: -865.81, Optimal reward -510.38
Iteration 49 took 3.65 seconds (mean sampled reward: -6804.19). Current reward after update: -941.33, Optimal reward -510.38
Iteration 50 took 3.52 seconds (mean sampled reward: -6737.45). Current reward after update: -578.92, Optimal reward -510.38
Iteration 51 took 3.58 seconds (mean sampled reward: -7099.96). Current reward after update: -980.77, Optimal reward -510.38
Iteration 52 took 3.46 seconds (mean sampled reward: -6942.67). Current reward after update: -600.47, Optimal reward -510.38
Iteration 53 took 3.60 seconds (mean sampled reward: -6689.71). Current reward after update: -680.21, Optimal reward -510.38
Iteration 54 took 3.47 seconds (mean sampled reward: -6850.20). Current reward after update: -758.40, Optimal reward -510.38
Iteration 55 took 3.60 seconds (mean sampled reward: -6329.87). Current reward after update: -505.74, Optimal reward -505.74
Iteration 56 took 3.43 seconds (mean sampled reward: -6389.83). Current reward after update: -467.09, Optimal reward -467.09
Iteration 57 took 3.48 seconds (mean sampled reward: -6914.34). Current reward after update: -564.91, Optimal reward -467.09
Iteration 58 took 3.51 seconds (mean sampled reward: -6705.96). Current reward after update: -668.18, Optimal reward -467.09
Iteration 59 took 3.54 seconds (mean sampled reward: -6508.31). Current reward after update: -745.34, Optimal reward -467.09
Iteration 60 took 3.37 seconds (mean sampled reward: -6768.77). Current reward after update: -793.80, Optimal reward -467.09
Iteration 61 took 3.53 seconds (mean sampled reward: -7022.92). Current reward after update: -1545.71, Optimal reward -467.09
Iteration 62 took 3.55 seconds (mean sampled reward: -6944.43). Current reward after update: -858.63, Optimal reward -467.09
Iteration 63 took 3.38 seconds (mean sampled reward: -7062.72). Current reward after update: -1179.21, Optimal reward -467.09
Iteration 64 took 3.49 seconds (mean sampled reward: -6397.76). Current reward after update: -824.01, Optimal reward -467.09
Iteration 65 took 3.39 seconds (mean sampled reward: -5980.49). Current reward after update: -630.58, Optimal reward -467.09
Iteration 66 took 3.32 seconds (mean sampled reward: -5820.33). Current reward after update: -915.25, Optimal reward -467.09
Iteration 67 took 3.30 seconds (mean sampled reward: -4696.98). Current reward after update: -661.51, Optimal reward -467.09
Iteration 68 took 3.37 seconds (mean sampled reward: -6185.38). Current reward after update: -629.03, Optimal reward -467.09
Iteration 69 took 3.29 seconds (mean sampled reward: -4283.62). Current reward after update: -525.23, Optimal reward -467.09
Iteration 70 took 3.46 seconds (mean sampled reward: -5888.51). Current reward after update: -462.14, Optimal reward -462.14
Iteration 71 took 3.42 seconds (mean sampled reward: -5859.04). Current reward after update: -562.08, Optimal reward -462.14
Iteration 72 took 3.44 seconds (mean sampled reward: -5995.77). Current reward after update: -692.53, Optimal reward -462.14
Iteration 73 took 3.35 seconds (mean sampled reward: -5401.30). Current reward after update: -700.16, Optimal reward -462.14
Iteration 74 took 3.30 seconds (mean sampled reward: -5348.93). Current reward after update: -637.07, Optimal reward -462.14
Iteration 75 took 3.42 seconds (mean sampled reward: -6148.96). Current reward after update: -733.94, Optimal reward -462.14
Iteration 76 took 3.29 seconds (mean sampled reward: -6487.49). Current reward after update: -687.41, Optimal reward -462.14
Iteration 77 took 3.36 seconds (mean sampled reward: -5406.65). Current reward after update: -566.83, Optimal reward -462.14
Iteration 78 took 3.38 seconds (mean sampled reward: -5783.22). Current reward after update: -570.42, Optimal reward -462.14
Iteration 79 took 3.29 seconds (mean sampled reward: -4909.96). Current reward after update: -670.21, Optimal reward -462.14
Iteration 80 took 3.33 seconds (mean sampled reward: -4749.50). Current reward after update: -564.63, Optimal reward -462.14
Iteration 81 took 3.36 seconds (mean sampled reward: -4251.16). Current reward after update: -479.15, Optimal reward -462.14
Iteration 82 took 3.31 seconds (mean sampled reward: -3596.49). Current reward after update: -510.83, Optimal reward -462.14
Iteration 83 took 3.35 seconds (mean sampled reward: -4391.41). Current reward after update: -566.58, Optimal reward -462.14
Iteration 84 took 3.29 seconds (mean sampled reward: -4466.32). Current reward after update: -523.23, Optimal reward -462.14
Iteration 85 took 3.26 seconds (mean sampled reward: -4217.68). Current reward after update: -687.39, Optimal reward -462.14
Iteration 86 took 3.30 seconds (mean sampled reward: -4603.16). Current reward after update: -584.80, Optimal reward -462.14
Iteration 87 took 3.30 seconds (mean sampled reward: -6120.14). Current reward after update: -538.25, Optimal reward -462.14
Iteration 88 took 3.34 seconds (mean sampled reward: -4874.81). Current reward after update: -655.95, Optimal reward -462.14
Iteration 89 took 3.32 seconds (mean sampled reward: -4385.12). Current reward after update: -1478.26, Optimal reward -462.14
Iteration 90 took 3.38 seconds (mean sampled reward: -4869.31). Current reward after update: -572.83, Optimal reward -462.14
Iteration 91 took 3.43 seconds (mean sampled reward: -4384.91). Current reward after update: -516.13, Optimal reward -462.14
Iteration 92 took 3.25 seconds (mean sampled reward: -4523.64). Current reward after update: -461.97, Optimal reward -461.97
Iteration 93 took 3.34 seconds (mean sampled reward: -5529.11). Current reward after update: -599.06, Optimal reward -461.97
Iteration 94 took 3.20 seconds (mean sampled reward: -4370.27). Current reward after update: -591.93, Optimal reward -461.97
Iteration 95 took 3.32 seconds (mean sampled reward: -5097.47). Current reward after update: -756.12, Optimal reward -461.97
Iteration 96 took 3.41 seconds (mean sampled reward: -5746.61). Current reward after update: -593.80, Optimal reward -461.97
Iteration 97 took 3.51 seconds (mean sampled reward: -5911.55). Current reward after update: -424.37, Optimal reward -424.37
Iteration 98 took 3.42 seconds (mean sampled reward: -5589.68). Current reward after update: -441.66, Optimal reward -424.37
Iteration 99 took 3.44 seconds (mean sampled reward: -5260.34). Current reward after update: -507.70, Optimal reward -424.37
Iteration 100 took 3.22 seconds (mean sampled reward: -3245.97). Current reward after update: -537.11, Optimal reward -424.37
Iteration 101 took 3.32 seconds (mean sampled reward: -3613.22). Current reward after update: -493.95, Optimal reward -424.37
Iteration 102 took 3.22 seconds (mean sampled reward: -3823.93). Current reward after update: -465.57, Optimal reward -424.37
Iteration 103 took 3.23 seconds (mean sampled reward: -3625.01). Current reward after update: -388.64, Optimal reward -388.64
Iteration 104 took 3.24 seconds (mean sampled reward: -4412.27). Current reward after update: -523.65, Optimal reward -388.64
Iteration 105 took 3.31 seconds (mean sampled reward: -3349.93). Current reward after update: -508.57, Optimal reward -388.64
Iteration 106 took 3.29 seconds (mean sampled reward: -3542.99). Current reward after update: -380.59, Optimal reward -380.59
Iteration 107 took 3.24 seconds (mean sampled reward: -3316.66). Current reward after update: -486.42, Optimal reward -380.59
Iteration 108 took 3.39 seconds (mean sampled reward: -2984.17). Current reward after update: -482.18, Optimal reward -380.59
Iteration 109 took 3.57 seconds (mean sampled reward: -3191.13). Current reward after update: -472.29, Optimal reward -380.59
Iteration 110 took 3.28 seconds (mean sampled reward: -2607.70). Current reward after update: -463.65, Optimal reward -380.59
Iteration 111 took 3.25 seconds (mean sampled reward: -2920.24). Current reward after update: -489.81, Optimal reward -380.59
Iteration 112 took 3.32 seconds (mean sampled reward: -2899.09). Current reward after update: -461.16, Optimal reward -380.59
Iteration 113 took 3.33 seconds (mean sampled reward: -2963.12). Current reward after update: -484.02, Optimal reward -380.59
Iteration 114 took 3.29 seconds (mean sampled reward: -3127.15). Current reward after update: -516.22, Optimal reward -380.59
Iteration 115 took 3.34 seconds (mean sampled reward: -3698.88). Current reward after update: -616.64, Optimal reward -380.59
Iteration 116 took 3.33 seconds (mean sampled reward: -3077.40). Current reward after update: -512.42, Optimal reward -380.59
Iteration 117 took 3.28 seconds (mean sampled reward: -2544.06). Current reward after update: -520.91, Optimal reward -380.59
Iteration 118 took 3.47 seconds (mean sampled reward: -3765.12). Current reward after update: -547.83, Optimal reward -380.59
Iteration 119 took 3.42 seconds (mean sampled reward: -3899.45). Current reward after update: -544.40, Optimal reward -380.59
Iteration 120 took 3.26 seconds (mean sampled reward: -2801.88). Current reward after update: -483.93, Optimal reward -380.59
Iteration 121 took 3.31 seconds (mean sampled reward: -3333.40). Current reward after update: -607.12, Optimal reward -380.59
Iteration 122 took 3.37 seconds (mean sampled reward: -3877.94). Current reward after update: -595.35, Optimal reward -380.59
Iteration 123 took 3.45 seconds (mean sampled reward: -5684.00). Current reward after update: -663.34, Optimal reward -380.59
Iteration 124 took 3.41 seconds (mean sampled reward: -5108.31). Current reward after update: -5275.66, Optimal reward -380.59
Iteration 125 took 3.53 seconds (mean sampled reward: -5809.11). Current reward after update: -642.90, Optimal reward -380.59
Iteration 126 took 3.40 seconds (mean sampled reward: -4227.54). Current reward after update: -669.68, Optimal reward -380.59
Iteration 127 took 3.43 seconds (mean sampled reward: -3978.81). Current reward after update: -545.40, Optimal reward -380.59
Iteration 128 took 3.43 seconds (mean sampled reward: -3919.34). Current reward after update: -530.12, Optimal reward -380.59
Iteration 129 took 3.37 seconds (mean sampled reward: -3351.00). Current reward after update: -569.08, Optimal reward -380.59
Iteration 130 took 3.43 seconds (mean sampled reward: -4098.40). Current reward after update: -565.44, Optimal reward -380.59
Iteration 131 took 3.45 seconds (mean sampled reward: -4095.52). Current reward after update: -539.23, Optimal reward -380.59
Iteration 132 took 3.32 seconds (mean sampled reward: -3316.74). Current reward after update: -504.96, Optimal reward -380.59
Iteration 133 took 3.34 seconds (mean sampled reward: -3476.74). Current reward after update: -581.53, Optimal reward -380.59
Iteration 134 took 3.63 seconds (mean sampled reward: -5271.58). Current reward after update: -477.45, Optimal reward -380.59
Iteration 135 took 3.56 seconds (mean sampled reward: -4862.34). Current reward after update: -545.16, Optimal reward -380.59
Iteration 136 took 3.63 seconds (mean sampled reward: -4749.38). Current reward after update: -509.91, Optimal reward -380.59
Iteration 137 took 3.74 seconds (mean sampled reward: -5306.18). Current reward after update: -541.71, Optimal reward -380.59
Iteration 138 took 3.72 seconds (mean sampled reward: -4862.87). Current reward after update: -692.45, Optimal reward -380.59
Iteration 139 took 3.32 seconds (mean sampled reward: -3214.92). Current reward after update: -516.44, Optimal reward -380.59
Iteration 140 took 3.60 seconds (mean sampled reward: -4071.37). Current reward after update: -532.51, Optimal reward -380.59
Iteration 141 took 3.62 seconds (mean sampled reward: -4276.95). Current reward after update: -643.05, Optimal reward -380.59
Iteration 142 took 3.45 seconds (mean sampled reward: -3600.03). Current reward after update: -508.06, Optimal reward -380.59
Iteration 143 took 3.53 seconds (mean sampled reward: -4309.32). Current reward after update: -528.53, Optimal reward -380.59
Iteration 144 took 3.73 seconds (mean sampled reward: -5232.83). Current reward after update: -551.13, Optimal reward -380.59
Iteration 145 took 3.43 seconds (mean sampled reward: -3288.65). Current reward after update: -554.17, Optimal reward -380.59
Iteration 146 took 3.36 seconds (mean sampled reward: -2650.01). Current reward after update: -504.31, Optimal reward -380.59
Iteration 147 took 3.32 seconds (mean sampled reward: -2312.70). Current reward after update: -400.81, Optimal reward -380.59
Iteration 148 took 3.39 seconds (mean sampled reward: -2606.15). Current reward after update: -488.63, Optimal reward -380.59
Iteration 149 took 3.33 seconds (mean sampled reward: -2424.08). Current reward after update: -487.05, Optimal reward -380.59
Iteration 150 took 3.36 seconds (mean sampled reward: -2751.29). Current reward after update: -474.90, Optimal reward -380.59
Iteration 151 took 3.43 seconds (mean sampled reward: -2979.48). Current reward after update: -457.28, Optimal reward -380.59
Iteration 152 took 3.53 seconds (mean sampled reward: -3467.36). Current reward after update: -571.20, Optimal reward -380.59
Iteration 153 took 3.47 seconds (mean sampled reward: -3814.70). Current reward after update: -530.29, Optimal reward -380.59
Iteration 154 took 3.53 seconds (mean sampled reward: -3937.76). Current reward after update: -506.77, Optimal reward -380.59
Iteration 155 took 3.51 seconds (mean sampled reward: -3861.97). Current reward after update: -487.33, Optimal reward -380.59
Iteration 156 took 3.53 seconds (mean sampled reward: -3541.48). Current reward after update: -441.43, Optimal reward -380.59
Iteration 157 took 3.67 seconds (mean sampled reward: -4667.54). Current reward after update: -483.92, Optimal reward -380.59
Iteration 158 took 3.73 seconds (mean sampled reward: -5129.38). Current reward after update: -426.39, Optimal reward -380.59
Iteration 159 took 3.76 seconds (mean sampled reward: -5886.03). Current reward after update: -406.07, Optimal reward -380.59
Iteration 160 took 3.58 seconds (mean sampled reward: -3623.86). Current reward after update: -527.71, Optimal reward -380.59
Iteration 161 took 3.38 seconds (mean sampled reward: -3154.26). Current reward after update: -542.10, Optimal reward -380.59
Iteration 162 took 3.47 seconds (mean sampled reward: -3680.78). Current reward after update: -411.51, Optimal reward -380.59
Iteration 163 took 3.52 seconds (mean sampled reward: -4237.46). Current reward after update: -704.16, Optimal reward -380.59
Iteration 164 took 3.44 seconds (mean sampled reward: -3294.00). Current reward after update: -431.17, Optimal reward -380.59
Iteration 165 took 3.48 seconds (mean sampled reward: -3588.45). Current reward after update: -465.62, Optimal reward -380.59
Iteration 166 took 3.85 seconds (mean sampled reward: -6396.59). Current reward after update: -536.55, Optimal reward -380.59
Iteration 167 took 3.52 seconds (mean sampled reward: -4061.22). Current reward after update: -441.36, Optimal reward -380.59
Iteration 168 took 3.31 seconds (mean sampled reward: -2927.52). Current reward after update: -449.46, Optimal reward -380.59
Iteration 169 took 3.32 seconds (mean sampled reward: -2400.68). Current reward after update: -484.43, Optimal reward -380.59
Iteration 170 took 3.33 seconds (mean sampled reward: -2402.60). Current reward after update: -441.61, Optimal reward -380.59
Iteration 171 took 3.57 seconds (mean sampled reward: -4318.29). Current reward after update: -460.39, Optimal reward -380.59
Iteration 172 took 3.56 seconds (mean sampled reward: -4509.78). Current reward after update: -399.25, Optimal reward -380.59
Iteration 173 took 3.55 seconds (mean sampled reward: -4214.22). Current reward after update: -455.22, Optimal reward -380.59
Iteration 174 took 3.41 seconds (mean sampled reward: -3749.70). Current reward after update: -507.95, Optimal reward -380.59
Iteration 175 took 3.54 seconds (mean sampled reward: -4495.76). Current reward after update: -447.68, Optimal reward -380.59
Iteration 176 took 3.54 seconds (mean sampled reward: -4551.81). Current reward after update: -773.83, Optimal reward -380.59
Iteration 177 took 3.58 seconds (mean sampled reward: -4911.06). Current reward after update: -452.23, Optimal reward -380.59
Iteration 178 took 3.49 seconds (mean sampled reward: -3956.74). Current reward after update: -481.67, Optimal reward -380.59
Iteration 179 took 3.55 seconds (mean sampled reward: -4681.48). Current reward after update: -447.00, Optimal reward -380.59
Iteration 180 took 3.52 seconds (mean sampled reward: -4332.16). Current reward after update: -647.07, Optimal reward -380.59
Iteration 181 took 3.42 seconds (mean sampled reward: -4082.94). Current reward after update: -507.70, Optimal reward -380.59
Iteration 182 took 3.49 seconds (mean sampled reward: -4288.86). Current reward after update: -438.21, Optimal reward -380.59
Iteration 183 took 3.59 seconds (mean sampled reward: -5104.86). Current reward after update: -484.22, Optimal reward -380.59
Iteration 184 took 3.50 seconds (mean sampled reward: -4115.68). Current reward after update: -468.81, Optimal reward -380.59
Iteration 185 took 3.44 seconds (mean sampled reward: -3690.84). Current reward after update: -421.04, Optimal reward -380.59
Iteration 186 took 3.45 seconds (mean sampled reward: -3626.99). Current reward after update: -459.37, Optimal reward -380.59
Iteration 187 took 3.43 seconds (mean sampled reward: -4156.93). Current reward after update: -471.06, Optimal reward -380.59
Iteration 188 took 3.61 seconds (mean sampled reward: -4992.47). Current reward after update: -483.94, Optimal reward -380.59
Iteration 189 took 3.66 seconds (mean sampled reward: -5393.21). Current reward after update: -549.40, Optimal reward -380.59
Iteration 190 took 3.66 seconds (mean sampled reward: -5367.53). Current reward after update: -558.93, Optimal reward -380.59
Iteration 191 took 3.51 seconds (mean sampled reward: -5314.59). Current reward after update: -735.63, Optimal reward -380.59
Iteration 192 took 3.68 seconds (mean sampled reward: -5439.47). Current reward after update: -576.87, Optimal reward -380.59
Iteration 193 took 3.76 seconds (mean sampled reward: -6340.51). Current reward after update: -508.88, Optimal reward -380.59
Iteration 194 took 3.70 seconds (mean sampled reward: -5910.39). Current reward after update: -505.70, Optimal reward -380.59
Iteration 195 took 3.57 seconds (mean sampled reward: -5066.98). Current reward after update: -660.37, Optimal reward -380.59
Iteration 196 took 3.58 seconds (mean sampled reward: -4801.86). Current reward after update: -553.79, Optimal reward -380.59
Iteration 197 took 3.56 seconds (mean sampled reward: -4852.44). Current reward after update: -562.17, Optimal reward -380.59
Iteration 198 took 3.59 seconds (mean sampled reward: -5073.50). Current reward after update: -668.48, Optimal reward -380.59
Iteration 199 took 3.76 seconds (mean sampled reward: -6624.81). Current reward after update: -710.90, Optimal reward -380.59
Iteration 200 took 3.68 seconds (mean sampled reward: -5626.84). Current reward after update: -538.60, Optimal reward -380.59
Iteration 201 took 3.73 seconds (mean sampled reward: -5472.72). Current reward after update: -595.25, Optimal reward -380.59
Iteration 202 took 3.70 seconds (mean sampled reward: -5666.47). Current reward after update: -634.95, Optimal reward -380.59
Iteration 203 took 3.71 seconds (mean sampled reward: -5093.34). Current reward after update: -530.11, Optimal reward -380.59
Iteration 204 took 3.80 seconds (mean sampled reward: -5828.88). Current reward after update: -623.68, Optimal reward -380.59
Iteration 205 took 3.72 seconds (mean sampled reward: -5788.62). Current reward after update: -523.70, Optimal reward -380.59
Iteration 206 took 3.62 seconds (mean sampled reward: -4809.80). Current reward after update: -589.03, Optimal reward -380.59
Iteration 207 took 3.70 seconds (mean sampled reward: -5580.58). Current reward after update: -592.43, Optimal reward -380.59
Iteration 208 took 3.62 seconds (mean sampled reward: -4786.15). Current reward after update: -617.54, Optimal reward -380.59
Iteration 209 took 3.62 seconds (mean sampled reward: -4663.12). Current reward after update: -555.48, Optimal reward -380.59
Iteration 210 took 3.55 seconds (mean sampled reward: -4153.00). Current reward after update: -510.02, Optimal reward -380.59
Iteration 211 took 3.63 seconds (mean sampled reward: -4459.21). Current reward after update: -475.69, Optimal reward -380.59
Iteration 212 took 3.93 seconds (mean sampled reward: -4297.47). Current reward after update: -506.90, Optimal reward -380.59
Iteration 213 took 3.63 seconds (mean sampled reward: -4609.20). Current reward after update: -492.95, Optimal reward -380.59
Iteration 214 took 3.69 seconds (mean sampled reward: -4206.17). Current reward after update: -472.01, Optimal reward -380.59
Iteration 215 took 3.70 seconds (mean sampled reward: -5164.45). Current reward after update: -463.20, Optimal reward -380.59
Iteration 216 took 3.83 seconds (mean sampled reward: -5825.24). Current reward after update: -436.86, Optimal reward -380.59
Iteration 217 took 3.84 seconds (mean sampled reward: -6193.82). Current reward after update: -468.75, Optimal reward -380.59
Iteration 218 took 3.84 seconds (mean sampled reward: -5718.83). Current reward after update: -538.97, Optimal reward -380.59
Iteration 219 took 3.72 seconds (mean sampled reward: -5308.35). Current reward after update: -527.19, Optimal reward -380.59
Iteration 220 took 3.67 seconds (mean sampled reward: -4822.90). Current reward after update: -1342.74, Optimal reward -380.59
Iteration 221 took 3.68 seconds (mean sampled reward: -5247.70). Current reward after update: -567.43, Optimal reward -380.59
Iteration 222 took 3.76 seconds (mean sampled reward: -6006.57). Current reward after update: -407.42, Optimal reward -380.59
Iteration 223 took 3.85 seconds (mean sampled reward: -6879.65). Current reward after update: -607.10, Optimal reward -380.59
Iteration 224 took 3.84 seconds (mean sampled reward: -6384.39). Current reward after update: -701.99, Optimal reward -380.59
Iteration 225 took 3.81 seconds (mean sampled reward: -5792.80). Current reward after update: -580.55, Optimal reward -380.59
Iteration 226 took 3.80 seconds (mean sampled reward: -6048.67). Current reward after update: -665.94, Optimal reward -380.59
Iteration 227 took 3.60 seconds (mean sampled reward: -4018.53). Current reward after update: -460.38, Optimal reward -380.59
Iteration 228 took 3.71 seconds (mean sampled reward: -4786.50). Current reward after update: -439.90, Optimal reward -380.59
Iteration 229 took 3.70 seconds (mean sampled reward: -4921.36). Current reward after update: -523.58, Optimal reward -380.59
Iteration 230 took 3.67 seconds (mean sampled reward: -5576.63). Current reward after update: -527.81, Optimal reward -380.59
Iteration 231 took 3.60 seconds (mean sampled reward: -4766.27). Current reward after update: -573.14, Optimal reward -380.59
Iteration 232 took 3.78 seconds (mean sampled reward: -4729.38). Current reward after update: -567.48, Optimal reward -380.59
Iteration 233 took 3.68 seconds (mean sampled reward: -5087.85). Current reward after update: -734.86, Optimal reward -380.59
Iteration 234 took 3.86 seconds (mean sampled reward: -5950.63). Current reward after update: -594.26, Optimal reward -380.59
Iteration 235 took 3.75 seconds (mean sampled reward: -6351.67). Current reward after update: -840.76, Optimal reward -380.59
Iteration 236 took 3.69 seconds (mean sampled reward: -5539.37). Current reward after update: -725.13, Optimal reward -380.59
Iteration 237 took 3.78 seconds (mean sampled reward: -5857.93). Current reward after update: -555.08, Optimal reward -380.59
Iteration 238 took 3.76 seconds (mean sampled reward: -5382.33). Current reward after update: -754.49, Optimal reward -380.59
Iteration 239 took 3.94 seconds (mean sampled reward: -6828.28). Current reward after update: -624.18, Optimal reward -380.59
Iteration 240 took 3.86 seconds (mean sampled reward: -6659.43). Current reward after update: -852.03, Optimal reward -380.59
Iteration 241 took 3.70 seconds (mean sampled reward: -5320.55). Current reward after update: -823.68, Optimal reward -380.59
Iteration 242 took 3.63 seconds (mean sampled reward: -5183.20). Current reward after update: -654.57, Optimal reward -380.59
Iteration 243 took 3.72 seconds (mean sampled reward: -5375.72). Current reward after update: -600.78, Optimal reward -380.59
Iteration 244 took 3.76 seconds (mean sampled reward: -6061.41). Current reward after update: -753.97, Optimal reward -380.59
Iteration 245 took 3.53 seconds (mean sampled reward: -4369.01). Current reward after update: -655.43, Optimal reward -380.59
Iteration 246 took 3.83 seconds (mean sampled reward: -6277.42). Current reward after update: -536.52, Optimal reward -380.59
Iteration 247 took 3.64 seconds (mean sampled reward: -6070.26). Current reward after update: -639.33, Optimal reward -380.59
Iteration 248 took 3.64 seconds (mean sampled reward: -5897.19). Current reward after update: -647.46, Optimal reward -380.59
Iteration 249 took 3.71 seconds (mean sampled reward: -6124.29). Current reward after update: -677.35, Optimal reward -380.59
Iteration 250 took 3.64 seconds (mean sampled reward: -5556.55). Current reward after update: -799.30, Optimal reward -380.59
Iteration 251 took 3.65 seconds (mean sampled reward: -5268.07). Current reward after update: -694.10, Optimal reward -380.59
Iteration 252 took 3.61 seconds (mean sampled reward: -5574.93). Current reward after update: -731.37, Optimal reward -380.59
Iteration 253 took 3.66 seconds (mean sampled reward: -5001.97). Current reward after update: -632.19, Optimal reward -380.59
Iteration 254 took 3.56 seconds (mean sampled reward: -5572.40). Current reward after update: -671.47, Optimal reward -380.59
Iteration 255 took 3.51 seconds (mean sampled reward: -5659.74). Current reward after update: -753.40, Optimal reward -380.59
Iteration 256 took 3.60 seconds (mean sampled reward: -5298.36). Current reward after update: -730.55, Optimal reward -380.59
Iteration 257 took 3.58 seconds (mean sampled reward: -4805.41). Current reward after update: -733.72, Optimal reward -380.59
Iteration 258 took 3.61 seconds (mean sampled reward: -4521.68). Current reward after update: -828.59, Optimal reward -380.59
Iteration 259 took 3.55 seconds (mean sampled reward: -4875.59). Current reward after update: -682.32, Optimal reward -380.59
Iteration 260 took 3.53 seconds (mean sampled reward: -4546.72). Current reward after update: -674.78, Optimal reward -380.59
Iteration 261 took 3.45 seconds (mean sampled reward: -3892.82). Current reward after update: -723.51, Optimal reward -380.59
Iteration 262 took 3.52 seconds (mean sampled reward: -4721.86). Current reward after update: -658.64, Optimal reward -380.59
Iteration 263 took 3.69 seconds (mean sampled reward: -4718.85). Current reward after update: -678.97, Optimal reward -380.59
Iteration 264 took 3.75 seconds (mean sampled reward: -5932.43). Current reward after update: -686.23, Optimal reward -380.59
Iteration 265 took 3.67 seconds (mean sampled reward: -5283.39). Current reward after update: -669.32, Optimal reward -380.59
Iteration 266 took 3.57 seconds (mean sampled reward: -4439.82). Current reward after update: -679.02, Optimal reward -380.59
Iteration 267 took 3.63 seconds (mean sampled reward: -4517.94). Current reward after update: -622.18, Optimal reward -380.59
Iteration 268 took 3.74 seconds (mean sampled reward: -5245.17). Current reward after update: -709.12, Optimal reward -380.59
Iteration 269 took 3.76 seconds (mean sampled reward: -5319.37). Current reward after update: -582.11, Optimal reward -380.59
Iteration 270 took 3.71 seconds (mean sampled reward: -4916.76). Current reward after update: -533.61, Optimal reward -380.59
Iteration 271 took 3.74 seconds (mean sampled reward: -5625.61). Current reward after update: -592.53, Optimal reward -380.59
Iteration 272 took 3.70 seconds (mean sampled reward: -5028.96). Current reward after update: -601.10, Optimal reward -380.59
Iteration 273 took 3.70 seconds (mean sampled reward: -5230.62). Current reward after update: -729.08, Optimal reward -380.59
Iteration 274 took 3.84 seconds (mean sampled reward: -6153.70). Current reward after update: -703.24, Optimal reward -380.59
Iteration 275 took 3.87 seconds (mean sampled reward: -6177.43). Current reward after update: -551.19, Optimal reward -380.59
Iteration 276 took 3.74 seconds (mean sampled reward: -5260.45). Current reward after update: -664.84, Optimal reward -380.59
Iteration 277 took 3.69 seconds (mean sampled reward: -5514.86). Current reward after update: -656.64, Optimal reward -380.59
Iteration 278 took 3.88 seconds (mean sampled reward: -5859.95). Current reward after update: -606.86, Optimal reward -380.59
Iteration 279 took 3.71 seconds (mean sampled reward: -6457.53). Current reward after update: -730.01, Optimal reward -380.59
Iteration 280 took 3.51 seconds (mean sampled reward: -5476.44). Current reward after update: -650.89, Optimal reward -380.59
Iteration 281 took 3.68 seconds (mean sampled reward: -5696.33). Current reward after update: -685.96, Optimal reward -380.59
Iteration 282 took 3.60 seconds (mean sampled reward: -4815.52). Current reward after update: -672.41, Optimal reward -380.59
Iteration 283 took 3.77 seconds (mean sampled reward: -6275.72). Current reward after update: -665.79, Optimal reward -380.59
Iteration 284 took 3.78 seconds (mean sampled reward: -6144.32). Current reward after update: -676.60, Optimal reward -380.59
Iteration 285 took 3.77 seconds (mean sampled reward: -6560.25). Current reward after update: -584.16, Optimal reward -380.59
Iteration 286 took 3.70 seconds (mean sampled reward: -6756.63). Current reward after update: -572.96, Optimal reward -380.59
Iteration 287 took 3.78 seconds (mean sampled reward: -6474.84). Current reward after update: -650.22, Optimal reward -380.59
Iteration 288 took 3.75 seconds (mean sampled reward: -6571.31). Current reward after update: -843.20, Optimal reward -380.59
Iteration 289 took 3.75 seconds (mean sampled reward: -6076.06). Current reward after update: -742.97, Optimal reward -380.59
Iteration 290 took 3.47 seconds (mean sampled reward: -4480.25). Current reward after update: -631.83, Optimal reward -380.59
Iteration 291 took 3.72 seconds (mean sampled reward: -6019.44). Current reward after update: -728.94, Optimal reward -380.59
Iteration 292 took 3.74 seconds (mean sampled reward: -6349.60). Current reward after update: -652.81, Optimal reward -380.59
Iteration 293 took 3.78 seconds (mean sampled reward: -6600.08). Current reward after update: -741.97, Optimal reward -380.59
Iteration 294 took 3.71 seconds (mean sampled reward: -7107.76). Current reward after update: -844.39, Optimal reward -380.59
Iteration 295 took 3.69 seconds (mean sampled reward: -7360.46). Current reward after update: -860.09, Optimal reward -380.59
Iteration 296 took 3.59 seconds (mean sampled reward: -5775.62). Current reward after update: -700.89, Optimal reward -380.59
Iteration 297 took 3.51 seconds (mean sampled reward: -6469.54). Current reward after update: -725.36, Optimal reward -380.59
Iteration 298 took 3.49 seconds (mean sampled reward: -5578.57). Current reward after update: -656.38, Optimal reward -380.59
Iteration 299 took 3.48 seconds (mean sampled reward: -5472.85). Current reward after update: -619.46, Optimal reward -380.59
Iteration 300 took 3.36 seconds (mean sampled reward: -4100.56). Current reward after update: -598.81, Optimal reward -380.59
Iteration 301 took 3.52 seconds (mean sampled reward: -5692.87). Current reward after update: -645.40, Optimal reward -380.59
Iteration 302 took 3.52 seconds (mean sampled reward: -5988.46). Current reward after update: -649.52, Optimal reward -380.59
Iteration 303 took 3.48 seconds (mean sampled reward: -5623.95). Current reward after update: -650.75, Optimal reward -380.59
Iteration 304 took 3.55 seconds (mean sampled reward: -6382.89). Current reward after update: -662.82, Optimal reward -380.59
Iteration 305 took 3.52 seconds (mean sampled reward: -5692.27). Current reward after update: -641.22, Optimal reward -380.59
Iteration 306 took 3.46 seconds (mean sampled reward: -4631.47). Current reward after update: -614.36, Optimal reward -380.59
Iteration 307 took 3.31 seconds (mean sampled reward: -3269.42). Current reward after update: -627.54, Optimal reward -380.59
Iteration 308 took 3.30 seconds (mean sampled reward: -2784.03). Current reward after update: -594.82, Optimal reward -380.59
Iteration 309 took 3.28 seconds (mean sampled reward: -2927.63). Current reward after update: -560.54, Optimal reward -380.59
Iteration 310 took 3.34 seconds (mean sampled reward: -4073.22). Current reward after update: -599.74, Optimal reward -380.59
Iteration 311 took 3.34 seconds (mean sampled reward: -4457.51). Current reward after update: -663.02, Optimal reward -380.59
Iteration 312 took 3.35 seconds (mean sampled reward: -3831.52). Current reward after update: -583.10, Optimal reward -380.59
Iteration 313 took 3.38 seconds (mean sampled reward: -3499.61). Current reward after update: -552.87, Optimal reward -380.59
Iteration 314 took 3.42 seconds (mean sampled reward: -4605.47). Current reward after update: -535.49, Optimal reward -380.59
Iteration 315 took 3.43 seconds (mean sampled reward: -4287.47). Current reward after update: -573.52, Optimal reward -380.59
Iteration 316 took 3.34 seconds (mean sampled reward: -3059.43). Current reward after update: -592.37, Optimal reward -380.59
Iteration 317 took 3.42 seconds (mean sampled reward: -4053.95). Current reward after update: -561.72, Optimal reward -380.59
Iteration 318 took 3.51 seconds (mean sampled reward: -5136.47). Current reward after update: -533.30, Optimal reward -380.59
Iteration 319 took 3.48 seconds (mean sampled reward: -4264.90). Current reward after update: -586.95, Optimal reward -380.59
Iteration 320 took 3.37 seconds (mean sampled reward: -3306.36). Current reward after update: -545.38, Optimal reward -380.59
Iteration 321 took 3.48 seconds (mean sampled reward: -4348.70). Current reward after update: -595.81, Optimal reward -380.59
Iteration 322 took 3.47 seconds (mean sampled reward: -3685.67). Current reward after update: -611.60, Optimal reward -380.59
Iteration 323 took 3.63 seconds (mean sampled reward: -5625.74). Current reward after update: -563.91, Optimal reward -380.59
Iteration 324 took 3.57 seconds (mean sampled reward: -5131.83). Current reward after update: -581.90, Optimal reward -380.59
Iteration 325 took 3.49 seconds (mean sampled reward: -5090.61). Current reward after update: -584.68, Optimal reward -380.59
Iteration 326 took 3.46 seconds (mean sampled reward: -5217.66). Current reward after update: -532.22, Optimal reward -380.59
Iteration 327 took 3.47 seconds (mean sampled reward: -5094.09). Current reward after update: -563.61, Optimal reward -380.59
Iteration 328 took 3.38 seconds (mean sampled reward: -3761.66). Current reward after update: -537.29, Optimal reward -380.59
Iteration 329 took 3.44 seconds (mean sampled reward: -4287.95). Current reward after update: -505.13, Optimal reward -380.59
Iteration 330 took 3.51 seconds (mean sampled reward: -5197.12). Current reward after update: -609.95, Optimal reward -380.59
Iteration 331 took 3.39 seconds (mean sampled reward: -3613.01). Current reward after update: -566.36, Optimal reward -380.59
Iteration 332 took 3.33 seconds (mean sampled reward: -2629.96). Current reward after update: -583.84, Optimal reward -380.59
Iteration 333 took 3.37 seconds (mean sampled reward: -2730.86). Current reward after update: -500.92, Optimal reward -380.59
Iteration 334 took 3.40 seconds (mean sampled reward: -3255.22). Current reward after update: -555.62, Optimal reward -380.59
Iteration 335 took 3.27 seconds (mean sampled reward: -2445.70). Current reward after update: -685.36, Optimal reward -380.59
Iteration 336 took 3.33 seconds (mean sampled reward: -2548.02). Current reward after update: -1113.73, Optimal reward -380.59
Iteration 337 took 3.35 seconds (mean sampled reward: -2565.22). Current reward after update: -561.29, Optimal reward -380.59
Iteration 338 took 3.30 seconds (mean sampled reward: -2795.75). Current reward after update: -578.69, Optimal reward -380.59
Iteration 339 took 3.41 seconds (mean sampled reward: -3389.68). Current reward after update: -582.95, Optimal reward -380.59
Iteration 340 took 3.41 seconds (mean sampled reward: -3300.69). Current reward after update: -717.32, Optimal reward -380.59
Iteration 341 took 3.34 seconds (mean sampled reward: -2528.59). Current reward after update: -549.42, Optimal reward -380.59
Iteration 342 took 3.33 seconds (mean sampled reward: -2520.18). Current reward after update: -495.13, Optimal reward -380.59
Iteration 343 took 3.34 seconds (mean sampled reward: -2736.78). Current reward after update: -497.03, Optimal reward -380.59
Iteration 344 took 3.41 seconds (mean sampled reward: -3106.09). Current reward after update: -521.40, Optimal reward -380.59
Iteration 345 took 3.38 seconds (mean sampled reward: -3418.16). Current reward after update: -631.21, Optimal reward -380.59
Iteration 346 took 3.53 seconds (mean sampled reward: -4197.80). Current reward after update: -639.00, Optimal reward -380.59
Iteration 347 took 3.31 seconds (mean sampled reward: -2588.04). Current reward after update: -584.96, Optimal reward -380.59
Iteration 348 took 3.32 seconds (mean sampled reward: -2467.62). Current reward after update: -542.82, Optimal reward -380.59
Iteration 349 took 3.36 seconds (mean sampled reward: -3591.23). Current reward after update: -626.62, Optimal reward -380.59
Iteration 350 took 3.46 seconds (mean sampled reward: -3228.98). Current reward after update: -7061.31, Optimal reward -380.59
Iteration 351 took 3.44 seconds (mean sampled reward: -4281.07). Current reward after update: -612.84, Optimal reward -380.59
Iteration 352 took 3.49 seconds (mean sampled reward: -4893.31). Current reward after update: -611.01, Optimal reward -380.59
Iteration 353 took 3.40 seconds (mean sampled reward: -3783.97). Current reward after update: -614.41, Optimal reward -380.59
Iteration 354 took 3.49 seconds (mean sampled reward: -3994.02). Current reward after update: -579.75, Optimal reward -380.59
Iteration 355 took 3.44 seconds (mean sampled reward: -4165.71). Current reward after update: -570.21, Optimal reward -380.59
Iteration 356 took 3.35 seconds (mean sampled reward: -2851.52). Current reward after update: -555.48, Optimal reward -380.59
Iteration 357 took 3.37 seconds (mean sampled reward: -2479.99). Current reward after update: -572.41, Optimal reward -380.59
Iteration 358 took 3.35 seconds (mean sampled reward: -3154.80). Current reward after update: -620.53, Optimal reward -380.59
Iteration 359 took 3.33 seconds (mean sampled reward: -2697.39). Current reward after update: -606.02, Optimal reward -380.59
Iteration 360 took 3.39 seconds (mean sampled reward: -3691.91). Current reward after update: -594.16, Optimal reward -380.59
Iteration 361 took 3.38 seconds (mean sampled reward: -3533.92). Current reward after update: -570.63, Optimal reward -380.59
Iteration 362 took 3.39 seconds (mean sampled reward: -3708.81). Current reward after update: -573.87, Optimal reward -380.59
Iteration 363 took 3.44 seconds (mean sampled reward: -4237.09). Current reward after update: -542.35, Optimal reward -380.59
Iteration 364 took 3.44 seconds (mean sampled reward: -3987.18). Current reward after update: -547.79, Optimal reward -380.59
Iteration 365 took 3.33 seconds (mean sampled reward: -2511.30). Current reward after update: -591.19, Optimal reward -380.59
Iteration 366 took 3.37 seconds (mean sampled reward: -2673.91). Current reward after update: -555.05, Optimal reward -380.59
Iteration 367 took 3.38 seconds (mean sampled reward: -3357.72). Current reward after update: -535.78, Optimal reward -380.59
Iteration 368 took 3.37 seconds (mean sampled reward: -4050.55). Current reward after update: -587.02, Optimal reward -380.59
Iteration 369 took 3.36 seconds (mean sampled reward: -4022.98). Current reward after update: -597.57, Optimal reward -380.59
Iteration 370 took 3.50 seconds (mean sampled reward: -4334.57). Current reward after update: -590.62, Optimal reward -380.59
Iteration 371 took 3.42 seconds (mean sampled reward: -4814.04). Current reward after update: -560.19, Optimal reward -380.59
Iteration 372 took 3.39 seconds (mean sampled reward: -4151.97). Current reward after update: -595.45, Optimal reward -380.59
Iteration 373 took 3.40 seconds (mean sampled reward: -4011.39). Current reward after update: -537.77, Optimal reward -380.59
Iteration 374 took 3.36 seconds (mean sampled reward: -3835.39). Current reward after update: -572.82, Optimal reward -380.59
Iteration 375 took 3.50 seconds (mean sampled reward: -4893.47). Current reward after update: -593.32, Optimal reward -380.59
Iteration 376 took 3.57 seconds (mean sampled reward: -5339.43). Current reward after update: -636.34, Optimal reward -380.59
Iteration 377 took 3.49 seconds (mean sampled reward: -4028.28). Current reward after update: -562.28, Optimal reward -380.59
Iteration 378 took 3.46 seconds (mean sampled reward: -4402.39). Current reward after update: -502.00, Optimal reward -380.59
Iteration 379 took 3.45 seconds (mean sampled reward: -4242.82). Current reward after update: -611.61, Optimal reward -380.59
Iteration 380 took 3.50 seconds (mean sampled reward: -3874.40). Current reward after update: -461.18, Optimal reward -380.59
Iteration 381 took 3.44 seconds (mean sampled reward: -4263.94). Current reward after update: -537.87, Optimal reward -380.59
Iteration 382 took 3.43 seconds (mean sampled reward: -4252.31). Current reward after update: -555.79, Optimal reward -380.59
Iteration 383 took 3.50 seconds (mean sampled reward: -4755.24). Current reward after update: -531.78, Optimal reward -380.59
Iteration 384 took 3.55 seconds (mean sampled reward: -4939.66). Current reward after update: -645.71, Optimal reward -380.59
Iteration 385 took 3.45 seconds (mean sampled reward: -5371.78). Current reward after update: -613.07, Optimal reward -380.59
Iteration 386 took 3.53 seconds (mean sampled reward: -4871.87). Current reward after update: -782.23, Optimal reward -380.59
Iteration 387 took 3.43 seconds (mean sampled reward: -4518.45). Current reward after update: -565.77, Optimal reward -380.59
Iteration 388 took 3.42 seconds (mean sampled reward: -4448.99). Current reward after update: -535.09, Optimal reward -380.59
Iteration 389 took 3.40 seconds (mean sampled reward: -4760.37). Current reward after update: -613.27, Optimal reward -380.59
Iteration 390 took 3.48 seconds (mean sampled reward: -4712.06). Current reward after update: -609.65, Optimal reward -380.59
Iteration 391 took 3.51 seconds (mean sampled reward: -5736.88). Current reward after update: -594.40, Optimal reward -380.59
Iteration 392 took 3.50 seconds (mean sampled reward: -4205.04). Current reward after update: -546.48, Optimal reward -380.59
Iteration 393 took 3.49 seconds (mean sampled reward: -4916.82). Current reward after update: -557.50, Optimal reward -380.59
Iteration 394 took 3.45 seconds (mean sampled reward: -3953.66). Current reward after update: -570.58, Optimal reward -380.59
Iteration 395 took 3.36 seconds (mean sampled reward: -3324.91). Current reward after update: -552.66, Optimal reward -380.59
Iteration 396 took 3.32 seconds (mean sampled reward: -2730.31). Current reward after update: -540.34, Optimal reward -380.59
Iteration 397 took 3.41 seconds (mean sampled reward: -3193.40). Current reward after update: -528.77, Optimal reward -380.59
Iteration 398 took 3.31 seconds (mean sampled reward: -2707.29). Current reward after update: -579.95, Optimal reward -380.59
Iteration 399 took 3.33 seconds (mean sampled reward: -2975.39). Current reward after update: -635.66, Optimal reward -380.59
Iteration 400 took 3.40 seconds (mean sampled reward: -3102.00). Current reward after update: -507.35, Optimal reward -380.59
Iteration 1 took 3.96 seconds (mean sampled reward: -7523.90). Current reward after update: -4320.60, Optimal reward -4320.60
Iteration 2 took 3.80 seconds (mean sampled reward: -6763.49). Current reward after update: -2779.85, Optimal reward -2779.85
Iteration 3 took 3.64 seconds (mean sampled reward: -6738.22). Current reward after update: -2706.25, Optimal reward -2706.25
Iteration 4 took 3.80 seconds (mean sampled reward: -6396.12). Current reward after update: -2383.30, Optimal reward -2383.30
Iteration 5 took 3.64 seconds (mean sampled reward: -6719.50). Current reward after update: -2496.45, Optimal reward -2383.30
Iteration 6 took 3.61 seconds (mean sampled reward: -5799.95). Current reward after update: -2268.32, Optimal reward -2268.32
Iteration 7 took 3.71 seconds (mean sampled reward: -6312.32). Current reward after update: -1693.49, Optimal reward -1693.49
Iteration 8 took 3.59 seconds (mean sampled reward: -6189.27). Current reward after update: -1646.05, Optimal reward -1646.05
Iteration 9 took 3.73 seconds (mean sampled reward: -6180.06). Current reward after update: -1244.98, Optimal reward -1244.98
Iteration 10 took 3.56 seconds (mean sampled reward: -6815.28). Current reward after update: -1942.06, Optimal reward -1244.98
Iteration 11 took 3.55 seconds (mean sampled reward: -6447.50). Current reward after update: -1680.03, Optimal reward -1244.98
Iteration 12 took 3.60 seconds (mean sampled reward: -5833.41). Current reward after update: -1953.95, Optimal reward -1244.98
Iteration 13 took 3.53 seconds (mean sampled reward: -5005.87). Current reward after update: -1489.97, Optimal reward -1244.98
Iteration 14 took 3.67 seconds (mean sampled reward: -5740.28). Current reward after update: -1309.38, Optimal reward -1244.98
Iteration 15 took 3.78 seconds (mean sampled reward: -6334.88). Current reward after update: -1363.97, Optimal reward -1244.98
Iteration 16 took 3.90 seconds (mean sampled reward: -6422.56). Current reward after update: -1229.98, Optimal reward -1229.98
Iteration 17 took 3.76 seconds (mean sampled reward: -5959.30). Current reward after update: -1742.69, Optimal reward -1229.98
Iteration 18 took 3.76 seconds (mean sampled reward: -5819.67). Current reward after update: -1678.99, Optimal reward -1229.98
Iteration 19 took 3.87 seconds (mean sampled reward: -6727.39). Current reward after update: -2542.70, Optimal reward -1229.98
Iteration 20 took 3.94 seconds (mean sampled reward: -6455.62). Current reward after update: -1282.42, Optimal reward -1229.98
Iteration 21 took 3.78 seconds (mean sampled reward: -5984.94). Current reward after update: -1509.20, Optimal reward -1229.98
Iteration 22 took 3.73 seconds (mean sampled reward: -5875.76). Current reward after update: -1497.56, Optimal reward -1229.98
Iteration 23 took 3.71 seconds (mean sampled reward: -5609.65). Current reward after update: -1276.02, Optimal reward -1229.98
Iteration 24 took 3.72 seconds (mean sampled reward: -6009.94). Current reward after update: -1470.88, Optimal reward -1229.98
Iteration 25 took 3.94 seconds (mean sampled reward: -6287.39). Current reward after update: -1390.84, Optimal reward -1229.98
Iteration 26 took 3.94 seconds (mean sampled reward: -6333.10). Current reward after update: -1219.25, Optimal reward -1219.25
Iteration 27 took 4.02 seconds (mean sampled reward: -5958.37). Current reward after update: -1452.50, Optimal reward -1219.25
Iteration 28 took 4.14 seconds (mean sampled reward: -6093.02). Current reward after update: -1456.83, Optimal reward -1219.25
Iteration 29 took 3.96 seconds (mean sampled reward: -6224.56). Current reward after update: -1524.66, Optimal reward -1219.25
Iteration 30 took 3.95 seconds (mean sampled reward: -5796.38). Current reward after update: -1266.61, Optimal reward -1219.25
Iteration 31 took 3.92 seconds (mean sampled reward: -6003.67). Current reward after update: -1353.14, Optimal reward -1219.25
Iteration 32 took 3.95 seconds (mean sampled reward: -6169.82). Current reward after update: -2957.68, Optimal reward -1219.25
Iteration 33 took 3.73 seconds (mean sampled reward: -5942.21). Current reward after update: -1524.71, Optimal reward -1219.25
Iteration 34 took 3.90 seconds (mean sampled reward: -6305.41). Current reward after update: -1431.29, Optimal reward -1219.25
Iteration 35 took 3.74 seconds (mean sampled reward: -6369.44). Current reward after update: -1324.47, Optimal reward -1219.25
Iteration 36 took 3.61 seconds (mean sampled reward: -5231.68). Current reward after update: -1355.81, Optimal reward -1219.25
Iteration 37 took 3.98 seconds (mean sampled reward: -5812.27). Current reward after update: -1433.35, Optimal reward -1219.25
Iteration 38 took 3.61 seconds (mean sampled reward: -5654.50). Current reward after update: -1418.55, Optimal reward -1219.25
Iteration 39 took 3.82 seconds (mean sampled reward: -5366.11). Current reward after update: -1285.09, Optimal reward -1219.25
Iteration 40 took 3.87 seconds (mean sampled reward: -5822.74). Current reward after update: -1399.92, Optimal reward -1219.25
Iteration 41 took 3.95 seconds (mean sampled reward: -5517.09). Current reward after update: -1265.38, Optimal reward -1219.25
Iteration 42 took 3.72 seconds (mean sampled reward: -5709.78). Current reward after update: -1333.92, Optimal reward -1219.25
Iteration 43 took 3.89 seconds (mean sampled reward: -5225.50). Current reward after update: -1270.44, Optimal reward -1219.25
Iteration 44 took 3.78 seconds (mean sampled reward: -5395.86). Current reward after update: -1233.96, Optimal reward -1219.25
Iteration 45 took 3.86 seconds (mean sampled reward: -5890.35). Current reward after update: -1269.99, Optimal reward -1219.25
Iteration 46 took 3.73 seconds (mean sampled reward: -5475.22). Current reward after update: -1204.53, Optimal reward -1204.53
Iteration 47 took 3.80 seconds (mean sampled reward: -5680.24). Current reward after update: -1356.74, Optimal reward -1204.53
Iteration 48 took 3.83 seconds (mean sampled reward: -5583.93). Current reward after update: -1179.75, Optimal reward -1179.75
Iteration 49 took 3.86 seconds (mean sampled reward: -5434.67). Current reward after update: -6427.30, Optimal reward -1179.75
Iteration 50 took 3.68 seconds (mean sampled reward: -5336.26). Current reward after update: -1245.26, Optimal reward -1179.75
Iteration 51 took 3.89 seconds (mean sampled reward: -5807.05). Current reward after update: -1308.62, Optimal reward -1179.75
Iteration 52 took 3.85 seconds (mean sampled reward: -5789.38). Current reward after update: -1175.37, Optimal reward -1175.37
Iteration 53 took 3.85 seconds (mean sampled reward: -5655.57). Current reward after update: -1063.83, Optimal reward -1063.83
Iteration 54 took 3.88 seconds (mean sampled reward: -5560.82). Current reward after update: -1156.38, Optimal reward -1063.83
Iteration 55 took 3.72 seconds (mean sampled reward: -5691.82). Current reward after update: -1225.08, Optimal reward -1063.83
Iteration 56 took 3.81 seconds (mean sampled reward: -5852.87). Current reward after update: -1302.65, Optimal reward -1063.83
Iteration 57 took 4.05 seconds (mean sampled reward: -5716.20). Current reward after update: -1344.98, Optimal reward -1063.83
Iteration 58 took 3.73 seconds (mean sampled reward: -4890.46). Current reward after update: -1146.79, Optimal reward -1063.83
Iteration 59 took 3.74 seconds (mean sampled reward: -5099.59). Current reward after update: -5362.88, Optimal reward -1063.83
Iteration 60 took 3.52 seconds (mean sampled reward: -5032.48). Current reward after update: -1247.41, Optimal reward -1063.83
Iteration 61 took 3.57 seconds (mean sampled reward: -5104.34). Current reward after update: -1292.24, Optimal reward -1063.83
Iteration 62 took 3.60 seconds (mean sampled reward: -5481.92). Current reward after update: -1245.50, Optimal reward -1063.83
Iteration 63 took 3.67 seconds (mean sampled reward: -5604.48). Current reward after update: -1197.22, Optimal reward -1063.83
Iteration 64 took 3.62 seconds (mean sampled reward: -5640.48). Current reward after update: -6831.49, Optimal reward -1063.83
Iteration 65 took 3.50 seconds (mean sampled reward: -5336.72). Current reward after update: -1183.76, Optimal reward -1063.83
Iteration 66 took 3.50 seconds (mean sampled reward: -5415.66). Current reward after update: -1251.84, Optimal reward -1063.83
Iteration 67 took 3.81 seconds (mean sampled reward: -5549.31). Current reward after update: -1001.92, Optimal reward -1001.92
Iteration 68 took 3.79 seconds (mean sampled reward: -6110.44). Current reward after update: -1075.23, Optimal reward -1001.92
Iteration 69 took 3.67 seconds (mean sampled reward: -5441.49). Current reward after update: -992.78, Optimal reward -992.78
Iteration 70 took 3.67 seconds (mean sampled reward: -5345.15). Current reward after update: -1142.92, Optimal reward -992.78
Iteration 71 took 3.67 seconds (mean sampled reward: -4547.93). Current reward after update: -7470.11, Optimal reward -992.78
Iteration 72 took 3.78 seconds (mean sampled reward: -5704.83). Current reward after update: -911.94, Optimal reward -911.94
Iteration 73 took 3.68 seconds (mean sampled reward: -5615.68). Current reward after update: -1002.11, Optimal reward -911.94
Iteration 74 took 3.63 seconds (mean sampled reward: -4202.98). Current reward after update: -1773.83, Optimal reward -911.94
Iteration 75 took 3.67 seconds (mean sampled reward: -3875.87). Current reward after update: -731.10, Optimal reward -731.10
Iteration 76 took 3.60 seconds (mean sampled reward: -3791.60). Current reward after update: -794.15, Optimal reward -731.10
Iteration 77 took 3.61 seconds (mean sampled reward: -4086.92). Current reward after update: -742.85, Optimal reward -731.10
Iteration 78 took 3.59 seconds (mean sampled reward: -3557.68). Current reward after update: -776.67, Optimal reward -731.10
Iteration 79 took 3.55 seconds (mean sampled reward: -3478.60). Current reward after update: -650.50, Optimal reward -650.50
Iteration 80 took 3.54 seconds (mean sampled reward: -3492.52). Current reward after update: -760.27, Optimal reward -650.50
Iteration 81 took 3.34 seconds (mean sampled reward: -3345.24). Current reward after update: -802.72, Optimal reward -650.50
Iteration 82 took 3.49 seconds (mean sampled reward: -2831.62). Current reward after update: -730.36, Optimal reward -650.50
Iteration 83 took 3.47 seconds (mean sampled reward: -3066.07). Current reward after update: -774.03, Optimal reward -650.50
Iteration 84 took 3.49 seconds (mean sampled reward: -3051.30). Current reward after update: -829.91, Optimal reward -650.50
Iteration 85 took 3.50 seconds (mean sampled reward: -3135.80). Current reward after update: -766.58, Optimal reward -650.50
Iteration 86 took 3.47 seconds (mean sampled reward: -3922.62). Current reward after update: -800.08, Optimal reward -650.50
Iteration 87 took 3.48 seconds (mean sampled reward: -3510.38). Current reward after update: -1029.78, Optimal reward -650.50
Iteration 88 took 3.49 seconds (mean sampled reward: -3421.79). Current reward after update: -816.16, Optimal reward -650.50
Iteration 89 took 3.49 seconds (mean sampled reward: -3228.54). Current reward after update: -762.41, Optimal reward -650.50
Iteration 90 took 3.62 seconds (mean sampled reward: -4455.58). Current reward after update: -663.30, Optimal reward -650.50
Iteration 91 took 3.55 seconds (mean sampled reward: -3288.07). Current reward after update: -721.56, Optimal reward -650.50
Iteration 92 took 3.57 seconds (mean sampled reward: -3500.95). Current reward after update: -882.89, Optimal reward -650.50
Iteration 93 took 3.73 seconds (mean sampled reward: -5073.74). Current reward after update: -988.04, Optimal reward -650.50
Iteration 94 took 3.70 seconds (mean sampled reward: -4758.70). Current reward after update: -765.66, Optimal reward -650.50
Iteration 95 took 3.58 seconds (mean sampled reward: -4602.88). Current reward after update: -789.48, Optimal reward -650.50
Iteration 96 took 3.56 seconds (mean sampled reward: -3940.57). Current reward after update: -589.86, Optimal reward -589.86
Iteration 97 took 3.70 seconds (mean sampled reward: -5077.95). Current reward after update: -560.04, Optimal reward -560.04
Iteration 98 took 3.73 seconds (mean sampled reward: -5424.00). Current reward after update: -732.40, Optimal reward -560.04
Iteration 99 took 3.74 seconds (mean sampled reward: -5588.80). Current reward after update: -802.77, Optimal reward -560.04
Iteration 100 took 3.75 seconds (mean sampled reward: -5486.21). Current reward after update: -860.74, Optimal reward -560.04
Iteration 101 took 3.65 seconds (mean sampled reward: -5525.65). Current reward after update: -829.75, Optimal reward -560.04
Iteration 102 took 3.65 seconds (mean sampled reward: -4096.22). Current reward after update: -892.05, Optimal reward -560.04
Iteration 103 took 3.75 seconds (mean sampled reward: -3684.26). Current reward after update: -859.10, Optimal reward -560.04
Iteration 104 took 3.51 seconds (mean sampled reward: -3225.90). Current reward after update: -610.54, Optimal reward -560.04
Iteration 105 took 3.68 seconds (mean sampled reward: -5272.46). Current reward after update: -840.13, Optimal reward -560.04
Iteration 106 took 3.55 seconds (mean sampled reward: -3755.87). Current reward after update: -957.47, Optimal reward -560.04
Iteration 107 took 3.73 seconds (mean sampled reward: -4474.97). Current reward after update: -916.29, Optimal reward -560.04
Iteration 108 took 3.85 seconds (mean sampled reward: -4217.55). Current reward after update: -919.68, Optimal reward -560.04
Iteration 109 took 3.49 seconds (mean sampled reward: -5502.77). Current reward after update: -988.07, Optimal reward -560.04
Iteration 110 took 3.47 seconds (mean sampled reward: -5538.40). Current reward after update: -943.28, Optimal reward -560.04
Iteration 111 took 3.54 seconds (mean sampled reward: -5508.43). Current reward after update: -834.83, Optimal reward -560.04
Iteration 112 took 3.56 seconds (mean sampled reward: -5698.38). Current reward after update: -1049.90, Optimal reward -560.04
Iteration 113 took 3.56 seconds (mean sampled reward: -5096.22). Current reward after update: -916.84, Optimal reward -560.04
Iteration 114 took 3.46 seconds (mean sampled reward: -4487.85). Current reward after update: -803.37, Optimal reward -560.04
Iteration 115 took 3.52 seconds (mean sampled reward: -5301.48). Current reward after update: -727.72, Optimal reward -560.04
Iteration 116 took 3.54 seconds (mean sampled reward: -5141.63). Current reward after update: -1670.34, Optimal reward -560.04
Iteration 117 took 3.46 seconds (mean sampled reward: -4358.77). Current reward after update: -981.35, Optimal reward -560.04
Iteration 118 took 3.37 seconds (mean sampled reward: -4185.08). Current reward after update: -1055.69, Optimal reward -560.04
Iteration 119 took 3.46 seconds (mean sampled reward: -4546.47). Current reward after update: -894.81, Optimal reward -560.04
Iteration 120 took 3.56 seconds (mean sampled reward: -5568.20). Current reward after update: -1257.27, Optimal reward -560.04
Iteration 121 took 3.52 seconds (mean sampled reward: -5939.52). Current reward after update: -873.74, Optimal reward -560.04
Iteration 122 took 3.44 seconds (mean sampled reward: -5334.64). Current reward after update: -996.16, Optimal reward -560.04
Iteration 123 took 3.59 seconds (mean sampled reward: -5785.87). Current reward after update: -1321.21, Optimal reward -560.04
Iteration 124 took 3.52 seconds (mean sampled reward: -5069.35). Current reward after update: -813.04, Optimal reward -560.04
Iteration 125 took 3.58 seconds (mean sampled reward: -5573.30). Current reward after update: -795.74, Optimal reward -560.04
Iteration 126 took 3.58 seconds (mean sampled reward: -5292.37). Current reward after update: -1143.44, Optimal reward -560.04
Iteration 127 took 3.59 seconds (mean sampled reward: -5095.02). Current reward after update: -931.97, Optimal reward -560.04
Iteration 128 took 3.62 seconds (mean sampled reward: -5348.33). Current reward after update: -758.55, Optimal reward -560.04
Iteration 129 took 3.73 seconds (mean sampled reward: -5643.69). Current reward after update: -688.04, Optimal reward -560.04
Iteration 130 took 3.76 seconds (mean sampled reward: -5668.97). Current reward after update: -739.24, Optimal reward -560.04
Iteration 131 took 3.85 seconds (mean sampled reward: -5315.64). Current reward after update: -1173.30, Optimal reward -560.04
Iteration 132 took 3.58 seconds (mean sampled reward: -4509.87). Current reward after update: -824.77, Optimal reward -560.04
Iteration 133 took 3.78 seconds (mean sampled reward: -4812.33). Current reward after update: -770.66, Optimal reward -560.04
Iteration 134 took 3.79 seconds (mean sampled reward: -4586.40). Current reward after update: -713.51, Optimal reward -560.04
Iteration 135 took 3.81 seconds (mean sampled reward: -5125.94). Current reward after update: -697.91, Optimal reward -560.04
Iteration 136 took 3.79 seconds (mean sampled reward: -5206.58). Current reward after update: -635.16, Optimal reward -560.04
Iteration 137 took 3.76 seconds (mean sampled reward: -4557.32). Current reward after update: -716.68, Optimal reward -560.04
Iteration 138 took 3.81 seconds (mean sampled reward: -4225.42). Current reward after update: -711.55, Optimal reward -560.04
Iteration 139 took 3.79 seconds (mean sampled reward: -5024.19). Current reward after update: -621.78, Optimal reward -560.04
Iteration 140 took 3.79 seconds (mean sampled reward: -5590.04). Current reward after update: -567.85, Optimal reward -560.04
Iteration 141 took 3.98 seconds (mean sampled reward: -5810.62). Current reward after update: -717.31, Optimal reward -560.04
Iteration 142 took 3.83 seconds (mean sampled reward: -5225.35). Current reward after update: -617.75, Optimal reward -560.04
Iteration 143 took 3.74 seconds (mean sampled reward: -4972.42). Current reward after update: -783.10, Optimal reward -560.04
Iteration 144 took 3.71 seconds (mean sampled reward: -5266.14). Current reward after update: -674.65, Optimal reward -560.04
Iteration 145 took 3.63 seconds (mean sampled reward: -4662.23). Current reward after update: -704.62, Optimal reward -560.04
Iteration 146 took 3.68 seconds (mean sampled reward: -5059.43). Current reward after update: -763.20, Optimal reward -560.04
Iteration 147 took 3.76 seconds (mean sampled reward: -5262.46). Current reward after update: -716.94, Optimal reward -560.04
Iteration 148 took 3.61 seconds (mean sampled reward: -4593.36). Current reward after update: -792.99, Optimal reward -560.04
Iteration 149 took 3.50 seconds (mean sampled reward: -4361.75). Current reward after update: -822.88, Optimal reward -560.04
Iteration 150 took 3.62 seconds (mean sampled reward: -5264.28). Current reward after update: -714.40, Optimal reward -560.04
Iteration 151 took 3.71 seconds (mean sampled reward: -4512.13). Current reward after update: -689.56, Optimal reward -560.04
Iteration 152 took 3.76 seconds (mean sampled reward: -5634.26). Current reward after update: -770.61, Optimal reward -560.04
Iteration 153 took 3.55 seconds (mean sampled reward: -4177.63). Current reward after update: -596.79, Optimal reward -560.04
Iteration 154 took 3.78 seconds (mean sampled reward: -5658.71). Current reward after update: -626.33, Optimal reward -560.04
Iteration 155 took 3.73 seconds (mean sampled reward: -5159.62). Current reward after update: -672.68, Optimal reward -560.04
Iteration 156 took 3.69 seconds (mean sampled reward: -4853.29). Current reward after update: -693.24, Optimal reward -560.04
Iteration 157 took 3.67 seconds (mean sampled reward: -5300.48). Current reward after update: -589.51, Optimal reward -560.04
Iteration 158 took 3.75 seconds (mean sampled reward: -5396.13). Current reward after update: -952.19, Optimal reward -560.04
Iteration 159 took 3.68 seconds (mean sampled reward: -4915.90). Current reward after update: -870.92, Optimal reward -560.04
Iteration 160 took 3.61 seconds (mean sampled reward: -4660.24). Current reward after update: -937.50, Optimal reward -560.04
Iteration 161 took 3.67 seconds (mean sampled reward: -4447.17). Current reward after update: -807.90, Optimal reward -560.04
Iteration 162 took 3.50 seconds (mean sampled reward: -3911.93). Current reward after update: -646.38, Optimal reward -560.04
Iteration 163 took 3.60 seconds (mean sampled reward: -4026.86). Current reward after update: -668.38, Optimal reward -560.04
Iteration 164 took 3.60 seconds (mean sampled reward: -4387.06). Current reward after update: -713.67, Optimal reward -560.04
Iteration 165 took 3.72 seconds (mean sampled reward: -5481.84). Current reward after update: -816.99, Optimal reward -560.04
Iteration 166 took 3.40 seconds (mean sampled reward: -4649.70). Current reward after update: -935.55, Optimal reward -560.04
Iteration 167 took 3.57 seconds (mean sampled reward: -5526.34). Current reward after update: -672.91, Optimal reward -560.04
Iteration 168 took 3.64 seconds (mean sampled reward: -4818.74). Current reward after update: -821.44, Optimal reward -560.04
Iteration 169 took 3.78 seconds (mean sampled reward: -5570.77). Current reward after update: -829.56, Optimal reward -560.04
Iteration 170 took 3.61 seconds (mean sampled reward: -4753.80). Current reward after update: -839.16, Optimal reward -560.04
Iteration 171 took 3.46 seconds (mean sampled reward: -3939.57). Current reward after update: -878.88, Optimal reward -560.04
Iteration 172 took 3.48 seconds (mean sampled reward: -4724.27). Current reward after update: -804.79, Optimal reward -560.04
Iteration 173 took 3.36 seconds (mean sampled reward: -3900.73). Current reward after update: -837.95, Optimal reward -560.04
Iteration 174 took 3.48 seconds (mean sampled reward: -4062.41). Current reward after update: -895.42, Optimal reward -560.04
Iteration 175 took 3.54 seconds (mean sampled reward: -4414.30). Current reward after update: -953.72, Optimal reward -560.04
Iteration 176 took 3.43 seconds (mean sampled reward: -4033.04). Current reward after update: -783.81, Optimal reward -560.04
Iteration 177 took 3.44 seconds (mean sampled reward: -4050.30). Current reward after update: -906.05, Optimal reward -560.04
Iteration 178 took 3.47 seconds (mean sampled reward: -3128.41). Current reward after update: -611.43, Optimal reward -560.04
Iteration 179 took 3.37 seconds (mean sampled reward: -3500.34). Current reward after update: -878.64, Optimal reward -560.04
Iteration 180 took 3.40 seconds (mean sampled reward: -3418.17). Current reward after update: -1034.11, Optimal reward -560.04
Iteration 181 took 3.39 seconds (mean sampled reward: -3214.53). Current reward after update: -802.91, Optimal reward -560.04
Iteration 182 took 3.47 seconds (mean sampled reward: -2914.99). Current reward after update: -860.83, Optimal reward -560.04
Iteration 183 took 3.54 seconds (mean sampled reward: -3411.86). Current reward after update: -842.34, Optimal reward -560.04
Iteration 184 took 3.40 seconds (mean sampled reward: -3582.24). Current reward after update: -942.55, Optimal reward -560.04
Iteration 185 took 3.32 seconds (mean sampled reward: -4089.22). Current reward after update: -754.09, Optimal reward -560.04
Iteration 186 took 3.34 seconds (mean sampled reward: -3640.19). Current reward after update: -823.04, Optimal reward -560.04
Iteration 187 took 3.43 seconds (mean sampled reward: -4391.47). Current reward after update: -951.85, Optimal reward -560.04
Iteration 188 took 3.46 seconds (mean sampled reward: -4342.46). Current reward after update: -839.49, Optimal reward -560.04
Iteration 189 took 3.53 seconds (mean sampled reward: -4575.34). Current reward after update: -813.66, Optimal reward -560.04
Iteration 190 took 3.54 seconds (mean sampled reward: -4456.72). Current reward after update: -965.35, Optimal reward -560.04
Iteration 191 took 3.42 seconds (mean sampled reward: -4759.56). Current reward after update: -1125.71, Optimal reward -560.04
Iteration 192 took 3.54 seconds (mean sampled reward: -4625.97). Current reward after update: -945.69, Optimal reward -560.04
Iteration 193 took 3.39 seconds (mean sampled reward: -3302.99). Current reward after update: -898.29, Optimal reward -560.04
Iteration 194 took 3.39 seconds (mean sampled reward: -2851.16). Current reward after update: -753.97, Optimal reward -560.04
Iteration 195 took 3.31 seconds (mean sampled reward: -2909.76). Current reward after update: -1781.66, Optimal reward -560.04
Iteration 196 took 3.36 seconds (mean sampled reward: -2575.05). Current reward after update: -818.35, Optimal reward -560.04
Iteration 197 took 3.36 seconds (mean sampled reward: -2595.63). Current reward after update: -904.87, Optimal reward -560.04
Iteration 198 took 3.43 seconds (mean sampled reward: -2384.97). Current reward after update: -842.41, Optimal reward -560.04
Iteration 199 took 3.40 seconds (mean sampled reward: -2623.33). Current reward after update: -843.24, Optimal reward -560.04
Iteration 200 took 3.44 seconds (mean sampled reward: -2714.50). Current reward after update: -771.76, Optimal reward -560.04
Iteration 201 took 3.42 seconds (mean sampled reward: -2967.51). Current reward after update: -743.50, Optimal reward -560.04
Iteration 202 took 3.43 seconds (mean sampled reward: -3096.54). Current reward after update: -780.78, Optimal reward -560.04
Iteration 203 took 3.30 seconds (mean sampled reward: -3146.29). Current reward after update: -886.97, Optimal reward -560.04
Iteration 204 took 3.40 seconds (mean sampled reward: -2650.88). Current reward after update: -618.09, Optimal reward -560.04
Iteration 205 took 3.39 seconds (mean sampled reward: -2686.88). Current reward after update: -719.68, Optimal reward -560.04
Iteration 206 took 3.41 seconds (mean sampled reward: -2769.12). Current reward after update: -706.70, Optimal reward -560.04
Iteration 207 took 3.34 seconds (mean sampled reward: -3016.62). Current reward after update: -641.42, Optimal reward -560.04
Iteration 208 took 3.27 seconds (mean sampled reward: -3092.98). Current reward after update: -609.04, Optimal reward -560.04
Iteration 209 took 3.34 seconds (mean sampled reward: -3222.95). Current reward after update: -729.24, Optimal reward -560.04
Iteration 210 took 3.29 seconds (mean sampled reward: -3243.83). Current reward after update: -752.07, Optimal reward -560.04
Iteration 211 took 3.36 seconds (mean sampled reward: -3393.35). Current reward after update: -792.26, Optimal reward -560.04
Iteration 212 took 3.66 seconds (mean sampled reward: -3235.98). Current reward after update: -673.26, Optimal reward -560.04
Iteration 213 took 3.30 seconds (mean sampled reward: -3359.67). Current reward after update: -764.90, Optimal reward -560.04
Iteration 214 took 3.30 seconds (mean sampled reward: -3384.79). Current reward after update: -835.38, Optimal reward -560.04
Iteration 215 took 3.56 seconds (mean sampled reward: -3449.75). Current reward after update: -899.47, Optimal reward -560.04
Iteration 216 took 3.28 seconds (mean sampled reward: -3887.62). Current reward after update: -772.69, Optimal reward -560.04
Iteration 217 took 3.32 seconds (mean sampled reward: -3592.63). Current reward after update: -815.06, Optimal reward -560.04
Iteration 218 took 3.67 seconds (mean sampled reward: -3696.81). Current reward after update: -726.37, Optimal reward -560.04
Iteration 219 took 3.35 seconds (mean sampled reward: -3443.80). Current reward after update: -776.14, Optimal reward -560.04
Iteration 220 took 3.57 seconds (mean sampled reward: -2964.61). Current reward after update: -637.93, Optimal reward -560.04
Iteration 221 took 3.38 seconds (mean sampled reward: -3139.06). Current reward after update: -671.54, Optimal reward -560.04
Iteration 222 took 3.33 seconds (mean sampled reward: -3076.22). Current reward after update: -815.36, Optimal reward -560.04
Iteration 223 took 3.44 seconds (mean sampled reward: -3112.82). Current reward after update: -682.22, Optimal reward -560.04
Iteration 224 took 3.41 seconds (mean sampled reward: -3681.74). Current reward after update: -724.00, Optimal reward -560.04
Iteration 225 took 3.42 seconds (mean sampled reward: -3673.06). Current reward after update: -744.92, Optimal reward -560.04
Iteration 226 took 3.56 seconds (mean sampled reward: -3551.88). Current reward after update: -742.42, Optimal reward -560.04
Iteration 227 took 3.36 seconds (mean sampled reward: -2890.11). Current reward after update: -590.01, Optimal reward -560.04
Iteration 228 took 3.45 seconds (mean sampled reward: -2896.36). Current reward after update: -553.60, Optimal reward -553.60
Iteration 229 took 3.35 seconds (mean sampled reward: -2908.58). Current reward after update: -624.93, Optimal reward -553.60
Iteration 230 took 3.43 seconds (mean sampled reward: -2811.30). Current reward after update: -726.15, Optimal reward -553.60
Iteration 231 took 3.42 seconds (mean sampled reward: -3217.00). Current reward after update: -667.14, Optimal reward -553.60
Iteration 232 took 3.52 seconds (mean sampled reward: -3534.24). Current reward after update: -701.33, Optimal reward -553.60
Iteration 233 took 3.48 seconds (mean sampled reward: -3704.22). Current reward after update: -1002.95, Optimal reward -553.60
Iteration 234 took 3.60 seconds (mean sampled reward: -3773.85). Current reward after update: -1035.79, Optimal reward -553.60
Iteration 235 took 3.54 seconds (mean sampled reward: -3716.01). Current reward after update: -933.78, Optimal reward -553.60
Iteration 236 took 3.59 seconds (mean sampled reward: -3742.82). Current reward after update: -986.64, Optimal reward -553.60
Iteration 237 took 3.58 seconds (mean sampled reward: -4580.84). Current reward after update: -726.80, Optimal reward -553.60
Iteration 238 took 3.47 seconds (mean sampled reward: -3317.56). Current reward after update: -685.23, Optimal reward -553.60
Iteration 239 took 3.41 seconds (mean sampled reward: -3907.39). Current reward after update: -914.30, Optimal reward -553.60
Iteration 240 took 3.39 seconds (mean sampled reward: -3550.78). Current reward after update: -794.72, Optimal reward -553.60
Iteration 241 took 3.58 seconds (mean sampled reward: -4445.74). Current reward after update: -781.99, Optimal reward -553.60
Iteration 242 took 3.47 seconds (mean sampled reward: -3678.33). Current reward after update: -732.00, Optimal reward -553.60
Iteration 243 took 3.37 seconds (mean sampled reward: -3759.02). Current reward after update: -657.95, Optimal reward -553.60
Iteration 244 took 3.34 seconds (mean sampled reward: -3673.27). Current reward after update: -710.51, Optimal reward -553.60
Iteration 245 took 3.38 seconds (mean sampled reward: -3434.04). Current reward after update: -716.19, Optimal reward -553.60
Iteration 246 took 3.43 seconds (mean sampled reward: -3445.68). Current reward after update: -813.42, Optimal reward -553.60
Iteration 247 took 3.36 seconds (mean sampled reward: -3391.22). Current reward after update: -703.53, Optimal reward -553.60
Iteration 248 took 3.48 seconds (mean sampled reward: -3477.25). Current reward after update: -777.74, Optimal reward -553.60
Iteration 249 took 3.37 seconds (mean sampled reward: -3151.85). Current reward after update: -695.57, Optimal reward -553.60
Iteration 250 took 3.40 seconds (mean sampled reward: -2998.45). Current reward after update: -696.94, Optimal reward -553.60
Iteration 251 took 3.35 seconds (mean sampled reward: -3375.52). Current reward after update: -705.43, Optimal reward -553.60
Iteration 252 took 3.52 seconds (mean sampled reward: -3149.38). Current reward after update: -653.45, Optimal reward -553.60
Iteration 253 took 3.51 seconds (mean sampled reward: -3212.34). Current reward after update: -635.52, Optimal reward -553.60
Iteration 254 took 3.41 seconds (mean sampled reward: -3140.01). Current reward after update: -712.77, Optimal reward -553.60
Iteration 255 took 3.44 seconds (mean sampled reward: -2999.04). Current reward after update: -684.70, Optimal reward -553.60
Iteration 256 took 3.48 seconds (mean sampled reward: -2877.59). Current reward after update: -607.67, Optimal reward -553.60
Iteration 257 took 3.62 seconds (mean sampled reward: -3061.93). Current reward after update: -748.58, Optimal reward -553.60
Iteration 258 took 3.58 seconds (mean sampled reward: -2980.94). Current reward after update: -807.71, Optimal reward -553.60
Iteration 259 took 3.58 seconds (mean sampled reward: -2845.77). Current reward after update: -908.48, Optimal reward -553.60
Iteration 260 took 3.60 seconds (mean sampled reward: -2512.81). Current reward after update: -720.60, Optimal reward -553.60
Iteration 261 took 3.66 seconds (mean sampled reward: -2702.12). Current reward after update: -603.38, Optimal reward -553.60
Iteration 262 took 3.71 seconds (mean sampled reward: -3134.92). Current reward after update: -767.02, Optimal reward -553.60
Iteration 263 took 3.74 seconds (mean sampled reward: -2819.05). Current reward after update: -771.36, Optimal reward -553.60
Iteration 264 took 3.70 seconds (mean sampled reward: -3117.80). Current reward after update: -675.88, Optimal reward -553.60
Iteration 265 took 3.68 seconds (mean sampled reward: -3140.71). Current reward after update: -713.58, Optimal reward -553.60
Iteration 266 took 3.73 seconds (mean sampled reward: -3573.27). Current reward after update: -718.00, Optimal reward -553.60
Iteration 267 took 3.72 seconds (mean sampled reward: -3348.43). Current reward after update: -779.63, Optimal reward -553.60
Iteration 268 took 3.79 seconds (mean sampled reward: -4576.97). Current reward after update: -687.65, Optimal reward -553.60
Iteration 269 took 3.72 seconds (mean sampled reward: -3804.64). Current reward after update: -1289.29, Optimal reward -553.60
Iteration 270 took 3.69 seconds (mean sampled reward: -3740.59). Current reward after update: -595.80, Optimal reward -553.60
Iteration 271 took 3.57 seconds (mean sampled reward: -3419.99). Current reward after update: -662.85, Optimal reward -553.60
Iteration 272 took 3.71 seconds (mean sampled reward: -4003.10). Current reward after update: -710.77, Optimal reward -553.60
Iteration 273 took 3.74 seconds (mean sampled reward: -4989.37). Current reward after update: -581.32, Optimal reward -553.60
Iteration 274 took 3.77 seconds (mean sampled reward: -4303.94). Current reward after update: -726.35, Optimal reward -553.60
Iteration 275 took 3.63 seconds (mean sampled reward: -3913.32). Current reward after update: -744.64, Optimal reward -553.60
Iteration 276 took 3.82 seconds (mean sampled reward: -5757.76). Current reward after update: -772.19, Optimal reward -553.60
Iteration 277 took 3.76 seconds (mean sampled reward: -4477.33). Current reward after update: -628.41, Optimal reward -553.60
Iteration 278 took 3.72 seconds (mean sampled reward: -3758.66). Current reward after update: -715.95, Optimal reward -553.60
Iteration 279 took 3.77 seconds (mean sampled reward: -3848.84). Current reward after update: -604.64, Optimal reward -553.60
Iteration 280 took 3.74 seconds (mean sampled reward: -3496.40). Current reward after update: -721.16, Optimal reward -553.60
Iteration 281 took 3.79 seconds (mean sampled reward: -4017.56). Current reward after update: -787.38, Optimal reward -553.60
Iteration 282 took 3.74 seconds (mean sampled reward: -3409.01). Current reward after update: -748.67, Optimal reward -553.60
Iteration 283 took 3.60 seconds (mean sampled reward: -2802.91). Current reward after update: -789.43, Optimal reward -553.60
Iteration 284 took 3.65 seconds (mean sampled reward: -3244.04). Current reward after update: -863.87, Optimal reward -553.60
Iteration 285 took 3.77 seconds (mean sampled reward: -3442.10). Current reward after update: -781.27, Optimal reward -553.60
Iteration 286 took 3.67 seconds (mean sampled reward: -2876.39). Current reward after update: -723.31, Optimal reward -553.60
Iteration 287 took 3.66 seconds (mean sampled reward: -2644.98). Current reward after update: -558.52, Optimal reward -553.60
Iteration 288 took 3.74 seconds (mean sampled reward: -3960.07). Current reward after update: -740.93, Optimal reward -553.60
Iteration 289 took 3.77 seconds (mean sampled reward: -3160.31). Current reward after update: -785.61, Optimal reward -553.60
Iteration 290 took 3.67 seconds (mean sampled reward: -3052.00). Current reward after update: -754.48, Optimal reward -553.60
Iteration 291 took 3.66 seconds (mean sampled reward: -3430.01). Current reward after update: -818.10, Optimal reward -553.60
Iteration 292 took 3.61 seconds (mean sampled reward: -3337.86). Current reward after update: -854.46, Optimal reward -553.60
Iteration 293 took 3.56 seconds (mean sampled reward: -2796.50). Current reward after update: -1625.54, Optimal reward -553.60
Iteration 294 took 3.58 seconds (mean sampled reward: -2771.16). Current reward after update: -791.03, Optimal reward -553.60
Iteration 295 took 3.60 seconds (mean sampled reward: -2721.34). Current reward after update: -872.76, Optimal reward -553.60
Iteration 296 took 3.76 seconds (mean sampled reward: -4201.32). Current reward after update: -808.62, Optimal reward -553.60
Iteration 297 took 3.86 seconds (mean sampled reward: -5014.13). Current reward after update: -683.65, Optimal reward -553.60
Iteration 298 took 3.83 seconds (mean sampled reward: -4687.77). Current reward after update: -980.93, Optimal reward -553.60
Iteration 299 took 3.69 seconds (mean sampled reward: -3507.61). Current reward after update: -657.64, Optimal reward -553.60
Iteration 300 took 3.73 seconds (mean sampled reward: -2993.04). Current reward after update: -944.62, Optimal reward -553.60
Iteration 301 took 3.68 seconds (mean sampled reward: -3506.04). Current reward after update: -835.95, Optimal reward -553.60
Iteration 302 took 3.75 seconds (mean sampled reward: -3267.31). Current reward after update: -835.73, Optimal reward -553.60
Iteration 303 took 3.72 seconds (mean sampled reward: -2927.32). Current reward after update: -813.70, Optimal reward -553.60
Iteration 304 took 3.76 seconds (mean sampled reward: -3869.31). Current reward after update: -754.88, Optimal reward -553.60
Iteration 305 took 3.76 seconds (mean sampled reward: -3889.86). Current reward after update: -770.50, Optimal reward -553.60
Iteration 306 took 3.74 seconds (mean sampled reward: -3986.03). Current reward after update: -702.50, Optimal reward -553.60
Iteration 307 took 3.78 seconds (mean sampled reward: -4276.56). Current reward after update: -770.45, Optimal reward -553.60
Iteration 308 took 3.65 seconds (mean sampled reward: -4052.66). Current reward after update: -733.71, Optimal reward -553.60
Iteration 309 took 3.58 seconds (mean sampled reward: -3078.64). Current reward after update: -601.05, Optimal reward -553.60
Iteration 310 took 3.78 seconds (mean sampled reward: -4260.61). Current reward after update: -509.61, Optimal reward -509.61
Iteration 311 took 3.76 seconds (mean sampled reward: -4406.10). Current reward after update: -646.38, Optimal reward -509.61
Iteration 312 took 3.70 seconds (mean sampled reward: -4276.75). Current reward after update: -770.93, Optimal reward -509.61
Iteration 313 took 3.62 seconds (mean sampled reward: -2824.43). Current reward after update: -570.80, Optimal reward -509.61
Iteration 314 took 3.61 seconds (mean sampled reward: -2973.17). Current reward after update: -580.91, Optimal reward -509.61
Iteration 315 took 3.60 seconds (mean sampled reward: -3046.84). Current reward after update: -669.52, Optimal reward -509.61
Iteration 316 took 3.72 seconds (mean sampled reward: -4181.75). Current reward after update: -662.93, Optimal reward -509.61
Iteration 317 took 3.73 seconds (mean sampled reward: -4325.25). Current reward after update: -630.68, Optimal reward -509.61
Iteration 318 took 3.61 seconds (mean sampled reward: -3121.91). Current reward after update: -642.18, Optimal reward -509.61
Iteration 319 took 3.70 seconds (mean sampled reward: -4021.08). Current reward after update: -720.31, Optimal reward -509.61
Iteration 320 took 3.75 seconds (mean sampled reward: -4332.75). Current reward after update: -750.02, Optimal reward -509.61
Iteration 321 took 3.75 seconds (mean sampled reward: -4367.87). Current reward after update: -576.03, Optimal reward -509.61
Iteration 322 took 3.71 seconds (mean sampled reward: -4183.16). Current reward after update: -670.81, Optimal reward -509.61
Iteration 323 took 3.50 seconds (mean sampled reward: -2798.00). Current reward after update: -748.10, Optimal reward -509.61
Iteration 324 took 3.50 seconds (mean sampled reward: -2989.99). Current reward after update: -593.89, Optimal reward -509.61
Iteration 325 took 3.46 seconds (mean sampled reward: -2668.22). Current reward after update: -622.54, Optimal reward -509.61
Iteration 326 took 3.51 seconds (mean sampled reward: -2872.54). Current reward after update: -575.94, Optimal reward -509.61
Iteration 327 took 3.67 seconds (mean sampled reward: -3995.71). Current reward after update: -617.54, Optimal reward -509.61
Iteration 328 took 3.59 seconds (mean sampled reward: -4199.11). Current reward after update: -719.48, Optimal reward -509.61
Iteration 329 took 3.54 seconds (mean sampled reward: -3128.90). Current reward after update: -637.46, Optimal reward -509.61
Iteration 330 took 3.44 seconds (mean sampled reward: -2984.96). Current reward after update: -740.59, Optimal reward -509.61
Iteration 331 took 3.46 seconds (mean sampled reward: -2891.26). Current reward after update: -690.89, Optimal reward -509.61
Iteration 332 took 3.52 seconds (mean sampled reward: -3759.01). Current reward after update: -782.70, Optimal reward -509.61
Iteration 333 took 3.41 seconds (mean sampled reward: -2770.03). Current reward after update: -696.72, Optimal reward -509.61
Iteration 334 took 3.50 seconds (mean sampled reward: -3446.07). Current reward after update: -985.80, Optimal reward -509.61
Iteration 335 took 3.55 seconds (mean sampled reward: -3872.76). Current reward after update: -603.07, Optimal reward -509.61
Iteration 336 took 3.42 seconds (mean sampled reward: -2710.72). Current reward after update: -674.87, Optimal reward -509.61
Iteration 337 took 3.36 seconds (mean sampled reward: -2912.87). Current reward after update: -732.07, Optimal reward -509.61
Iteration 338 took 3.44 seconds (mean sampled reward: -2793.80). Current reward after update: -791.13, Optimal reward -509.61
Iteration 339 took 3.64 seconds (mean sampled reward: -4447.04). Current reward after update: -778.62, Optimal reward -509.61
Iteration 340 took 3.53 seconds (mean sampled reward: -4078.48). Current reward after update: -742.31, Optimal reward -509.61
Iteration 341 took 3.64 seconds (mean sampled reward: -4559.44). Current reward after update: -661.81, Optimal reward -509.61
Iteration 342 took 3.73 seconds (mean sampled reward: -5406.12). Current reward after update: -707.14, Optimal reward -509.61
Iteration 343 took 3.73 seconds (mean sampled reward: -5086.50). Current reward after update: -736.91, Optimal reward -509.61
Iteration 344 took 3.73 seconds (mean sampled reward: -4706.79). Current reward after update: -636.77, Optimal reward -509.61
Iteration 345 took 3.76 seconds (mean sampled reward: -4922.43). Current reward after update: -743.75, Optimal reward -509.61
Iteration 346 took 3.86 seconds (mean sampled reward: -5130.56). Current reward after update: -616.39, Optimal reward -509.61
Iteration 347 took 3.81 seconds (mean sampled reward: -4934.08). Current reward after update: -807.43, Optimal reward -509.61
Iteration 348 took 3.75 seconds (mean sampled reward: -4673.60). Current reward after update: -615.61, Optimal reward -509.61
Iteration 349 took 3.59 seconds (mean sampled reward: -3893.86). Current reward after update: -776.12, Optimal reward -509.61
Iteration 350 took 3.73 seconds (mean sampled reward: -4931.80). Current reward after update: -722.27, Optimal reward -509.61
Iteration 351 took 3.80 seconds (mean sampled reward: -4906.67). Current reward after update: -590.73, Optimal reward -509.61
Iteration 352 took 3.51 seconds (mean sampled reward: -3371.20). Current reward after update: -676.45, Optimal reward -509.61
Iteration 353 took 3.79 seconds (mean sampled reward: -4806.36). Current reward after update: -732.20, Optimal reward -509.61
Iteration 354 took 3.79 seconds (mean sampled reward: -4739.70). Current reward after update: -638.86, Optimal reward -509.61
Iteration 355 took 3.70 seconds (mean sampled reward: -4079.91). Current reward after update: -722.21, Optimal reward -509.61
Iteration 356 took 3.67 seconds (mean sampled reward: -4436.93). Current reward after update: -745.51, Optimal reward -509.61
Iteration 357 took 3.66 seconds (mean sampled reward: -4582.78). Current reward after update: -632.62, Optimal reward -509.61
Iteration 358 took 3.71 seconds (mean sampled reward: -4636.59). Current reward after update: -854.90, Optimal reward -509.61
Iteration 359 took 3.50 seconds (mean sampled reward: -3317.80). Current reward after update: -726.86, Optimal reward -509.61
Iteration 360 took 3.50 seconds (mean sampled reward: -3089.05). Current reward after update: -1627.89, Optimal reward -509.61
Iteration 361 took 3.39 seconds (mean sampled reward: -2902.21). Current reward after update: -813.59, Optimal reward -509.61
Iteration 362 took 3.40 seconds (mean sampled reward: -2846.11). Current reward after update: -780.58, Optimal reward -509.61
Iteration 363 took 3.32 seconds (mean sampled reward: -3002.99). Current reward after update: -919.97, Optimal reward -509.61
Iteration 364 took 3.34 seconds (mean sampled reward: -3189.25). Current reward after update: -1095.30, Optimal reward -509.61
Iteration 365 took 3.39 seconds (mean sampled reward: -3298.61). Current reward after update: -900.95, Optimal reward -509.61
Iteration 366 took 3.36 seconds (mean sampled reward: -3262.26). Current reward after update: -992.97, Optimal reward -509.61
Iteration 367 took 3.34 seconds (mean sampled reward: -2953.46). Current reward after update: -763.97, Optimal reward -509.61
Iteration 368 took 3.37 seconds (mean sampled reward: -2837.87). Current reward after update: -807.04, Optimal reward -509.61
Iteration 369 took 3.33 seconds (mean sampled reward: -2872.83). Current reward after update: -694.19, Optimal reward -509.61
Iteration 370 took 3.38 seconds (mean sampled reward: -3004.93). Current reward after update: -1117.07, Optimal reward -509.61
Iteration 371 took 3.34 seconds (mean sampled reward: -3122.81). Current reward after update: -701.63, Optimal reward -509.61
Iteration 372 took 3.31 seconds (mean sampled reward: -3020.98). Current reward after update: -697.30, Optimal reward -509.61
Iteration 373 took 3.43 seconds (mean sampled reward: -2923.41). Current reward after update: -746.58, Optimal reward -509.61
Iteration 374 took 3.30 seconds (mean sampled reward: -2992.71). Current reward after update: -778.67, Optimal reward -509.61
Iteration 375 took 3.28 seconds (mean sampled reward: -2829.63). Current reward after update: -679.74, Optimal reward -509.61
Iteration 376 took 3.46 seconds (mean sampled reward: -2714.58). Current reward after update: -675.43, Optimal reward -509.61
Iteration 377 took 3.30 seconds (mean sampled reward: -2685.22). Current reward after update: -750.35, Optimal reward -509.61
Iteration 378 took 3.31 seconds (mean sampled reward: -2789.92). Current reward after update: -760.98, Optimal reward -509.61
Iteration 379 took 3.35 seconds (mean sampled reward: -2911.44). Current reward after update: -811.61, Optimal reward -509.61
Iteration 380 took 3.53 seconds (mean sampled reward: -4503.69). Current reward after update: -746.61, Optimal reward -509.61
Iteration 381 took 3.33 seconds (mean sampled reward: -2873.81). Current reward after update: -697.67, Optimal reward -509.61
Iteration 382 took 3.28 seconds (mean sampled reward: -2910.11). Current reward after update: -670.74, Optimal reward -509.61
Iteration 383 took 3.36 seconds (mean sampled reward: -3618.69). Current reward after update: -671.52, Optimal reward -509.61
Iteration 384 took 3.43 seconds (mean sampled reward: -4256.80). Current reward after update: -672.96, Optimal reward -509.61
Iteration 385 took 3.57 seconds (mean sampled reward: -5262.94). Current reward after update: -718.07, Optimal reward -509.61
Iteration 386 took 3.58 seconds (mean sampled reward: -5202.40). Current reward after update: -784.41, Optimal reward -509.61
Iteration 387 took 3.63 seconds (mean sampled reward: -5274.03). Current reward after update: -867.34, Optimal reward -509.61
Iteration 388 took 3.62 seconds (mean sampled reward: -5368.93). Current reward after update: -800.60, Optimal reward -509.61
Iteration 389 took 3.63 seconds (mean sampled reward: -5589.82). Current reward after update: -770.98, Optimal reward -509.61
Iteration 390 took 3.58 seconds (mean sampled reward: -5031.54). Current reward after update: -722.76, Optimal reward -509.61
Iteration 391 took 3.52 seconds (mean sampled reward: -4611.69). Current reward after update: -807.73, Optimal reward -509.61
Iteration 392 took 3.67 seconds (mean sampled reward: -5682.90). Current reward after update: -2519.09, Optimal reward -509.61
Iteration 393 took 3.66 seconds (mean sampled reward: -5763.77). Current reward after update: -685.43, Optimal reward -509.61
Iteration 394 took 3.50 seconds (mean sampled reward: -4789.05). Current reward after update: -692.58, Optimal reward -509.61
Iteration 395 took 3.65 seconds (mean sampled reward: -5626.68). Current reward after update: -677.12, Optimal reward -509.61
Iteration 396 took 3.55 seconds (mean sampled reward: -5518.22). Current reward after update: -713.47, Optimal reward -509.61
Iteration 397 took 3.45 seconds (mean sampled reward: -4519.39). Current reward after update: -778.86, Optimal reward -509.61
Iteration 398 took 3.44 seconds (mean sampled reward: -4502.25). Current reward after update: -845.32, Optimal reward -509.61
Iteration 399 took 3.45 seconds (mean sampled reward: -4584.92). Current reward after update: -826.09, Optimal reward -509.61
Iteration 400 took 3.28 seconds (mean sampled reward: -4210.39). Current reward after update: -1016.23, Optimal reward -509.61
Max force: 50 Sigma: 0.8 mean rewards: -331.7503871145624, best rewards:-99.37247202384168

argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
pybullet build time: Jan 28 2022 20:17:22
Iteration 1 took 3.96 seconds (mean sampled reward: -6419.01). Current reward after update: -3405.62, Optimal reward -3405.62
Iteration 2 took 4.07 seconds (mean sampled reward: -5500.96). Current reward after update: -2283.80, Optimal reward -2283.80
Iteration 3 took 3.72 seconds (mean sampled reward: -5522.10). Current reward after update: -2108.58, Optimal reward -2108.58
Iteration 4 took 3.69 seconds (mean sampled reward: -4761.36). Current reward after update: -1752.33, Optimal reward -1752.33
Iteration 5 took 3.86 seconds (mean sampled reward: -4986.07). Current reward after update: -1421.54, Optimal reward -1421.54
Iteration 6 took 3.68 seconds (mean sampled reward: -4915.32). Current reward after update: -1083.90, Optimal reward -1083.90
Iteration 7 took 3.76 seconds (mean sampled reward: -4438.54). Current reward after update: -977.15, Optimal reward -977.15
Iteration 8 took 3.83 seconds (mean sampled reward: -4077.17). Current reward after update: -623.53, Optimal reward -623.53
Iteration 9 took 3.83 seconds (mean sampled reward: -3452.02). Current reward after update: -485.41, Optimal reward -485.41
Iteration 10 took 3.67 seconds (mean sampled reward: -3993.41). Current reward after update: -592.79, Optimal reward -485.41
Iteration 11 took 3.89 seconds (mean sampled reward: -4646.26). Current reward after update: -538.15, Optimal reward -485.41
Iteration 12 took 3.66 seconds (mean sampled reward: -5130.95). Current reward after update: -571.25, Optimal reward -485.41
Iteration 13 took 3.64 seconds (mean sampled reward: -4446.84). Current reward after update: -582.40, Optimal reward -485.41
Iteration 14 took 3.67 seconds (mean sampled reward: -4594.27). Current reward after update: -422.35, Optimal reward -422.35
Iteration 15 took 3.61 seconds (mean sampled reward: -4452.59). Current reward after update: -810.72, Optimal reward -422.35
Iteration 16 took 3.63 seconds (mean sampled reward: -4527.36). Current reward after update: -353.55, Optimal reward -353.55
Iteration 17 took 3.65 seconds (mean sampled reward: -4619.36). Current reward after update: -268.24, Optimal reward -268.24
Iteration 18 took 3.82 seconds (mean sampled reward: -5163.27). Current reward after update: -424.46, Optimal reward -268.24
Iteration 19 took 3.68 seconds (mean sampled reward: -4325.68). Current reward after update: -421.52, Optimal reward -268.24
Iteration 20 took 3.69 seconds (mean sampled reward: -4712.40). Current reward after update: -447.41, Optimal reward -268.24
Iteration 21 took 3.67 seconds (mean sampled reward: -4876.15). Current reward after update: -416.02, Optimal reward -268.24
Iteration 22 took 3.67 seconds (mean sampled reward: -4226.14). Current reward after update: -237.17, Optimal reward -237.17
Iteration 23 took 3.66 seconds (mean sampled reward: -3928.87). Current reward after update: -209.90, Optimal reward -209.90
Iteration 24 took 3.79 seconds (mean sampled reward: -4779.92). Current reward after update: -213.03, Optimal reward -209.90
Iteration 25 took 3.75 seconds (mean sampled reward: -4802.80). Current reward after update: -246.86, Optimal reward -209.90
Iteration 26 took 3.60 seconds (mean sampled reward: -3664.49). Current reward after update: -217.59, Optimal reward -209.90
Iteration 27 took 3.61 seconds (mean sampled reward: -3256.40). Current reward after update: -214.03, Optimal reward -209.90
Iteration 28 took 3.91 seconds (mean sampled reward: -4112.18). Current reward after update: -157.39, Optimal reward -157.39
Iteration 29 took 3.85 seconds (mean sampled reward: -3356.32). Current reward after update: -186.55, Optimal reward -157.39
Iteration 30 took 3.67 seconds (mean sampled reward: -4194.67). Current reward after update: -243.33, Optimal reward -157.39
Iteration 31 took 3.71 seconds (mean sampled reward: -5230.91). Current reward after update: -200.50, Optimal reward -157.39
Iteration 32 took 4.03 seconds (mean sampled reward: -5344.63). Current reward after update: -319.34, Optimal reward -157.39
Iteration 33 took 3.85 seconds (mean sampled reward: -5573.11). Current reward after update: -308.43, Optimal reward -157.39
Iteration 34 took 3.84 seconds (mean sampled reward: -5362.28). Current reward after update: -504.61, Optimal reward -157.39
Iteration 35 took 3.74 seconds (mean sampled reward: -5174.30). Current reward after update: -333.71, Optimal reward -157.39
Iteration 36 took 3.84 seconds (mean sampled reward: -4675.20). Current reward after update: -430.90, Optimal reward -157.39
Iteration 37 took 3.85 seconds (mean sampled reward: -4579.63). Current reward after update: -429.59, Optimal reward -157.39
Iteration 38 took 3.72 seconds (mean sampled reward: -3760.72). Current reward after update: -269.79, Optimal reward -157.39
Iteration 39 took 3.77 seconds (mean sampled reward: -3666.11). Current reward after update: -299.89, Optimal reward -157.39
Iteration 40 took 3.75 seconds (mean sampled reward: -3951.38). Current reward after update: -318.78, Optimal reward -157.39
Iteration 41 took 3.80 seconds (mean sampled reward: -4301.76). Current reward after update: -391.01, Optimal reward -157.39
Iteration 42 took 3.74 seconds (mean sampled reward: -3693.92). Current reward after update: -247.04, Optimal reward -157.39
Iteration 43 took 3.76 seconds (mean sampled reward: -3581.02). Current reward after update: -268.67, Optimal reward -157.39
Iteration 44 took 3.74 seconds (mean sampled reward: -4060.57). Current reward after update: -338.98, Optimal reward -157.39
Iteration 45 took 3.74 seconds (mean sampled reward: -4125.41). Current reward after update: -280.23, Optimal reward -157.39
Iteration 46 took 3.69 seconds (mean sampled reward: -4028.16). Current reward after update: -238.63, Optimal reward -157.39
Iteration 47 took 3.67 seconds (mean sampled reward: -3950.73). Current reward after update: -274.09, Optimal reward -157.39
Iteration 48 took 3.63 seconds (mean sampled reward: -4115.94). Current reward after update: -356.62, Optimal reward -157.39
Iteration 49 took 3.66 seconds (mean sampled reward: -4348.18). Current reward after update: -331.43, Optimal reward -157.39
Iteration 50 took 3.60 seconds (mean sampled reward: -4239.32). Current reward after update: -270.53, Optimal reward -157.39
Iteration 51 took 3.65 seconds (mean sampled reward: -4292.08). Current reward after update: -344.83, Optimal reward -157.39
Iteration 52 took 3.64 seconds (mean sampled reward: -4458.43). Current reward after update: -227.48, Optimal reward -157.39
Iteration 53 took 3.69 seconds (mean sampled reward: -4271.28). Current reward after update: -224.40, Optimal reward -157.39
Iteration 54 took 3.67 seconds (mean sampled reward: -4249.16). Current reward after update: -276.60, Optimal reward -157.39
Iteration 55 took 3.96 seconds (mean sampled reward: -4011.65). Current reward after update: -256.84, Optimal reward -157.39
Iteration 56 took 3.70 seconds (mean sampled reward: -3526.62). Current reward after update: -245.15, Optimal reward -157.39
Iteration 57 took 3.87 seconds (mean sampled reward: -4370.65). Current reward after update: -1091.81, Optimal reward -157.39
Iteration 58 took 3.55 seconds (mean sampled reward: -2821.86). Current reward after update: -222.83, Optimal reward -157.39
Iteration 59 took 3.48 seconds (mean sampled reward: -3481.97). Current reward after update: -284.64, Optimal reward -157.39
Iteration 60 took 3.57 seconds (mean sampled reward: -4400.35). Current reward after update: -239.74, Optimal reward -157.39
Iteration 61 took 3.62 seconds (mean sampled reward: -4434.68). Current reward after update: -232.68, Optimal reward -157.39
Iteration 62 took 3.64 seconds (mean sampled reward: -3736.89). Current reward after update: -351.49, Optimal reward -157.39
Iteration 63 took 3.63 seconds (mean sampled reward: -4414.92). Current reward after update: -232.50, Optimal reward -157.39
Iteration 64 took 3.52 seconds (mean sampled reward: -3884.52). Current reward after update: -230.77, Optimal reward -157.39
Iteration 65 took 3.63 seconds (mean sampled reward: -3556.83). Current reward after update: -254.13, Optimal reward -157.39
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
Iteration 66 took 3.62 seconds (mean sampled reward: -3863.03). Current reward after update: -254.13, Optimal reward -157.39
Iteration 67 took 3.52 seconds (mean sampled reward: -4011.38). Current reward after update: -243.00, Optimal reward -157.39
Iteration 68 took 3.52 seconds (mean sampled reward: -4136.77). Current reward after update: -271.40, Optimal reward -157.39
Iteration 69 took 3.49 seconds (mean sampled reward: -3969.76). Current reward after update: -286.16, Optimal reward -157.39
Iteration 70 took 3.41 seconds (mean sampled reward: -3608.83). Current reward after update: -224.28, Optimal reward -157.39
Iteration 71 took 3.56 seconds (mean sampled reward: -4294.55). Current reward after update: -381.65, Optimal reward -157.39
Iteration 72 took 3.55 seconds (mean sampled reward: -3969.82). Current reward after update: -297.49, Optimal reward -157.39
Iteration 73 took 3.47 seconds (mean sampled reward: -3715.82). Current reward after update: -220.95, Optimal reward -157.39
Iteration 74 took 3.41 seconds (mean sampled reward: -3613.74). Current reward after update: -295.55, Optimal reward -157.39
Iteration 75 took 3.37 seconds (mean sampled reward: -3602.43). Current reward after update: -274.38, Optimal reward -157.39
Iteration 76 took 3.43 seconds (mean sampled reward: -4133.57). Current reward after update: -272.58, Optimal reward -157.39
Iteration 77 took 3.44 seconds (mean sampled reward: -3839.16). Current reward after update: -278.30, Optimal reward -157.39
Iteration 78 took 3.43 seconds (mean sampled reward: -3806.81). Current reward after update: -301.23, Optimal reward -157.39
Iteration 79 took 3.37 seconds (mean sampled reward: -3805.75). Current reward after update: -284.93, Optimal reward -157.39
Iteration 80 took 3.45 seconds (mean sampled reward: -3291.36). Current reward after update: -262.59, Optimal reward -157.39
Iteration 81 took 3.39 seconds (mean sampled reward: -3562.00). Current reward after update: -237.12, Optimal reward -157.39
Iteration 82 took 3.42 seconds (mean sampled reward: -3728.90). Current reward after update: -253.68, Optimal reward -157.39
Iteration 83 took 3.45 seconds (mean sampled reward: -4367.41). Current reward after update: -296.20, Optimal reward -157.39
Iteration 84 took 3.52 seconds (mean sampled reward: -4196.48). Current reward after update: -240.18, Optimal reward -157.39
Iteration 85 took 3.47 seconds (mean sampled reward: -4039.30). Current reward after update: -294.73, Optimal reward -157.39
Iteration 86 took 3.46 seconds (mean sampled reward: -3205.08). Current reward after update: -287.46, Optimal reward -157.39
Iteration 87 took 3.46 seconds (mean sampled reward: -3661.24). Current reward after update: -269.75, Optimal reward -157.39
Iteration 88 took 3.41 seconds (mean sampled reward: -4508.92). Current reward after update: -290.73, Optimal reward -157.39
Iteration 89 took 3.46 seconds (mean sampled reward: -4366.45). Current reward after update: -258.32, Optimal reward -157.39
Iteration 90 took 3.50 seconds (mean sampled reward: -4644.66). Current reward after update: -251.93, Optimal reward -157.39
Iteration 91 took 3.46 seconds (mean sampled reward: -3901.25). Current reward after update: -296.90, Optimal reward -157.39
Iteration 92 took 3.44 seconds (mean sampled reward: -4309.26). Current reward after update: -256.27, Optimal reward -157.39
Iteration 93 took 3.46 seconds (mean sampled reward: -4530.89). Current reward after update: -234.06, Optimal reward -157.39
Iteration 94 took 3.50 seconds (mean sampled reward: -4847.58). Current reward after update: -203.62, Optimal reward -157.39
Iteration 95 took 3.42 seconds (mean sampled reward: -4712.46). Current reward after update: -286.84, Optimal reward -157.39
Iteration 96 took 3.44 seconds (mean sampled reward: -4974.64). Current reward after update: -243.52, Optimal reward -157.39
Iteration 97 took 3.48 seconds (mean sampled reward: -4916.91). Current reward after update: -267.07, Optimal reward -157.39
Iteration 98 took 3.58 seconds (mean sampled reward: -5100.38). Current reward after update: -232.79, Optimal reward -157.39
Iteration 99 took 3.56 seconds (mean sampled reward: -5758.94). Current reward after update: -271.76, Optimal reward -157.39
Iteration 100 took 3.50 seconds (mean sampled reward: -5295.26). Current reward after update: -334.03, Optimal reward -157.39
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
Iteration 1 took 3.92 seconds (mean sampled reward: -6436.78). Current reward after update: -3980.01, Optimal reward -3980.01
Iteration 2 took 3.67 seconds (mean sampled reward: -5751.59). Current reward after update: -2618.54, Optimal reward -2618.54
Iteration 3 took 3.71 seconds (mean sampled reward: -5314.12). Current reward after update: -1498.31, Optimal reward -1498.31
Iteration 4 took 3.82 seconds (mean sampled reward: -5308.24). Current reward after update: -1640.76, Optimal reward -1498.31
Iteration 5 took 3.99 seconds (mean sampled reward: -5320.20). Current reward after update: -1519.50, Optimal reward -1498.31
Iteration 6 took 3.90 seconds (mean sampled reward: -4906.82). Current reward after update: -957.98, Optimal reward -957.98
Iteration 7 took 3.90 seconds (mean sampled reward: -5095.97). Current reward after update: -1438.16, Optimal reward -957.98
Iteration 8 took 4.00 seconds (mean sampled reward: -5157.39). Current reward after update: -1419.95, Optimal reward -957.98
Iteration 9 took 3.88 seconds (mean sampled reward: -4878.66). Current reward after update: -965.38, Optimal reward -957.98
Iteration 10 took 3.74 seconds (mean sampled reward: -4204.22). Current reward after update: -919.96, Optimal reward -919.96
Iteration 11 took 3.88 seconds (mean sampled reward: -4346.74). Current reward after update: -725.92, Optimal reward -725.92
Iteration 12 took 3.75 seconds (mean sampled reward: -2821.66). Current reward after update: -651.69, Optimal reward -651.69
Iteration 13 took 3.63 seconds (mean sampled reward: -3968.32). Current reward after update: -577.42, Optimal reward -577.42
Iteration 14 took 3.98 seconds (mean sampled reward: -4099.29). Current reward after update: -451.29, Optimal reward -451.29
Iteration 15 took 3.64 seconds (mean sampled reward: -3916.95). Current reward after update: -467.60, Optimal reward -451.29
Iteration 16 took 3.97 seconds (mean sampled reward: -4300.32). Current reward after update: -507.83, Optimal reward -451.29
Iteration 17 took 3.49 seconds (mean sampled reward: -3298.30). Current reward after update: -322.67, Optimal reward -322.67
Iteration 18 took 3.61 seconds (mean sampled reward: -3410.95). Current reward after update: -250.98, Optimal reward -250.98
Iteration 19 took 3.69 seconds (mean sampled reward: -3426.76). Current reward after update: -282.76, Optimal reward -250.98
Iteration 20 took 3.56 seconds (mean sampled reward: -3466.93). Current reward after update: -312.56, Optimal reward -250.98
Iteration 21 took 3.50 seconds (mean sampled reward: -3386.79). Current reward after update: -276.54, Optimal reward -250.98
Iteration 22 took 3.61 seconds (mean sampled reward: -2888.80). Current reward after update: -275.04, Optimal reward -250.98
Iteration 23 took 3.58 seconds (mean sampled reward: -4111.95). Current reward after update: -321.25, Optimal reward -250.98
Iteration 24 took 3.70 seconds (mean sampled reward: -3235.91). Current reward after update: -287.44, Optimal reward -250.98
Iteration 25 took 3.52 seconds (mean sampled reward: -1789.35). Current reward after update: -260.72, Optimal reward -250.98
Iteration 26 took 3.53 seconds (mean sampled reward: -2545.32). Current reward after update: -411.89, Optimal reward -250.98
Iteration 27 took 3.47 seconds (mean sampled reward: -1919.75). Current reward after update: -213.38, Optimal reward -213.38
Iteration 28 took 3.91 seconds (mean sampled reward: -2131.06). Current reward after update: -194.11, Optimal reward -194.11
Iteration 29 took 3.82 seconds (mean sampled reward: -1321.55). Current reward after update: -168.12, Optimal reward -168.12
Iteration 30 took 3.52 seconds (mean sampled reward: -1275.36). Current reward after update: -284.28, Optimal reward -168.12
Iteration 31 took 3.56 seconds (mean sampled reward: -2226.94). Current reward after update: -144.12, Optimal reward -144.12
Iteration 32 took 3.52 seconds (mean sampled reward: -2017.52). Current reward after update: -138.83, Optimal reward -138.83
Iteration 33 took 3.82 seconds (mean sampled reward: -2552.19). Current reward after update: -181.21, Optimal reward -138.83
Iteration 34 took 3.52 seconds (mean sampled reward: -3569.02). Current reward after update: -228.07, Optimal reward -138.83
Iteration 35 took 3.49 seconds (mean sampled reward: -3658.91). Current reward after update: -114.76, Optimal reward -114.76
Iteration 36 took 3.64 seconds (mean sampled reward: -2231.54). Current reward after update: -96.59, Optimal reward -96.59
Iteration 37 took 3.50 seconds (mean sampled reward: -3154.22). Current reward after update: -195.20, Optimal reward -96.59
Iteration 38 took 3.56 seconds (mean sampled reward: -2480.07). Current reward after update: -139.52, Optimal reward -96.59
Iteration 39 took 3.49 seconds (mean sampled reward: -2762.03). Current reward after update: -55.65, Optimal reward -55.65
Iteration 40 took 3.46 seconds (mean sampled reward: -2988.93). Current reward after update: -71.08, Optimal reward -55.65
Iteration 41 took 3.44 seconds (mean sampled reward: -1636.25). Current reward after update: -312.63, Optimal reward -55.65
Iteration 42 took 3.38 seconds (mean sampled reward: -1694.27). Current reward after update: -89.35, Optimal reward -55.65
Iteration 43 took 3.43 seconds (mean sampled reward: -1952.34). Current reward after update: -97.44, Optimal reward -55.65
Iteration 44 took 3.49 seconds (mean sampled reward: -1603.30). Current reward after update: -49.66, Optimal reward -49.66
Iteration 45 took 3.49 seconds (mean sampled reward: -1912.61). Current reward after update: -191.02, Optimal reward -49.66
Iteration 46 took 3.47 seconds (mean sampled reward: -1648.09). Current reward after update: -68.29, Optimal reward -49.66
Iteration 47 took 3.39 seconds (mean sampled reward: -1869.08). Current reward after update: -85.22, Optimal reward -49.66
Iteration 48 took 3.44 seconds (mean sampled reward: -1415.48). Current reward after update: -119.25, Optimal reward -49.66
Iteration 49 took 3.42 seconds (mean sampled reward: -2099.72). Current reward after update: -126.76, Optimal reward -49.66
Iteration 50 took 3.53 seconds (mean sampled reward: -2298.70). Current reward after update: -77.28, Optimal reward -49.66
Iteration 51 took 3.50 seconds (mean sampled reward: -1935.76). Current reward after update: -77.28, Optimal reward -49.66
Iteration 52 took 3.51 seconds (mean sampled reward: -1676.00). Current reward after update: -90.75, Optimal reward -49.66
Iteration 53 took 3.53 seconds (mean sampled reward: -1499.04). Current reward after update: -129.94, Optimal reward -49.66
Iteration 54 took 3.60 seconds (mean sampled reward: -1892.47). Current reward after update: -402.62, Optimal reward -49.66
Iteration 55 took 3.49 seconds (mean sampled reward: -2429.41). Current reward after update: -105.37, Optimal reward -49.66
Iteration 56 took 3.72 seconds (mean sampled reward: -1433.67). Current reward after update: -55.63, Optimal reward -49.66
Iteration 57 took 3.46 seconds (mean sampled reward: -1255.79). Current reward after update: -60.44, Optimal reward -49.66
Iteration 58 took 3.50 seconds (mean sampled reward: -1523.09). Current reward after update: -61.53, Optimal reward -49.66
Iteration 59 took 3.83 seconds (mean sampled reward: -1329.92). Current reward after update: -12.01, Optimal reward -12.01
Iteration 60 took 3.63 seconds (mean sampled reward: -1626.14). Current reward after update: -29.21, Optimal reward -12.01
Iteration 61 took 3.49 seconds (mean sampled reward: -1574.91). Current reward after update: -68.05, Optimal reward -12.01
Iteration 62 took 3.49 seconds (mean sampled reward: -1633.36). Current reward after update: -65.59, Optimal reward -12.01
Iteration 63 took 3.41 seconds (mean sampled reward: -1002.26). Current reward after update: -25.48, Optimal reward -12.01
Iteration 64 took 3.45 seconds (mean sampled reward: -1046.35). Current reward after update: -64.76, Optimal reward -12.01
Iteration 65 took 3.41 seconds (mean sampled reward: -1097.60). Current reward after update: -7.74, Optimal reward -7.74
Iteration 66 took 3.62 seconds (mean sampled reward: -1197.79). Current reward after update: -17.30, Optimal reward -7.74
Iteration 67 took 3.46 seconds (mean sampled reward: -1259.44). Current reward after update: -84.82, Optimal reward -7.74
Iteration 68 took 3.48 seconds (mean sampled reward: -1135.88). Current reward after update: -32.10, Optimal reward -7.74
Iteration 69 took 3.43 seconds (mean sampled reward: -1472.94). Current reward after update: -81.80, Optimal reward -7.74
Iteration 70 took 3.40 seconds (mean sampled reward: -1460.63). Current reward after update: -31.44, Optimal reward -7.74
Iteration 71 took 3.42 seconds (mean sampled reward: -1462.64). Current reward after update: -92.81, Optimal reward -7.74
Iteration 72 took 3.43 seconds (mean sampled reward: -1451.19). Current reward after update: -189.35, Optimal reward -7.74
Iteration 73 took 3.50 seconds (mean sampled reward: -1691.30). Current reward after update: -89.31, Optimal reward -7.74
Iteration 74 took 3.47 seconds (mean sampled reward: -2385.82). Current reward after update: -99.58, Optimal reward -7.74
Iteration 75 took 3.58 seconds (mean sampled reward: -2139.14). Current reward after update: -184.73, Optimal reward -7.74
Iteration 76 took 3.60 seconds (mean sampled reward: -2138.99). Current reward after update: -84.60, Optimal reward -7.74
Iteration 77 took 3.46 seconds (mean sampled reward: -2328.76). Current reward after update: -54.15, Optimal reward -7.74
Iteration 78 took 3.47 seconds (mean sampled reward: -1902.97). Current reward after update: -117.33, Optimal reward -7.74
Iteration 79 took 3.45 seconds (mean sampled reward: -1683.89). Current reward after update: -143.57, Optimal reward -7.74
Iteration 80 took 3.49 seconds (mean sampled reward: -2301.16). Current reward after update: -84.17, Optimal reward -7.74
Iteration 81 took 3.57 seconds (mean sampled reward: -2447.18). Current reward after update: -38.28, Optimal reward -7.74
Iteration 82 took 3.44 seconds (mean sampled reward: -1841.76). Current reward after update: -27.91, Optimal reward -7.74
Iteration 83 took 3.49 seconds (mean sampled reward: -2132.27). Current reward after update: -111.39, Optimal reward -7.74
Iteration 84 took 3.48 seconds (mean sampled reward: -2170.67). Current reward after update: -96.46, Optimal reward -7.74
Iteration 85 took 3.57 seconds (mean sampled reward: -2736.52). Current reward after update: -59.18, Optimal reward -7.74
Iteration 86 took 3.59 seconds (mean sampled reward: -2767.54). Current reward after update: -108.05, Optimal reward -7.74
Iteration 87 took 3.55 seconds (mean sampled reward: -2067.53). Current reward after update: -177.28, Optimal reward -7.74
Iteration 88 took 3.58 seconds (mean sampled reward: -1855.73). Current reward after update: -50.55, Optimal reward -7.74
Iteration 89 took 3.56 seconds (mean sampled reward: -1952.54). Current reward after update: -63.51, Optimal reward -7.74
Iteration 90 took 3.54 seconds (mean sampled reward: -1874.77). Current reward after update: -80.41, Optimal reward -7.74
Iteration 91 took 3.46 seconds (mean sampled reward: -1802.94). Current reward after update: -65.57, Optimal reward -7.74
Iteration 92 took 3.50 seconds (mean sampled reward: -1957.70). Current reward after update: -49.59, Optimal reward -7.74
Iteration 93 took 3.57 seconds (mean sampled reward: -2076.53). Current reward after update: -59.69, Optimal reward -7.74
Iteration 94 took 3.51 seconds (mean sampled reward: -1758.30). Current reward after update: -42.72, Optimal reward -7.74
Iteration 95 took 3.53 seconds (mean sampled reward: -2088.63). Current reward after update: -29.98, Optimal reward -7.74
Iteration 96 took 3.56 seconds (mean sampled reward: -1748.57). Current reward after update: -52.30, Optimal reward -7.74
Iteration 97 took 3.61 seconds (mean sampled reward: -1812.19). Current reward after update: -45.90, Optimal reward -7.74
Iteration 98 took 3.71 seconds (mean sampled reward: -3318.52). Current reward after update: -127.70, Optimal reward -7.74
Iteration 99 took 3.62 seconds (mean sampled reward: -2351.21). Current reward after update: -74.36, Optimal reward -7.74
Iteration 100 took 3.46 seconds (mean sampled reward: -1417.96). Current reward after update: -10.49, Optimal reward -7.74
Iteration 1 took 3.90 seconds (mean sampled reward: -6458.68). Current reward after update: -3619.57, Optimal reward -3619.57
Iteration 2 took 3.84 seconds (mean sampled reward: -6061.91). Current reward after update: -2624.78, Optimal reward -2624.78
Iteration 3 took 3.74 seconds (mean sampled reward: -5640.86). Current reward after update: -1884.58, Optimal reward -1884.58
Iteration 4 took 3.68 seconds (mean sampled reward: -5769.56). Current reward after update: -1808.49, Optimal reward -1808.49
Iteration 5 took 3.76 seconds (mean sampled reward: -5134.68). Current reward after update: -2032.77, Optimal reward -1808.49
Iteration 6 took 3.63 seconds (mean sampled reward: -5193.55). Current reward after update: -2037.43, Optimal reward -1808.49
Iteration 7 took 3.70 seconds (mean sampled reward: -5133.68). Current reward after update: -1668.02, Optimal reward -1668.02
Iteration 8 took 3.73 seconds (mean sampled reward: -5214.60). Current reward after update: -1635.40, Optimal reward -1635.40
Iteration 9 took 3.73 seconds (mean sampled reward: -5162.68). Current reward after update: -1269.93, Optimal reward -1269.93
Iteration 10 took 3.68 seconds (mean sampled reward: -5403.37). Current reward after update: -1401.26, Optimal reward -1269.93
Iteration 11 took 3.72 seconds (mean sampled reward: -5316.27). Current reward after update: -915.89, Optimal reward -915.89
Iteration 12 took 3.80 seconds (mean sampled reward: -4815.37). Current reward after update: -461.44, Optimal reward -461.44
Iteration 13 took 3.76 seconds (mean sampled reward: -4432.55). Current reward after update: -446.74, Optimal reward -446.74
Iteration 14 took 3.64 seconds (mean sampled reward: -3904.87). Current reward after update: -372.73, Optimal reward -372.73
Iteration 15 took 3.68 seconds (mean sampled reward: -4984.47). Current reward after update: -558.53, Optimal reward -372.73
Iteration 16 took 3.62 seconds (mean sampled reward: -4309.72). Current reward after update: -371.92, Optimal reward -371.92
Iteration 17 took 3.65 seconds (mean sampled reward: -4655.13). Current reward after update: -651.59, Optimal reward -371.92
Iteration 18 took 3.77 seconds (mean sampled reward: -3610.43). Current reward after update: -195.14, Optimal reward -195.14
Iteration 19 took 3.72 seconds (mean sampled reward: -3712.89). Current reward after update: -244.14, Optimal reward -195.14
Iteration 20 took 3.63 seconds (mean sampled reward: -3547.46). Current reward after update: -347.58, Optimal reward -195.14
Iteration 21 took 3.54 seconds (mean sampled reward: -3691.23). Current reward after update: -252.17, Optimal reward -195.14
Iteration 22 took 3.48 seconds (mean sampled reward: -2530.74). Current reward after update: -326.63, Optimal reward -195.14
Iteration 23 took 3.51 seconds (mean sampled reward: -2434.96). Current reward after update: -303.41, Optimal reward -195.14
Iteration 24 took 3.58 seconds (mean sampled reward: -2702.66). Current reward after update: -233.60, Optimal reward -195.14
Iteration 25 took 3.45 seconds (mean sampled reward: -2195.65). Current reward after update: -186.33, Optimal reward -186.33
Iteration 26 took 3.53 seconds (mean sampled reward: -2346.53). Current reward after update: -161.18, Optimal reward -161.18
Iteration 27 took 3.44 seconds (mean sampled reward: -1933.47). Current reward after update: -83.88, Optimal reward -83.88
Iteration 28 took 3.47 seconds (mean sampled reward: -2468.98). Current reward after update: -208.67, Optimal reward -83.88
Iteration 29 took 3.60 seconds (mean sampled reward: -2422.25). Current reward after update: -274.56, Optimal reward -83.88
Iteration 30 took 3.61 seconds (mean sampled reward: -3563.76). Current reward after update: -363.00, Optimal reward -83.88
Iteration 31 took 3.71 seconds (mean sampled reward: -4646.71). Current reward after update: -1259.54, Optimal reward -83.88
Iteration 32 took 3.72 seconds (mean sampled reward: -3619.43). Current reward after update: -540.67, Optimal reward -83.88
Iteration 33 took 3.72 seconds (mean sampled reward: -4901.29). Current reward after update: -389.17, Optimal reward -83.88
Iteration 34 took 3.75 seconds (mean sampled reward: -4975.64). Current reward after update: -438.66, Optimal reward -83.88
Iteration 35 took 3.70 seconds (mean sampled reward: -4510.93). Current reward after update: -266.22, Optimal reward -83.88
Iteration 36 took 3.68 seconds (mean sampled reward: -5558.46). Current reward after update: -1009.44, Optimal reward -83.88
Iteration 37 took 3.66 seconds (mean sampled reward: -4865.74). Current reward after update: -665.04, Optimal reward -83.88
Iteration 38 took 3.69 seconds (mean sampled reward: -4515.04). Current reward after update: -421.57, Optimal reward -83.88
Iteration 39 took 3.66 seconds (mean sampled reward: -4267.27). Current reward after update: -388.11, Optimal reward -83.88
Iteration 40 took 3.56 seconds (mean sampled reward: -4203.43). Current reward after update: -176.59, Optimal reward -83.88
Iteration 41 took 3.72 seconds (mean sampled reward: -4228.01). Current reward after update: -230.58, Optimal reward -83.88
Iteration 42 took 3.55 seconds (mean sampled reward: -4372.46). Current reward after update: -180.41, Optimal reward -83.88
Iteration 43 took 3.45 seconds (mean sampled reward: -2995.29). Current reward after update: -191.43, Optimal reward -83.88
Iteration 44 took 3.48 seconds (mean sampled reward: -4169.58). Current reward after update: -102.98, Optimal reward -83.88
Iteration 45 took 3.49 seconds (mean sampled reward: -4940.62). Current reward after update: -189.01, Optimal reward -83.88
Iteration 46 took 3.52 seconds (mean sampled reward: -4454.91). Current reward after update: -55.53, Optimal reward -55.53
Iteration 47 took 3.52 seconds (mean sampled reward: -5363.15). Current reward after update: -87.42, Optimal reward -55.53
Iteration 48 took 3.71 seconds (mean sampled reward: -5038.08). Current reward after update: -229.29, Optimal reward -55.53
Iteration 49 took 3.57 seconds (mean sampled reward: -5278.46). Current reward after update: -277.76, Optimal reward -55.53
Iteration 50 took 3.48 seconds (mean sampled reward: -5467.50). Current reward after update: -181.16, Optimal reward -55.53
Iteration 51 took 3.41 seconds (mean sampled reward: -5323.61). Current reward after update: -126.05, Optimal reward -55.53
Iteration 52 took 3.55 seconds (mean sampled reward: -5386.35). Current reward after update: -27.94, Optimal reward -27.94
Iteration 53 took 3.49 seconds (mean sampled reward: -5085.47). Current reward after update: -93.58, Optimal reward -27.94
Iteration 54 took 3.50 seconds (mean sampled reward: -5301.68). Current reward after update: -108.87, Optimal reward -27.94
Iteration 55 took 3.64 seconds (mean sampled reward: -5261.39). Current reward after update: -107.71, Optimal reward -27.94
Iteration 56 took 3.54 seconds (mean sampled reward: -4984.25). Current reward after update: -145.80, Optimal reward -27.94
Iteration 57 took 3.42 seconds (mean sampled reward: -5480.82). Current reward after update: -302.20, Optimal reward -27.94
Iteration 58 took 3.54 seconds (mean sampled reward: -5179.68). Current reward after update: -200.67, Optimal reward -27.94
Iteration 59 took 3.43 seconds (mean sampled reward: -4560.90). Current reward after update: -199.88, Optimal reward -27.94
Iteration 60 took 3.43 seconds (mean sampled reward: -3546.82). Current reward after update: -203.85, Optimal reward -27.94
Iteration 61 took 3.39 seconds (mean sampled reward: -3919.13). Current reward after update: -297.92, Optimal reward -27.94
Iteration 62 took 3.35 seconds (mean sampled reward: -2556.07). Current reward after update: -95.72, Optimal reward -27.94
Iteration 63 took 3.31 seconds (mean sampled reward: -2433.15). Current reward after update: -596.48, Optimal reward -27.94
Iteration 64 took 3.32 seconds (mean sampled reward: -2365.60). Current reward after update: -96.47, Optimal reward -27.94
Iteration 65 took 3.28 seconds (mean sampled reward: -2363.06). Current reward after update: -66.79, Optimal reward -27.94
Iteration 66 took 3.32 seconds (mean sampled reward: -3272.36). Current reward after update: -46.07, Optimal reward -27.94
Iteration 67 took 3.42 seconds (mean sampled reward: -3811.20). Current reward after update: 61.08, Optimal reward 61.08
Iteration 68 took 3.36 seconds (mean sampled reward: -4128.11). Current reward after update: 89.22, Optimal reward 89.22
Iteration 69 took 3.51 seconds (mean sampled reward: -2614.31). Current reward after update: 25.30, Optimal reward 89.22
Iteration 70 took 3.36 seconds (mean sampled reward: -3279.87). Current reward after update: 51.66, Optimal reward 89.22
Iteration 71 took 3.60 seconds (mean sampled reward: -3477.10). Current reward after update: 23.74, Optimal reward 89.22
Iteration 72 took 3.73 seconds (mean sampled reward: -2667.14). Current reward after update: 118.57, Optimal reward 118.57
Iteration 73 took 3.74 seconds (mean sampled reward: -1695.26). Current reward after update: 134.41, Optimal reward 134.41
Iteration 74 took 3.47 seconds (mean sampled reward: -2175.70). Current reward after update: 122.71, Optimal reward 134.41
Iteration 75 took 3.35 seconds (mean sampled reward: -2059.07). Current reward after update: 160.62, Optimal reward 160.62
Iteration 76 took 3.37 seconds (mean sampled reward: -2355.38). Current reward after update: 86.58, Optimal reward 160.62
Iteration 77 took 3.35 seconds (mean sampled reward: -1960.98). Current reward after update: 84.71, Optimal reward 160.62
Iteration 78 took 3.34 seconds (mean sampled reward: -2454.27). Current reward after update: 52.25, Optimal reward 160.62
Iteration 79 took 3.42 seconds (mean sampled reward: -2248.58). Current reward after update: 94.41, Optimal reward 160.62
Iteration 80 took 3.41 seconds (mean sampled reward: -4109.99). Current reward after update: 92.70, Optimal reward 160.62
Iteration 81 took 3.48 seconds (mean sampled reward: -3586.42). Current reward after update: -65.71, Optimal reward 160.62
Iteration 82 took 3.54 seconds (mean sampled reward: -3656.95). Current reward after update: -12.91, Optimal reward 160.62
Iteration 83 took 3.51 seconds (mean sampled reward: -3141.77). Current reward after update: 11.19, Optimal reward 160.62
Iteration 84 took 3.76 seconds (mean sampled reward: -3257.32). Current reward after update: 35.51, Optimal reward 160.62
Iteration 85 took 3.66 seconds (mean sampled reward: -4346.11). Current reward after update: -54.23, Optimal reward 160.62
Iteration 86 took 3.71 seconds (mean sampled reward: -3533.05). Current reward after update: 1.62, Optimal reward 160.62
Iteration 87 took 3.76 seconds (mean sampled reward: -3038.33). Current reward after update: 16.45, Optimal reward 160.62
Iteration 88 took 3.54 seconds (mean sampled reward: -3737.68). Current reward after update: -1522.84, Optimal reward 160.62
Iteration 89 took 3.57 seconds (mean sampled reward: -4722.24). Current reward after update: 3.22, Optimal reward 160.62
Iteration 90 took 3.52 seconds (mean sampled reward: -4517.22). Current reward after update: -0.04, Optimal reward 160.62
Iteration 91 took 3.57 seconds (mean sampled reward: -3954.38). Current reward after update: 20.35, Optimal reward 160.62
Iteration 92 took 3.54 seconds (mean sampled reward: -2682.44). Current reward after update: 102.28, Optimal reward 160.62
Iteration 93 took 3.49 seconds (mean sampled reward: -3238.87). Current reward after update: -7.26, Optimal reward 160.62
Iteration 94 took 3.52 seconds (mean sampled reward: -2308.37). Current reward after update: -62.91, Optimal reward 160.62
Iteration 95 took 3.53 seconds (mean sampled reward: -3161.21). Current reward after update: 4.27, Optimal reward 160.62
Iteration 96 took 3.53 seconds (mean sampled reward: -2481.08). Current reward after update: 109.01, Optimal reward 160.62
Iteration 97 took 3.69 seconds (mean sampled reward: -2654.05). Current reward after update: 102.89, Optimal reward 160.62
Iteration 98 took 3.51 seconds (mean sampled reward: -3922.84). Current reward after update: -3.58, Optimal reward 160.62
Iteration 99 took 3.49 seconds (mean sampled reward: -4572.01). Current reward after update: -30.55, Optimal reward 160.62
Iteration 100 took 3.46 seconds (mean sampled reward: -4155.00). Current reward after update: -6.53, Optimal reward 160.62
Max force: 50 Sigma: 0.8 mean rewards: -1.502221001007932, best rewards:160.62159211480667

argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
