nohup: ignoring input
pybullet build time: May 20 2022 19:44:17
Iteration 1 took 4.60 seconds (mean sampled reward: -913.36). Current reward after update: -651.56, Optimal reward -651.56
Iteration 2 took 4.55 seconds (mean sampled reward: -904.07). Current reward after update: -716.48, Optimal reward -651.56
Iteration 3 took 4.67 seconds (mean sampled reward: -858.21). Current reward after update: -626.80, Optimal reward -626.80
Iteration 4 took 4.57 seconds (mean sampled reward: -866.80). Current reward after update: -695.28, Optimal reward -626.80
Iteration 5 took 4.53 seconds (mean sampled reward: -956.20). Current reward after update: -382.49, Optimal reward -382.49
Iteration 6 took 4.52 seconds (mean sampled reward: -861.64). Current reward after update: -489.25, Optimal reward -382.49
Iteration 7 took 4.53 seconds (mean sampled reward: -869.71). Current reward after update: -571.28, Optimal reward -382.49
Iteration 8 took 4.48 seconds (mean sampled reward: -830.18). Current reward after update: -403.73, Optimal reward -382.49
Iteration 9 took 4.53 seconds (mean sampled reward: -939.74). Current reward after update: -470.22, Optimal reward -382.49
Iteration 10 took 4.46 seconds (mean sampled reward: -908.37). Current reward after update: -393.28, Optimal reward -382.49
Iteration 11 took 4.48 seconds (mean sampled reward: -885.50). Current reward after update: -404.54, Optimal reward -382.49
Iteration 12 took 4.54 seconds (mean sampled reward: -866.69). Current reward after update: -394.99, Optimal reward -382.49
Iteration 13 took 4.71 seconds (mean sampled reward: -894.47). Current reward after update: -340.54, Optimal reward -340.54
Iteration 14 took 4.43 seconds (mean sampled reward: -829.94). Current reward after update: -294.31, Optimal reward -294.31
Iteration 15 took 4.53 seconds (mean sampled reward: -818.62). Current reward after update: -330.12, Optimal reward -294.31
Iteration 16 took 4.44 seconds (mean sampled reward: -828.16). Current reward after update: -309.15, Optimal reward -294.31
Iteration 17 took 4.45 seconds (mean sampled reward: -851.07). Current reward after update: -357.73, Optimal reward -294.31
Iteration 18 took 4.46 seconds (mean sampled reward: -843.07). Current reward after update: -325.67, Optimal reward -294.31
Iteration 19 took 4.47 seconds (mean sampled reward: -837.86). Current reward after update: -306.63, Optimal reward -294.31
Iteration 20 took 4.49 seconds (mean sampled reward: -847.06). Current reward after update: -318.22, Optimal reward -294.31
Iteration 21 took 4.42 seconds (mean sampled reward: -835.70). Current reward after update: -324.62, Optimal reward -294.31
Iteration 22 took 4.52 seconds (mean sampled reward: -846.80). Current reward after update: -324.43, Optimal reward -294.31
Iteration 23 took 4.56 seconds (mean sampled reward: -879.13). Current reward after update: -327.52, Optimal reward -294.31
Iteration 24 took 4.61 seconds (mean sampled reward: -890.84). Current reward after update: -327.72, Optimal reward -294.31
Iteration 25 took 4.54 seconds (mean sampled reward: -861.44). Current reward after update: -322.61, Optimal reward -294.31
Iteration 26 took 4.51 seconds (mean sampled reward: -810.39). Current reward after update: -308.72, Optimal reward -294.31
Iteration 27 took 4.51 seconds (mean sampled reward: -810.84). Current reward after update: -299.64, Optimal reward -294.31
Iteration 28 took 4.50 seconds (mean sampled reward: -903.64). Current reward after update: -316.20, Optimal reward -294.31
Iteration 29 took 4.54 seconds (mean sampled reward: -875.13). Current reward after update: -317.40, Optimal reward -294.31
Iteration 30 took 4.38 seconds (mean sampled reward: -909.64). Current reward after update: -323.39, Optimal reward -294.31
Iteration 31 took 4.39 seconds (mean sampled reward: -905.97). Current reward after update: -319.17, Optimal reward -294.31
Iteration 32 took 4.50 seconds (mean sampled reward: -958.65). Current reward after update: -330.85, Optimal reward -294.31
Iteration 33 took 4.46 seconds (mean sampled reward: -873.93). Current reward after update: -377.59, Optimal reward -294.31
Iteration 34 took 4.48 seconds (mean sampled reward: -896.17). Current reward after update: -347.24, Optimal reward -294.31
Iteration 35 took 4.44 seconds (mean sampled reward: -955.42). Current reward after update: -385.83, Optimal reward -294.31
Iteration 36 took 4.49 seconds (mean sampled reward: -932.85). Current reward after update: -362.56, Optimal reward -294.31
Iteration 37 took 4.47 seconds (mean sampled reward: -855.71). Current reward after update: -365.24, Optimal reward -294.31
Iteration 38 took 4.49 seconds (mean sampled reward: -843.87). Current reward after update: -343.02, Optimal reward -294.31
Iteration 39 took 4.50 seconds (mean sampled reward: -846.14). Current reward after update: -289.24, Optimal reward -289.24
Iteration 40 took 4.57 seconds (mean sampled reward: -855.64). Current reward after update: -325.41, Optimal reward -289.24
Iteration 41 took 4.49 seconds (mean sampled reward: -873.22). Current reward after update: -348.35, Optimal reward -289.24
Iteration 42 took 4.56 seconds (mean sampled reward: -862.48). Current reward after update: -362.92, Optimal reward -289.24
Iteration 43 took 4.48 seconds (mean sampled reward: -853.91). Current reward after update: -371.69, Optimal reward -289.24
Iteration 44 took 4.55 seconds (mean sampled reward: -850.33). Current reward after update: -355.49, Optimal reward -289.24
Iteration 45 took 4.44 seconds (mean sampled reward: -860.81). Current reward after update: -342.02, Optimal reward -289.24
Iteration 46 took 4.50 seconds (mean sampled reward: -832.37). Current reward after update: -305.42, Optimal reward -289.24
Iteration 47 took 4.47 seconds (mean sampled reward: -799.91). Current reward after update: -286.53, Optimal reward -286.53
Iteration 48 took 4.45 seconds (mean sampled reward: -797.51). Current reward after update: -289.61, Optimal reward -286.53
Iteration 49 took 4.56 seconds (mean sampled reward: -795.00). Current reward after update: -275.42, Optimal reward -275.42
Iteration 50 took 4.46 seconds (mean sampled reward: -866.03). Current reward after update: -315.48, Optimal reward -275.42
Iteration 51 took 4.50 seconds (mean sampled reward: -804.46). Current reward after update: -290.71, Optimal reward -275.42
Iteration 52 took 4.55 seconds (mean sampled reward: -802.50). Current reward after update: -267.61, Optimal reward -267.61
Iteration 53 took 4.54 seconds (mean sampled reward: -832.82). Current reward after update: -288.37, Optimal reward -267.61
Iteration 54 took 4.53 seconds (mean sampled reward: -820.34). Current reward after update: -280.07, Optimal reward -267.61
Iteration 55 took 4.54 seconds (mean sampled reward: -796.31). Current reward after update: -263.91, Optimal reward -263.91
Iteration 56 took 4.37 seconds (mean sampled reward: -804.13). Current reward after update: -260.00, Optimal reward -260.00
Iteration 57 took 4.49 seconds (mean sampled reward: -791.46). Current reward after update: -309.33, Optimal reward -260.00
Iteration 58 took 4.45 seconds (mean sampled reward: -793.25). Current reward after update: -278.46, Optimal reward -260.00
Iteration 59 took 4.51 seconds (mean sampled reward: -793.16). Current reward after update: -265.00, Optimal reward -260.00
Iteration 60 took 4.49 seconds (mean sampled reward: -790.92). Current reward after update: -257.73, Optimal reward -257.73
Iteration 61 took 4.40 seconds (mean sampled reward: -871.80). Current reward after update: -277.81, Optimal reward -257.73
Iteration 62 took 4.50 seconds (mean sampled reward: -883.21). Current reward after update: -286.14, Optimal reward -257.73
Iteration 63 took 4.53 seconds (mean sampled reward: -803.33). Current reward after update: -261.37, Optimal reward -257.73
Iteration 64 took 4.49 seconds (mean sampled reward: -902.73). Current reward after update: -268.01, Optimal reward -257.73
Iteration 65 took 4.56 seconds (mean sampled reward: -953.84). Current reward after update: -273.12, Optimal reward -257.73
Iteration 66 took 4.53 seconds (mean sampled reward: -877.48). Current reward after update: -247.94, Optimal reward -247.94
Iteration 67 took 4.44 seconds (mean sampled reward: -876.86). Current reward after update: -265.53, Optimal reward -247.94
Iteration 68 took 4.43 seconds (mean sampled reward: -902.61). Current reward after update: -265.27, Optimal reward -247.94
Iteration 69 took 4.36 seconds (mean sampled reward: -848.37). Current reward after update: -260.70, Optimal reward -247.94
Iteration 70 took 4.50 seconds (mean sampled reward: -813.04). Current reward after update: -249.06, Optimal reward -247.94
Iteration 71 took 4.46 seconds (mean sampled reward: -948.25). Current reward after update: -304.05, Optimal reward -247.94
Iteration 72 took 4.52 seconds (mean sampled reward: -860.54). Current reward after update: -265.00, Optimal reward -247.94
Iteration 73 took 4.39 seconds (mean sampled reward: -894.67). Current reward after update: -667.95, Optimal reward -247.94
Iteration 74 took 4.46 seconds (mean sampled reward: -894.46). Current reward after update: -279.48, Optimal reward -247.94
Iteration 75 took 4.52 seconds (mean sampled reward: -825.40). Current reward after update: -268.67, Optimal reward -247.94
Iteration 76 took 4.49 seconds (mean sampled reward: -780.20). Current reward after update: -271.17, Optimal reward -247.94
Iteration 77 took 4.51 seconds (mean sampled reward: -778.02). Current reward after update: -259.64, Optimal reward -247.94
Iteration 78 took 4.44 seconds (mean sampled reward: -795.23). Current reward after update: -266.08, Optimal reward -247.94
Iteration 79 took 4.50 seconds (mean sampled reward: -809.75). Current reward after update: -266.59, Optimal reward -247.94
Iteration 80 took 4.59 seconds (mean sampled reward: -841.72). Current reward after update: -292.61, Optimal reward -247.94
Iteration 81 took 4.44 seconds (mean sampled reward: -842.00). Current reward after update: -258.66, Optimal reward -247.94
Iteration 82 took 4.59 seconds (mean sampled reward: -803.05). Current reward after update: -273.38, Optimal reward -247.94
Iteration 83 took 4.61 seconds (mean sampled reward: -805.39). Current reward after update: -254.00, Optimal reward -247.94
Iteration 84 took 4.52 seconds (mean sampled reward: -807.33). Current reward after update: -247.81, Optimal reward -247.81
Iteration 85 took 4.58 seconds (mean sampled reward: -803.69). Current reward after update: -269.88, Optimal reward -247.81
Iteration 86 took 4.60 seconds (mean sampled reward: -811.92). Current reward after update: -274.19, Optimal reward -247.81
Iteration 87 took 4.65 seconds (mean sampled reward: -815.97). Current reward after update: -280.36, Optimal reward -247.81
Iteration 88 took 4.54 seconds (mean sampled reward: -806.24). Current reward after update: -273.56, Optimal reward -247.81
Iteration 89 took 4.47 seconds (mean sampled reward: -813.96). Current reward after update: -247.64, Optimal reward -247.64
Iteration 90 took 4.52 seconds (mean sampled reward: -818.37). Current reward after update: -262.37, Optimal reward -247.64
Iteration 91 took 4.50 seconds (mean sampled reward: -844.19). Current reward after update: -294.59, Optimal reward -247.64
Iteration 92 took 4.52 seconds (mean sampled reward: -821.29). Current reward after update: -286.58, Optimal reward -247.64
Iteration 93 took 4.57 seconds (mean sampled reward: -859.02). Current reward after update: -302.66, Optimal reward -247.64
Iteration 94 took 4.51 seconds (mean sampled reward: -843.77). Current reward after update: -292.34, Optimal reward -247.64
Iteration 95 took 4.59 seconds (mean sampled reward: -864.41). Current reward after update: -280.54, Optimal reward -247.64
Iteration 96 took 4.47 seconds (mean sampled reward: -802.30). Current reward after update: -284.13, Optimal reward -247.64
Iteration 97 took 4.47 seconds (mean sampled reward: -818.62). Current reward after update: -284.64, Optimal reward -247.64
Iteration 98 took 4.52 seconds (mean sampled reward: -841.79). Current reward after update: -305.87, Optimal reward -247.64
Iteration 99 took 4.43 seconds (mean sampled reward: -797.73). Current reward after update: -305.94, Optimal reward -247.64
Iteration 100 took 4.56 seconds (mean sampled reward: -859.21). Current reward after update: -297.57, Optimal reward -247.64
Iteration 101 took 4.55 seconds (mean sampled reward: -873.61). Current reward after update: -316.09, Optimal reward -247.64
Iteration 102 took 4.46 seconds (mean sampled reward: -832.23). Current reward after update: -294.57, Optimal reward -247.64
Iteration 103 took 4.47 seconds (mean sampled reward: -856.02). Current reward after update: -302.34, Optimal reward -247.64
Iteration 104 took 4.56 seconds (mean sampled reward: -874.98). Current reward after update: -309.85, Optimal reward -247.64
Iteration 105 took 4.58 seconds (mean sampled reward: -834.14). Current reward after update: -282.52, Optimal reward -247.64
Iteration 106 took 4.54 seconds (mean sampled reward: -852.66). Current reward after update: -291.64, Optimal reward -247.64
Iteration 107 took 4.54 seconds (mean sampled reward: -851.61). Current reward after update: -272.53, Optimal reward -247.64
Iteration 108 took 4.52 seconds (mean sampled reward: -821.35). Current reward after update: -276.82, Optimal reward -247.64
Iteration 109 took 4.56 seconds (mean sampled reward: -863.70). Current reward after update: -284.44, Optimal reward -247.64
Iteration 110 took 4.51 seconds (mean sampled reward: -820.96). Current reward after update: -275.57, Optimal reward -247.64
Iteration 111 took 4.57 seconds (mean sampled reward: -866.84). Current reward after update: -287.60, Optimal reward -247.64
Iteration 112 took 4.60 seconds (mean sampled reward: -826.84). Current reward after update: -271.26, Optimal reward -247.64
Iteration 113 took 4.57 seconds (mean sampled reward: -855.65). Current reward after update: -264.72, Optimal reward -247.64
Iteration 114 took 4.57 seconds (mean sampled reward: -843.27). Current reward after update: -260.00, Optimal reward -247.64
Iteration 115 took 4.56 seconds (mean sampled reward: -847.40). Current reward after update: -287.24, Optimal reward -247.64
Iteration 116 took 4.47 seconds (mean sampled reward: -806.15). Current reward after update: -246.69, Optimal reward -246.69
Iteration 117 took 4.43 seconds (mean sampled reward: -832.00). Current reward after update: -280.35, Optimal reward -246.69
Iteration 118 took 4.52 seconds (mean sampled reward: -857.31). Current reward after update: -300.07, Optimal reward -246.69
Iteration 119 took 4.43 seconds (mean sampled reward: -805.77). Current reward after update: -251.23, Optimal reward -246.69
Iteration 120 took 4.48 seconds (mean sampled reward: -791.41). Current reward after update: -590.41, Optimal reward -246.69
Iteration 121 took 4.43 seconds (mean sampled reward: -801.99). Current reward after update: -249.29, Optimal reward -246.69
Iteration 122 took 4.48 seconds (mean sampled reward: -769.60). Current reward after update: -257.99, Optimal reward -246.69
Iteration 123 took 4.41 seconds (mean sampled reward: -797.40). Current reward after update: -267.39, Optimal reward -246.69
Iteration 124 took 4.59 seconds (mean sampled reward: -834.13). Current reward after update: -279.78, Optimal reward -246.69
Iteration 125 took 4.48 seconds (mean sampled reward: -797.74). Current reward after update: -274.07, Optimal reward -246.69
Iteration 126 took 4.47 seconds (mean sampled reward: -835.41). Current reward after update: -273.50, Optimal reward -246.69
Iteration 127 took 4.59 seconds (mean sampled reward: -852.87). Current reward after update: -311.20, Optimal reward -246.69
Iteration 128 took 4.56 seconds (mean sampled reward: -861.46). Current reward after update: -276.50, Optimal reward -246.69
Iteration 129 took 4.45 seconds (mean sampled reward: -814.70). Current reward after update: -274.20, Optimal reward -246.69
Iteration 130 took 4.46 seconds (mean sampled reward: -855.53). Current reward after update: -271.46, Optimal reward -246.69
Iteration 131 took 4.45 seconds (mean sampled reward: -869.53). Current reward after update: -275.94, Optimal reward -246.69
Iteration 132 took 4.43 seconds (mean sampled reward: -839.97). Current reward after update: -257.81, Optimal reward -246.69
Iteration 133 took 4.46 seconds (mean sampled reward: -785.47). Current reward after update: -246.98, Optimal reward -246.69
Iteration 134 took 4.50 seconds (mean sampled reward: -825.66). Current reward after update: -253.68, Optimal reward -246.69
Iteration 135 took 4.49 seconds (mean sampled reward: -832.25). Current reward after update: -255.12, Optimal reward -246.69
Iteration 136 took 4.46 seconds (mean sampled reward: -823.94). Current reward after update: -255.82, Optimal reward -246.69
Iteration 137 took 4.44 seconds (mean sampled reward: -849.34). Current reward after update: -275.21, Optimal reward -246.69
Iteration 138 took 4.36 seconds (mean sampled reward: -847.01). Current reward after update: -246.31, Optimal reward -246.31
Iteration 139 took 4.47 seconds (mean sampled reward: -805.74). Current reward after update: -243.69, Optimal reward -243.69
Iteration 140 took 4.52 seconds (mean sampled reward: -799.99). Current reward after update: -264.66, Optimal reward -243.69
Iteration 141 took 4.49 seconds (mean sampled reward: -839.49). Current reward after update: -260.62, Optimal reward -243.69
Iteration 142 took 4.46 seconds (mean sampled reward: -848.29). Current reward after update: -226.13, Optimal reward -226.13
Iteration 143 took 4.43 seconds (mean sampled reward: -906.23). Current reward after update: -274.03, Optimal reward -226.13
Iteration 144 took 4.52 seconds (mean sampled reward: -843.06). Current reward after update: -274.92, Optimal reward -226.13
Iteration 145 took 4.46 seconds (mean sampled reward: -809.49). Current reward after update: -258.05, Optimal reward -226.13
Iteration 146 took 4.47 seconds (mean sampled reward: -835.83). Current reward after update: -280.71, Optimal reward -226.13
Iteration 147 took 4.48 seconds (mean sampled reward: -831.17). Current reward after update: -249.58, Optimal reward -226.13
Iteration 148 took 4.52 seconds (mean sampled reward: -887.97). Current reward after update: -222.28, Optimal reward -222.28
Iteration 149 took 4.51 seconds (mean sampled reward: -854.05). Current reward after update: -282.48, Optimal reward -222.28
Iteration 150 took 4.50 seconds (mean sampled reward: -836.28). Current reward after update: -294.26, Optimal reward -222.28
Iteration 151 took 4.51 seconds (mean sampled reward: -854.22). Current reward after update: -286.35, Optimal reward -222.28
Iteration 152 took 4.53 seconds (mean sampled reward: -866.29). Current reward after update: -282.26, Optimal reward -222.28
Iteration 153 took 4.49 seconds (mean sampled reward: -831.54). Current reward after update: -282.10, Optimal reward -222.28
Iteration 154 took 4.46 seconds (mean sampled reward: -813.29). Current reward after update: -259.73, Optimal reward -222.28
Iteration 155 took 4.45 seconds (mean sampled reward: -791.14). Current reward after update: -271.10, Optimal reward -222.28
Iteration 156 took 4.47 seconds (mean sampled reward: -776.78). Current reward after update: -287.98, Optimal reward -222.28
Iteration 157 took 4.49 seconds (mean sampled reward: -786.05). Current reward after update: -265.72, Optimal reward -222.28
Iteration 158 took 4.49 seconds (mean sampled reward: -786.00). Current reward after update: -268.00, Optimal reward -222.28
Iteration 159 took 4.42 seconds (mean sampled reward: -793.42). Current reward after update: -278.72, Optimal reward -222.28
Iteration 160 took 4.56 seconds (mean sampled reward: -790.89). Current reward after update: -276.86, Optimal reward -222.28
Iteration 161 took 4.52 seconds (mean sampled reward: -784.28). Current reward after update: -273.70, Optimal reward -222.28
Iteration 162 took 4.56 seconds (mean sampled reward: -827.11). Current reward after update: -290.16, Optimal reward -222.28
Iteration 163 took 4.55 seconds (mean sampled reward: -793.30). Current reward after update: -276.57, Optimal reward -222.28
Iteration 164 took 4.58 seconds (mean sampled reward: -811.53). Current reward after update: -268.68, Optimal reward -222.28
Iteration 165 took 4.63 seconds (mean sampled reward: -803.86). Current reward after update: -277.78, Optimal reward -222.28
Iteration 166 took 4.51 seconds (mean sampled reward: -791.92). Current reward after update: -272.99, Optimal reward -222.28
Iteration 167 took 4.62 seconds (mean sampled reward: -786.49). Current reward after update: -270.32, Optimal reward -222.28
Iteration 168 took 4.55 seconds (mean sampled reward: -776.87). Current reward after update: -263.00, Optimal reward -222.28
Iteration 169 took 4.64 seconds (mean sampled reward: -793.50). Current reward after update: -278.80, Optimal reward -222.28
Iteration 170 took 4.55 seconds (mean sampled reward: -788.72). Current reward after update: -273.15, Optimal reward -222.28
Iteration 171 took 4.58 seconds (mean sampled reward: -807.61). Current reward after update: -280.84, Optimal reward -222.28
Iteration 172 took 4.52 seconds (mean sampled reward: -791.06). Current reward after update: -277.86, Optimal reward -222.28
Iteration 173 took 4.65 seconds (mean sampled reward: -800.30). Current reward after update: -272.32, Optimal reward -222.28
Iteration 174 took 4.60 seconds (mean sampled reward: -798.61). Current reward after update: -262.80, Optimal reward -222.28
Iteration 175 took 4.58 seconds (mean sampled reward: -797.61). Current reward after update: -255.61, Optimal reward -222.28
Iteration 176 took 4.63 seconds (mean sampled reward: -802.27). Current reward after update: -269.03, Optimal reward -222.28
Iteration 177 took 4.66 seconds (mean sampled reward: -816.71). Current reward after update: -263.14, Optimal reward -222.28
Iteration 178 took 4.61 seconds (mean sampled reward: -795.79). Current reward after update: -256.23, Optimal reward -222.28
Iteration 179 took 4.63 seconds (mean sampled reward: -847.93). Current reward after update: -290.30, Optimal reward -222.28
Iteration 180 took 4.58 seconds (mean sampled reward: -825.45). Current reward after update: -268.57, Optimal reward -222.28
Iteration 181 took 4.50 seconds (mean sampled reward: -793.68). Current reward after update: -278.13, Optimal reward -222.28
Iteration 182 took 4.55 seconds (mean sampled reward: -789.37). Current reward after update: -269.56, Optimal reward -222.28
Iteration 183 took 4.57 seconds (mean sampled reward: -801.17). Current reward after update: -288.41, Optimal reward -222.28
Iteration 184 took 4.61 seconds (mean sampled reward: -799.00). Current reward after update: -274.24, Optimal reward -222.28
Iteration 185 took 4.52 seconds (mean sampled reward: -801.22). Current reward after update: -277.19, Optimal reward -222.28
Iteration 186 took 4.55 seconds (mean sampled reward: -804.50). Current reward after update: -273.67, Optimal reward -222.28
Iteration 187 took 4.60 seconds (mean sampled reward: -771.67). Current reward after update: -316.20, Optimal reward -222.28
Iteration 188 took 4.60 seconds (mean sampled reward: -780.45). Current reward after update: -277.85, Optimal reward -222.28
Iteration 189 took 4.54 seconds (mean sampled reward: -795.27). Current reward after update: -276.08, Optimal reward -222.28
Iteration 190 took 4.48 seconds (mean sampled reward: -777.02). Current reward after update: -275.24, Optimal reward -222.28
Iteration 191 took 4.57 seconds (mean sampled reward: -811.83). Current reward after update: -280.72, Optimal reward -222.28
Iteration 192 took 4.51 seconds (mean sampled reward: -780.78). Current reward after update: -267.84, Optimal reward -222.28
Iteration 193 took 4.53 seconds (mean sampled reward: -769.40). Current reward after update: -273.83, Optimal reward -222.28
Iteration 194 took 4.57 seconds (mean sampled reward: -777.83). Current reward after update: -262.46, Optimal reward -222.28
Iteration 195 took 4.62 seconds (mean sampled reward: -762.69). Current reward after update: -279.66, Optimal reward -222.28
Iteration 196 took 4.55 seconds (mean sampled reward: -762.34). Current reward after update: -281.61, Optimal reward -222.28
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
Iteration 197 took 4.54 seconds (mean sampled reward: -761.94). Current reward after update: -257.43, Optimal reward -222.28
Iteration 198 took 4.56 seconds (mean sampled reward: -763.26). Current reward after update: -270.89, Optimal reward -222.28
Iteration 199 took 4.51 seconds (mean sampled reward: -779.70). Current reward after update: -301.63, Optimal reward -222.28
Iteration 200 took 4.48 seconds (mean sampled reward: -810.26). Current reward after update: -263.48, Optimal reward -222.28
/home/sirius/anaconda3/envs/nimble_dev/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
Iteration 1 took 4.53 seconds (mean sampled reward: -920.47). Current reward after update: -705.77, Optimal reward -705.77
Iteration 2 took 4.58 seconds (mean sampled reward: -877.10). Current reward after update: -687.07, Optimal reward -687.07
Iteration 3 took 4.41 seconds (mean sampled reward: -895.08). Current reward after update: -702.73, Optimal reward -687.07
Iteration 4 took 4.56 seconds (mean sampled reward: -820.47). Current reward after update: -678.79, Optimal reward -678.79
Iteration 5 took 4.62 seconds (mean sampled reward: -821.66). Current reward after update: -688.04, Optimal reward -678.79
Iteration 6 took 4.60 seconds (mean sampled reward: -818.09). Current reward after update: -672.10, Optimal reward -672.10
Iteration 7 took 4.62 seconds (mean sampled reward: -791.76). Current reward after update: -654.06, Optimal reward -654.06
Iteration 8 took 4.61 seconds (mean sampled reward: -804.73). Current reward after update: -630.26, Optimal reward -630.26
Iteration 9 took 4.54 seconds (mean sampled reward: -832.35). Current reward after update: -626.35, Optimal reward -626.35
Iteration 10 took 4.49 seconds (mean sampled reward: -777.23). Current reward after update: -532.90, Optimal reward -532.90
Iteration 11 took 4.51 seconds (mean sampled reward: -860.55). Current reward after update: -587.22, Optimal reward -532.90
Iteration 12 took 4.53 seconds (mean sampled reward: -858.38). Current reward after update: -501.90, Optimal reward -501.90
Iteration 13 took 4.56 seconds (mean sampled reward: -867.46). Current reward after update: -569.16, Optimal reward -501.90
Iteration 14 took 4.58 seconds (mean sampled reward: -934.29). Current reward after update: -494.52, Optimal reward -494.52
Iteration 15 took 4.58 seconds (mean sampled reward: -915.70). Current reward after update: -547.21, Optimal reward -494.52
Iteration 16 took 4.65 seconds (mean sampled reward: -820.15). Current reward after update: -439.20, Optimal reward -439.20
Iteration 17 took 4.59 seconds (mean sampled reward: -868.84). Current reward after update: -485.14, Optimal reward -439.20
Iteration 18 took 4.57 seconds (mean sampled reward: -912.37). Current reward after update: -552.14, Optimal reward -439.20
Iteration 19 took 4.62 seconds (mean sampled reward: -879.14). Current reward after update: -517.54, Optimal reward -439.20
Iteration 20 took 4.53 seconds (mean sampled reward: -872.06). Current reward after update: -516.48, Optimal reward -439.20
Iteration 21 took 4.62 seconds (mean sampled reward: -812.33). Current reward after update: -589.62, Optimal reward -439.20
Iteration 22 took 4.59 seconds (mean sampled reward: -923.34). Current reward after update: -527.04, Optimal reward -439.20
Iteration 23 took 4.63 seconds (mean sampled reward: -891.48). Current reward after update: -534.96, Optimal reward -439.20
Iteration 24 took 4.65 seconds (mean sampled reward: -894.19). Current reward after update: -541.14, Optimal reward -439.20
Iteration 25 took 4.62 seconds (mean sampled reward: -831.45). Current reward after update: -593.73, Optimal reward -439.20
Iteration 26 took 4.57 seconds (mean sampled reward: -869.52). Current reward after update: -517.70, Optimal reward -439.20
Iteration 27 took 4.48 seconds (mean sampled reward: -856.27). Current reward after update: -517.31, Optimal reward -439.20
Iteration 28 took 4.58 seconds (mean sampled reward: -825.38). Current reward after update: -509.57, Optimal reward -439.20
Iteration 29 took 4.54 seconds (mean sampled reward: -792.78). Current reward after update: -545.94, Optimal reward -439.20
Iteration 30 took 4.67 seconds (mean sampled reward: -750.22). Current reward after update: -513.47, Optimal reward -439.20
Iteration 31 took 4.52 seconds (mean sampled reward: -765.36). Current reward after update: -516.38, Optimal reward -439.20
Iteration 32 took 4.57 seconds (mean sampled reward: -796.00). Current reward after update: -491.44, Optimal reward -439.20
Iteration 33 took 4.62 seconds (mean sampled reward: -744.41). Current reward after update: -533.23, Optimal reward -439.20
Iteration 34 took 4.49 seconds (mean sampled reward: -758.02). Current reward after update: -567.13, Optimal reward -439.20
Iteration 35 took 4.45 seconds (mean sampled reward: -748.49). Current reward after update: -512.86, Optimal reward -439.20
Iteration 36 took 4.54 seconds (mean sampled reward: -805.71). Current reward after update: -530.25, Optimal reward -439.20
Iteration 37 took 4.51 seconds (mean sampled reward: -767.92). Current reward after update: -509.76, Optimal reward -439.20
Iteration 38 took 4.53 seconds (mean sampled reward: -777.87). Current reward after update: -492.56, Optimal reward -439.20
Iteration 39 took 4.53 seconds (mean sampled reward: -801.70). Current reward after update: -471.30, Optimal reward -439.20
Iteration 40 took 4.62 seconds (mean sampled reward: -823.52). Current reward after update: -476.98, Optimal reward -439.20
Iteration 41 took 4.55 seconds (mean sampled reward: -813.00). Current reward after update: -491.70, Optimal reward -439.20
Iteration 42 took 4.65 seconds (mean sampled reward: -743.61). Current reward after update: -539.34, Optimal reward -439.20
Iteration 43 took 4.60 seconds (mean sampled reward: -798.64). Current reward after update: -511.68, Optimal reward -439.20
Iteration 44 took 4.77 seconds (mean sampled reward: -773.87). Current reward after update: -497.80, Optimal reward -439.20
Iteration 45 took 4.61 seconds (mean sampled reward: -772.40). Current reward after update: -480.25, Optimal reward -439.20
Iteration 46 took 4.68 seconds (mean sampled reward: -775.26). Current reward after update: -497.22, Optimal reward -439.20
Iteration 47 took 4.62 seconds (mean sampled reward: -805.37). Current reward after update: -462.54, Optimal reward -439.20
Iteration 48 took 4.70 seconds (mean sampled reward: -768.16). Current reward after update: -492.10, Optimal reward -439.20
Iteration 49 took 4.61 seconds (mean sampled reward: -798.90). Current reward after update: -492.25, Optimal reward -439.20
Iteration 50 took 4.65 seconds (mean sampled reward: -807.22). Current reward after update: -470.71, Optimal reward -439.20
Iteration 51 took 4.63 seconds (mean sampled reward: -781.90). Current reward after update: -505.53, Optimal reward -439.20
Iteration 52 took 4.64 seconds (mean sampled reward: -759.64). Current reward after update: -470.23, Optimal reward -439.20
Iteration 53 took 4.60 seconds (mean sampled reward: -748.00). Current reward after update: -519.20, Optimal reward -439.20
Iteration 54 took 4.66 seconds (mean sampled reward: -759.91). Current reward after update: -501.48, Optimal reward -439.20
Iteration 55 took 4.66 seconds (mean sampled reward: -756.20). Current reward after update: -508.12, Optimal reward -439.20
Iteration 56 took 4.64 seconds (mean sampled reward: -767.57). Current reward after update: -507.11, Optimal reward -439.20
Iteration 57 took 4.69 seconds (mean sampled reward: -772.90). Current reward after update: -517.07, Optimal reward -439.20
Iteration 58 took 4.65 seconds (mean sampled reward: -766.76). Current reward after update: -473.11, Optimal reward -439.20
Iteration 59 took 4.68 seconds (mean sampled reward: -754.75). Current reward after update: -508.51, Optimal reward -439.20
Iteration 60 took 4.61 seconds (mean sampled reward: -778.40). Current reward after update: -512.42, Optimal reward -439.20
Iteration 61 took 4.66 seconds (mean sampled reward: -822.05). Current reward after update: -542.72, Optimal reward -439.20
Iteration 62 took 4.58 seconds (mean sampled reward: -805.63). Current reward after update: -524.50, Optimal reward -439.20
Iteration 63 took 4.58 seconds (mean sampled reward: -756.40). Current reward after update: -516.52, Optimal reward -439.20
Iteration 64 took 4.60 seconds (mean sampled reward: -735.21). Current reward after update: -491.27, Optimal reward -439.20
Iteration 65 took 4.62 seconds (mean sampled reward: -753.44). Current reward after update: -491.52, Optimal reward -439.20
Iteration 66 took 4.65 seconds (mean sampled reward: -747.59). Current reward after update: -505.02, Optimal reward -439.20
Iteration 67 took 4.70 seconds (mean sampled reward: -752.72). Current reward after update: -477.99, Optimal reward -439.20
Iteration 68 took 4.56 seconds (mean sampled reward: -745.01). Current reward after update: -518.24, Optimal reward -439.20
Iteration 69 took 4.66 seconds (mean sampled reward: -753.78). Current reward after update: -518.03, Optimal reward -439.20
Iteration 70 took 4.65 seconds (mean sampled reward: -752.52). Current reward after update: -500.05, Optimal reward -439.20
Iteration 71 took 4.51 seconds (mean sampled reward: -771.94). Current reward after update: -518.16, Optimal reward -439.20
Iteration 72 took 4.57 seconds (mean sampled reward: -796.47). Current reward after update: -512.21, Optimal reward -439.20
Iteration 73 took 4.52 seconds (mean sampled reward: -794.30). Current reward after update: -546.25, Optimal reward -439.20
Iteration 74 took 4.54 seconds (mean sampled reward: -786.27). Current reward after update: -517.91, Optimal reward -439.20
Iteration 75 took 4.57 seconds (mean sampled reward: -790.12). Current reward after update: -532.01, Optimal reward -439.20
Iteration 76 took 4.59 seconds (mean sampled reward: -773.52). Current reward after update: -481.72, Optimal reward -439.20
Iteration 77 took 4.52 seconds (mean sampled reward: -830.80). Current reward after update: -506.10, Optimal reward -439.20
Iteration 78 took 4.53 seconds (mean sampled reward: -795.08). Current reward after update: -593.01, Optimal reward -439.20
Iteration 79 took 4.60 seconds (mean sampled reward: -796.69). Current reward after update: -487.61, Optimal reward -439.20
Iteration 80 took 4.54 seconds (mean sampled reward: -789.56). Current reward after update: -591.78, Optimal reward -439.20
Iteration 81 took 4.54 seconds (mean sampled reward: -779.12). Current reward after update: -505.60, Optimal reward -439.20
Iteration 82 took 4.52 seconds (mean sampled reward: -751.81). Current reward after update: -531.45, Optimal reward -439.20
Iteration 83 took 4.57 seconds (mean sampled reward: -755.89). Current reward after update: -542.40, Optimal reward -439.20
Iteration 84 took 4.56 seconds (mean sampled reward: -759.92). Current reward after update: -533.50, Optimal reward -439.20
Iteration 85 took 4.54 seconds (mean sampled reward: -762.90). Current reward after update: -516.24, Optimal reward -439.20
Iteration 86 took 4.60 seconds (mean sampled reward: -747.24). Current reward after update: -509.58, Optimal reward -439.20
Iteration 87 took 4.63 seconds (mean sampled reward: -749.57). Current reward after update: -516.09, Optimal reward -439.20
Iteration 88 took 4.68 seconds (mean sampled reward: -751.33). Current reward after update: -520.36, Optimal reward -439.20
Iteration 89 took 4.74 seconds (mean sampled reward: -747.43). Current reward after update: -513.49, Optimal reward -439.20
Iteration 90 took 4.48 seconds (mean sampled reward: -750.41). Current reward after update: -552.93, Optimal reward -439.20
Iteration 91 took 4.60 seconds (mean sampled reward: -755.86). Current reward after update: -578.69, Optimal reward -439.20
Iteration 92 took 4.50 seconds (mean sampled reward: -749.74). Current reward after update: -536.24, Optimal reward -439.20
Iteration 93 took 4.51 seconds (mean sampled reward: -754.03). Current reward after update: -493.85, Optimal reward -439.20
Iteration 94 took 4.56 seconds (mean sampled reward: -746.27). Current reward after update: -526.28, Optimal reward -439.20
Iteration 95 took 4.56 seconds (mean sampled reward: -737.21). Current reward after update: -525.60, Optimal reward -439.20
Iteration 96 took 4.59 seconds (mean sampled reward: -739.67). Current reward after update: -517.99, Optimal reward -439.20
Iteration 97 took 4.65 seconds (mean sampled reward: -749.62). Current reward after update: -536.41, Optimal reward -439.20
Iteration 98 took 4.58 seconds (mean sampled reward: -744.17). Current reward after update: -533.86, Optimal reward -439.20
Iteration 99 took 4.65 seconds (mean sampled reward: -771.72). Current reward after update: -575.84, Optimal reward -439.20
Iteration 100 took 4.52 seconds (mean sampled reward: -756.82). Current reward after update: -549.07, Optimal reward -439.20
Iteration 101 took 4.47 seconds (mean sampled reward: -739.72). Current reward after update: -545.71, Optimal reward -439.20
Iteration 102 took 4.56 seconds (mean sampled reward: -741.05). Current reward after update: -535.88, Optimal reward -439.20
Iteration 103 took 4.55 seconds (mean sampled reward: -747.12). Current reward after update: -508.75, Optimal reward -439.20
Iteration 104 took 4.54 seconds (mean sampled reward: -754.88). Current reward after update: -513.03, Optimal reward -439.20
Iteration 105 took 4.52 seconds (mean sampled reward: -757.93). Current reward after update: -539.38, Optimal reward -439.20
Iteration 106 took 4.62 seconds (mean sampled reward: -740.70). Current reward after update: -524.02, Optimal reward -439.20
Iteration 107 took 4.60 seconds (mean sampled reward: -738.76). Current reward after update: -516.35, Optimal reward -439.20
Iteration 108 took 4.56 seconds (mean sampled reward: -733.44). Current reward after update: -522.47, Optimal reward -439.20
Iteration 109 took 4.58 seconds (mean sampled reward: -737.61). Current reward after update: -513.55, Optimal reward -439.20
Iteration 110 took 4.67 seconds (mean sampled reward: -746.38). Current reward after update: -518.31, Optimal reward -439.20
Iteration 111 took 4.63 seconds (mean sampled reward: -750.66). Current reward after update: -505.44, Optimal reward -439.20
Iteration 112 took 4.62 seconds (mean sampled reward: -741.08). Current reward after update: -500.90, Optimal reward -439.20
Iteration 113 took 4.62 seconds (mean sampled reward: -743.36). Current reward after update: -519.87, Optimal reward -439.20
Iteration 114 took 4.70 seconds (mean sampled reward: -737.78). Current reward after update: -517.54, Optimal reward -439.20
Iteration 115 took 4.76 seconds (mean sampled reward: -743.61). Current reward after update: -549.51, Optimal reward -439.20
Iteration 116 took 4.78 seconds (mean sampled reward: -738.29). Current reward after update: -541.90, Optimal reward -439.20
Iteration 117 took 4.93 seconds (mean sampled reward: -737.59). Current reward after update: -523.62, Optimal reward -439.20
Iteration 118 took 4.82 seconds (mean sampled reward: -737.43). Current reward after update: -526.30, Optimal reward -439.20
Iteration 119 took 4.82 seconds (mean sampled reward: -746.28). Current reward after update: -544.24, Optimal reward -439.20
Iteration 120 took 4.77 seconds (mean sampled reward: -745.27). Current reward after update: -569.56, Optimal reward -439.20
Iteration 121 took 4.74 seconds (mean sampled reward: -738.21). Current reward after update: -533.13, Optimal reward -439.20
Iteration 122 took 4.91 seconds (mean sampled reward: -737.17). Current reward after update: -525.03, Optimal reward -439.20
Iteration 123 took 4.95 seconds (mean sampled reward: -736.14). Current reward after update: -511.69, Optimal reward -439.20
Iteration 124 took 4.85 seconds (mean sampled reward: -733.90). Current reward after update: -517.40, Optimal reward -439.20
Iteration 125 took 4.78 seconds (mean sampled reward: -733.85). Current reward after update: -502.49, Optimal reward -439.20
Iteration 126 took 4.89 seconds (mean sampled reward: -728.35). Current reward after update: -487.43, Optimal reward -439.20
Iteration 127 took 4.84 seconds (mean sampled reward: -718.83). Current reward after update: -683.77, Optimal reward -439.20
Iteration 128 took 4.77 seconds (mean sampled reward: -716.23). Current reward after update: -503.58, Optimal reward -439.20
Iteration 129 took 4.71 seconds (mean sampled reward: -716.35). Current reward after update: -489.90, Optimal reward -439.20
Iteration 130 took 4.67 seconds (mean sampled reward: -717.35). Current reward after update: -494.15, Optimal reward -439.20
Iteration 131 took 4.71 seconds (mean sampled reward: -718.51). Current reward after update: -479.04, Optimal reward -439.20
Iteration 132 took 4.88 seconds (mean sampled reward: -717.27). Current reward after update: -470.17, Optimal reward -439.20
Iteration 133 took 4.88 seconds (mean sampled reward: -725.64). Current reward after update: -503.82, Optimal reward -439.20
Iteration 134 took 4.82 seconds (mean sampled reward: -727.23). Current reward after update: -504.84, Optimal reward -439.20
Iteration 135 took 4.77 seconds (mean sampled reward: -721.49). Current reward after update: -504.91, Optimal reward -439.20
Iteration 136 took 4.81 seconds (mean sampled reward: -726.43). Current reward after update: -501.15, Optimal reward -439.20
Iteration 137 took 4.81 seconds (mean sampled reward: -726.15). Current reward after update: -625.30, Optimal reward -439.20
Iteration 138 took 4.79 seconds (mean sampled reward: -721.54). Current reward after update: -504.41, Optimal reward -439.20
Iteration 139 took 4.70 seconds (mean sampled reward: -720.27). Current reward after update: -493.62, Optimal reward -439.20
Iteration 140 took 4.71 seconds (mean sampled reward: -751.95). Current reward after update: -514.88, Optimal reward -439.20
Iteration 141 took 4.76 seconds (mean sampled reward: -786.04). Current reward after update: -495.63, Optimal reward -439.20
Iteration 142 took 4.76 seconds (mean sampled reward: -759.93). Current reward after update: -495.67, Optimal reward -439.20
Iteration 143 took 4.70 seconds (mean sampled reward: -778.34). Current reward after update: -509.41, Optimal reward -439.20
Iteration 144 took 4.75 seconds (mean sampled reward: -802.84). Current reward after update: -515.75, Optimal reward -439.20
Iteration 145 took 4.79 seconds (mean sampled reward: -776.96). Current reward after update: -480.23, Optimal reward -439.20
Iteration 146 took 4.74 seconds (mean sampled reward: -766.57). Current reward after update: -481.92, Optimal reward -439.20
Iteration 147 took 4.74 seconds (mean sampled reward: -768.28). Current reward after update: -474.43, Optimal reward -439.20
Iteration 148 took 4.75 seconds (mean sampled reward: -753.17). Current reward after update: -518.57, Optimal reward -439.20
Iteration 149 took 4.78 seconds (mean sampled reward: -739.71). Current reward after update: -469.99, Optimal reward -439.20
Iteration 150 took 4.72 seconds (mean sampled reward: -722.02). Current reward after update: -462.26, Optimal reward -439.20
Iteration 151 took 4.71 seconds (mean sampled reward: -716.68). Current reward after update: -466.92, Optimal reward -439.20
Iteration 152 took 4.79 seconds (mean sampled reward: -717.37). Current reward after update: -485.89, Optimal reward -439.20
Iteration 153 took 4.74 seconds (mean sampled reward: -717.05). Current reward after update: -468.81, Optimal reward -439.20
Iteration 154 took 4.76 seconds (mean sampled reward: -734.70). Current reward after update: -493.09, Optimal reward -439.20
Iteration 155 took 4.82 seconds (mean sampled reward: -729.95). Current reward after update: -507.89, Optimal reward -439.20
Iteration 156 took 4.78 seconds (mean sampled reward: -730.59). Current reward after update: -498.91, Optimal reward -439.20
Iteration 157 took 4.73 seconds (mean sampled reward: -724.94). Current reward after update: -474.40, Optimal reward -439.20
Iteration 158 took 4.72 seconds (mean sampled reward: -739.68). Current reward after update: -470.68, Optimal reward -439.20
Iteration 159 took 4.76 seconds (mean sampled reward: -728.31). Current reward after update: -452.60, Optimal reward -439.20
Iteration 160 took 4.76 seconds (mean sampled reward: -731.27). Current reward after update: -482.51, Optimal reward -439.20
Iteration 161 took 4.68 seconds (mean sampled reward: -727.30). Current reward after update: -499.28, Optimal reward -439.20
Iteration 162 took 4.76 seconds (mean sampled reward: -720.17). Current reward after update: -505.43, Optimal reward -439.20
Iteration 163 took 4.72 seconds (mean sampled reward: -716.71). Current reward after update: -500.88, Optimal reward -439.20
Iteration 164 took 4.80 seconds (mean sampled reward: -716.34). Current reward after update: -495.38, Optimal reward -439.20
Iteration 165 took 4.75 seconds (mean sampled reward: -716.02). Current reward after update: -508.94, Optimal reward -439.20
Iteration 166 took 4.73 seconds (mean sampled reward: -773.71). Current reward after update: -496.96, Optimal reward -439.20
Iteration 167 took 4.80 seconds (mean sampled reward: -726.90). Current reward after update: -508.13, Optimal reward -439.20
Iteration 168 took 4.69 seconds (mean sampled reward: -717.96). Current reward after update: -506.24, Optimal reward -439.20
Iteration 169 took 4.69 seconds (mean sampled reward: -720.26). Current reward after update: -676.33, Optimal reward -439.20
Iteration 170 took 4.84 seconds (mean sampled reward: -722.21). Current reward after update: -493.06, Optimal reward -439.20
Iteration 171 took 4.68 seconds (mean sampled reward: -721.41). Current reward after update: -508.82, Optimal reward -439.20
Iteration 172 took 4.82 seconds (mean sampled reward: -717.74). Current reward after update: -508.31, Optimal reward -439.20
Iteration 173 took 4.80 seconds (mean sampled reward: -717.82). Current reward after update: -507.07, Optimal reward -439.20
Iteration 174 took 4.73 seconds (mean sampled reward: -715.04). Current reward after update: -493.53, Optimal reward -439.20
Iteration 175 took 4.73 seconds (mean sampled reward: -718.65). Current reward after update: -485.97, Optimal reward -439.20
Iteration 176 took 4.85 seconds (mean sampled reward: -725.91). Current reward after update: -461.59, Optimal reward -439.20
Iteration 177 took 4.71 seconds (mean sampled reward: -718.48). Current reward after update: -507.66, Optimal reward -439.20
Iteration 178 took 4.83 seconds (mean sampled reward: -725.12). Current reward after update: -511.31, Optimal reward -439.20
Iteration 179 took 4.74 seconds (mean sampled reward: -719.14). Current reward after update: -518.53, Optimal reward -439.20
Iteration 180 took 4.80 seconds (mean sampled reward: -722.35). Current reward after update: -600.79, Optimal reward -439.20
Iteration 181 took 4.79 seconds (mean sampled reward: -720.33). Current reward after update: -471.58, Optimal reward -439.20
Iteration 182 took 4.81 seconds (mean sampled reward: -728.14). Current reward after update: -494.14, Optimal reward -439.20
Iteration 183 took 4.70 seconds (mean sampled reward: -718.73). Current reward after update: -461.09, Optimal reward -439.20
Iteration 184 took 4.76 seconds (mean sampled reward: -716.23). Current reward after update: -473.77, Optimal reward -439.20
Iteration 185 took 4.71 seconds (mean sampled reward: -712.73). Current reward after update: -472.75, Optimal reward -439.20
Iteration 186 took 4.82 seconds (mean sampled reward: -712.84). Current reward after update: -475.12, Optimal reward -439.20
Iteration 187 took 4.77 seconds (mean sampled reward: -722.14). Current reward after update: -485.07, Optimal reward -439.20
Iteration 188 took 4.74 seconds (mean sampled reward: -718.52). Current reward after update: -485.97, Optimal reward -439.20
Iteration 189 took 4.72 seconds (mean sampled reward: -730.94). Current reward after update: -470.20, Optimal reward -439.20
Iteration 190 took 4.79 seconds (mean sampled reward: -755.37). Current reward after update: -486.31, Optimal reward -439.20
Iteration 191 took 4.77 seconds (mean sampled reward: -793.82). Current reward after update: -456.44, Optimal reward -439.20
Iteration 192 took 4.76 seconds (mean sampled reward: -792.59). Current reward after update: -476.90, Optimal reward -439.20
Iteration 193 took 4.70 seconds (mean sampled reward: -879.81). Current reward after update: -495.81, Optimal reward -439.20
Iteration 194 took 4.75 seconds (mean sampled reward: -856.62). Current reward after update: -467.78, Optimal reward -439.20
Iteration 195 took 4.65 seconds (mean sampled reward: -885.11). Current reward after update: -481.57, Optimal reward -439.20
Iteration 196 took 4.74 seconds (mean sampled reward: -920.74). Current reward after update: -509.37, Optimal reward -439.20
Iteration 197 took 4.67 seconds (mean sampled reward: -922.57). Current reward after update: -504.53, Optimal reward -439.20
Iteration 198 took 4.64 seconds (mean sampled reward: -770.80). Current reward after update: -479.91, Optimal reward -439.20
Iteration 199 took 4.64 seconds (mean sampled reward: -729.10). Current reward after update: -466.61, Optimal reward -439.20
Iteration 200 took 4.64 seconds (mean sampled reward: -786.85). Current reward after update: -464.30, Optimal reward -439.20
Iteration 1 took 4.57 seconds (mean sampled reward: -916.63). Current reward after update: -697.00, Optimal reward -697.00
Iteration 2 took 4.70 seconds (mean sampled reward: -978.04). Current reward after update: -698.82, Optimal reward -697.00
Iteration 3 took 4.73 seconds (mean sampled reward: -987.01). Current reward after update: -687.22, Optimal reward -687.22
Iteration 4 took 4.52 seconds (mean sampled reward: -920.14). Current reward after update: -656.77, Optimal reward -656.77
Iteration 5 took 4.50 seconds (mean sampled reward: -959.94). Current reward after update: -652.40, Optimal reward -652.40
Iteration 6 took 4.45 seconds (mean sampled reward: -892.60). Current reward after update: -655.90, Optimal reward -652.40
Iteration 7 took 4.40 seconds (mean sampled reward: -858.65). Current reward after update: -604.58, Optimal reward -604.58
Iteration 8 took 4.51 seconds (mean sampled reward: -903.70). Current reward after update: -669.93, Optimal reward -604.58
Iteration 9 took 4.46 seconds (mean sampled reward: -910.88). Current reward after update: -701.03, Optimal reward -604.58
Iteration 10 took 4.52 seconds (mean sampled reward: -964.20). Current reward after update: -699.49, Optimal reward -604.58
Iteration 11 took 4.40 seconds (mean sampled reward: -872.46). Current reward after update: -597.54, Optimal reward -597.54
Iteration 12 took 4.49 seconds (mean sampled reward: -922.58). Current reward after update: -642.85, Optimal reward -597.54
Iteration 13 took 4.49 seconds (mean sampled reward: -862.33). Current reward after update: -646.64, Optimal reward -597.54
Iteration 14 took 4.57 seconds (mean sampled reward: -860.67). Current reward after update: -613.80, Optimal reward -597.54
Iteration 15 took 4.62 seconds (mean sampled reward: -856.53). Current reward after update: -632.37, Optimal reward -597.54
Iteration 16 took 4.49 seconds (mean sampled reward: -869.16). Current reward after update: -614.44, Optimal reward -597.54
Iteration 17 took 4.52 seconds (mean sampled reward: -861.14). Current reward after update: -636.99, Optimal reward -597.54
Iteration 18 took 4.58 seconds (mean sampled reward: -919.21). Current reward after update: -653.37, Optimal reward -597.54
Iteration 19 took 4.54 seconds (mean sampled reward: -896.35). Current reward after update: -620.88, Optimal reward -597.54
Iteration 20 took 4.47 seconds (mean sampled reward: -830.79). Current reward after update: -619.14, Optimal reward -597.54
Iteration 21 took 4.40 seconds (mean sampled reward: -820.01). Current reward after update: -636.16, Optimal reward -597.54
Iteration 22 took 4.37 seconds (mean sampled reward: -832.47). Current reward after update: -583.46, Optimal reward -583.46
Iteration 23 took 4.36 seconds (mean sampled reward: -850.70). Current reward after update: -568.49, Optimal reward -568.49
Iteration 24 took 4.52 seconds (mean sampled reward: -872.26). Current reward after update: -764.44, Optimal reward -568.49
Iteration 25 took 4.42 seconds (mean sampled reward: -849.61). Current reward after update: -655.21, Optimal reward -568.49
Iteration 26 took 4.39 seconds (mean sampled reward: -880.35). Current reward after update: -631.66, Optimal reward -568.49
Iteration 27 took 4.34 seconds (mean sampled reward: -842.91). Current reward after update: -625.36, Optimal reward -568.49
Iteration 28 took 4.28 seconds (mean sampled reward: -794.08). Current reward after update: -628.24, Optimal reward -568.49
Iteration 29 took 4.38 seconds (mean sampled reward: -810.67). Current reward after update: -628.92, Optimal reward -568.49
Iteration 30 took 4.55 seconds (mean sampled reward: -811.13). Current reward after update: -636.60, Optimal reward -568.49
Iteration 31 took 4.52 seconds (mean sampled reward: -822.36). Current reward after update: -649.02, Optimal reward -568.49
Iteration 32 took 4.60 seconds (mean sampled reward: -848.23). Current reward after update: -622.21, Optimal reward -568.49
Iteration 33 took 4.60 seconds (mean sampled reward: -868.21). Current reward after update: -664.07, Optimal reward -568.49
Iteration 34 took 4.62 seconds (mean sampled reward: -884.66). Current reward after update: -624.41, Optimal reward -568.49
Iteration 35 took 4.67 seconds (mean sampled reward: -831.02). Current reward after update: -635.22, Optimal reward -568.49
Iteration 36 took 4.71 seconds (mean sampled reward: -853.09). Current reward after update: -637.67, Optimal reward -568.49
Iteration 37 took 4.54 seconds (mean sampled reward: -888.15). Current reward after update: -615.54, Optimal reward -568.49
Iteration 38 took 4.42 seconds (mean sampled reward: -917.07). Current reward after update: -622.67, Optimal reward -568.49
Iteration 39 took 4.41 seconds (mean sampled reward: -879.47). Current reward after update: -612.25, Optimal reward -568.49
Iteration 40 took 4.36 seconds (mean sampled reward: -824.93). Current reward after update: -620.95, Optimal reward -568.49
Iteration 41 took 4.16 seconds (mean sampled reward: -837.16). Current reward after update: -607.03, Optimal reward -568.49
Iteration 42 took 4.34 seconds (mean sampled reward: -822.08). Current reward after update: -619.55, Optimal reward -568.49
Iteration 43 took 4.53 seconds (mean sampled reward: -809.41). Current reward after update: -634.87, Optimal reward -568.49
Iteration 44 took 4.68 seconds (mean sampled reward: -849.12). Current reward after update: -644.24, Optimal reward -568.49
Iteration 45 took 4.59 seconds (mean sampled reward: -808.68). Current reward after update: -618.72, Optimal reward -568.49
Iteration 46 took 4.57 seconds (mean sampled reward: -802.64). Current reward after update: -623.12, Optimal reward -568.49
Iteration 47 took 4.76 seconds (mean sampled reward: -814.87). Current reward after update: -647.55, Optimal reward -568.49
Iteration 48 took 4.63 seconds (mean sampled reward: -846.46). Current reward after update: -624.78, Optimal reward -568.49
Iteration 49 took 4.70 seconds (mean sampled reward: -806.73). Current reward after update: -605.29, Optimal reward -568.49
Iteration 50 took 4.68 seconds (mean sampled reward: -843.21). Current reward after update: -603.90, Optimal reward -568.49
Iteration 51 took 4.69 seconds (mean sampled reward: -839.92). Current reward after update: -601.88, Optimal reward -568.49
Iteration 52 took 4.62 seconds (mean sampled reward: -893.05). Current reward after update: -611.79, Optimal reward -568.49
Iteration 53 took 4.61 seconds (mean sampled reward: -911.50). Current reward after update: -625.16, Optimal reward -568.49
Iteration 54 took 4.57 seconds (mean sampled reward: -864.73). Current reward after update: -633.18, Optimal reward -568.49
Iteration 55 took 4.42 seconds (mean sampled reward: -907.17). Current reward after update: -640.76, Optimal reward -568.49
Iteration 56 took 4.19 seconds (mean sampled reward: -911.60). Current reward after update: -619.46, Optimal reward -568.49
Iteration 57 took 4.22 seconds (mean sampled reward: -868.72). Current reward after update: -631.50, Optimal reward -568.49
Iteration 58 took 4.23 seconds (mean sampled reward: -859.99). Current reward after update: -640.13, Optimal reward -568.49
Iteration 59 took 4.16 seconds (mean sampled reward: -830.37). Current reward after update: -622.64, Optimal reward -568.49
Iteration 60 took 4.15 seconds (mean sampled reward: -831.13). Current reward after update: -618.38, Optimal reward -568.49
Iteration 61 took 4.24 seconds (mean sampled reward: -851.27). Current reward after update: -552.04, Optimal reward -552.04
Iteration 62 took 4.19 seconds (mean sampled reward: -823.54). Current reward after update: -613.35, Optimal reward -552.04
Iteration 63 took 4.12 seconds (mean sampled reward: -808.26). Current reward after update: -554.21, Optimal reward -552.04
Iteration 64 took 4.12 seconds (mean sampled reward: -817.49). Current reward after update: -560.50, Optimal reward -552.04
Iteration 65 took 4.04 seconds (mean sampled reward: -815.40). Current reward after update: -562.68, Optimal reward -552.04
Iteration 66 took 4.09 seconds (mean sampled reward: -825.31). Current reward after update: -593.53, Optimal reward -552.04
Iteration 67 took 4.13 seconds (mean sampled reward: -816.17). Current reward after update: -565.34, Optimal reward -552.04
Iteration 68 took 4.16 seconds (mean sampled reward: -814.71). Current reward after update: -594.55, Optimal reward -552.04
Iteration 69 took 4.30 seconds (mean sampled reward: -824.62). Current reward after update: -562.45, Optimal reward -552.04
Iteration 70 took 4.18 seconds (mean sampled reward: -816.81). Current reward after update: -541.92, Optimal reward -541.92
Iteration 71 took 4.15 seconds (mean sampled reward: -809.96). Current reward after update: -579.37, Optimal reward -541.92
Iteration 72 took 4.33 seconds (mean sampled reward: -804.75). Current reward after update: -608.93, Optimal reward -541.92
Iteration 73 took 4.30 seconds (mean sampled reward: -822.51). Current reward after update: -581.12, Optimal reward -541.92
Iteration 74 took 4.04 seconds (mean sampled reward: -810.64). Current reward after update: -556.48, Optimal reward -541.92
Iteration 75 took 4.14 seconds (mean sampled reward: -820.63). Current reward after update: -556.96, Optimal reward -541.92
Iteration 76 took 4.17 seconds (mean sampled reward: -823.95). Current reward after update: -573.88, Optimal reward -541.92
Iteration 77 took 4.07 seconds (mean sampled reward: -826.92). Current reward after update: -583.42, Optimal reward -541.92
Iteration 78 took 4.24 seconds (mean sampled reward: -809.75). Current reward after update: -558.64, Optimal reward -541.92
Iteration 79 took 4.15 seconds (mean sampled reward: -824.40). Current reward after update: -559.95, Optimal reward -541.92
Iteration 80 took 4.24 seconds (mean sampled reward: -844.34). Current reward after update: -516.37, Optimal reward -516.37
Iteration 81 took 4.30 seconds (mean sampled reward: -907.80). Current reward after update: -634.32, Optimal reward -516.37
Iteration 82 took 4.44 seconds (mean sampled reward: -863.68). Current reward after update: -524.37, Optimal reward -516.37
Iteration 83 took 4.11 seconds (mean sampled reward: -835.75). Current reward after update: -546.27, Optimal reward -516.37
Iteration 84 took 4.14 seconds (mean sampled reward: -906.73). Current reward after update: -541.06, Optimal reward -516.37
Iteration 85 took 4.23 seconds (mean sampled reward: -894.08). Current reward after update: -519.33, Optimal reward -516.37
Iteration 86 took 4.17 seconds (mean sampled reward: -846.31). Current reward after update: -613.56, Optimal reward -516.37
Iteration 87 took 4.23 seconds (mean sampled reward: -829.72). Current reward after update: -604.57, Optimal reward -516.37
Iteration 88 took 4.26 seconds (mean sampled reward: -852.19). Current reward after update: -547.93, Optimal reward -516.37
Iteration 89 took 4.22 seconds (mean sampled reward: -809.85). Current reward after update: -587.43, Optimal reward -516.37
Iteration 90 took 4.12 seconds (mean sampled reward: -818.97). Current reward after update: -574.72, Optimal reward -516.37
Iteration 91 took 4.17 seconds (mean sampled reward: -806.80). Current reward after update: -623.02, Optimal reward -516.37
Iteration 92 took 4.28 seconds (mean sampled reward: -849.55). Current reward after update: -636.60, Optimal reward -516.37
Iteration 93 took 4.26 seconds (mean sampled reward: -853.85). Current reward after update: -589.14, Optimal reward -516.37
Iteration 94 took 4.12 seconds (mean sampled reward: -834.70). Current reward after update: -701.51, Optimal reward -516.37
Iteration 95 took 4.12 seconds (mean sampled reward: -810.41). Current reward after update: -621.06, Optimal reward -516.37
Iteration 96 took 4.08 seconds (mean sampled reward: -812.63). Current reward after update: -614.14, Optimal reward -516.37
Iteration 97 took 4.19 seconds (mean sampled reward: -811.79). Current reward after update: -609.05, Optimal reward -516.37
Iteration 98 took 4.09 seconds (mean sampled reward: -816.84). Current reward after update: -509.40, Optimal reward -509.40
Iteration 99 took 4.11 seconds (mean sampled reward: -815.57). Current reward after update: -567.63, Optimal reward -509.40
Iteration 100 took 4.04 seconds (mean sampled reward: -803.31). Current reward after update: -530.97, Optimal reward -509.40
Iteration 101 took 4.10 seconds (mean sampled reward: -818.25). Current reward after update: -570.78, Optimal reward -509.40
Iteration 102 took 4.09 seconds (mean sampled reward: -829.69). Current reward after update: -531.25, Optimal reward -509.40
Iteration 103 took 4.17 seconds (mean sampled reward: -813.12). Current reward after update: -580.91, Optimal reward -509.40
Iteration 104 took 4.16 seconds (mean sampled reward: -829.59). Current reward after update: -613.57, Optimal reward -509.40
Iteration 105 took 4.31 seconds (mean sampled reward: -919.47). Current reward after update: -568.21, Optimal reward -509.40
Iteration 106 took 4.17 seconds (mean sampled reward: -842.26). Current reward after update: -535.59, Optimal reward -509.40
Iteration 107 took 4.18 seconds (mean sampled reward: -824.84). Current reward after update: -517.92, Optimal reward -509.40
Iteration 108 took 4.11 seconds (mean sampled reward: -822.71). Current reward after update: -582.17, Optimal reward -509.40
Iteration 109 took 4.05 seconds (mean sampled reward: -822.11). Current reward after update: -596.11, Optimal reward -509.40
Iteration 110 took 4.15 seconds (mean sampled reward: -823.14). Current reward after update: -529.46, Optimal reward -509.40
Iteration 111 took 4.05 seconds (mean sampled reward: -809.35). Current reward after update: -539.90, Optimal reward -509.40
Iteration 112 took 4.21 seconds (mean sampled reward: -814.19). Current reward after update: -578.14, Optimal reward -509.40
Iteration 113 took 4.28 seconds (mean sampled reward: -851.60). Current reward after update: -605.09, Optimal reward -509.40
Iteration 114 took 4.15 seconds (mean sampled reward: -823.09). Current reward after update: -526.46, Optimal reward -509.40
Iteration 115 took 4.07 seconds (mean sampled reward: -808.09). Current reward after update: -509.55, Optimal reward -509.40
Iteration 116 took 4.21 seconds (mean sampled reward: -819.26). Current reward after update: -667.26, Optimal reward -509.40
Iteration 117 took 4.28 seconds (mean sampled reward: -818.74). Current reward after update: -592.44, Optimal reward -509.40
Iteration 118 took 4.17 seconds (mean sampled reward: -806.40). Current reward after update: -607.25, Optimal reward -509.40
Iteration 119 took 4.46 seconds (mean sampled reward: -822.56). Current reward after update: -616.39, Optimal reward -509.40
Iteration 120 took 4.33 seconds (mean sampled reward: -822.26). Current reward after update: -564.89, Optimal reward -509.40
Iteration 121 took 4.28 seconds (mean sampled reward: -834.03). Current reward after update: -578.00, Optimal reward -509.40
Iteration 122 took 4.24 seconds (mean sampled reward: -870.55). Current reward after update: -538.28, Optimal reward -509.40
Iteration 123 took 4.19 seconds (mean sampled reward: -859.54). Current reward after update: -570.06, Optimal reward -509.40
Iteration 124 took 4.16 seconds (mean sampled reward: -833.56). Current reward after update: -604.88, Optimal reward -509.40
Iteration 125 took 4.15 seconds (mean sampled reward: -840.96). Current reward after update: -577.42, Optimal reward -509.40
Iteration 126 took 4.18 seconds (mean sampled reward: -822.57). Current reward after update: -624.41, Optimal reward -509.40
Iteration 127 took 4.13 seconds (mean sampled reward: -832.38). Current reward after update: -588.96, Optimal reward -509.40
Iteration 128 took 4.15 seconds (mean sampled reward: -833.71). Current reward after update: -571.98, Optimal reward -509.40
Iteration 129 took 4.13 seconds (mean sampled reward: -831.68). Current reward after update: -622.34, Optimal reward -509.40
Iteration 130 took 4.14 seconds (mean sampled reward: -838.46). Current reward after update: -579.45, Optimal reward -509.40
Iteration 131 took 4.18 seconds (mean sampled reward: -846.17). Current reward after update: -589.93, Optimal reward -509.40
Iteration 132 took 4.20 seconds (mean sampled reward: -900.72). Current reward after update: -659.60, Optimal reward -509.40
Iteration 133 took 4.18 seconds (mean sampled reward: -846.25). Current reward after update: -743.86, Optimal reward -509.40
Iteration 134 took 4.25 seconds (mean sampled reward: -895.70). Current reward after update: -600.63, Optimal reward -509.40
Iteration 135 took 4.12 seconds (mean sampled reward: -834.63). Current reward after update: -638.08, Optimal reward -509.40
Iteration 136 took 4.25 seconds (mean sampled reward: -835.50). Current reward after update: -593.81, Optimal reward -509.40
Iteration 137 took 4.26 seconds (mean sampled reward: -856.34). Current reward after update: -582.91, Optimal reward -509.40
Iteration 138 took 4.25 seconds (mean sampled reward: -847.01). Current reward after update: -637.36, Optimal reward -509.40
Iteration 139 took 4.19 seconds (mean sampled reward: -834.08). Current reward after update: -583.82, Optimal reward -509.40
Iteration 140 took 4.09 seconds (mean sampled reward: -826.62). Current reward after update: -596.38, Optimal reward -509.40
Iteration 141 took 4.15 seconds (mean sampled reward: -825.88). Current reward after update: -624.41, Optimal reward -509.40
Iteration 142 took 4.53 seconds (mean sampled reward: -831.03). Current reward after update: -601.49, Optimal reward -509.40
Iteration 143 took 4.62 seconds (mean sampled reward: -820.72). Current reward after update: -605.19, Optimal reward -509.40
Iteration 144 took 4.43 seconds (mean sampled reward: -808.34). Current reward after update: -599.55, Optimal reward -509.40
Iteration 145 took 4.53 seconds (mean sampled reward: -809.26). Current reward after update: -597.99, Optimal reward -509.40
Iteration 146 took 4.32 seconds (mean sampled reward: -825.96). Current reward after update: -620.84, Optimal reward -509.40
Iteration 147 took 4.14 seconds (mean sampled reward: -817.89). Current reward after update: -550.34, Optimal reward -509.40
Iteration 148 took 4.25 seconds (mean sampled reward: -820.42). Current reward after update: -563.41, Optimal reward -509.40
Iteration 149 took 4.08 seconds (mean sampled reward: -867.45). Current reward after update: -488.72, Optimal reward -488.72
Iteration 150 took 4.25 seconds (mean sampled reward: -809.74). Current reward after update: -631.26, Optimal reward -488.72
Iteration 151 took 4.18 seconds (mean sampled reward: -859.41). Current reward after update: -605.62, Optimal reward -488.72
Iteration 152 took 4.18 seconds (mean sampled reward: -837.71). Current reward after update: -656.06, Optimal reward -488.72
Iteration 153 took 4.21 seconds (mean sampled reward: -882.75). Current reward after update: -640.54, Optimal reward -488.72
Iteration 154 took 4.08 seconds (mean sampled reward: -852.00). Current reward after update: -652.31, Optimal reward -488.72
Iteration 155 took 4.13 seconds (mean sampled reward: -812.67). Current reward after update: -606.45, Optimal reward -488.72
Iteration 156 took 4.33 seconds (mean sampled reward: -821.04). Current reward after update: -643.06, Optimal reward -488.72
Iteration 157 took 4.25 seconds (mean sampled reward: -831.78). Current reward after update: -699.65, Optimal reward -488.72
Iteration 158 took 4.29 seconds (mean sampled reward: -839.87). Current reward after update: -582.92, Optimal reward -488.72
Iteration 159 took 4.15 seconds (mean sampled reward: -823.37). Current reward after update: -619.38, Optimal reward -488.72
Iteration 160 took 4.20 seconds (mean sampled reward: -814.55). Current reward after update: -624.11, Optimal reward -488.72
Iteration 161 took 4.20 seconds (mean sampled reward: -800.87). Current reward after update: -566.52, Optimal reward -488.72
Iteration 162 took 4.12 seconds (mean sampled reward: -795.83). Current reward after update: -575.08, Optimal reward -488.72
Iteration 163 took 4.01 seconds (mean sampled reward: -816.85). Current reward after update: -595.36, Optimal reward -488.72
Iteration 164 took 4.08 seconds (mean sampled reward: -808.04). Current reward after update: -569.36, Optimal reward -488.72
Iteration 165 took 4.16 seconds (mean sampled reward: -834.23). Current reward after update: -556.57, Optimal reward -488.72
Iteration 166 took 4.14 seconds (mean sampled reward: -806.94). Current reward after update: -552.24, Optimal reward -488.72
Iteration 167 took 4.14 seconds (mean sampled reward: -839.55). Current reward after update: -557.37, Optimal reward -488.72
Iteration 168 took 4.09 seconds (mean sampled reward: -855.21). Current reward after update: -633.84, Optimal reward -488.72
Iteration 169 took 4.13 seconds (mean sampled reward: -827.57). Current reward after update: -578.57, Optimal reward -488.72
Iteration 170 took 4.17 seconds (mean sampled reward: -820.10). Current reward after update: -632.48, Optimal reward -488.72
Iteration 171 took 4.12 seconds (mean sampled reward: -800.52). Current reward after update: -561.41, Optimal reward -488.72
Iteration 172 took 4.12 seconds (mean sampled reward: -804.52). Current reward after update: -619.01, Optimal reward -488.72
Iteration 173 took 4.13 seconds (mean sampled reward: -833.90). Current reward after update: -621.32, Optimal reward -488.72
Iteration 174 took 4.16 seconds (mean sampled reward: -813.11). Current reward after update: -635.19, Optimal reward -488.72
Iteration 175 took 4.17 seconds (mean sampled reward: -809.55). Current reward after update: -517.13, Optimal reward -488.72
Iteration 176 took 4.10 seconds (mean sampled reward: -813.45). Current reward after update: -638.15, Optimal reward -488.72
Iteration 177 took 4.17 seconds (mean sampled reward: -810.80). Current reward after update: -626.29, Optimal reward -488.72
Iteration 178 took 4.19 seconds (mean sampled reward: -812.73). Current reward after update: -636.61, Optimal reward -488.72
Iteration 179 took 4.14 seconds (mean sampled reward: -857.28). Current reward after update: -567.95, Optimal reward -488.72
Iteration 180 took 4.11 seconds (mean sampled reward: -832.16). Current reward after update: -613.53, Optimal reward -488.72
Iteration 181 took 4.18 seconds (mean sampled reward: -819.44). Current reward after update: -593.02, Optimal reward -488.72
Iteration 182 took 4.18 seconds (mean sampled reward: -845.93). Current reward after update: -605.40, Optimal reward -488.72
Iteration 183 took 4.26 seconds (mean sampled reward: -851.37). Current reward after update: -745.25, Optimal reward -488.72
Iteration 184 took 4.47 seconds (mean sampled reward: -828.89). Current reward after update: -617.51, Optimal reward -488.72
Iteration 185 took 4.49 seconds (mean sampled reward: -813.59). Current reward after update: -605.77, Optimal reward -488.72
Iteration 186 took 4.65 seconds (mean sampled reward: -835.36). Current reward after update: -605.61, Optimal reward -488.72
Iteration 187 took 4.63 seconds (mean sampled reward: -810.83). Current reward after update: -610.35, Optimal reward -488.72
Iteration 188 took 4.63 seconds (mean sampled reward: -814.97). Current reward after update: -631.97, Optimal reward -488.72
Iteration 189 took 4.42 seconds (mean sampled reward: -804.04). Current reward after update: -633.33, Optimal reward -488.72
Iteration 190 took 4.50 seconds (mean sampled reward: -805.82). Current reward after update: -623.90, Optimal reward -488.72
Iteration 191 took 4.63 seconds (mean sampled reward: -820.72). Current reward after update: -646.87, Optimal reward -488.72
Iteration 192 took 4.69 seconds (mean sampled reward: -807.81). Current reward after update: -617.12, Optimal reward -488.72
Iteration 193 took 4.67 seconds (mean sampled reward: -788.06). Current reward after update: -658.41, Optimal reward -488.72
Iteration 194 took 4.69 seconds (mean sampled reward: -840.51). Current reward after update: -596.69, Optimal reward -488.72
Iteration 195 took 4.65 seconds (mean sampled reward: -820.23). Current reward after update: -611.81, Optimal reward -488.72
Iteration 196 took 4.64 seconds (mean sampled reward: -793.38). Current reward after update: -588.78, Optimal reward -488.72
Iteration 197 took 4.57 seconds (mean sampled reward: -801.10). Current reward after update: -594.52, Optimal reward -488.72
Iteration 198 took 4.50 seconds (mean sampled reward: -816.33). Current reward after update: -614.61, Optimal reward -488.72
Iteration 199 took 4.51 seconds (mean sampled reward: -814.68). Current reward after update: -601.64, Optimal reward -488.72
Iteration 200 took 4.73 seconds (mean sampled reward: -794.61). Current reward after update: -608.74, Optimal reward -488.72
Sigma: 0.8 mean rewards: -383.3992415698608, best rewards:-222.28055835639356

argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
